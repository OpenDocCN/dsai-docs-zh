# scipy.optimize.minimize

> 原文链接：[https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize)

```py
scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)
```

最小化一个或多个变量的标量函数。

参数：

**fun** 可调用对象

要最小化的目标函数。

> `fun(x, *args) -> float`

其中 `x` 是形状为 (n,) 的一维数组，`args` 是一个元组，包含完全指定函数所需的固定参数。

**x0** 数组，形状为 (n,)

初始猜测。大小为 (n,) 的实数元素数组，其中 `n` 是独立变量的数量。

**args** 元组，可选

传递给目标函数及其导数（*fun*、*jac* 和 *hess* 函数）的额外参数。

**method** 字符串或可调用对象，可选

求解器的类型。应为以下之一：

> +   ‘Nelder-Mead’ [(详见此处)](../optimize.minimize-neldermead.html#optimize-minimize-neldermead)
> +   
> +   ‘Powell’ [(详见此处)](../optimize.minimize-powell.html#optimize-minimize-powell)
> +   
> +   ‘CG’ [(详见此处)](../optimize.minimize-cg.html#optimize-minimize-cg)
> +   
> +   ‘BFGS’ [(详见此处)](../optimize.minimize-bfgs.html#optimize-minimize-bfgs)
> +   
> +   ‘Newton-CG’ [(详见此处)](../optimize.minimize-newtoncg.html#optimize-minimize-newtoncg)
> +   
> +   ‘L-BFGS-B’ [(详见此处)](../optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb)
> +   
> +   ‘TNC’ [(详见此处)](../optimize.minimize-tnc.html#optimize-minimize-tnc)
> +   
> +   ‘COBYLA’ [(详见此处)](../optimize.minimize-cobyla.html#optimize-minimize-cobyla)
> +   
> +   ‘SLSQP’ [(详见此处)](../optimize.minimize-slsqp.html#optimize-minimize-slsqp)
> +   
> +   ‘trust-constr’[(详见此处)](../optimize.minimize-trustconstr.html#optimize-minimize-trustconstr)
> +   
> +   ‘dogleg’ [(详见此处)](../optimize.minimize-dogleg.html#optimize-minimize-dogleg)
> +   
> +   ‘trust-ncg’ [(详见此处)](../optimize.minimize-trustncg.html#optimize-minimize-trustncg)
> +   
> +   ‘trust-exact’ [(详见此处)](../optimize.minimize-trustexact.html#optimize-minimize-trustexact)
> +   
> +   ‘trust-krylov’ [(详见此处)](../optimize.minimize-trustkrylov.html#optimize-minimize-trustkrylov)
> +   
> +   custom - 一个可调用对象，请参阅下文进行描述。

如果未提供，则根据问题是否有约束或边界选择 `BFGS`、`L-BFGS-B`、`SLSQP` 中的一种。

**jac**{callable, ‘2-point’, ‘3-point’, ‘cs’, bool}, optional

计算梯度向量的方法。仅适用于 CG、BFGS、Newton-CG、L-BFGS-B、TNC、SLSQP、dogleg、trust-ncg、trust-krylov、trust-exact 和 trust-constr。如果是可调用对象，则应为返回梯度向量的函数：

> `jac(x, *args) -> array_like, shape (n,)`

其中 `x` 是形状为 (n,) 的数组，`args` 是具有固定参数的元组。如果 *jac* 是布尔值且为 True，则假定 *fun* 返回包含目标函数和梯度的元组 `(f, g)`。方法 ‘Newton-CG’、‘trust-ncg’、‘dogleg’、‘trust-exact’ 和 ‘trust-krylov’ 要求提供一个可调用对象，或者 *fun* 返回目标函数和梯度。如果为 None 或 False，则使用绝对步长进行二点有限差分估计梯度。或者，关键字 {‘2-point’、‘3-point’、‘cs’} 可用于选择用于数值梯度估计的有限差分方案，并使用相对步长。这些有限差分方案遵循任何指定的 *bounds*。

**hess**{可调用对象，‘2-point’，‘3-point’，‘cs’，HessianUpdateStrategy}，可选

计算 Hessian 矩阵的方法。仅适用于 Newton-CG、dogleg、trust-ncg、trust-krylov、trust-exact 和 trust-constr。如果是可调用对象，则应返回 Hessian 矩阵：

> `hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)`

其中 `x` 是形状为 (n,) 的 ndarray，`args` 是具有固定参数的元组。关键字 {‘2-point’、‘3-point’、‘cs’} 也可用于选择用于数值估计 Hessian 的有限差分方案。或者，实现 [`HessianUpdateStrategy`](https://docs.scipy.org/doc/scipy/reference/optimize.HessianUpdateStrategy.html#scipy.optimize.HessianUpdateStrategy "scipy.optimize.HessianUpdateStrategy") 接口的对象可用于近似 Hessian。实现此接口的可用拟牛顿方法包括：

> +   [`BFGS`](https://docs.scipy.org/doc/scipy/reference/optimize.BFGS.html#scipy.optimize.BFGS "scipy.optimize.BFGS");
> +   
> +   [`SR1`](https://docs.scipy.org/doc/scipy/reference/optimize.SR1.html#scipy.optimize.SR1 "scipy.optimize.SR1")。

并非每种方法都有所有选项；可参考注释中的可用性。

**hessp**可调用对象，可选

目标函数的 Hessian 矩阵乘以任意向量 p。仅适用于 Newton-CG、trust-ncg、trust-krylov、trust-constr。*hessp* 或 *hess* 之一需要提供。如果提供了 *hess*，则将忽略 *hessp*。*hessp* 必须计算 Hessian 乘以任意向量：

> `hessp(x, p, *args) ->  ndarray 形状 (n,)`

其中 `x` 是形状为 (n,) 的 ndarray，`p` 是维度为 (n,) 的任意向量，`args` 是具有固定参数的元组。

**bounds**序列或 [`Bounds`](https://docs.scipy.org/doc/scipy/reference/optimize.Bounds.html#scipy.optimize.Bounds "scipy.optimize.Bounds")，可选

变量的界限用于 Nelder-Mead、L-BFGS-B、TNC、SLSQP、Powell、trust-constr 和 COBYLA 方法。有两种指定界限的方式：

> 1.  [`Bounds`](https://docs.scipy.org/doc/scipy/reference/optimize.Bounds.html#scipy.optimize.Bounds "scipy.optimize.Bounds") 类的实例。
> 1.  
> 1.  对于 *x* 中的每个元素，`(min, max)` 对用于指定界限。使用 None 表示无界限。

**约束**{约束，字典} 或 约束列表 {约束，字典}，可选

约束定义。仅适用于 COBYLA、SLSQP 和 trust-constr。

‘trust-constr’ 的约束被定义为一个单一对象或指定优化问题约束的对象列表。可用的约束类型包括：

> +   [`LinearConstraint`](scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint "scipy.optimize.LinearConstraint")
> +   
> +   [`NonlinearConstraint`](scipy.optimize.NonlinearConstraint.html#scipy.optimize.NonlinearConstraint "scipy.optimize.NonlinearConstraint")

对于 COBYLA、SLSQP，约束被定义为一个包含字段的字典列表：

> typestr
> 
> 约束类型：‘eq’ 表示等式约束，‘ineq’ 表示不等式约束。
> 
> funcallable
> 
> 定义约束的函数。
> 
> jaccallable，可选
> 
> *fun* 的雅可比矩阵（仅适用于 SLSQP）。
> 
> argssequence，可选
> 
> 传递给函数和雅可比矩阵的额外参数。

等式约束意味着约束函数的结果应为零，而不等式则意味着其应为非负。请注意，COBYLA 只支持不等式约束。

**tol**float，可选

终止容差。当指定 *tol* 时，所选的最小化算法设置一些相关的特定解算器容差为 *tol*。要进行详细的控制，请使用特定于解算器的选项。

**options**dict，可选

解算器选项的字典。除了 *TNC* 方法外，所有方法都接受以下通用选项：

> maxiterint
> 
> 执行的最大迭代次数。根据方法，每次迭代可能会使用多个函数评估。
> 
> 对于 *TNC* 方法，请使用 *maxfun* 而不是 *maxiter*。
> 
> dispbool
> 
> 设置为 True 以打印收敛消息。

对于特定方法的选项，请参见 [`show_options`](scipy.optimize.show_options.html#scipy.optimize.show_options "scipy.optimize.show_options")。

**callback**callable，可选

每次迭代后调用的可调用函数。

除了 TNC、SLSQP 和 COBYLA 方法之外的所有方法都支持具有以下签名的可调用函数：

> `callback(intermediate_result: OptimizeResult)`

其中 `intermediate_result` 是一个关键字参数，包含一个 [`OptimizeResult`](scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult "scipy.optimize.OptimizeResult")，具有参数向量和目标函数当前值的属性。请注意，回调函数必须命名为 `intermediate_result`，以便传递一个 [`OptimizeResult`](scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult "scipy.optimize.OptimizeResult")。如果回调函数引发 `StopIteration`，这些方法也将终止。

除了 trust-constr 方法之外的所有方法都支持以下形式的签名：

> `callback(xk)`

其中 `xk` 是当前的参数向量。

使用内省来确定要调用上述哪种签名。

返回：

**res**OptimizeResult

优化结果表示为 `OptimizeResult` 对象。重要属性包括：`x` 解数组，`success` 表示优化器是否成功退出的布尔标志，`message` 描述终止原因。请参阅 [`OptimizeResult`](scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult "scipy.optimize.OptimizeResult") 了解其他属性的描述。

参见

[`minimize_scalar`](scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar "scipy.optimize.minimize_scalar") 标量函数最小化接口。

标量单变量函数最小化算法接口

[`show_options`](scipy.optimize.show_options.html#scipy.optimize.show_options "scipy.optimize.show_options") 函数显示选项。

求解器接受的额外选项

注意事项

本节描述可以通过 'method' 参数选择的可用求解器。默认方法为 *BFGS*。

**无约束最小化**

方法 [CG](../optimize.minimize-cg.html#optimize-minimize-cg) 使用 Polak 和 Ribiere 的非线性共轭梯度算法，是 Fletcher-Reeves 方法的变种，详见 [[5]](#rdd2e1855725e-5) pp.120-122\. 仅使用一阶导数。

方法 [BFGS](../optimize.minimize-bfgs.html#optimize-minimize-bfgs) 使用 Broyden、Fletcher、Goldfarb 和 Shanno（BFGS）拟牛顿法 [[5]](#rdd2e1855725e-5) pp. 136\. 仅使用一阶导数。即使在非平滑优化中，BFGS 也表现良好。此方法还返回存储在 OptimizeResult 对象的 *hess_inv* 中的海森矩阵逆的近似值。

方法 [Newton-CG](../optimize.minimize-newtoncg.html#optimize-minimize-newtoncg) 使用 Newton-CG 算法 [[5]](#rdd2e1855725e-5) pp. 168（也称为截断牛顿法）。它使用共轭梯度方法计算搜索方向。参见 *TNC* 方法，该方法类似，但适用于有边界约束的最小化问题。适用于大规模问题。

方法 [dogleg](../optimize.minimize-dogleg.html#optimize-minimize-dogleg) 使用狗腿信任域算法 [[5]](#rdd2e1855725e-5) 进行无约束最小化。该算法需要梯度和海森矩阵；此外，海森矩阵要求正定。

方法 [trust-ncg](../optimize.minimize-trustncg.html#optimize-minimize-trustncg) 使用牛顿共轭梯度信任域算法 [[5]](#rdd2e1855725e-5) 进行无约束最小化。该算法需要梯度和海森矩阵或计算给定向量与海森矩阵乘积的函数。适用于大规模问题。

方法[trust-krylov](../optimize.minimize-trustkrylov.html#optimize-minimize-trustkrylov)使用Newton GLTR信赖域算法[[14]](#rdd2e1855725e-14)，[[15]](#rdd2e1855725e-15)进行无约束最小化。此算法要求梯度和Hessian矩阵或计算给定向量与Hessian矩阵乘积的函数。适用于大规模问题。在不定问题上，通常比*trust-ncg*方法需要更少的迭代，推荐用于中等和大规模问题。

方法[trust-exact](../optimize.minimize-trustexact.html#optimize-minimize-trustexact)是一种信赖域方法，用于无约束最小化，几乎完全解决二次子问题[[13]](#rdd2e1855725e-13)。此算法要求梯度和Hessian矩阵（*不*要求为正定）。在许多情况下，这种方法收敛迭代较少，是小型和中型问题中最推荐的方法。

**边界约束最小化**

方法[Nelder-Mead](../optimize.minimize-neldermead.html#optimize-minimize-neldermead)使用Simplex算法[[1]](#rdd2e1855725e-1)，[[2]](#rdd2e1855725e-2)。此算法在许多应用中表现稳健。但是，如果可以信任数值导数的计算，其他利用一阶和/或二阶导数信息的算法可能更适合于其在一般情况下的更好性能。

方法[L-BFGS-B](../optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb)使用L-BFGS-B算法[[6]](#rdd2e1855725e-6)，[[7]](#rdd2e1855725e-7)进行边界约束最小化。

方法[Powell](../optimize.minimize-powell.html#optimize-minimize-powell)是Powell方法的改进[[3]](#rdd2e1855725e-3)，[[4]](#rdd2e1855725e-4)，它是一种共轭方向方法。它沿着每个方向集合的每个向量（*options*和*info*中的*direc*字段）顺序进行一维最小化，每次主最小化循环迭代时更新。函数不需要可微，也不计算导数。如果未提供边界，则将使用无界线搜索。如果提供了边界，并且初始猜测在边界内，则最小化过程中的每个函数评估都将在边界内。如果提供了边界，初始猜测超出边界，并且*direc*具有完整秩（默认具有完整秩），则第一次迭代期间的某些函数评估可能超出边界，但第一次迭代后的每个函数评估都将在边界内。如果*direc*秩不完整，则某些参数可能不会被优化，并且不能保证解在边界内。

方法 [TNC](../optimize.minimize-tnc.html#optimize-minimize-tnc) 使用截断牛顿算法[[5]](#rdd2e1855725e-5), [[8]](#rdd2e1855725e-8)来最小化带有变量界限的函数。此算法利用梯度信息；它也称为牛顿共轭梯度法。它与上述*Newton-CG*方法不同，因为它封装了C实现，并允许每个变量都有上限和下限。

**约束最小化**

方法 [COBYLA](../optimize.minimize-cobyla.html#optimize-minimize-cobyla) 使用约束优化BY线性近似（COBYLA）方法[[9]](#rdd2e1855725e-9), [[10]](#rdd2e1855725e-10), [[11]](#rdd2e1855725e-11)。该算法基于目标函数和每个约束的线性近似。该方法封装了该算法的FORTRAN实现。约束函数‘fun’可以返回单个数字或数字数组或列表。

方法 [SLSQP](../optimize.minimize-slsqp.html#optimize-minimize-slsqp) 使用顺序最小二乘编程来最小化多变量函数，可以有各种边界、等式和不等式约束的组合。该方法封装了最初由Dieter Kraft实现的SLSQP优化子程序[[12]](#rdd2e1855725e-12)。请注意，包装器通过将边界中的无限值转换为大浮点值来处理边界中的无限值。

方法 [trust-constr](../optimize.minimize-trustconstr.html#optimize-minimize-trustconstr) 是一种用于约束优化的信赖域算法。它根据问题定义切换两种实现方式。这是SciPy中最通用的约束最小化算法，特别适用于大规模问题。对于等式约束问题，它是Byrd-Omojokun信赖域SQP方法的实现，详见[[17]](#rdd2e1855725e-17)和[[5]](#rdd2e1855725e-5)，第549页。当还有不等式约束时，它切换到信赖域内点法，详见[[16]](#rdd2e1855725e-16)。这种内点算法通过引入松弛变量并解决一系列逐渐减小的约束问题，以逐步减小的障碍参数。先前描述的等式约束SQP方法用于解决越来越精确的子问题，因为迭代逐渐接近解决方案。

**有限差分期权**

对于方法 [trust-constr](../optimize.minimize-trustconstr.html#optimize-minimize-trustconstr) ，可以使用三种有限差分方案来近似梯度和海森矩阵：{‘2-point’, ‘3-point’, ‘cs’}。方案 ‘cs’ 可能是最精确的，但它要求函数能够正确处理复杂输入并在复平面上可微分。方案 ‘3-point’ 比 ‘2-point’ 更精确，但需要两倍的操作。如果通过有限差分法估计梯度，则必须使用一种拟牛顿策略来估计海森矩阵。

**关于** *hess* **关键字的方法特定选项**

| method/Hess | None | callable | ‘2-point/’3-point’/’cs’ | HUS |
| --- | --- | --- | --- | --- |
| Newton-CG | x | (n, n) LO | x | x |
| dogleg |  | (n, n) |  |  |
| trust-ncg |  | (n, n) | x | x |
| trust-krylov |  | (n, n) | x | x |
| trust-exact |  | (n, n) |  |  |
| trust-constr | x | (n, n) LO sp | x | x |

其中 LO=LinearOperator，sp=Sparse matrix，HUS=HessianUpdateStrategy

**自定义最小化器**

可能在使用此方法的前端（例如[`scipy.optimize.basinhopping`](scipy.optimize.basinhopping.html#scipy.optimize.basinhopping "scipy.optimize.basinhopping")）或其他库时，传递自定义的最小化方法可能很有用。您可以简单地将可调用对象作为`method`参数传递。

可调用对象被调用为 `method(fun, x0, args, **kwargs, **options)`，其中 `kwargs` 对应于传递给[`minimize`](#scipy.optimize.minimize "scipy.optimize.minimize")的任何其他参数（如 *callback*, *hess* 等），除了 *options* 字典，其内容也会逐对传递为 *method* 参数。此外，如果 *jac* 被传递为布尔类型，则 *jac* 和 *fun* 将被篡改，使 *fun* 仅返回函数值，而 *jac* 被转换为返回雅可比矩阵的函数。该方法应返回一个[`OptimizeResult`](scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult "scipy.optimize.OptimizeResult")对象。

提供的 *method* 可调用对象必须能够接受（并可能忽略）任意参数；由于[`minimize`](#scipy.optimize.minimize "scipy.optimize.minimize")接受的参数集可能会在未来版本中扩展，这些参数将被传递给该方法。您可以在scipy.optimize教程中找到一个示例。

参考文献

[[1](#id9)]

Nelder, J A 和 R Mead. 1965\. 函数最小化的单纯形法。《计算机期刊》 7: 308-13.

[[2](#id10)]

Wright M H. 1996\. 直接搜索方法：曾经被蔑视，现在倍受尊重，收录于《数值分析1995：1995年邓迪双年会数值分析会议论文集》（主编 D F Griffiths 和 G A Watson）。Addison Wesley Longman, Harlow, UK. 191-208.

[[3](#id13)]

Powell, M J D. 1964\. 一种在不计算导数的情况下找到多变量函数最小值的高效方法。《计算机期刊》 7: 155-162.

[[4](#id14)]

Press W, S A Teukolsky, W T Vetterling和B P Flannery. Numerical Recipes（任何版本），剑桥大学出版社。

[5] ([1](#id1),[2](#id2),[3](#id3),[4](#id4),[5](#id5),[6](#id15),[7](#id22),[8](#id41))

Nocedal, J和S J Wright. 2006.数值优化。Springer New York。

[[6](#id11)]

Byrd, R H和P Lu和J. Nocedal. 1995.用于有界约束优化的有限内存算法。SIAM Journal on Scientific and Statistical Computing 16（5）：1190-1208。

[[7](#id12)]

Zhu, C和R H Byrd和J Nocedal. 1997. L-BFGS-B：算法778：L-BFGS-B，FORTRAN大规模有界约束优化的例程。ACM Transactions on Mathematical Software 23（4）：550-560。

[[8](#id16)]

Nash, S G. 通过Lanczos方法的牛顿型最小化。1984. SIAM Journal of Numerical Analysis 21：770-778。

[[9](#id17)]

Powell, M J D. 一种直接搜索优化方法，通过线性插值模拟目标和约束函数。1994.优化和数值分析进展，主编S. Gomez和J-P Hennart，Kluwer Academic（Dordrecht），51-67。

[[10](#id18)]

Powell M J D. 用于优化计算的直接搜索算法。1998. Acta Numerica 7：287-336。

[[11](#id19)]

Powell M J D. 无导数优化算法概览。2007.剑桥大学技术报告DAMTP 2007/NA03

[[12](#id20)]

Kraft, D. 用于顺序二次规划的软件包。1988. Tech. Rep. DFVLR-FB 88-28，DLR German Aerospace Center – Institute for Flight Mechanics，Koln，Germany。

[[13](#id8)]

Conn, A. R., Gould, N. I.,和Toint, P. L. 信任区域方法。2000. Siam. pp. 169-200.

[[14](#id6)]

F. Lenders, C. Kirches, A. Potschka: “trlib：用于迭代解决信任区域问题的无向量实现”，[arXiv:1611.04718](https://arxiv.org/abs/1611.04718)

[[15](#id7)]

N. Gould, S. Lucidi, M. Roma, P. Toint: “使用Lanczos方法解决信任区域子问题”，SIAM J. Optim., 9(2), 504–525, (1999).

[[16](#id23)]

Byrd, Richard H., Mary E. Hribar和Jorge Nocedal. 1999.大规模非线性规划的内点算法。SIAM Journal on Optimization 9.4：877-900。

[[17](#id21)]

Lalee, Marucha，Jorge Nocedal和Todd Plantega. 1998.关于大规模等式约束优化算法的实现。SIAM Journal on Optimization 8.3：682-706。

例子

让我们考虑最小化Rosenbrock函数的问题。该函数（及其相应的导数）在[`rosen`](scipy.optimize.rosen.html#scipy.optimize.rosen "scipy.optimize.rosen")（分别在[`rosen_der`](scipy.optimize.rosen_der.html#scipy.optimize.rosen_der "scipy.optimize.rosen_der")，[`rosen_hess`](scipy.optimize.rosen_hess.html#scipy.optimize.rosen_hess "scipy.optimize.rosen_hess")中实现）中。[`scipy.optimize`](../optimize.html#module-scipy.optimize "scipy.optimize")。

```py
>>> from scipy.optimize import minimize, rosen, rosen_der 
```

*Nelder-Mead*方法的一个简单应用是：

```py
>>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]
>>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)
>>> res.x
array([ 1.,  1.,  1.,  1.,  1.]) 
```

现在使用*BFGS*算法，使用第一阶导数和一些选项：

```py
>>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,
...                options={'gtol': 1e-6, 'disp': True})
Optimization terminated successfully.
 Current function value: 0.000000
 Iterations: 26
 Function evaluations: 31
 Gradient evaluations: 31
>>> res.x
array([ 1.,  1.,  1.,  1.,  1.])
>>> print(res.message)
Optimization terminated successfully.
>>> res.hess_inv
array([
 [ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary
 [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],
 [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],
 [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],
 [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]
]) 
```

接下来，考虑一个带有多个约束条件的最小化问题（即来自[[5]](#rdd2e1855725e-5)的示例16.4）。目标函数是：

```py
>>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2 
```

有三个定义为约束条件：

```py
>>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},
...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},
...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2}) 
```

变量必须为正数，因此以下是界限：

```py
>>> bnds = ((0, None), (0, None)) 
```

优化问题使用SLSQP方法求解如下：

```py
>>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,
...                constraints=cons) 
```

它应该收敛到理论解（1.4, 1.7）。
