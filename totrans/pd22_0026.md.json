["```py\nIn [1]: import pandas as pd\n\nIn [2]: from io import StringIO\n\nIn [3]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [4]: pd.read_csv(StringIO(data))\nOut[4]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [5]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"COL1\", \"COL3\"])\nOut[5]: \n col1  col3\n0    a     1\n1    a     2\n2    c     3 \n```", "```py\nIn [6]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [7]: pd.read_csv(StringIO(data))\nOut[7]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [8]: pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)\nOut[8]: \n col1 col2  col3\n0    a    b     2 \n```", "```py\nIn [9]: import numpy as np\n\nIn [10]: data = \"a,b,c,d\\n1,2,3,4\\n5,6,7,8\\n9,10,11\"\n\nIn [11]: print(data)\na,b,c,d\n1,2,3,4\n5,6,7,8\n9,10,11\n\nIn [12]: df = pd.read_csv(StringIO(data), dtype=object)\n\nIn [13]: df\nOut[13]: \n a   b   c    d\n0  1   2   3    4\n1  5   6   7    8\n2  9  10  11  NaN\n\nIn [14]: df[\"a\"][0]\nOut[14]: '1'\n\nIn [15]: df = pd.read_csv(StringIO(data), dtype={\"b\": object, \"c\": np.float64, \"d\": \"Int64\"})\n\nIn [16]: df.dtypes\nOut[16]: \na      int64\nb     object\nc    float64\nd      Int64\ndtype: object \n```", "```py\nIn [17]: data = \"col_1\\n1\\n2\\n'A'\\n4.22\"\n\nIn [18]: df = pd.read_csv(StringIO(data), converters={\"col_1\": str})\n\nIn [19]: df\nOut[19]: \n col_1\n0     1\n1     2\n2   'A'\n3  4.22\n\nIn [20]: df[\"col_1\"].apply(type).value_counts()\nOut[20]: \ncol_1\n<class 'str'>    4\nName: count, dtype: int64 \n```", "```py\nIn [21]: df2 = pd.read_csv(StringIO(data))\n\nIn [22]: df2[\"col_1\"] = pd.to_numeric(df2[\"col_1\"], errors=\"coerce\")\n\nIn [23]: df2\nOut[23]: \n col_1\n0   1.00\n1   2.00\n2    NaN\n3   4.22\n\nIn [24]: df2[\"col_1\"].apply(type).value_counts()\nOut[24]: \ncol_1\n<class 'float'>    4\nName: count, dtype: int64 \n```", "```py\nIn [25]: col_1 = list(range(500000)) + [\"a\", \"b\"] + list(range(500000))\n\nIn [26]: df = pd.DataFrame({\"col_1\": col_1})\n\nIn [27]: df.to_csv(\"foo.csv\")\n\nIn [28]: mixed_df = pd.read_csv(\"foo.csv\")\n\nIn [29]: mixed_df[\"col_1\"].apply(type).value_counts()\nOut[29]: \ncol_1\n<class 'int'>    737858\n<class 'str'>    262144\nName: count, dtype: int64\n\nIn [30]: mixed_df[\"col_1\"].dtype\nOut[30]: dtype('O') \n```", "```py\nIn [31]: data = \"\"\"a,b,c,d,e,f,g,h,i,j\n ....: 1,2.5,True,a,,,,,12-31-2019,\n ....: 3,4.5,False,b,6,7.5,True,a,12-31-2019,\n ....: \"\"\"\n ....: \n\nIn [32]: df = pd.read_csv(StringIO(data), dtype_backend=\"numpy_nullable\", parse_dates=[\"i\"])\n\nIn [33]: df\nOut[33]: \n a    b      c  d     e     f     g     h          i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA> 2019-12-31  <NA>\n1  3  4.5  False  b     6   7.5  True     a 2019-12-31  <NA>\n\nIn [34]: df.dtypes\nOut[34]: \na             Int64\nb           Float64\nc           boolean\nd    string[python]\ne             Int64\nf           Float64\ng           boolean\nh    string[python]\ni    datetime64[ns]\nj             Int64\ndtype: object \n```", "```py\nIn [35]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [36]: pd.read_csv(StringIO(data))\nOut[36]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [37]: pd.read_csv(StringIO(data)).dtypes\nOut[37]: \ncol1    object\ncol2    object\ncol3     int64\ndtype: object\n\nIn [38]: pd.read_csv(StringIO(data), dtype=\"category\").dtypes\nOut[38]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object \n```", "```py\nIn [39]: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\nOut[39]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object \n```", "```py\nIn [40]: from pandas.api.types import CategoricalDtype\n\nIn [41]: dtype = CategoricalDtype([\"d\", \"c\", \"b\", \"a\"], ordered=True)\n\nIn [42]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).dtypes\nOut[42]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object \n```", "```py\nIn [43]: dtype = CategoricalDtype([\"a\", \"b\", \"d\"])  # No 'c'\n\nIn [44]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).col1\nOut[44]: \n0      a\n1      a\n2    NaN\nName: col1, dtype: category\nCategories (3, object): ['a', 'b', 'd'] \n```", "```py\nIn [45]: df = pd.read_csv(StringIO(data), dtype=\"category\")\n\nIn [46]: df.dtypes\nOut[46]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object\n\nIn [47]: df[\"col3\"]\nOut[47]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, object): ['1', '2', '3']\n\nIn [48]: new_categories = pd.to_numeric(df[\"col3\"].cat.categories)\n\nIn [49]: df[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\n\nIn [50]: df[\"col3\"]\nOut[50]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, int64): [1, 2, 3] \n```", "```py\nIn [51]: data = \"a,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [52]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [53]: pd.read_csv(StringIO(data))\nOut[53]: \n a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9 \n```", "```py\nIn [54]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [55]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=0)\nOut[55]: \n foo  bar  baz\n0    1    2    3\n1    4    5    6\n2    7    8    9\n\nIn [56]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=None)\nOut[56]: \n foo bar baz\n0   a   b   c\n1   1   2   3\n2   4   5   6\n3   7   8   9 \n```", "```py\nIn [57]: data = \"skip this skip it\\na,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [58]: pd.read_csv(StringIO(data), header=1)\nOut[58]: \n a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9 \n```", "```py\nIn [59]: data = \"a,b,a\\n0,1,2\\n3,4,5\"\n\nIn [60]: pd.read_csv(StringIO(data))\nOut[60]: \n a  b  a.1\n0  0  1    2\n1  3  4    5 \n```", "```py\nIn [61]: data = \"a,b,c,d\\n1,2,3,foo\\n4,5,6,bar\\n7,8,9,baz\"\n\nIn [62]: pd.read_csv(StringIO(data))\nOut[62]: \n a  b  c    d\n0  1  2  3  foo\n1  4  5  6  bar\n2  7  8  9  baz\n\nIn [63]: pd.read_csv(StringIO(data), usecols=[\"b\", \"d\"])\nOut[63]: \n b    d\n0  2  foo\n1  5  bar\n2  8  baz\n\nIn [64]: pd.read_csv(StringIO(data), usecols=[0, 2, 3])\nOut[64]: \n a  c    d\n0  1  3  foo\n1  4  6  bar\n2  7  9  baz\n\nIn [65]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"A\", \"C\"])\nOut[65]: \n a  c\n0  1  3\n1  4  6\n2  7  9 \n```", "```py\nIn [66]: pd.read_csv(StringIO(data), usecols=lambda x: x not in [\"a\", \"c\"])\nOut[66]: \n b    d\n0  2  foo\n1  5  bar\n2  8  baz \n```", "```py\nIn [67]: data = \"\\na,b,c\\n  \\n# commented line\\n1,2,3\\n\\n4,5,6\"\n\nIn [68]: print(data)\n\na,b,c\n\n# commented line\n1,2,3\n\n4,5,6\n\nIn [69]: pd.read_csv(StringIO(data), comment=\"#\")\nOut[69]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [70]: data = \"a,b,c\\n\\n1,2,3\\n\\n\\n4,5,6\"\n\nIn [71]: pd.read_csv(StringIO(data), skip_blank_lines=False)\nOut[71]: \n a    b    c\n0  NaN  NaN  NaN\n1  1.0  2.0  3.0\n2  NaN  NaN  NaN\n3  NaN  NaN  NaN\n4  4.0  5.0  6.0 \n```", "```py\nIn [72]: data = \"#comment\\na,b,c\\nA,B,C\\n1,2,3\"\n\nIn [73]: pd.read_csv(StringIO(data), comment=\"#\", header=1)\nOut[73]: \n A  B  C\n0  1  2  3\n\nIn [74]: data = \"A,B,C\\n#comment\\na,b,c\\n1,2,3\"\n\nIn [75]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=2)\nOut[75]: \n a  b  c\n0  1  2  3 \n```", "```py\nIn [76]: data = (\n ....:    \"# empty\\n\"\n ....:    \"# second empty line\\n\"\n ....:    \"# third emptyline\\n\"\n ....:    \"X,Y,Z\\n\"\n ....:    \"1,2,3\\n\"\n ....:    \"A,B,C\\n\"\n ....:    \"1,2.,4.\\n\"\n ....:    \"5.,NaN,10.0\\n\"\n ....: )\n ....: \n\nIn [77]: print(data)\n# empty\n# second empty line\n# third emptyline\nX,Y,Z\n1,2,3\nA,B,C\n1,2.,4.\n5.,NaN,10.0\n\nIn [78]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=4, header=1)\nOut[78]: \n A    B     C\n0  1.0  2.0   4.0\n1  5.0  NaN  10.0 \n```", "```py\nIn [79]: data = (\n ....:    \"ID,level,category\\n\"\n ....:    \"Patient1,123000,x # really unpleasant\\n\"\n ....:    \"Patient2,23000,y # wouldn't take his medicine\\n\"\n ....:    \"Patient3,1234018,z # awesome\"\n ....: )\n ....: \n\nIn [80]: with open(\"tmp.csv\", \"w\") as fh:\n ....:    fh.write(data)\n ....: \n\nIn [81]: print(open(\"tmp.csv\").read())\nID,level,category\nPatient1,123000,x # really unpleasant\nPatient2,23000,y # wouldn't take his medicine\nPatient3,1234018,z # awesome \n```", "```py\nIn [82]: df = pd.read_csv(\"tmp.csv\")\n\nIn [83]: df\nOut[83]: \n ID    level                        category\n0  Patient1   123000           x # really unpleasant\n1  Patient2    23000  y # wouldn't take his medicine\n2  Patient3  1234018                     z # awesome \n```", "```py\nIn [84]: df = pd.read_csv(\"tmp.csv\", comment=\"#\")\n\nIn [85]: df\nOut[85]: \n ID    level category\n0  Patient1   123000       x \n1  Patient2    23000       y \n2  Patient3  1234018       z \n```", "```py\nIn [86]: from io import BytesIO\n\nIn [87]: data = b\"word,length\\n\" b\"Tr\\xc3\\xa4umen,7\\n\" b\"Gr\\xc3\\xbc\\xc3\\x9fe,5\"\n\nIn [88]: data = data.decode(\"utf8\").encode(\"latin-1\")\n\nIn [89]: df = pd.read_csv(BytesIO(data), encoding=\"latin-1\")\n\nIn [90]: df\nOut[90]: \n word  length\n0  Tr\u00e4umen       7\n1    Gr\u00fc\u00dfe       5\n\nIn [91]: df[\"word\"][1]\nOut[91]: 'Gr\u00fc\u00dfe' \n```", "```py\nIn [92]: data = \"a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [93]: pd.read_csv(StringIO(data))\nOut[93]: \n a    b     c\n4   apple  bat   5.7\n8  orange  cow  10.0 \n```", "```py\nIn [94]: data = \"index,a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [95]: pd.read_csv(StringIO(data), index_col=0)\nOut[95]: \n a    b     c\nindex \n4       apple  bat   5.7\n8      orange  cow  10.0 \n```", "```py\nIn [96]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [97]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [98]: pd.read_csv(StringIO(data))\nOut[98]: \n a    b   c\n4   apple  bat NaN\n8  orange  cow NaN\n\nIn [99]: pd.read_csv(StringIO(data), index_col=False)\nOut[99]: \n a       b    c\n0  4   apple  bat\n1  8  orange  cow \n```", "```py\nIn [100]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [101]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [102]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"])\nOut[102]: \n b   c\n4  bat NaN\n8  cow NaN\n\nIn [103]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"], index_col=0)\nOut[103]: \n b   c\n4  bat NaN\n8  cow NaN \n```", "```py\nIn [104]: with open(\"foo.csv\", mode=\"w\") as f:\n .....:    f.write(\"date,A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\")\n .....: \n\n# Use a column as an index, and parse it as dates.\nIn [105]: df = pd.read_csv(\"foo.csv\", index_col=0, parse_dates=True)\n\nIn [106]: df\nOut[106]: \n A  B  C\ndate \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5\n\n# These are Python datetime objects\nIn [107]: df.index\nOut[107]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', name='date', freq=None) \n```", "```py\nIn [108]: data = (\n .....:    \"KORD,19990127, 19:00:00, 18:56:00, 0.8100\\n\"\n .....:    \"KORD,19990127, 20:00:00, 19:56:00, 0.0100\\n\"\n .....:    \"KORD,19990127, 21:00:00, 20:56:00, -0.5900\\n\"\n .....:    \"KORD,19990127, 21:00:00, 21:18:00, -0.9900\\n\"\n .....:    \"KORD,19990127, 22:00:00, 21:56:00, -0.5900\\n\"\n .....:    \"KORD,19990127, 23:00:00, 22:56:00, -0.5900\"\n .....: )\n .....: \n\nIn [109]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [110]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]])\n\nIn [111]: df\nOut[111]: \n 1_2                 1_3     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [112]: df = pd.read_csv(\n .....:    \"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]], keep_date_col=True\n .....: )\n .....: \n\nIn [113]: df\nOut[113]: \n 1_2                 1_3     0  ...          2          3     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  ...   19:00:00   18:56:00  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  ...   20:00:00   19:56:00  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD  ...   21:00:00   20:56:00 -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD  ...   21:00:00   21:18:00 -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD  ...   22:00:00   21:56:00 -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD  ...   23:00:00   22:56:00 -0.59\n\n[6 rows x 7 columns] \n```", "```py\nIn [114]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [115]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=date_spec)\n\nIn [116]: df\nOut[116]: \n nominal              actual     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [117]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [118]: df = pd.read_csv(\n .....:    \"tmp.csv\", header=None, parse_dates=date_spec, index_col=0\n .....: )  # index is the nominal column\n .....: \n\nIn [119]: df\nOut[119]: \n actual     0     4\nnominal \n1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [120]: content = \"\"\"\\\n .....: a\n .....: 2000-01-01T00:00:00+05:00\n .....: 2000-01-01T00:00:00+06:00\"\"\"\n .....: \n\nIn [121]: df = pd.read_csv(StringIO(content))\n\nIn [122]: df[\"a\"] = pd.to_datetime(df[\"a\"], utc=True)\n\nIn [123]: df[\"a\"]\nOut[123]: \n0   1999-12-31 19:00:00+00:00\n1   1999-12-31 18:00:00+00:00\nName: a, dtype: datetime64[ns, UTC] \n```", "```py\nIn [124]: df = pd.read_csv(\n .....:    \"foo.csv\",\n .....:    index_col=0,\n .....:    parse_dates=True,\n .....: )\n .....: \n\nIn [125]: df\nOut[125]: \n A  B  C\ndate \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5 \n```", "```py\nIn [126]: data = StringIO(\"date\\n12 Jan 2000\\n2000-01-13\\n\")\n\nIn [127]: df = pd.read_csv(data)\n\nIn [128]: df['date'] = pd.to_datetime(df['date'], format='mixed')\n\nIn [129]: df\nOut[129]: \n date\n0 2000-01-12\n1 2000-01-13 \n```", "```py\nIn [130]: data = StringIO(\"date\\n2020-01-01\\n2020-01-01 03:00\\n\")\n\nIn [131]: df = pd.read_csv(data)\n\nIn [132]: df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n\nIn [133]: df\nOut[133]: \n date\n0 2020-01-01 00:00:00\n1 2020-01-01 03:00:00 \n```", "```py\nIn [134]: data = \"date,value,cat\\n1/6/2000,5,a\\n2/6/2000,10,b\\n3/6/2000,15,c\"\n\nIn [135]: print(data)\ndate,value,cat\n1/6/2000,5,a\n2/6/2000,10,b\n3/6/2000,15,c\n\nIn [136]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [137]: pd.read_csv(\"tmp.csv\", parse_dates=[0])\nOut[137]: \n date  value cat\n0 2000-01-06      5   a\n1 2000-02-06     10   b\n2 2000-03-06     15   c\n\nIn [138]: pd.read_csv(\"tmp.csv\", dayfirst=True, parse_dates=[0])\nOut[138]: \n date  value cat\n0 2000-06-01      5   a\n1 2000-06-02     10   b\n2 2000-06-03     15   c \n```", "```py\nIn [139]: import io\n\nIn [140]: data = pd.DataFrame([0, 1, 2])\n\nIn [141]: buffer = io.BytesIO()\n\nIn [142]: data.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\") \n```", "```py\nIn [143]: val = \"0.3066101993807095471566981359501369297504425048828125\"\n\nIn [144]: data = \"a,b,c\\n1,2,{0}\".format(val)\n\nIn [145]: abs(\n .....:    pd.read_csv(\n .....:        StringIO(data),\n .....:        engine=\"c\",\n .....:        float_precision=None,\n .....:    )[\"c\"][0] - float(val)\n .....: )\n .....: \nOut[145]: 5.551115123125783e-17\n\nIn [146]: abs(\n .....:    pd.read_csv(\n .....:        StringIO(data),\n .....:        engine=\"c\",\n .....:        float_precision=\"high\",\n .....:    )[\"c\"][0] - float(val)\n .....: )\n .....: \nOut[146]: 5.551115123125783e-17\n\nIn [147]: abs(\n .....:    pd.read_csv(StringIO(data), engine=\"c\", float_precision=\"round_trip\")[\"c\"][0]\n .....:    - float(val)\n .....: )\n .....: \nOut[147]: 0.0 \n```", "```py\nIn [148]: data = (\n .....:    \"ID|level|category\\n\"\n .....:    \"Patient1|123,000|x\\n\"\n .....:    \"Patient2|23,000|y\\n\"\n .....:    \"Patient3|1,234,018|z\"\n .....: )\n .....: \n\nIn [149]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [150]: df = pd.read_csv(\"tmp.csv\", sep=\"|\")\n\nIn [151]: df\nOut[151]: \n ID      level category\n0  Patient1    123,000        x\n1  Patient2     23,000        y\n2  Patient3  1,234,018        z\n\nIn [152]: df.level.dtype\nOut[152]: dtype('O') \n```", "```py\nIn [153]: df = pd.read_csv(\"tmp.csv\", sep=\"|\", thousands=\",\")\n\nIn [154]: df\nOut[154]: \n ID    level category\n0  Patient1   123000        x\n1  Patient2    23000        y\n2  Patient3  1234018        z\n\nIn [155]: df.level.dtype\nOut[155]: dtype('int64') \n```", "```py\npd.read_csv(\"path_to_file.csv\", na_values=[5]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"\"]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"NA\", \"0\"]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", na_values=[\"Nope\"]) \n```", "```py\nIn [156]: data = \"a,b,c\\n1,Yes,2\\n3,No,4\"\n\nIn [157]: print(data)\na,b,c\n1,Yes,2\n3,No,4\n\nIn [158]: pd.read_csv(StringIO(data))\nOut[158]: \n a    b  c\n0  1  Yes  2\n1  3   No  4\n\nIn [159]: pd.read_csv(StringIO(data), true_values=[\"Yes\"], false_values=[\"No\"])\nOut[159]: \n a      b  c\n0  1   True  2\n1  3  False  4 \n```", "```py\nIn [160]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [161]: pd.read_csv(StringIO(data))\n---------------------------------------------------------------------------\nParserError  Traceback (most recent call last)\nCell In[161], line 1\n----> 1 pd.read_csv(StringIO(data))\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:626, in _read(filepath_or_buffer, kwds)\n  623     return parser\n  625 with parser:\n--> 626     return parser.read(nrows)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1923, in TextFileReader.read(self, nrows)\n  1916 nrows = validate_integer(\"nrows\", nrows)\n  1917 try:\n  1918     # error: \"ParserBase\" has no attribute \"read\"\n  1919     (\n  1920         index,\n  1921         columns,\n  1922         col_dict,\n-> 1923     ) = self._engine.read(  # type: ignore[attr-defined]\n  1924         nrows\n  1925     )\n  1926 except Exception:\n  1927     self.close()\n\nFile ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:234, in CParserWrapper.read(self, nrows)\n  232 try:\n  233     if self.low_memory:\n--> 234         chunks = self._reader.read_low_memory(nrows)\n  235         # destructive to chunks\n  236         data = _concatenate_chunks(chunks)\n\nFile parsers.pyx:838, in pandas._libs.parsers.TextReader.read_low_memory()\n\nFile parsers.pyx:905, in pandas._libs.parsers.TextReader._read_rows()\n\nFile parsers.pyx:874, in pandas._libs.parsers.TextReader._tokenize_rows()\n\nFile parsers.pyx:891, in pandas._libs.parsers.TextReader._check_tokenize_status()\n\nFile parsers.pyx:2061, in pandas._libs.parsers.raise_parser_error()\n\nParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4 \n```", "```py\nIn [162]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [163]: pd.read_csv(StringIO(data), on_bad_lines=\"skip\")\nOut[163]: \n a  b   c\n0  1  2   3\n1  8  9  10 \n```", "```py\nIn [164]: external_list = []\n\nIn [165]: def bad_lines_func(line):\n .....:    external_list.append(line)\n .....:    return line[-3:]\n .....: \n\nIn [166]: external_list\nOut[166]: [] \n```", "```py\nIn [167]: bad_lines_func = lambda line: print(line)\n\nIn [168]: data = 'name,type\\nname a,a is of type a\\nname b,\"b\\\" is of type b\"'\n\nIn [169]: data\nOut[169]: 'name,type\\nname a,a is of type a\\nname b,\"b\" is of type b\"'\n\nIn [170]: pd.read_csv(StringIO(data), on_bad_lines=bad_lines_func, engine=\"python\")\nOut[170]: \n name            type\n0  name a  a is of type a \n```", "```py\nIn [171]: pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n---------------------------------------------------------------------------\nValueError  Traceback (most recent call last)\nCell In[171], line 1\n----> 1 pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n  617 _validate_names(kwds.get(\"names\", None))\n  619 # Create the parser.\n--> 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n  622 if chunksize or iterator:\n  623     return parser\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n  1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n  1619 self.handles: IOHandles | None = None\n-> 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1898, in TextFileReader._make_engine(self, f, engine)\n  1895     raise ValueError(msg)\n  1897 try:\n-> 1898     return mapping[engine](f, **self.options)\n  1899 except Exception:\n  1900     if self.handles is not None:\n\nFile ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:155, in CParserWrapper.__init__(self, src, **kwds)\n  152     # error: Cannot determine type of 'names'\n  153     if len(self.names) < len(usecols):  # type: ignore[has-type]\n  154         # error: Cannot determine type of 'names'\n--> 155         self._validate_usecols_names(\n  156             usecols,\n  157             self.names,  # type: ignore[has-type]\n  158         )\n  160 # error: Cannot determine type of 'names'\n  161 self._validate_parse_dates_presence(self.names)  # type: ignore[has-type]\n\nFile ~/work/pandas/pandas/pandas/io/parsers/base_parser.py:979, in ParserBase._validate_usecols_names(self, usecols, names)\n  977 missing = [c for c in usecols if c not in names]\n  978 if len(missing) > 0:\n--> 979     raise ValueError(\n  980         f\"Usecols do not match columns, columns expected but not found: \"\n  981         f\"{missing}\"\n  982     )\n  984 return usecols\n\nValueError: Usecols do not match columns, columns expected but not found: [0, 1, 2] \n```", "```py\nIn [172]: pd.read_csv(StringIO(data), names=['a', 'b', 'c', 'd'])\nOut[172]: \n a                b   c   d\n0    name             type NaN NaN\n1  name a   a is of type a NaN NaN\n2  name b  b is of type b\" NaN NaN \n```", "```py\nIn [173]: data = \"label1,label2,label3\\n\" 'index1,\"a,c,e\\n' \"index2,b,d,f\"\n\nIn [174]: print(data)\nlabel1,label2,label3\nindex1,\"a,c,e\nindex2,b,d,f \n```", "```py\nIn [175]: import csv\n\nIn [176]: dia = csv.excel()\n\nIn [177]: dia.quoting = csv.QUOTE_NONE\n\nIn [178]: pd.read_csv(StringIO(data), dialect=dia)\nOut[178]: \n label1 label2 label3\nindex1     \"a      c      e\nindex2      b      d      f \n```", "```py\nIn [179]: data = \"a,b,c~1,2,3~4,5,6\"\n\nIn [180]: pd.read_csv(StringIO(data), lineterminator=\"~\")\nOut[180]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [181]: data = \"a, b, c\\n1, 2, 3\\n4, 5, 6\"\n\nIn [182]: print(data)\na, b, c\n1, 2, 3\n4, 5, 6\n\nIn [183]: pd.read_csv(StringIO(data), skipinitialspace=True)\nOut[183]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [184]: data = 'a,b\\n\"hello, \\\\\"Bob\\\\\", nice to see you\",5'\n\nIn [185]: print(data)\na,b\n\"hello, \\\"Bob\\\", nice to see you\",5\n\nIn [186]: pd.read_csv(StringIO(data), escapechar=\"\\\\\")\nOut[186]: \n a  b\n0  hello, \"Bob\", nice to see you  5 \n```", "```py\nIn [187]: data1 = (\n .....:    \"id8141    360.242940   149.910199   11950.7\\n\"\n .....:    \"id1594    444.953632   166.985655   11788.4\\n\"\n .....:    \"id1849    364.136849   183.628767   11806.2\\n\"\n .....:    \"id1230    413.836124   184.375703   11916.8\\n\"\n .....:    \"id1948    502.953953   173.237159   12468.3\"\n .....: )\n .....: \n\nIn [188]: with open(\"bar.csv\", \"w\") as f:\n .....:    f.write(data1)\n .....: \n```", "```py\n# Column specifications are a list of half-intervals\nIn [189]: colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]\n\nIn [190]: df = pd.read_fwf(\"bar.csv\", colspecs=colspecs, header=None, index_col=0)\n\nIn [191]: df\nOut[191]: \n 1           2        3\n0 \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3 \n```", "```py\n# Widths are a list of integers\nIn [192]: widths = [6, 14, 13, 10]\n\nIn [193]: df = pd.read_fwf(\"bar.csv\", widths=widths, header=None)\n\nIn [194]: df\nOut[194]: \n 0           1           2        3\n0  id8141  360.242940  149.910199  11950.7\n1  id1594  444.953632  166.985655  11788.4\n2  id1849  364.136849  183.628767  11806.2\n3  id1230  413.836124  184.375703  11916.8\n4  id1948  502.953953  173.237159  12468.3 \n```", "```py\nIn [195]: df = pd.read_fwf(\"bar.csv\", header=None, index_col=0)\n\nIn [196]: df\nOut[196]: \n 1           2        3\n0 \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3 \n```", "```py\nIn [197]: pd.read_fwf(\"bar.csv\", header=None, index_col=0).dtypes\nOut[197]: \n1    float64\n2    float64\n3    float64\ndtype: object\n\nIn [198]: pd.read_fwf(\"bar.csv\", header=None, dtype={2: \"object\"}).dtypes\nOut[198]: \n0     object\n1    float64\n2     object\n3    float64\ndtype: object \n```", "```py\nIn [199]: data = \"A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\"\n\nIn [200]: print(data)\nA,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5\n\nIn [201]: with open(\"foo.csv\", \"w\") as f:\n .....:    f.write(data)\n .....: \n```", "```py\nIn [202]: pd.read_csv(\"foo.csv\")\nOut[202]: \n A  B  C\n20090101  a  1  2\n20090102  b  3  4\n20090103  c  4  5 \n```", "```py\nIn [203]: df = pd.read_csv(\"foo.csv\", parse_dates=True)\n\nIn [204]: df.index\nOut[204]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [205]: data = 'year,indiv,zit,xit\\n1977,\"A\",1.2,.6\\n1977,\"B\",1.5,.5'\n\nIn [206]: print(data)\nyear,indiv,zit,xit\n1977,\"A\",1.2,.6\n1977,\"B\",1.5,.5\n\nIn [207]: with open(\"mindex_ex.csv\", mode=\"w\") as f:\n .....:    f.write(data)\n .....: \n```", "```py\nIn [208]: df = pd.read_csv(\"mindex_ex.csv\", index_col=[0, 1])\n\nIn [209]: df\nOut[209]: \n zit  xit\nyear indiv \n1977 A      1.2  0.6\n B      1.5  0.5\n\nIn [210]: df.loc[1977]\nOut[210]: \n zit  xit\nindiv \nA      1.2  0.6\nB      1.5  0.5 \n```", "```py\nIn [211]: mi_idx = pd.MultiIndex.from_arrays([[1, 2, 3, 4], list(\"abcd\")], names=list(\"ab\"))\n\nIn [212]: mi_col = pd.MultiIndex.from_arrays([[1, 2], list(\"ab\")], names=list(\"cd\"))\n\nIn [213]: df = pd.DataFrame(np.ones((4, 2)), index=mi_idx, columns=mi_col)\n\nIn [214]: df.to_csv(\"mi.csv\")\n\nIn [215]: print(open(\"mi.csv\").read())\nc,,1,2\nd,,a,b\na,b,,\n1,a,1.0,1.0\n2,b,1.0,1.0\n3,c,1.0,1.0\n4,d,1.0,1.0\n\nIn [216]: pd.read_csv(\"mi.csv\", header=[0, 1, 2, 3], index_col=[0, 1])\nOut[216]: \nc                    1                  2\nd                    a                  b\na   Unnamed: 2_level_2 Unnamed: 3_level_2\n1                  1.0                1.0\n2 b                1.0                1.0\n3 c                1.0                1.0\n4 d                1.0                1.0 \n```", "```py\nIn [217]: data = \",a,a,a,b,c,c\\n,q,r,s,t,u,v\\none,1,2,3,4,5,6\\ntwo,7,8,9,10,11,12\"\n\nIn [218]: print(data)\n,a,a,a,b,c,c\n,q,r,s,t,u,v\none,1,2,3,4,5,6\ntwo,7,8,9,10,11,12\n\nIn [219]: with open(\"mi2.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [220]: pd.read_csv(\"mi2.csv\", header=[0, 1], index_col=0)\nOut[220]: \n a         b   c \n q  r  s   t   u   v\none  1  2  3   4   5   6\ntwo  7  8  9  10  11  12 \n```", "```py\nIn [221]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [222]: df.to_csv(\"tmp2.csv\", sep=\":\", index=False)\n\nIn [223]: pd.read_csv(\"tmp2.csv\", sep=None, engine=\"python\")\nOut[223]: \n 0         1         2         3\n0  0.469112 -0.282863 -1.509059 -1.135632\n1  1.212112 -0.173215  0.119209 -1.044236\n2 -0.861849 -2.104569 -0.494929  1.071804\n3  0.721555 -0.706771 -1.039575  0.271860\n4 -0.424972  0.567020  0.276232 -1.087401\n5 -0.673690  0.113648 -1.478427  0.524988\n6  0.404705  0.577046 -1.715002 -1.039268\n7 -0.370647 -1.157892 -1.344312  0.844885\n8  1.075770 -0.109050  1.643563 -1.469388\n9  0.357021 -0.674600 -1.776904 -0.968914 \n```", "```py\nIn [224]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [225]: df.to_csv(\"tmp.csv\", index=False)\n\nIn [226]: table = pd.read_csv(\"tmp.csv\")\n\nIn [227]: table\nOut[227]: \n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n4  0.410835  0.813850  0.132003 -0.827317\n5 -0.076467 -1.187678  1.130127 -1.436737\n6 -1.413681  1.607920  1.024180  0.569605\n7  0.875906 -2.211372  0.974466 -2.006747\n8 -0.410001 -0.078638  0.545952 -1.219217\n9 -1.226825  0.769804 -1.281247 -0.727707 \n```", "```py\nIn [228]: with pd.read_csv(\"tmp.csv\", chunksize=4) as reader:\n .....:    print(reader)\n .....:    for chunk in reader:\n .....:        print(chunk)\n .....: \n<pandas.io.parsers.readers.TextFileReader object at 0x7ff2e5421db0>\n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n 0         1         2         3\n4  0.410835  0.813850  0.132003 -0.827317\n5 -0.076467 -1.187678  1.130127 -1.436737\n6 -1.413681  1.607920  1.024180  0.569605\n7  0.875906 -2.211372  0.974466 -2.006747\n 0         1         2         3\n8 -0.410001 -0.078638  0.545952 -1.219217\n9 -1.226825  0.769804 -1.281247 -0.727707 \n```", "```py\nIn [229]: with pd.read_csv(\"tmp.csv\", iterator=True) as reader:\n .....:    print(reader.get_chunk(5))\n .....: \n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n4  0.410835  0.813850  0.132003 -0.827317 \n```", "```py\ndf = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\") \n```", "```py\nheaders = {\"User-Agent\": \"pandas\"}\ndf = pd.read_csv(\n    \"https://download.bls.gov/pub/time.series/cu/cu.item\",\n    sep=\"\\t\",\n    storage_options=headers\n) \n```", "```py\ndf = pd.read_json(\"s3://pandas-test/adatafile.json\") \n```", "```py\nstorage_options = {\"client_kwargs\": {\"endpoint_url\": \"http://127.0.0.1:5555\"}}}\ndf = pd.read_json(\"s3://pandas-test/test-1\", storage_options=storage_options) \n```", "```py\npd.read_csv(\n    \"s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013\"\n    \"-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"anon\": True},\n) \n```", "```py\npd.read_csv(\n    \"simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/\"\n    \"SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"s3\": {\"anon\": True}},\n) \n```", "```py\nIn [230]: dfj = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [231]: json = dfj.to_json()\n\nIn [232]: json\nOut[232]: '{\"A\":{\"0\":-0.1213062281,\"1\":0.6957746499,\"2\":0.9597255933,\"3\":-0.6199759194,\"4\":-0.7323393705},\"B\":{\"0\":-0.0978826728,\"1\":0.3417343559,\"2\":-1.1103361029,\"3\":0.1497483186,\"4\":0.6877383895}}' \n```", "```py\nIn [233]: dfjo = pd.DataFrame(\n .....:    dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),\n .....:    columns=list(\"ABC\"),\n .....:    index=list(\"xyz\"),\n .....: )\n .....: \n\nIn [234]: dfjo\nOut[234]: \n A  B  C\nx  1  4  7\ny  2  5  8\nz  3  6  9\n\nIn [235]: sjo = pd.Series(dict(x=15, y=16, z=17), name=\"D\")\n\nIn [236]: sjo\nOut[236]: \nx    15\ny    16\nz    17\nName: D, dtype: int64 \n```", "```py\nIn [237]: dfjo.to_json(orient=\"columns\")\nOut[237]: '{\"A\":{\"x\":1,\"y\":2,\"z\":3},\"B\":{\"x\":4,\"y\":5,\"z\":6},\"C\":{\"x\":7,\"y\":8,\"z\":9}}'\n\n# Not available for Series \n```", "```py\nIn [238]: dfjo.to_json(orient=\"index\")\nOut[238]: '{\"x\":{\"A\":1,\"B\":4,\"C\":7},\"y\":{\"A\":2,\"B\":5,\"C\":8},\"z\":{\"A\":3,\"B\":6,\"C\":9}}'\n\nIn [239]: sjo.to_json(orient=\"index\")\nOut[239]: '{\"x\":15,\"y\":16,\"z\":17}' \n```", "```py\nIn [240]: dfjo.to_json(orient=\"records\")\nOut[240]: '[{\"A\":1,\"B\":4,\"C\":7},{\"A\":2,\"B\":5,\"C\":8},{\"A\":3,\"B\":6,\"C\":9}]'\n\nIn [241]: sjo.to_json(orient=\"records\")\nOut[241]: '[15,16,17]' \n```", "```py\nIn [242]: dfjo.to_json(orient=\"values\")\nOut[242]: '[[1,4,7],[2,5,8],[3,6,9]]'\n\n# Not available for Series \n```", "```py\nIn [243]: dfjo.to_json(orient=\"split\")\nOut[243]: '{\"columns\":[\"A\",\"B\",\"C\"],\"index\":[\"x\",\"y\",\"z\"],\"data\":[[1,4,7],[2,5,8],[3,6,9]]}'\n\nIn [244]: sjo.to_json(orient=\"split\")\nOut[244]: '{\"name\":\"D\",\"index\":[\"x\",\"y\",\"z\"],\"data\":[15,16,17]}' \n```", "```py\nIn [245]: dfd = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [246]: dfd[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [247]: dfd = dfd.sort_index(axis=1, ascending=False)\n\nIn [248]: json = dfd.to_json(date_format=\"iso\")\n\nIn [249]: json\nOut[249]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000\",\"1\":\"2013-01-01T00:00:00.000\",\"2\":\"2013-01-01T00:00:00.000\",\"3\":\"2013-01-01T00:00:00.000\",\"4\":\"2013-01-01T00:00:00.000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [250]: json = dfd.to_json(date_format=\"iso\", date_unit=\"us\")\n\nIn [251]: json\nOut[251]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000000\",\"1\":\"2013-01-01T00:00:00.000000\",\"2\":\"2013-01-01T00:00:00.000000\",\"3\":\"2013-01-01T00:00:00.000000\",\"4\":\"2013-01-01T00:00:00.000000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [252]: json = dfd.to_json(date_format=\"epoch\", date_unit=\"s\")\n\nIn [253]: json\nOut[253]: '{\"date\":{\"0\":1,\"1\":1,\"2\":1,\"3\":1,\"4\":1},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [254]: dfj2 = dfj.copy()\n\nIn [255]: dfj2[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [256]: dfj2[\"ints\"] = list(range(5))\n\nIn [257]: dfj2[\"bools\"] = True\n\nIn [258]: dfj2.index = pd.date_range(\"20130101\", periods=5)\n\nIn [259]: dfj2.to_json(\"test.json\")\n\nIn [260]: with open(\"test.json\") as fh:\n .....:    print(fh.read())\n .....: \n{\"A\":{\"1356998400000\":-0.1213062281,\"1357084800000\":0.6957746499,\"1357171200000\":0.9597255933,\"1357257600000\":-0.6199759194,\"1357344000000\":-0.7323393705},\"B\":{\"1356998400000\":-0.0978826728,\"1357084800000\":0.3417343559,\"1357171200000\":-1.1103361029,\"1357257600000\":0.1497483186,\"1357344000000\":0.6877383895},\"date\":{\"1356998400000\":1356,\"1357084800000\":1356,\"1357171200000\":1356,\"1357257600000\":1356,\"1357344000000\":1356},\"ints\":{\"1356998400000\":0,\"1357084800000\":1,\"1357171200000\":2,\"1357257600000\":3,\"1357344000000\":4},\"bools\":{\"1356998400000\":true,\"1357084800000\":true,\"1357171200000\":true,\"1357257600000\":true,\"1357344000000\":true}} \n```", "```py\n>>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json()  # raises\nRuntimeError: Unhandled numpy dtype 15 \n```", "```py\nIn [261]: pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)\nOut[261]: '{\"0\":{\"0\":\"(1+0j)\",\"1\":\"(2+0j)\",\"2\":\"(1+2j)\"}}' \n```", "```py\nIn [262]: from io import StringIO\n\nIn [263]: pd.read_json(StringIO(json))\nOut[263]: \n date         B         A\n0     1  0.403310  0.176444\n1     1  0.301624 -0.154951\n2     1 -1.369849 -2.179861\n3     1  1.462696 -0.954208\n4     1 -0.826591 -1.743161 \n```", "```py\nIn [264]: pd.read_json(\"test.json\")\nOut[264]: \n A         B  date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356     0   True\n2013-01-02  0.695775  0.341734  1356     1   True\n2013-01-03  0.959726 -1.110336  1356     2   True\n2013-01-04 -0.619976  0.149748  1356     3   True\n2013-01-05 -0.732339  0.687738  1356     4   True \n```", "```py\nIn [265]: pd.read_json(\"test.json\", dtype=object).dtypes\nOut[265]: \nA        object\nB        object\ndate     object\nints     object\nbools    object\ndtype: object \n```", "```py\nIn [266]: pd.read_json(\"test.json\", dtype={\"A\": \"float32\", \"bools\": \"int8\"}).dtypes\nOut[266]: \nA        float32\nB        float64\ndate       int64\nints       int64\nbools       int8\ndtype: object \n```", "```py\nIn [267]: from io import StringIO\n\nIn [268]: si = pd.DataFrame(\n .....:    np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)]\n .....: )\n .....: \n\nIn [269]: si\nOut[269]: \n 0    1    2    3\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n\nIn [270]: si.index\nOut[270]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [271]: si.columns\nOut[271]: Index([0, 1, 2, 3], dtype='int64')\n\nIn [272]: json = si.to_json()\n\nIn [273]: sij = pd.read_json(StringIO(json), convert_axes=False)\n\nIn [274]: sij\nOut[274]: \n 0  1  2  3\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n3  0  0  0  0\n\nIn [275]: sij.index\nOut[275]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [276]: sij.columns\nOut[276]: Index(['0', '1', '2', '3'], dtype='object') \n```", "```py\nIn [277]: from io import StringIO\n\nIn [278]: json = dfj2.to_json(date_unit=\"ns\")\n\n# Try to parse timestamps as milliseconds -> Won't Work\nIn [279]: dfju = pd.read_json(StringIO(json), date_unit=\"ms\")\n\nIn [280]: dfju\nOut[280]: \n A         B        date  ints  bools\n1356998400000000000 -0.121306 -0.097883  1356998400     0   True\n1357084800000000000  0.695775  0.341734  1356998400     1   True\n1357171200000000000  0.959726 -1.110336  1356998400     2   True\n1357257600000000000 -0.619976  0.149748  1356998400     3   True\n1357344000000000000 -0.732339  0.687738  1356998400     4   True\n\n# Let pandas detect the correct precision\nIn [281]: dfju = pd.read_json(StringIO(json))\n\nIn [282]: dfju\nOut[282]: \n A         B       date  ints  bools\n2013-01-01 -0.121306 -0.097883 2013-01-01     0   True\n2013-01-02  0.695775  0.341734 2013-01-01     1   True\n2013-01-03  0.959726 -1.110336 2013-01-01     2   True\n2013-01-04 -0.619976  0.149748 2013-01-01     3   True\n2013-01-05 -0.732339  0.687738 2013-01-01     4   True\n\n# Or specify that all timestamps are in nanoseconds\nIn [283]: dfju = pd.read_json(StringIO(json), date_unit=\"ns\")\n\nIn [284]: dfju\nOut[284]: \n A         B        date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356998400     0   True\n2013-01-02  0.695775  0.341734  1356998400     1   True\n2013-01-03  0.959726 -1.110336  1356998400     2   True\n2013-01-04 -0.619976  0.149748  1356998400     3   True\n2013-01-05 -0.732339  0.687738  1356998400     4   True \n```", "```py\nIn [285]: data = (\n .....: '{\"a\":{\"0\":1,\"1\":3},\"b\":{\"0\":2.5,\"1\":4.5},\"c\":{\"0\":true,\"1\":false},\"d\":{\"0\":\"a\",\"1\":\"b\"},'\n .....: '\"e\":{\"0\":null,\"1\":6.0},\"f\":{\"0\":null,\"1\":7.5},\"g\":{\"0\":null,\"1\":true},\"h\":{\"0\":null,\"1\":\"a\"},'\n .....: '\"i\":{\"0\":\"12-31-2019\",\"1\":\"12-31-2019\"},\"j\":{\"0\":null,\"1\":null}}'\n .....: )\n .....: \n\nIn [286]: df = pd.read_json(StringIO(data), dtype_backend=\"pyarrow\")\n\nIn [287]: df\nOut[287]: \n a    b      c  d     e     f     g     h           i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA>  12-31-2019  None\n1  3  4.5  False  b     6   7.5  True     a  12-31-2019  None\n\nIn [288]: df.dtypes\nOut[288]: \na     int64[pyarrow]\nb    double[pyarrow]\nc      bool[pyarrow]\nd    string[pyarrow]\ne     int64[pyarrow]\nf    double[pyarrow]\ng      bool[pyarrow]\nh    string[pyarrow]\ni    string[pyarrow]\nj      null[pyarrow]\ndtype: object \n```", "```py\nIn [289]: data = [\n .....:    {\"id\": 1, \"name\": {\"first\": \"Coleen\", \"last\": \"Volk\"}},\n .....:    {\"name\": {\"given\": \"Mark\", \"family\": \"Regner\"}},\n .....:    {\"id\": 2, \"name\": \"Faye Raker\"},\n .....: ]\n .....: \n\nIn [290]: pd.json_normalize(data)\nOut[290]: \n id name.first name.last name.given name.family        name\n0  1.0     Coleen      Volk        NaN         NaN         NaN\n1  NaN        NaN       NaN       Mark      Regner         NaN\n2  2.0        NaN       NaN        NaN         NaN  Faye Raker \n```", "```py\nIn [291]: data = [\n .....:    {\n .....:        \"state\": \"Florida\",\n .....:        \"shortname\": \"FL\",\n .....:        \"info\": {\"governor\": \"Rick Scott\"},\n .....:        \"county\": [\n .....:            {\"name\": \"Dade\", \"population\": 12345},\n .....:            {\"name\": \"Broward\", \"population\": 40000},\n .....:            {\"name\": \"Palm Beach\", \"population\": 60000},\n .....:        ],\n .....:    },\n .....:    {\n .....:        \"state\": \"Ohio\",\n .....:        \"shortname\": \"OH\",\n .....:        \"info\": {\"governor\": \"John Kasich\"},\n .....:        \"county\": [\n .....:            {\"name\": \"Summit\", \"population\": 1234},\n .....:            {\"name\": \"Cuyahoga\", \"population\": 1337},\n .....:        ],\n .....:    },\n .....: ]\n .....: \n\nIn [292]: pd.json_normalize(data, \"county\", [\"state\", \"shortname\", [\"info\", \"governor\"]])\nOut[292]: \n name  population    state shortname info.governor\n0        Dade       12345  Florida        FL    Rick Scott\n1     Broward       40000  Florida        FL    Rick Scott\n2  Palm Beach       60000  Florida        FL    Rick Scott\n3      Summit        1234     Ohio        OH   John Kasich\n4    Cuyahoga        1337     Ohio        OH   John Kasich \n```", "```py\nIn [293]: data = [\n .....:    {\n .....:        \"CreatedBy\": {\"Name\": \"User001\"},\n .....:        \"Lookup\": {\n .....:            \"TextField\": \"Some text\",\n .....:            \"UserField\": {\"Id\": \"ID001\", \"Name\": \"Name001\"},\n .....:        },\n .....:        \"Image\": {\"a\": \"b\"},\n .....:    }\n .....: ]\n .....: \n\nIn [294]: pd.json_normalize(data, max_level=1)\nOut[294]: \n CreatedBy.Name Lookup.TextField                    Lookup.UserField Image.a\n0        User001        Some text  {'Id': 'ID001', 'Name': 'Name001'}       b \n```", "```py\nIn [295]: from io import StringIO\n\nIn [296]: jsonl = \"\"\"\n .....:    {\"a\": 1, \"b\": 2}\n .....:    {\"a\": 3, \"b\": 4}\n .....: \"\"\"\n .....: \n\nIn [297]: df = pd.read_json(StringIO(jsonl), lines=True)\n\nIn [298]: df\nOut[298]: \n a  b\n0  1  2\n1  3  4\n\nIn [299]: df.to_json(orient=\"records\", lines=True)\nOut[299]: '{\"a\":1,\"b\":2}\\n{\"a\":3,\"b\":4}\\n'\n\n# reader is an iterator that returns ``chunksize`` lines each iteration\nIn [300]: with pd.read_json(StringIO(jsonl), lines=True, chunksize=1) as reader:\n .....:    reader\n .....:    for chunk in reader:\n .....:        print(chunk)\n .....: \nEmpty DataFrame\nColumns: []\nIndex: []\n a  b\n0  1  2\n a  b\n1  3  4 \n```", "```py\nIn [301]: from io import BytesIO\n\nIn [302]: df = pd.read_json(BytesIO(jsonl.encode()), lines=True, engine=\"pyarrow\")\n\nIn [303]: df\nOut[303]: \n a  b\n0  1  2\n1  3  4 \n```", "```py\nIn [304]: df = pd.DataFrame(\n .....:    {\n .....:        \"A\": [1, 2, 3],\n .....:        \"B\": [\"a\", \"b\", \"c\"],\n .....:        \"C\": pd.date_range(\"2016-01-01\", freq=\"d\", periods=3),\n .....:    },\n .....:    index=pd.Index(range(3), name=\"idx\"),\n .....: )\n .....: \n\nIn [305]: df\nOut[305]: \n A  B          C\nidx \n0    1  a 2016-01-01\n1    2  b 2016-01-02\n2    3  c 2016-01-03\n\nIn [306]: df.to_json(orient=\"table\", date_format=\"iso\")\nOut[306]: '{\"schema\":{\"fields\":[{\"name\":\"idx\",\"type\":\"integer\"},{\"name\":\"A\",\"type\":\"integer\"},{\"name\":\"B\",\"type\":\"string\"},{\"name\":\"C\",\"type\":\"datetime\"}],\"primaryKey\":[\"idx\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"idx\":0,\"A\":1,\"B\":\"a\",\"C\":\"2016-01-01T00:00:00.000\"},{\"idx\":1,\"A\":2,\"B\":\"b\",\"C\":\"2016-01-02T00:00:00.000\"},{\"idx\":2,\"A\":3,\"B\":\"c\",\"C\":\"2016-01-03T00:00:00.000\"}]}' \n```", "```py\n    In [307]: from pandas.io.json import build_table_schema\n\n    In [308]: s = pd.Series(pd.date_range(\"2016\", periods=4))\n\n    In [309]: build_table_schema(s)\n    Out[309]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'datetime'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [310]: s_tz = pd.Series(pd.date_range(\"2016\", periods=12, tz=\"US/Central\"))\n\n    In [311]: build_table_schema(s_tz)\n    Out[311]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'datetime', 'tz': 'US/Central'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [312]: s_per = pd.Series(1, index=pd.period_range(\"2016\", freq=\"Y-DEC\", periods=4))\n\n    In [313]: build_table_schema(s_per)\n    Out[313]: \n    {'fields': [{'name': 'index', 'type': 'datetime', 'freq': 'YE-DEC'},\n     {'name': 'values', 'type': 'integer'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [314]: s_cat = pd.Series(pd.Categorical([\"a\", \"b\", \"a\"]))\n\n    In [315]: build_table_schema(s_cat)\n    Out[315]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values',\n     'type': 'any',\n     'constraints': {'enum': ['a', 'b']},\n     'ordered': False}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [316]: s_dupe = pd.Series([1, 2], index=[1, 1])\n\n    In [317]: build_table_schema(s_dupe)\n    Out[317]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'integer'}],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [318]: s_multi = pd.Series(1, index=pd.MultiIndex.from_product([(\"a\", \"b\"), (0, 1)]))\n\n    In [319]: build_table_schema(s_multi)\n    Out[319]: \n    {'fields': [{'name': 'level_0', 'type': 'string'},\n     {'name': 'level_1', 'type': 'integer'},\n     {'name': 'values', 'type': 'integer'}],\n     'primaryKey': FrozenList(['level_0', 'level_1']),\n     'pandas_version': '1.4.0'} \n    ```", "```py\nIn [320]: df = pd.DataFrame(\n .....:    {\n .....:        \"foo\": [1, 2, 3, 4],\n .....:        \"bar\": [\"a\", \"b\", \"c\", \"d\"],\n .....:        \"baz\": pd.date_range(\"2018-01-01\", freq=\"d\", periods=4),\n .....:        \"qux\": pd.Categorical([\"a\", \"b\", \"c\", \"c\"]),\n .....:    },\n .....:    index=pd.Index(range(4), name=\"idx\"),\n .....: )\n .....: \n\nIn [321]: df\nOut[321]: \n foo bar        baz qux\nidx \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [322]: df.dtypes\nOut[322]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object\n\nIn [323]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [324]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [325]: new_df\nOut[325]: \n foo bar        baz qux\nidx \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [326]: new_df.dtypes\nOut[326]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object \n```", "```py\nIn [327]: df.index.name = \"index\"\n\nIn [328]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [329]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [330]: print(new_df.index.name)\nNone \n```", "```py\nIn [320]: url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\nIn [321]: pd.read_html(url)\nOut[321]:\n[                         Bank NameBank           CityCity StateSt  ...              Acquiring InstitutionAI Closing DateClosing FundFund\n 0                    Almena State Bank             Almena      KS  ...                          Equity Bank    October 23, 2020    10538\n 1           First City Bank of Florida  Fort Walton Beach      FL  ...            United Fidelity Bank, fsb    October 16, 2020    10537\n 2                 The First State Bank      Barboursville      WV  ...                       MVB Bank, Inc.       April 3, 2020    10536\n 3                   Ericson State Bank            Ericson      NE  ...           Farmers and Merchants Bank   February 14, 2020    10535\n 4     City National Bank of New Jersey             Newark      NJ  ...                      Industrial Bank    November 1, 2019    10534\n ..                                 ...                ...     ...  ...                                  ...                 ...      ...\n 558                 Superior Bank, FSB           Hinsdale      IL  ...                Superior Federal, FSB       July 27, 2001     6004\n 559                Malta National Bank              Malta      OH  ...                    North Valley Bank         May 3, 2001     4648\n 560    First Alliance Bank & Trust Co.         Manchester      NH  ...  Southern New Hampshire Bank & Trust    February 2, 2001     4647\n 561  National State Bank of Metropolis         Metropolis      IL  ...              Banterra Bank of Marion   December 14, 2000     4646\n 562                   Bank of Honolulu           Honolulu      HI  ...                   Bank of the Orient    October 13, 2000     4645\n\n [563 rows x 7 columns]] \n```", "```py\nIn [322]: url = 'https://www.sump.org/notes/request/' # HTTP request reflector\nIn [323]: pd.read_html(url)\nOut[323]:\n[                   0                    1\n 0     Remote Socket:  51.15.105.256:51760\n 1  Protocol Version:             HTTP/1.1\n 2    Request Method:                  GET\n 3       Request URI:      /notes/request/\n 4     Request Query:                  NaN,\n 0   Accept-Encoding:             identity\n 1              Host:         www.sump.org\n 2        User-Agent:    Python-urllib/3.8\n 3        Connection:                close]\nIn [324]: headers = {\nIn [325]:    'User-Agent':'Mozilla Firefox v14.0',\nIn [326]:    'Accept':'application/json',\nIn [327]:    'Connection':'keep-alive',\nIn [328]:    'Auth':'Bearer 2*/f3+fe68df*4'\nIn [329]: }\nIn [340]: pd.read_html(url, storage_options=headers)\nOut[340]:\n[                   0                    1\n 0     Remote Socket:  51.15.105.256:51760\n 1  Protocol Version:             HTTP/1.1\n 2    Request Method:                  GET\n 3       Request URI:      /notes/request/\n 4     Request Query:                  NaN,\n 0        User-Agent: Mozilla Firefox v14.0\n 1    AcceptEncoding:   gzip,  deflate,  br\n 2            Accept:      application/json\n 3        Connection:             keep-alive\n 4              Auth:  Bearer 2*/f3+fe68df*4] \n```", "```py\nIn [331]: html_str = \"\"\"\n .....:         <table>\n .....:             <tr>\n .....:                 <th>A</th>\n .....:                 <th colspan=\"1\">B</th>\n .....:                 <th rowspan=\"1\">C</th>\n .....:             </tr>\n .....:             <tr>\n .....:                 <td>a</td>\n .....:                 <td>b</td>\n .....:                 <td>c</td>\n .....:             </tr>\n .....:         </table>\n .....:     \"\"\"\n .....: \n\nIn [332]: with open(\"tmp.html\", \"w\") as f:\n .....:    f.write(html_str)\n .....: \n\nIn [333]: df = pd.read_html(\"tmp.html\")\n\nIn [334]: df[0]\nOut[334]: \n A  B  C\n0  a  b  c \n```", "```py\nIn [335]: dfs = pd.read_html(StringIO(html_str))\n\nIn [336]: dfs[0]\nOut[336]: \n A  B  C\n0  a  b  c \n```", "```py\nmatch = \"Metcalf Bank\"\ndf_list = pd.read_html(url, match=match) \n```", "```py\ndfs = pd.read_html(url, header=0) \n```", "```py\ndfs = pd.read_html(url, index_col=0) \n```", "```py\ndfs = pd.read_html(url, skiprows=0) \n```", "```py\ndfs = pd.read_html(url, skiprows=range(2)) \n```", "```py\ndfs1 = pd.read_html(url, attrs={\"id\": \"table\"})\ndfs2 = pd.read_html(url, attrs={\"class\": \"sortable\"})\nprint(np.array_equal(dfs1[0], dfs2[0]))  # Should be True \n```", "```py\ndfs = pd.read_html(url, na_values=[\"No Acquirer\"]) \n```", "```py\ndfs = pd.read_html(url, keep_default_na=False) \n```", "```py\nurl_mcc = \"https://en.wikipedia.org/wiki/Mobile_country_code?oldid=899173761\"\ndfs = pd.read_html(\n    url_mcc,\n    match=\"Telekom Albania\",\n    header=0,\n    converters={\"MNC\": str},\n) \n```", "```py\ndfs = pd.read_html(url, match=\"Metcalf Bank\", index_col=0) \n```", "```py\ndf = pd.DataFrame(np.random.randn(2, 2))\ns = df.to_html(float_format=\"{0:.40g}\".format)\ndfin = pd.read_html(s, index_col=0) \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\"]) \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=\"lxml\") \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\", \"bs4\"]) \n```", "```py\nIn [337]: html_table = \"\"\"\n .....: <table>\n .....:  <tr>\n .....:    <th>GitHub</th>\n .....:  </tr>\n .....:  <tr>\n .....:    <td><a href=\"https://github.com/pandas-dev/pandas\">pandas</a></td>\n .....:  </tr>\n .....: </table>\n .....: \"\"\"\n .....: \n\nIn [338]: df = pd.read_html(\n .....:    StringIO(html_table),\n .....:    extract_links=\"all\"\n .....: )[0]\n .....: \n\nIn [339]: df\nOut[339]: \n (GitHub, None)\n0  (pandas, https://github.com/pandas-dev/pandas)\n\nIn [340]: df[(\"GitHub\", None)]\nOut[340]: \n0    (pandas, https://github.com/pandas-dev/pandas)\nName: (GitHub, None), dtype: object\n\nIn [341]: df[(\"GitHub\", None)].str[1]\nOut[341]: \n0    https://github.com/pandas-dev/pandas\nName: (GitHub, None), dtype: object \n```", "```py\nIn [342]: from IPython.display import display, HTML\n\nIn [343]: df = pd.DataFrame(np.random.randn(2, 2))\n\nIn [344]: df\nOut[344]: \n 0         1\n0 -0.345352  1.314232\n1  0.690579  0.995761\n\nIn [345]: html = df.to_html()\n\nIn [346]: print(html)  # raw html\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table>\n\nIn [347]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [348]: html = df.to_html(columns=[0])\n\nIn [349]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n </tr>\n </tbody>\n</table>\n\nIn [350]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [351]: html = df.to_html(float_format=\"{0:.10f}\".format)\n\nIn [352]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.3453521949</td>\n <td>1.3142323796</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.6905793352</td>\n <td>0.9957609037</td>\n </tr>\n </tbody>\n</table>\n\nIn [353]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [354]: html = df.to_html(bold_rows=False)\n\nIn [355]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <td>0</td>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <td>1</td>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table>\n\nIn [356]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [357]: print(df.to_html(classes=[\"awesome_table_class\", \"even_more_awesome_class\"]))\n<table border=\"1\" class=\"dataframe awesome_table_class even_more_awesome_class\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table> \n```", "```py\nIn [358]: url_df = pd.DataFrame(\n .....:    {\n .....:        \"name\": [\"Python\", \"pandas\"],\n .....:        \"url\": [\"https://www.python.org/\", \"https://pandas.pydata.org\"],\n .....:    }\n .....: )\n .....: \n\nIn [359]: html = url_df.to_html(render_links=True)\n\nIn [360]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>name</th>\n <th>url</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>Python</td>\n <td><a href=\"https://www.python.org/\" target=\"_blank\">https://www.python.org/</a></td>\n </tr>\n <tr>\n <th>1</th>\n <td>pandas</td>\n <td><a href=\"https://pandas.pydata.org\" target=\"_blank\">https://pandas.pydata.org</a></td>\n </tr>\n </tbody>\n</table>\n\nIn [361]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [362]: df = pd.DataFrame({\"a\": list(\"&<>\"), \"b\": np.random.randn(3)}) \n```", "```py\nIn [363]: html = df.to_html()\n\nIn [364]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>a</th>\n <th>b</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>&amp;</td>\n <td>2.396780</td>\n </tr>\n <tr>\n <th>1</th>\n <td>&lt;</td>\n <td>0.014871</td>\n </tr>\n <tr>\n <th>2</th>\n <td>&gt;</td>\n <td>3.357427</td>\n </tr>\n </tbody>\n</table>\n\nIn [365]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [366]: html = df.to_html(escape=False)\n\nIn [367]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>a</th>\n <th>b</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>&</td>\n <td>2.396780</td>\n </tr>\n <tr>\n <th>1</th>\n <td><</td>\n <td>0.014871</td>\n </tr>\n <tr>\n <th>2</th>\n <td>></td>\n <td>3.357427</td>\n </tr>\n </tbody>\n</table>\n\nIn [368]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [369]: df = pd.DataFrame([[1, 2], [3, 4]], index=[\"a\", \"b\"], columns=[\"c\", \"d\"])\n\nIn [370]: print(df.style.to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & 1 & 2 \\\\\nb & 3 & 4 \\\\\n\\end{tabular} \n```", "```py\nIn [371]: print(df.style.format(\"\u20ac {}\").to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & \u20ac 1 & \u20ac 2 \\\\\nb & \u20ac 3 & \u20ac 4 \\\\\n\\end{tabular} \n```", "```py\nIn [372]: from io import StringIO\n\nIn [373]: xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n .....: <bookstore>\n .....:  <book category=\"cooking\">\n .....:    <title lang=\"en\">Everyday Italian</title>\n .....:    <author>Giada De Laurentiis</author>\n .....:    <year>2005</year>\n .....:    <price>30.00</price>\n .....:  </book>\n .....:  <book category=\"children\">\n .....:    <title lang=\"en\">Harry Potter</title>\n .....:    <author>J K. Rowling</author>\n .....:    <year>2005</year>\n .....:    <price>29.99</price>\n .....:  </book>\n .....:  <book category=\"web\">\n .....:    <title lang=\"en\">Learning XML</title>\n .....:    <author>Erik T. Ray</author>\n .....:    <year>2003</year>\n .....:    <price>39.95</price>\n .....:  </book>\n .....: </bookstore>\"\"\"\n .....: \n\nIn [374]: df = pd.read_xml(StringIO(xml))\n\nIn [375]: df\nOut[375]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [376]: df = pd.read_xml(\"https://www.w3schools.com/xml/books.xml\")\n\nIn [377]: df\nOut[377]: \n category              title                  author  year  price      cover\n0   cooking   Everyday Italian     Giada De Laurentiis  2005  30.00       None\n1  children       Harry Potter            J K. Rowling  2005  29.99       None\n2       web  XQuery Kick Start  Vaidyanathan Nagarajan  2003  49.99       None\n3       web       Learning XML             Erik T. Ray  2003  39.95  paperback \n```", "```py\nIn [378]: file_path = \"books.xml\"\n\nIn [379]: with open(file_path, \"w\") as f:\n .....:    f.write(xml)\n .....: \n\nIn [380]: with open(file_path, \"r\") as f:\n .....:    df = pd.read_xml(StringIO(f.read()))\n .....: \n\nIn [381]: df\nOut[381]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [382]: with open(file_path, \"r\") as f:\n .....:    sio = StringIO(f.read())\n .....: \n\nIn [383]: df = pd.read_xml(sio)\n\nIn [384]: df\nOut[384]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [385]: with open(file_path, \"rb\") as f:\n .....:    bio = BytesIO(f.read())\n .....: \n\nIn [386]: df = pd.read_xml(bio)\n\nIn [387]: df\nOut[387]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [388]: df = pd.read_xml(\n .....:    \"s3://pmc-oa-opendata/oa_comm/xml/all/PMC1236943.xml\",\n .....:    xpath=\".//journal-meta\",\n .....: )\n .....: \n\nIn [389]: df\nOut[389]: \n journal-id              journal-title       issn  publisher\n0  Cardiovasc Ultrasound  Cardiovascular Ultrasound  1476-7120        NaN \n```", "```py\nIn [390]: df = pd.read_xml(file_path, xpath=\"//book[year=2005]\")\n\nIn [391]: df\nOut[391]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99 \n```", "```py\nIn [392]: df = pd.read_xml(file_path, elems_only=True)\n\nIn [393]: df\nOut[393]: \n title               author  year  price\n0  Everyday Italian  Giada De Laurentiis  2005  30.00\n1      Harry Potter         J K. Rowling  2005  29.99\n2      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [394]: df = pd.read_xml(file_path, attrs_only=True)\n\nIn [395]: df\nOut[395]: \n category\n0   cooking\n1  children\n2       web \n```", "```py\nIn [396]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <doc:data >\n .....:  <doc:row>\n .....:    <doc:shape>square</doc:shape>\n .....:    <doc:degrees>360</doc:degrees>\n .....:    <doc:sides>4.0</doc:sides>\n .....:  </doc:row>\n .....:  <doc:row>\n .....:    <doc:shape>circle</doc:shape>\n .....:    <doc:degrees>360</doc:degrees>\n .....:    <doc:sides/>\n .....:  </doc:row>\n .....:  <doc:row>\n .....:    <doc:shape>triangle</doc:shape>\n .....:    <doc:degrees>180</doc:degrees>\n .....:    <doc:sides>3.0</doc:sides>\n .....:  </doc:row>\n .....: </doc:data>\"\"\"\n .....: \n\nIn [397]: df = pd.read_xml(StringIO(xml),\n .....:                 xpath=\"//doc:row\",\n .....:                 namespaces={\"doc\": \"https://example.com\"})\n .....: \n\nIn [398]: df\nOut[398]: \n shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0 \n```", "```py\nIn [399]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <data >\n .....: <row>\n .....:   <shape>square</shape>\n .....:   <degrees>360</degrees>\n .....:   <sides>4.0</sides>\n .....: </row>\n .....: <row>\n .....:   <shape>circle</shape>\n .....:   <degrees>360</degrees>\n .....:   <sides/>\n .....: </row>\n .....: <row>\n .....:   <shape>triangle</shape>\n .....:   <degrees>180</degrees>\n .....:   <sides>3.0</sides>\n .....: </row>\n .....: </data>\"\"\"\n .....: \n\nIn [400]: df = pd.read_xml(StringIO(xml),\n .....:                 xpath=\"//pandas:row\",\n .....:                 namespaces={\"pandas\": \"https://example.com\"})\n .....: \n\nIn [401]: df\nOut[401]: \n shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0 \n```", "```py\nIn [402]: xml = \"\"\"\n .....: <data>\n .....:  <row>\n .....:    <shape sides=\"4\">square</shape>\n .....:    <degrees>360</degrees>\n .....:  </row>\n .....:  <row>\n .....:    <shape sides=\"0\">circle</shape>\n .....:    <degrees>360</degrees>\n .....:  </row>\n .....:  <row>\n .....:    <shape sides=\"3\">triangle</shape>\n .....:    <degrees>180</degrees>\n .....:  </row>\n .....: </data>\"\"\"\n .....: \n\nIn [403]: df = pd.read_xml(StringIO(xml), xpath=\"./row\")\n\nIn [404]: df\nOut[404]: \n shape  degrees\n0    square      360\n1    circle      360\n2  triangle      180 \n```", "```py\nIn [405]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <response>\n .....:  <row>\n .....:    <station id=\"40850\" name=\"Library\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>864.2</avg_weekday_rides>\n .....:      <avg_saturday_rides>534</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....:  <row>\n .....:    <station id=\"41700\" name=\"Washington/Wabash\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>2707.4</avg_weekday_rides>\n .....:      <avg_saturday_rides>1909.8</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....:  <row>\n .....:    <station id=\"40380\" name=\"Clark/Lake\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>2949.6</avg_weekday_rides>\n .....:      <avg_saturday_rides>1657</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....: </response>\"\"\"\n .....: \n\nIn [406]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" >\n .....:   <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n .....:   <xsl:strip-space elements=\"*\"/>\n .....:   <xsl:template match=\"/response\">\n .....:      <xsl:copy>\n .....:        <xsl:apply-templates select=\"row\"/>\n .....:      </xsl:copy>\n .....:   </xsl:template>\n .....:   <xsl:template match=\"row\">\n .....:      <xsl:copy>\n .....:        <station_id><xsl:value-of select=\"station/@id\"/></station_id>\n .....:        <station_name><xsl:value-of select=\"station/@name\"/></station_name>\n .....:        <xsl:copy-of select=\"month|rides/*\"/>\n .....:      </xsl:copy>\n .....:   </xsl:template>\n .....: </xsl:stylesheet>\"\"\"\n .....: \n\nIn [407]: output = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <response>\n .....:   <row>\n .....:      <station_id>40850</station_id>\n .....:      <station_name>Library</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>864.2</avg_weekday_rides>\n .....:      <avg_saturday_rides>534</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n .....:   </row>\n .....:   <row>\n .....:      <station_id>41700</station_id>\n .....:      <station_name>Washington/Wabash</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>2707.4</avg_weekday_rides>\n .....:      <avg_saturday_rides>1909.8</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n .....:   </row>\n .....:   <row>\n .....:      <station_id>40380</station_id>\n .....:      <station_name>Clark/Lake</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>2949.6</avg_weekday_rides>\n .....:      <avg_saturday_rides>1657</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n .....:   </row>\n .....: </response>\"\"\"\n .....: \n\nIn [408]: df = pd.read_xml(StringIO(xml), stylesheet=xsl)\n\nIn [409]: df\nOut[409]: \n station_id       station_name  ... avg_saturday_rides  avg_sunday_holiday_rides\n0       40850            Library  ...              534.0                     417.2\n1       41700  Washington/Wabash  ...             1909.8                    1438.6\n2       40380         Clark/Lake  ...             1657.0                    1453.8\n\n[3 rows x 6 columns] \n```", "```py\nIn [1]: df = pd.read_xml(\n...         \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n...         iterparse = {\"page\": [\"title\", \"ns\", \"id\"]}\n...     )\n...     df\nOut[2]:\n title   ns        id\n0                                       Gettysburg Address    0     21450\n1                                                Main Page    0     42950\n2                            Declaration by United Nations    0      8435\n3             Constitution of the United States of America    0      8435\n4                     Declaration of Independence (Israel)    0     17858\n...                                                    ...  ...       ...\n3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649\n3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649\n3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649\n3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291\n3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450\n\n[3578765 rows x 3 columns] \n```", "```py\nIn [410]: geom_df = pd.DataFrame(\n .....:    {\n .....:        \"shape\": [\"square\", \"circle\", \"triangle\"],\n .....:        \"degrees\": [360, 360, 180],\n .....:        \"sides\": [4, np.nan, 3],\n .....:    }\n .....: )\n .....: \n\nIn [411]: print(geom_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </row>\n <row>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [412]: print(geom_df.to_xml(root_name=\"geometry\", row_name=\"objects\"))\n<?xml version='1.0' encoding='utf-8'?>\n<geometry>\n <objects>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </objects>\n <objects>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </objects>\n <objects>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </objects>\n</geometry> \n```", "```py\nIn [413]: print(geom_df.to_xml(attr_cols=geom_df.columns.tolist()))\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/>\n <row index=\"1\" shape=\"circle\" degrees=\"360\"/>\n <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/>\n</data> \n```", "```py\nIn [414]: print(\n .....:    geom_df.to_xml(\n .....:        index=False,\n .....:        attr_cols=['shape'],\n .....:        elem_cols=['degrees', 'sides'])\n .....: )\n .....: \n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row shape=\"square\">\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row shape=\"circle\">\n <degrees>360</degrees>\n <sides/>\n </row>\n <row shape=\"triangle\">\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [415]: ext_geom_df = pd.DataFrame(\n .....:    {\n .....:        \"type\": [\"polygon\", \"other\", \"polygon\"],\n .....:        \"shape\": [\"square\", \"circle\", \"triangle\"],\n .....:        \"degrees\": [360, 360, 180],\n .....:        \"sides\": [4, np.nan, 3],\n .....:    }\n .....: )\n .....: \n\nIn [416]: pvt_df = ext_geom_df.pivot_table(index='shape',\n .....:                                 columns='type',\n .....:                                 values=['degrees', 'sides'],\n .....:                                 aggfunc='sum')\n .....: \n\nIn [417]: pvt_df\nOut[417]: \n degrees         sides \ntype       other polygon other polygon\nshape \ncircle     360.0     NaN   0.0     NaN\nsquare       NaN   360.0   NaN     4.0\ntriangle     NaN   180.0   NaN     3.0\n\nIn [418]: print(pvt_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row>\n <shape>circle</shape>\n <degrees_other>360.0</degrees_other>\n <degrees_polygon/>\n <sides_other>0.0</sides_other>\n <sides_polygon/>\n </row>\n <row>\n <shape>square</shape>\n <degrees_other/>\n <degrees_polygon>360.0</degrees_polygon>\n <sides_other/>\n <sides_polygon>4.0</sides_polygon>\n </row>\n <row>\n <shape>triangle</shape>\n <degrees_other/>\n <degrees_polygon>180.0</degrees_polygon>\n <sides_other/>\n <sides_polygon>3.0</sides_polygon>\n </row>\n</data> \n```", "```py\nIn [419]: print(geom_df.to_xml(namespaces={\"\": \"https://example.com\"}))\n<?xml version='1.0' encoding='utf-8'?>\n<data >\n <row>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </row>\n <row>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [420]: print(\n .....:    geom_df.to_xml(namespaces={\"doc\": \"https://example.com\"},\n .....:                   prefix=\"doc\")\n .....: )\n .....: \n<?xml version='1.0' encoding='utf-8'?>\n<doc:data >\n <doc:row>\n <doc:index>0</doc:index>\n <doc:shape>square</doc:shape>\n <doc:degrees>360</doc:degrees>\n <doc:sides>4.0</doc:sides>\n </doc:row>\n <doc:row>\n <doc:index>1</doc:index>\n <doc:shape>circle</doc:shape>\n <doc:degrees>360</doc:degrees>\n <doc:sides/>\n </doc:row>\n <doc:row>\n <doc:index>2</doc:index>\n <doc:shape>triangle</doc:shape>\n <doc:degrees>180</doc:degrees>\n <doc:sides>3.0</doc:sides>\n </doc:row>\n</doc:data> \n```", "```py\nIn [421]: print(\n .....:    geom_df.to_xml(xml_declaration=False,\n .....:                   pretty_print=False)\n .....: )\n .....: \n<data><row><index>0</index><shape>square</shape><degrees>360</degrees><sides>4.0</sides></row><row><index>1</index><shape>circle</shape><degrees>360</degrees><sides/></row><row><index>2</index><shape>triangle</shape><degrees>180</degrees><sides>3.0</sides></row></data> \n```", "```py\nIn [422]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" >\n .....:   <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n .....:   <xsl:strip-space elements=\"*\"/>\n .....:   <xsl:template match=\"/data\">\n .....:     <geometry>\n .....:       <xsl:apply-templates select=\"row\"/>\n .....:     </geometry>\n .....:   </xsl:template>\n .....:   <xsl:template match=\"row\">\n .....:     <object index=\"{index}\">\n .....:       <xsl:if test=\"shape!='circle'\">\n .....:           <xsl:attribute name=\"type\">polygon</xsl:attribute>\n .....:       </xsl:if>\n .....:       <xsl:copy-of select=\"shape\"/>\n .....:       <property>\n .....:           <xsl:copy-of select=\"degrees|sides\"/>\n .....:       </property>\n .....:     </object>\n .....:   </xsl:template>\n .....: </xsl:stylesheet>\"\"\"\n .....: \n\nIn [423]: print(geom_df.to_xml(stylesheet=xsl))\n<?xml version=\"1.0\"?>\n<geometry>\n <object index=\"0\" type=\"polygon\">\n <shape>square</shape>\n <property>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </property>\n </object>\n <object index=\"1\">\n <shape>circle</shape>\n <property>\n <degrees>360</degrees>\n <sides/>\n </property>\n </object>\n <object index=\"2\" type=\"polygon\">\n <shape>triangle</shape>\n <property>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </property>\n </object>\n</geometry> \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", sheet_name=\"Sheet1\") \n```", "```py\nxlsx = pd.ExcelFile(\"path_to_file.xls\")\ndf = pd.read_excel(xlsx, \"Sheet1\") \n```", "```py\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\") \n```", "```py\ndata = {}\n# For when Sheet1's format differs from Sheet2\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=1) \n```", "```py\n# using the ExcelFile class\ndata = {}\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=None, na_values=[\"NA\"])\n\n# equivalent using the read_excel function\ndata = pd.read_excel(\n    \"path_to_file.xls\", [\"Sheet1\", \"Sheet2\"], index_col=None, na_values=[\"NA\"]\n) \n```", "```py\nimport xlrd\n\nxlrd_book = xlrd.open_workbook(\"path_to_file.xls\", on_demand=True)\nwith pd.ExcelFile(xlrd_book) as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", index_col=None, na_values=[\"NA\"]) \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", 0, index_col=None, na_values=[\"NA\"]) \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\") \n```", "```py\n# Returns a dictionary of DataFrames\npd.read_excel(\"path_to_file.xls\", sheet_name=None) \n```", "```py\n# Returns the 1st and 4th sheet, as a dictionary of DataFrames.\npd.read_excel(\"path_to_file.xls\", sheet_name=[\"Sheet1\", 3]) \n```", "```py\nIn [424]: df = pd.DataFrame(\n .....:    {\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]},\n .....:    index=pd.MultiIndex.from_product([[\"a\", \"b\"], [\"c\", \"d\"]]),\n .....: )\n .....: \n\nIn [425]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [426]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [427]: df\nOut[427]: \n a  b\na c  1  5\n d  2  6\nb c  3  7\n d  4  8 \n```", "```py\nIn [428]: df.index = df.index.set_names([\"lvl1\", \"lvl2\"])\n\nIn [429]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [430]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [431]: df\nOut[431]: \n a  b\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\nIn [432]: df.columns = pd.MultiIndex.from_product([[\"a\"], [\"b\", \"d\"]], names=[\"c1\", \"c2\"])\n\nIn [433]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [434]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1], header=[0, 1])\n\nIn [435]: df\nOut[435]: \nc1         a \nc2         b  d\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=\"A,C:E\") \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[0, 2, 3]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[\"foo\", \"bar\"]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=lambda x: x.isalpha()) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", parse_dates=[\"date_strings\"]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyBools\": bool}) \n```", "```py\ndef cfun(x):\n    return int(x) if x else -1\n\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyInts\": cfun}) \n```", "```py\npd.read_excel(\"path_to_file.xls\", dtype={\"MyInts\": \"int64\", \"MyText\": str}) \n```", "```py\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\") \n```", "```py\ndf.to_excel(\"path_to_file.xlsx\", index_label=\"label\", merge_cells=False) \n```", "```py\nwith pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Sheet1\")\n    df2.to_excel(writer, sheet_name=\"Sheet2\") \n```", "```py\nfrom io import BytesIO\n\nbio = BytesIO()\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(bio, engine=\"xlsxwriter\")\ndf.to_excel(writer, sheet_name=\"Sheet1\")\n\n# Save the workbook\nwriter.save()\n\n# Seek to the beginning and read to copy the workbook to a variable in memory\nbio.seek(0)\nworkbook = bio.read() \n```", "```py\n# By setting the 'engine' in the DataFrame 'to_excel()' methods.\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\", engine=\"xlsxwriter\")\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(\"path_to_file.xlsx\", engine=\"xlsxwriter\")\n\n# Or via pandas configuration.\nfrom pandas import options  # noqa: E402\n\noptions.io.excel.xlsx.writer = \"xlsxwriter\"\n\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.ods\", engine=\"odf\") \n```", "```py\n# Writes DataFrame to a .ods file\ndf.to_excel(\"path_to_file.ods\", engine=\"odf\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"pyxlsb\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"calamine\") \n```", "```py\n A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r \n```", "```py\n>>> clipdf = pd.read_clipboard()\n>>> clipdf\n A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r \n```", "```py\n>>> df = pd.DataFrame(\n...     {\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [\"p\", \"q\", \"r\"]}, index=[\"x\", \"y\", \"z\"]\n... )\n\n>>> df\n A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n>>> df.to_clipboard()\n>>> pd.read_clipboard()\n A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r \n```", "```py\nIn [436]: df\nOut[436]: \nc1         a \nc2         b  d\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8\n\nIn [437]: df.to_pickle(\"foo.pkl\") \n```", "```py\nIn [438]: pd.read_pickle(\"foo.pkl\")\nOut[438]: \nc1         a \nc2         b  d\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\nIn [439]: df = pd.DataFrame(\n .....:    {\n .....:        \"A\": np.random.randn(1000),\n .....:        \"B\": \"foo\",\n .....:        \"C\": pd.date_range(\"20130101\", periods=1000, freq=\"s\"),\n .....:    }\n .....: )\n .....: \n\nIn [440]: df\nOut[440]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [441]: df.to_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [442]: rt = pd.read_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [443]: rt\nOut[443]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [444]: df.to_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [445]: rt = pd.read_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [446]: rt\nOut[446]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [447]: df.to_pickle(\"data.pkl.gz\")\n\nIn [448]: rt = pd.read_pickle(\"data.pkl.gz\")\n\nIn [449]: rt\nOut[449]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]\n\nIn [450]: df[\"A\"].to_pickle(\"s1.pkl.bz2\")\n\nIn [451]: rt = pd.read_pickle(\"s1.pkl.bz2\")\n\nIn [452]: rt\nOut[452]: \n0     -0.317441\n1     -1.236269\n2      0.896171\n3     -0.487602\n4     -0.082240\n ... \n995   -0.171092\n996    1.786173\n997   -0.575189\n998    0.820750\n999   -1.256530\nName: A, Length: 1000, dtype: float64 \n```", "```py\nIn [453]: df.to_pickle(\"data.pkl.gz\", compression={\"method\": \"gzip\", \"compresslevel\": 1}) \n```", "```py\nIn [454]: store = pd.HDFStore(\"store.h5\")\n\nIn [455]: print(store)\n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\nIn [456]: index = pd.date_range(\"1/1/2000\", periods=8)\n\nIn [457]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [458]: df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\n# store.put('s', s) is an equivalent method\nIn [459]: store[\"s\"] = s\n\nIn [460]: store[\"df\"] = df\n\nIn [461]: store\nOut[461]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\n# store.get('df') is an equivalent method\nIn [462]: store[\"df\"]\nOut[462]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\n\n# dotted (attribute) access provides get as well\nIn [463]: store.df\nOut[463]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\n# store.remove('df') is an equivalent method\nIn [464]: del store[\"df\"]\n\nIn [465]: store\nOut[465]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\nIn [466]: store.close()\n\nIn [467]: store\nOut[467]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\nIn [468]: store.is_open\nOut[468]: False\n\n# Working with, and automatically closing the store using a context manager\nIn [469]: with pd.HDFStore(\"store.h5\") as store:\n .....:    store.keys()\n .....: \n```", "```py\nIn [470]: df_tl = pd.DataFrame({\"A\": list(range(5)), \"B\": list(range(5))})\n\nIn [471]: df_tl.to_hdf(\"store_tl.h5\", key=\"table\", append=True)\n\nIn [472]: pd.read_hdf(\"store_tl.h5\", \"table\", where=[\"index>2\"])\nOut[472]: \n A  B\n3  3  3\n4  4  4 \n```", "```py\nIn [473]: df_with_missing = pd.DataFrame(\n .....:    {\n .....:        \"col1\": [0, np.nan, 2],\n .....:        \"col2\": [1, np.nan, np.nan],\n .....:    }\n .....: )\n .....: \n\nIn [474]: df_with_missing\nOut[474]: \n col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [475]: df_with_missing.to_hdf(\"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\")\n\nIn [476]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[476]: \n col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [477]: df_with_missing.to_hdf(\n .....:    \"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\", dropna=True\n .....: )\n .....: \n\nIn [478]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[478]: \n col1  col2\n0   0.0   1.0\n2   2.0   NaN \n```", "```py\nIn [479]: pd.DataFrame(np.random.randn(10, 2)).to_hdf(\"test_fixed.h5\", key=\"df\")\n\nIn [480]: pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n---------------------------------------------------------------------------\nTypeError  Traceback (most recent call last)\nCell In[480], line 1\n----> 1 pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:452, in read_hdf(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\n  447                 raise ValueError(\n  448                     \"key must be provided when HDF5 \"\n  449                     \"file contains multiple datasets.\"\n  450                 )\n  451         key = candidate_only_group._v_pathname\n--> 452     return store.select(\n  453         key,\n  454         where=where,\n  455         start=start,\n  456         stop=stop,\n  457         columns=columns,\n  458         iterator=iterator,\n  459         chunksize=chunksize,\n  460         auto_close=auto_close,\n  461     )\n  462 except (ValueError, TypeError, LookupError):\n  463     if not isinstance(path_or_buf, HDFStore):\n  464         # if there is an error, close the store if we opened it.\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:906, in HDFStore.select(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\n  892 # create the iterator\n  893 it = TableIterator(\n  894     self,\n  895     s,\n   (...)\n  903     auto_close=auto_close,\n  904 )\n--> 906 return it.get_result()\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:2029, in TableIterator.get_result(self, coordinates)\n  2026     where = self.where\n  2028 # directly return the result\n-> 2029 results = self.func(self.start, self.stop, where)\n  2030 self.close()\n  2031 return results\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:890, in HDFStore.select.<locals>.func(_start, _stop, _where)\n  889 def func(_start, _stop, _where):\n--> 890     return s.read(start=_start, stop=_stop, where=_where, columns=columns)\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:3278, in BlockManagerFixed.read(self, where, columns, start, stop)\n  3270 def read(\n  3271     self,\n  3272     where=None,\n   (...)\n  3276 ) -> DataFrame:\n  3277     # start, stop applied to rows, so 0th axis only\n-> 3278     self.validate_read(columns, where)\n  3279     select_axis = self.obj_type()._get_block_manager_axis(0)\n  3281     axes = []\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:2922, in GenericFixed.validate_read(self, columns, where)\n  2917     raise TypeError(\n  2918         \"cannot pass a column specification when reading \"\n  2919         \"a Fixed format store. this store must be selected in its entirety\"\n  2920     )\n  2921 if where is not None:\n-> 2922     raise TypeError(\n  2923         \"cannot pass a where specification when reading \"\n  2924         \"from a Fixed format store. this store must be selected in its entirety\"\n  2925     )\n\nTypeError: cannot pass a where specification when reading from a Fixed format store. this store must be selected in its entirety \n```", "```py\nIn [481]: store = pd.HDFStore(\"store.h5\")\n\nIn [482]: df1 = df[0:4]\n\nIn [483]: df2 = df[4:]\n\n# append data (creates a table automatically)\nIn [484]: store.append(\"df\", df1)\n\nIn [485]: store.append(\"df\", df2)\n\nIn [486]: store\nOut[486]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# select the entire object\nIn [487]: store.select(\"df\")\nOut[487]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\n\n# the type of stored data\nIn [488]: store.root.df._v_attrs.pandas_type\nOut[488]: 'frame_table' \n```", "```py\nIn [489]: store.put(\"foo/bar/bah\", df)\n\nIn [490]: store.append(\"food/orange\", df)\n\nIn [491]: store.append(\"food/apple\", df)\n\nIn [492]: store\nOut[492]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# a list of keys are returned\nIn [493]: store.keys()\nOut[493]: ['/df', '/food/apple', '/food/orange', '/foo/bar/bah']\n\n# remove all nodes under this level\nIn [494]: store.remove(\"food\")\n\nIn [495]: store\nOut[495]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\nIn [496]: for (path, subgroups, subkeys) in store.walk():\n .....:    for subgroup in subgroups:\n .....:        print(\"GROUP: {}/{}\".format(path, subgroup))\n .....:    for subkey in subkeys:\n .....:        key = \"/\".join([path, subkey])\n .....:        print(\"KEY: {}\".format(key))\n .....:        print(store.get(key))\n .....: \nGROUP: /foo\nKEY: /df\n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\nGROUP: /foo/bar\nKEY: /foo/bar/bah\n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nIn [497]: store.foo.bar.bah\n---------------------------------------------------------------------------\nTypeError  Traceback (most recent call last)\nCell In[497], line 1\n----> 1 store.foo.bar.bah\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:613, in HDFStore.__getattr__(self, name)\n  611  \"\"\"allow attribute access to get stores\"\"\"\n  612 try:\n--> 613     return self.get(name)\n  614 except (KeyError, ClosedFileError):\n  615     pass\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:813, in HDFStore.get(self, key)\n  811 if group is None:\n  812     raise KeyError(f\"No object named {key} in the file\")\n--> 813 return self._read_group(group)\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:1878, in HDFStore._read_group(self, group)\n  1877 def _read_group(self, group: Node):\n-> 1878     s = self._create_storer(group)\n  1879     s.infer_axes()\n  1880     return s.read()\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:1752, in HDFStore._create_storer(self, group, format, value, encoding, errors)\n  1750         tt = \"generic_table\"\n  1751     else:\n-> 1752         raise TypeError(\n  1753             \"cannot create a storer if the object is not existing \"\n  1754             \"nor a value are passed\"\n  1755         )\n  1756 else:\n  1757     if isinstance(value, Series):\n\nTypeError: cannot create a storer if the object is not existing nor a value are passed \n```", "```py\n# you can directly access the actual PyTables node but using the root node\nIn [498]: store.root.foo.bar.bah\nOut[498]: \n/foo/bar/bah (Group) ''\n children := ['axis0' (Array), 'axis1' (Array), 'block0_items' (Array), 'block0_values' (Array)] \n```", "```py\nIn [499]: store[\"foo/bar/bah\"]\nOut[499]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nIn [500]: df_mixed = pd.DataFrame(\n .....:    {\n .....:        \"A\": np.random.randn(8),\n .....:        \"B\": np.random.randn(8),\n .....:        \"C\": np.array(np.random.randn(8), dtype=\"float32\"),\n .....:        \"string\": \"string\",\n .....:        \"int\": 1,\n .....:        \"bool\": True,\n .....:        \"datetime64\": pd.Timestamp(\"20010102\"),\n .....:    },\n .....:    index=list(range(8)),\n .....: )\n .....: \n\nIn [501]: df_mixed.loc[df_mixed.index[3:5], [\"A\", \"B\", \"string\", \"datetime64\"]] = np.nan\n\nIn [502]: store.append(\"df_mixed\", df_mixed, min_itemsize={\"values\": 50})\n\nIn [503]: df_mixed1 = store.select(\"df_mixed\")\n\nIn [504]: df_mixed1\nOut[504]: \n A         B         C  ... int  bool                    datetime64\n0  0.013747 -1.166078 -1.292080  ...   1  True 1970-01-01 00:00:00.978393600\n1 -0.712009  0.247572  1.526911  ...   1  True 1970-01-01 00:00:00.978393600\n2 -0.645096  1.687406  0.288504  ...   1  True 1970-01-01 00:00:00.978393600\n3       NaN       NaN  0.097771  ...   1  True                           NaT\n4       NaN       NaN  1.536408  ...   1  True                           NaT\n5 -0.023202  0.043702  0.926790  ...   1  True 1970-01-01 00:00:00.978393600\n6  2.359782  0.088224 -0.676448  ...   1  True 1970-01-01 00:00:00.978393600\n7 -0.143428 -0.813360 -0.179724  ...   1  True 1970-01-01 00:00:00.978393600\n\n[8 rows x 7 columns]\n\nIn [505]: df_mixed1.dtypes.value_counts()\nOut[505]: \nfloat64           2\nfloat32           1\nobject            1\nint64             1\nbool              1\ndatetime64[ns]    1\nName: count, dtype: int64\n\n# we have provided a minimum string column size\nIn [506]: store.root.df_mixed.table\nOut[506]: \n/df_mixed/table (Table(8,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1),\n \"values_block_1\": Float32Col(shape=(1,), dflt=0.0, pos=2),\n \"values_block_2\": StringCol(itemsize=50, shape=(1,), dflt=b'', pos=3),\n \"values_block_3\": Int64Col(shape=(1,), dflt=0, pos=4),\n \"values_block_4\": BoolCol(shape=(1,), dflt=False, pos=5),\n \"values_block_5\": Int64Col(shape=(1,), dflt=0, pos=6)}\n byteorder := 'little'\n chunkshape := (689,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [507]: index = pd.MultiIndex(\n .....:   levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n .....:   codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n .....:   names=[\"foo\", \"bar\"],\n .....: )\n .....: \n\nIn [508]: df_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [509]: df_mi\nOut[509]: \n A         B         C\nfoo bar \nfoo one   -1.303456 -0.642994 -0.649456\n two    1.012694  0.414147  1.950460\n three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n two   -1.660522  0.929553 -1.298649\n three  3.565769  0.682402  1.041927\n\nIn [510]: store.append(\"df_mi\", df_mi)\n\nIn [511]: store.select(\"df_mi\")\nOut[511]: \n A         B         C\nfoo bar \nfoo one   -1.303456 -0.642994 -0.649456\n two    1.012694  0.414147  1.950460\n three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n two   -1.660522  0.929553 -1.298649\n three  3.565769  0.682402  1.041927\n\n# the levels are automatically included as data columns\nIn [512]: store.select(\"df_mi\", \"foo=bar\")\nOut[512]: \n A         B         C\nfoo bar \nbar one  0.410395  0.618321  0.560398\n two  1.434027 -0.033270  0.343197 \n```", "```py\nstring = \"HolyMoly'\"\nstore.select(\"df\", \"index == string\") \n```", "```py\nstring = \"HolyMoly'\"\nstore.select('df', f'index == {string}') \n```", "```py\nstore.select(\"df\", \"index == %r\" % string) \n```", "```py\nIn [513]: dfq = pd.DataFrame(\n .....:    np.random.randn(10, 4),\n .....:    columns=list(\"ABCD\"),\n .....:    index=pd.date_range(\"20130101\", periods=10),\n .....: )\n .....: \n\nIn [514]: store.append(\"dfq\", dfq, format=\"table\", data_columns=True) \n```", "```py\nIn [515]: store.select(\"dfq\", \"index>pd.Timestamp('20130104') & columns=['A', 'B']\")\nOut[515]: \n A         B\n2013-01-05 -0.830545 -0.457071\n2013-01-06  0.431186  1.049421\n2013-01-07  0.617509 -0.811230\n2013-01-08  0.947422 -0.671233\n2013-01-09 -0.183798 -1.211230\n2013-01-10  0.361428  0.887304 \n```", "```py\nIn [516]: store.select(\"dfq\", where=\"A>0 or C>0\")\nOut[516]: \n A         B         C         D\n2013-01-02  0.658179  0.362814 -0.917897  0.010165\n2013-01-03  0.905122  1.848731 -1.184241  0.932053\n2013-01-05 -0.830545 -0.457071  1.565581  1.148032\n2013-01-06  0.431186  1.049421  0.383309  0.595013\n2013-01-07  0.617509 -0.811230 -2.088563 -1.393500\n2013-01-08  0.947422 -0.671233 -0.847097 -1.187785\n2013-01-10  0.361428  0.887304  0.266457 -0.399641 \n```", "```py\nIn [517]: store.select(\"df\", \"columns=['A', 'B']\")\nOut[517]: \n A         B\n2000-01-01  0.858644 -0.851236\n2000-01-02 -0.080372 -1.268121\n2000-01-03  0.816983  1.965656\n2000-01-04  0.712795 -0.062433\n2000-01-05 -0.298721 -1.988045\n2000-01-06  1.103675  1.382242\n2000-01-07 -0.729161 -0.142928\n2000-01-08 -1.005977  0.465222 \n```", "```py\nIn [518]: from datetime import timedelta\n\nIn [519]: dftd = pd.DataFrame(\n .....:    {\n .....:        \"A\": pd.Timestamp(\"20130101\"),\n .....:        \"B\": [\n .....:            pd.Timestamp(\"20130101\") + timedelta(days=i, seconds=10)\n .....:            for i in range(10)\n .....:        ],\n .....:    }\n .....: )\n .....: \n\nIn [520]: dftd[\"C\"] = dftd[\"A\"] - dftd[\"B\"]\n\nIn [521]: dftd\nOut[521]: \n A                   B                  C\n0 2013-01-01 2013-01-01 00:00:10  -1 days +23:59:50\n1 2013-01-01 2013-01-02 00:00:10  -2 days +23:59:50\n2 2013-01-01 2013-01-03 00:00:10  -3 days +23:59:50\n3 2013-01-01 2013-01-04 00:00:10  -4 days +23:59:50\n4 2013-01-01 2013-01-05 00:00:10  -5 days +23:59:50\n5 2013-01-01 2013-01-06 00:00:10  -6 days +23:59:50\n6 2013-01-01 2013-01-07 00:00:10  -7 days +23:59:50\n7 2013-01-01 2013-01-08 00:00:10  -8 days +23:59:50\n8 2013-01-01 2013-01-09 00:00:10  -9 days +23:59:50\n9 2013-01-01 2013-01-10 00:00:10 -10 days +23:59:50\n\nIn [522]: store.append(\"dftd\", dftd, data_columns=True)\n\nIn [523]: store.select(\"dftd\", \"C<'-3.5D'\")\nOut[523]: \n A                   B                  C\n4 1970-01-01 00:00:01.356998400 2013-01-05 00:00:10  -5 days +23:59:50\n5 1970-01-01 00:00:01.356998400 2013-01-06 00:00:10  -6 days +23:59:50\n6 1970-01-01 00:00:01.356998400 2013-01-07 00:00:10  -7 days +23:59:50\n7 1970-01-01 00:00:01.356998400 2013-01-08 00:00:10  -8 days +23:59:50\n8 1970-01-01 00:00:01.356998400 2013-01-09 00:00:10  -9 days +23:59:50\n9 1970-01-01 00:00:01.356998400 2013-01-10 00:00:10 -10 days +23:59:50 \n```", "```py\nIn [524]: df_mi.index.names\nOut[524]: FrozenList(['foo', 'bar'])\n\nIn [525]: store.select(\"df_mi\", \"foo=baz and bar=two\")\nOut[525]: \n A         B         C\nfoo bar \nbaz two -1.646063 -0.695847 -0.429156 \n```", "```py\nIn [526]: index = pd.MultiIndex(\n .....:    levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n .....:    codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n .....: )\n .....: \n\nIn [527]: df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [528]: df_mi_2\nOut[528]: \n A         B         C\nfoo one   -0.219582  1.186860 -1.437189\n two    0.053768  1.872644 -1.469813\n three -0.564201  0.876341  0.407749\nbar one   -0.232583  0.179812  0.922152\n two   -1.820952 -0.641360  2.133239\nbaz two   -0.941248 -0.136307 -1.271305\n three -0.099774 -0.061438 -0.845172\nqux one    0.465793  0.756995 -0.541690\n two   -0.802241  0.877657 -2.553831\n three  0.094899 -2.319519  0.293601\n\nIn [529]: store.append(\"df_mi_2\", df_mi_2)\n\n# the levels are automatically included as data columns with keyword level_n\nIn [530]: store.select(\"df_mi_2\", \"level_0=foo and level_1=two\")\nOut[530]: \n A         B         C\nfoo two  0.053768  1.872644 -1.469813 \n```", "```py\n# we have automagically already created an index (in the first section)\nIn [531]: i = store.root.df.table.cols.index.index\n\nIn [532]: i.optlevel, i.kind\nOut[532]: (6, 'medium')\n\n# change an index by passing new parameters\nIn [533]: store.create_table_index(\"df\", optlevel=9, kind=\"full\")\n\nIn [534]: i = store.root.df.table.cols.index.index\n\nIn [535]: i.optlevel, i.kind\nOut[535]: (9, 'full') \n```", "```py\nIn [536]: df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [537]: df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [538]: st = pd.HDFStore(\"appends.h5\", mode=\"w\")\n\nIn [539]: st.append(\"df\", df_1, data_columns=[\"B\"], index=False)\n\nIn [540]: st.append(\"df\", df_2, data_columns=[\"B\"], index=False)\n\nIn [541]: st.get_storer(\"df\").table\nOut[541]: \n/df/table (Table(20,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n byteorder := 'little'\n chunkshape := (2730,) \n```", "```py\nIn [542]: st.create_table_index(\"df\", columns=[\"B\"], optlevel=9, kind=\"full\")\n\nIn [543]: st.get_storer(\"df\").table\nOut[543]: \n/df/table (Table(20,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n byteorder := 'little'\n chunkshape := (2730,)\n autoindex := True\n colindexes := {\n \"B\": Index(9, fullshuffle, zlib(1)).is_csi=True}\n\nIn [544]: st.close() \n```", "```py\nIn [545]: df_dc = df.copy()\n\nIn [546]: df_dc[\"string\"] = \"foo\"\n\nIn [547]: df_dc.loc[df_dc.index[4:6], \"string\"] = np.nan\n\nIn [548]: df_dc.loc[df_dc.index[7:9], \"string\"] = \"bar\"\n\nIn [549]: df_dc[\"string2\"] = \"cool\"\n\nIn [550]: df_dc.loc[df_dc.index[1:3], [\"B\", \"C\"]] = 1.0\n\nIn [551]: df_dc\nOut[551]: \n A         B         C string string2\n2000-01-01  0.858644 -0.851236  1.058006    foo    cool\n2000-01-02 -0.080372  1.000000  1.000000    foo    cool\n2000-01-03  0.816983  1.000000  1.000000    foo    cool\n2000-01-04  0.712795 -0.062433  0.736755    foo    cool\n2000-01-05 -0.298721 -1.988045  1.475308    NaN    cool\n2000-01-06  1.103675  1.382242 -0.650762    NaN    cool\n2000-01-07 -0.729161 -0.142928 -1.063038    foo    cool\n2000-01-08 -1.005977  0.465222 -0.094517    bar    cool\n\n# on-disk operations\nIn [552]: store.append(\"df_dc\", df_dc, data_columns=[\"B\", \"C\", \"string\", \"string2\"])\n\nIn [553]: store.select(\"df_dc\", where=\"B > 0\")\nOut[553]: \n A         B         C string string2\n2000-01-02 -0.080372  1.000000  1.000000    foo    cool\n2000-01-03  0.816983  1.000000  1.000000    foo    cool\n2000-01-06  1.103675  1.382242 -0.650762    NaN    cool\n2000-01-08 -1.005977  0.465222 -0.094517    bar    cool\n\n# getting creative\nIn [554]: store.select(\"df_dc\", \"B > 0 & C > 0 & string == foo\")\nOut[554]: \n A    B    C string string2\n2000-01-02 -0.080372  1.0  1.0    foo    cool\n2000-01-03  0.816983  1.0  1.0    foo    cool\n\n# this is in-memory version of this type of selection\nIn [555]: df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == \"foo\")]\nOut[555]: \n A    B    C string string2\n2000-01-02 -0.080372  1.0  1.0    foo    cool\n2000-01-03  0.816983  1.0  1.0    foo    cool\n\n# we have automagically created this index and the B/C/string/string2\n# columns are stored separately as ``PyTables`` columns\nIn [556]: store.root.df_dc.table\nOut[556]: \n/df_dc/table (Table(8,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2),\n \"C\": Float64Col(shape=(), dflt=0.0, pos=3),\n \"string\": StringCol(itemsize=3, shape=(), dflt=b'', pos=4),\n \"string2\": StringCol(itemsize=4, shape=(), dflt=b'', pos=5)}\n byteorder := 'little'\n chunkshape := (1680,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"B\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"C\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"string\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"string2\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [557]: for df in store.select(\"df\", chunksize=3):\n .....:    print(df)\n .....: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n A         B         C\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n A         B         C\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nfor df in pd.read_hdf(\"store.h5\", \"df\", chunksize=3):\n    print(df) \n```", "```py\nIn [558]: dfeq = pd.DataFrame({\"number\": np.arange(1, 11)})\n\nIn [559]: dfeq\nOut[559]: \n number\n0       1\n1       2\n2       3\n3       4\n4       5\n5       6\n6       7\n7       8\n8       9\n9      10\n\nIn [560]: store.append(\"dfeq\", dfeq, data_columns=[\"number\"])\n\nIn [561]: def chunks(l, n):\n .....:    return [l[i: i + n] for i in range(0, len(l), n)]\n .....: \n\nIn [562]: evens = [2, 4, 6, 8, 10]\n\nIn [563]: coordinates = store.select_as_coordinates(\"dfeq\", \"number=evens\")\n\nIn [564]: for c in chunks(coordinates, 2):\n .....:    print(store.select(\"dfeq\", where=c))\n .....: \n number\n1       2\n3       4\n number\n5       6\n7       8\n number\n9      10 \n```", "```py\nIn [565]: store.select_column(\"df_dc\", \"index\")\nOut[565]: \n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\n3   2000-01-04\n4   2000-01-05\n5   2000-01-06\n6   2000-01-07\n7   2000-01-08\nName: index, dtype: datetime64[ns]\n\nIn [566]: store.select_column(\"df_dc\", \"string\")\nOut[566]: \n0    foo\n1    foo\n2    foo\n3    foo\n4    NaN\n5    NaN\n6    foo\n7    bar\nName: string, dtype: object \n```", "```py\nIn [567]: df_coord = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [568]: store.append(\"df_coord\", df_coord)\n\nIn [569]: c = store.select_as_coordinates(\"df_coord\", \"index > 20020101\")\n\nIn [570]: c\nOut[570]: \nIndex([732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n ...\n 990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n dtype='int64', length=268)\n\nIn [571]: store.select(\"df_coord\", where=c)\nOut[571]: \n 0         1\n2002-01-02  0.007717  1.168386\n2002-01-03  0.759328 -0.638934\n2002-01-04 -1.154018 -0.324071\n2002-01-05 -0.804551 -1.280593\n2002-01-06 -0.047208  1.260503\n...              ...       ...\n2002-09-22 -1.139583  0.344316\n2002-09-23 -0.760643 -1.306704\n2002-09-24  0.059018  1.775482\n2002-09-25  1.242255 -0.055457\n2002-09-26  0.410317  2.194489\n\n[268 rows x 2 columns] \n```", "```py\nIn [572]: df_mask = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [573]: store.append(\"df_mask\", df_mask)\n\nIn [574]: c = store.select_column(\"df_mask\", \"index\")\n\nIn [575]: where = c[pd.DatetimeIndex(c).month == 5].index\n\nIn [576]: store.select(\"df_mask\", where=where)\nOut[576]: \n 0         1\n2000-05-01  1.479511  0.516433\n2000-05-02 -0.334984 -1.493537\n2000-05-03  0.900321  0.049695\n2000-05-04  0.614266 -1.077151\n2000-05-05  0.233881  0.493246\n...              ...       ...\n2002-05-27  0.294122  0.457407\n2002-05-28 -1.102535  1.215650\n2002-05-29 -0.432911  0.753606\n2002-05-30 -1.105212  2.311877\n2002-05-31  2.567296  2.610691\n\n[93 rows x 2 columns] \n```", "```py\nIn [577]: store.get_storer(\"df_dc\").nrows\nOut[577]: 8 \n```", "```py\nIn [578]: df_mt = pd.DataFrame(\n .....:    np.random.randn(8, 6),\n .....:    index=pd.date_range(\"1/1/2000\", periods=8),\n .....:    columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n .....: )\n .....: \n\nIn [579]: df_mt[\"foo\"] = \"bar\"\n\nIn [580]: df_mt.loc[df_mt.index[1], (\"A\", \"B\")] = np.nan\n\n# you can also create the tables individually\nIn [581]: store.append_to_multiple(\n .....:    {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\"\n .....: )\n .....: \n\nIn [582]: store\nOut[582]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# individual tables were created\nIn [583]: store.select(\"df1_mt\")\nOut[583]: \n A         B\n2000-01-01  0.162291 -0.430489\n2000-01-02       NaN       NaN\n2000-01-03  0.429207 -1.099274\n2000-01-04  1.869081 -1.466039\n2000-01-05  0.092130 -1.726280\n2000-01-06  0.266901 -0.036854\n2000-01-07 -0.517871 -0.990317\n2000-01-08 -0.231342  0.557402\n\nIn [584]: store.select(\"df2_mt\")\nOut[584]: \n C         D         E         F  foo\n2000-01-01 -2.502042  0.668149  0.460708  1.834518  bar\n2000-01-02  0.130441 -0.608465  0.439872  0.506364  bar\n2000-01-03 -1.069546  1.236277  0.116634 -1.772519  bar\n2000-01-04  0.137462  0.313939  0.748471 -0.943009  bar\n2000-01-05  0.836517  2.049798  0.562167  0.189952  bar\n2000-01-06  1.112750 -0.151596  1.503311  0.939470  bar\n2000-01-07 -0.294348  0.335844 -0.794159  1.495614  bar\n2000-01-08  0.860312 -0.538674 -0.541986 -1.759606  bar\n\n# as a multiple\nIn [585]: store.select_as_multiple(\n .....:    [\"df1_mt\", \"df2_mt\"],\n .....:    where=[\"A>0\", \"B>0\"],\n .....:    selector=\"df1_mt\",\n .....: )\n .....: \nOut[585]: \nEmpty DataFrame\nColumns: [A, B, C, D, E, F, foo]\nIndex: [] \n```", "```py\nstore_compressed = pd.HDFStore(\n    \"store_compressed.h5\", complevel=9, complib=\"blosc:blosclz\"\n) \n```", "```py\nstore.append(\"df\", df, complib=\"zlib\", complevel=5) \n```", "```py\nptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5 \n```", "```py\nIn [586]: dfcat = pd.DataFrame(\n .....:    {\"A\": pd.Series(list(\"aabbcdba\")).astype(\"category\"), \"B\": np.random.randn(8)}\n .....: )\n .....: \n\nIn [587]: dfcat\nOut[587]: \n A         B\n0  a -1.520478\n1  a -1.069391\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n5  d  0.301911\n6  b -0.640843\n7  a -2.253022\n\nIn [588]: dfcat.dtypes\nOut[588]: \nA    category\nB     float64\ndtype: object\n\nIn [589]: cstore = pd.HDFStore(\"cats.h5\", mode=\"w\")\n\nIn [590]: cstore.append(\"dfcat\", dfcat, format=\"table\", data_columns=[\"A\"])\n\nIn [591]: result = cstore.select(\"dfcat\", where=\"A in ['b', 'c']\")\n\nIn [592]: result\nOut[592]: \n A         B\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n6  b -0.640843\n\nIn [593]: result.dtypes\nOut[593]: \nA    category\nB     float64\ndtype: object \n```", "```py\nIn [594]: dfs = pd.DataFrame({\"A\": \"foo\", \"B\": \"bar\"}, index=list(range(5)))\n\nIn [595]: dfs\nOut[595]: \n A    B\n0  foo  bar\n1  foo  bar\n2  foo  bar\n3  foo  bar\n4  foo  bar\n\n# A and B have a size of 30\nIn [596]: store.append(\"dfs\", dfs, min_itemsize=30)\n\nIn [597]: store.get_storer(\"dfs\").table\nOut[597]: \n/dfs/table (Table(5,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": StringCol(itemsize=30, shape=(2,), dflt=b'', pos=1)}\n byteorder := 'little'\n chunkshape := (963,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False}\n\n# A is created as a data_column with a size of 30\n# B is size is calculated\nIn [598]: store.append(\"dfs2\", dfs, min_itemsize={\"A\": 30})\n\nIn [599]: store.get_storer(\"dfs2\").table\nOut[599]: \n/dfs2/table (Table(5,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": StringCol(itemsize=3, shape=(1,), dflt=b'', pos=1),\n \"A\": StringCol(itemsize=30, shape=(), dflt=b'', pos=2)}\n byteorder := 'little'\n chunkshape := (1598,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"A\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [600]: dfss = pd.DataFrame({\"A\": [\"foo\", \"bar\", \"nan\"]})\n\nIn [601]: dfss\nOut[601]: \n A\n0  foo\n1  bar\n2  nan\n\nIn [602]: store.append(\"dfss\", dfss)\n\nIn [603]: store.select(\"dfss\")\nOut[603]: \n A\n0  foo\n1  bar\n2  NaN\n\n# here you need to specify a different nan rep\nIn [604]: store.append(\"dfss2\", dfss, nan_rep=\"_nan_\")\n\nIn [605]: store.select(\"dfss2\")\nOut[605]: \n A\n0  foo\n1  bar\n2  nan \n```", "```py\nIn [606]: df = pd.DataFrame(\n .....:    {\n .....:        \"a\": list(\"abc\"),\n .....:        \"b\": list(range(1, 4)),\n .....:        \"c\": np.arange(3, 6).astype(\"u1\"),\n .....:        \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n .....:        \"e\": [True, False, True],\n .....:        \"f\": pd.Categorical(list(\"abc\")),\n .....:        \"g\": pd.date_range(\"20130101\", periods=3),\n .....:        \"h\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n .....:        \"i\": pd.date_range(\"20130101\", periods=3, freq=\"ns\"),\n .....:    }\n .....: )\n .....: \n\nIn [607]: df\nOut[607]: \n a  b  c  ...          g                         h                             i\n0  a  1  3  ... 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000\n1  b  2  4  ... 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001\n2  c  3  5  ... 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002\n\n[3 rows x 9 columns]\n\nIn [608]: df.dtypes\nOut[608]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                      category\ng                datetime64[ns]\nh    datetime64[ns, US/Eastern]\ni                datetime64[ns]\ndtype: object \n```", "```py\nIn [609]: df.to_feather(\"example.feather\") \n```", "```py\nIn [610]: result = pd.read_feather(\"example.feather\")\n\nIn [611]: result\nOut[611]: \n a  b  c  ...          g                         h                             i\n0  a  1  3  ... 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000\n1  b  2  4  ... 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001\n2  c  3  5  ... 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002\n\n[3 rows x 9 columns]\n\n# we preserve dtypes\nIn [612]: result.dtypes\nOut[612]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                      category\ng                datetime64[ns]\nh    datetime64[ns, US/Eastern]\ni                datetime64[ns]\ndtype: object \n```", "```py\nIn [613]: df = pd.DataFrame(\n .....:    {\n .....:        \"a\": list(\"abc\"),\n .....:        \"b\": list(range(1, 4)),\n .....:        \"c\": np.arange(3, 6).astype(\"u1\"),\n .....:        \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n .....:        \"e\": [True, False, True],\n .....:        \"f\": pd.date_range(\"20130101\", periods=3),\n .....:        \"g\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n .....:        \"h\": pd.Categorical(list(\"abc\")),\n .....:        \"i\": pd.Categorical(list(\"abc\"), ordered=True),\n .....:    }\n .....: )\n .....: \n\nIn [614]: df\nOut[614]: \n a  b  c    d      e          f                         g  h  i\n0  a  1  3  4.0   True 2013-01-01 2013-01-01 00:00:00-05:00  a  a\n1  b  2  4  5.0  False 2013-01-02 2013-01-02 00:00:00-05:00  b  b\n2  c  3  5  6.0   True 2013-01-03 2013-01-03 00:00:00-05:00  c  c\n\nIn [615]: df.dtypes\nOut[615]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                datetime64[ns]\ng    datetime64[ns, US/Eastern]\nh                      category\ni                      category\ndtype: object \n```", "```py\nIn [616]: df.to_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\n\nIn [617]: df.to_parquet(\"example_fp.parquet\", engine=\"fastparquet\") \n```", "```py\nIn [618]: result = pd.read_parquet(\"example_fp.parquet\", engine=\"fastparquet\")\n\nIn [619]: result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\n\nIn [620]: result.dtypes\nOut[620]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                datetime64[ns]\ng    datetime64[ns, US/Eastern]\nh                      category\ni                      category\ndtype: object \n```", "```py\nIn [621]: result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n\nIn [622]: result.dtypes\nOut[622]: \na                                      string[pyarrow]\nb                                       int64[pyarrow]\nc                                       uint8[pyarrow]\nd                                      double[pyarrow]\ne                                        bool[pyarrow]\nf                               timestamp[ns][pyarrow]\ng                timestamp[ns, tz=US/Eastern][pyarrow]\nh    dictionary<values=string, indices=int32, order...\ni    dictionary<values=string, indices=int32, order...\ndtype: object \n```", "```py\nIn [623]: result = pd.read_parquet(\n .....:    \"example_fp.parquet\",\n .....:    engine=\"fastparquet\",\n .....:    columns=[\"a\", \"b\"],\n .....: )\n .....: \n\nIn [624]: result = pd.read_parquet(\n .....:    \"example_pa.parquet\",\n .....:    engine=\"pyarrow\",\n .....:    columns=[\"a\", \"b\"],\n .....: )\n .....: \n\nIn [625]: result.dtypes\nOut[625]: \na    object\nb     int64\ndtype: object \n```", "```py\nIn [626]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n\nIn [627]: df.to_parquet(\"test.parquet\", engine=\"pyarrow\") \n```", "```py\nIn [628]: df.to_parquet(\"test.parquet\", index=False) \n```", "```py\nIn [629]: df = pd.DataFrame({\"a\": [0, 0, 1, 1], \"b\": [0, 1, 0, 1]})\n\nIn [630]: df.to_parquet(path=\"test\", engine=\"pyarrow\", partition_cols=[\"a\"], compression=None) \n```", "```py\ntest\n\u251c\u2500\u2500 a=0\n\u2502   \u251c\u2500\u2500 0bac803e32dc42ae83fddfd029cbdebc.parquet\n\u2502   \u2514\u2500\u2500  ...\n\u2514\u2500\u2500 a=1\n    \u251c\u2500\u2500 e6ab24a4f45147b49b54a662f0c412a3.parquet\n    \u2514\u2500\u2500 ... \n```", "```py\nIn [631]: df = pd.DataFrame(\n .....:    {\n .....:        \"a\": list(\"abc\"),\n .....:        \"b\": list(range(1, 4)),\n .....:        \"c\": np.arange(4.0, 7.0, dtype=\"float64\"),\n .....:        \"d\": [True, False, True],\n .....:        \"e\": pd.date_range(\"20130101\", periods=3),\n .....:    }\n .....: )\n .....: \n\nIn [632]: df\nOut[632]: \n a  b    c      d          e\n0  a  1  4.0   True 2013-01-01\n1  b  2  5.0  False 2013-01-02\n2  c  3  6.0   True 2013-01-03\n\nIn [633]: df.dtypes\nOut[633]: \na            object\nb             int64\nc           float64\nd              bool\ne    datetime64[ns]\ndtype: object \n```", "```py\nIn [634]: df.to_orc(\"example_pa.orc\", engine=\"pyarrow\") \n```", "```py\nIn [635]: result = pd.read_orc(\"example_pa.orc\")\n\nIn [636]: result.dtypes\nOut[636]: \na            object\nb             int64\nc           float64\nd              bool\ne    datetime64[ns]\ndtype: object \n```", "```py\nIn [637]: result = pd.read_orc(\n .....:    \"example_pa.orc\",\n .....:    columns=[\"a\", \"b\"],\n .....: )\n .....: \n\nIn [638]: result.dtypes\nOut[638]: \na    object\nb     int64\ndtype: object \n```", "```py\nimport adbc_driver_sqlite.dbapi as sqlite_dbapi\n\n# Create the connection\nwith sqlite_dbapi.connect(\"sqlite:///:memory:\") as conn:\n     df = pd.read_sql_table(\"data\", conn) \n```", "```py\nIn [639]: from sqlalchemy import create_engine\n\n# Create your engine.\nIn [640]: engine = create_engine(\"sqlite:///:memory:\") \n```", "```py\nwith engine.connect() as conn, conn.begin():\n    data = pd.read_sql_table(\"data\", conn) \n```", "```py\nIn [641]: import datetime\n\nIn [642]: c = [\"id\", \"Date\", \"Col_1\", \"Col_2\", \"Col_3\"]\n\nIn [643]: d = [\n .....:    (26, datetime.datetime(2010, 10, 18), \"X\", 27.5, True),\n .....:    (42, datetime.datetime(2010, 10, 19), \"Y\", -12.5, False),\n .....:    (63, datetime.datetime(2010, 10, 20), \"Z\", 5.73, True),\n .....: ]\n .....: \n\nIn [644]: data = pd.DataFrame(d, columns=c)\n\nIn [645]: data\nOut[645]: \n id       Date Col_1  Col_2  Col_3\n0  26 2010-10-18     X  27.50   True\n1  42 2010-10-19     Y -12.50  False\n2  63 2010-10-20     Z   5.73   True\n\nIn [646]: data.to_sql(\"data\", con=engine)\nOut[646]: 3 \n```", "```py\nIn [647]: data.to_sql(\"data_chunked\", con=engine, chunksize=1000)\nOut[647]: 3 \n```", "```py\n# for roundtripping\nwith pg_dbapi.connect(uri) as conn:\n    df2 = pd.read_sql(\"pandas_table\", conn, dtype_backend=\"pyarrow\") \n```", "```py\nIn [648]: from sqlalchemy.types import String\n\nIn [649]: data.to_sql(\"data_dtype\", con=engine, dtype={\"Col_1\": String})\nOut[649]: 3 \n```", "```py\n# Alternative to_sql() *method* for DBs that support COPY FROM\nimport csv\nfrom io import StringIO\n\ndef psql_insert_copy(table, conn, keys, data_iter):\n  \"\"\"\n Execute SQL statement inserting data\n\n Parameters\n ----------\n table : pandas.io.sql.SQLTable\n conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n keys : list of str\n Column names\n data_iter : Iterable that iterates the values to be inserted\n \"\"\"\n    # gets a DBAPI connection that can provide a cursor\n    dbapi_conn = conn.connection\n    with dbapi_conn.cursor() as cur:\n        s_buf = StringIO()\n        writer = csv.writer(s_buf)\n        writer.writerows(data_iter)\n        s_buf.seek(0)\n\n        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n        if table.schema:\n            table_name = '{}.{}'.format(table.schema, table.name)\n        else:\n            table_name = table.name\n\n        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n            table_name, columns)\n        cur.copy_expert(sql=sql, file=s_buf) \n```", "```py\nIn [650]: pd.read_sql_table(\"data\", engine)\nOut[650]: \n index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True \n```", "```py\nIn [651]: pd.read_sql_table(\"data\", engine, index_col=\"id\")\nOut[651]: \n index       Date Col_1  Col_2  Col_3\nid \n26      0 2010-10-18     X  27.50   True\n42      1 2010-10-19     Y -12.50  False\n63      2 2010-10-20     Z   5.73   True\n\nIn [652]: pd.read_sql_table(\"data\", engine, columns=[\"Col_1\", \"Col_2\"])\nOut[652]: \n Col_1  Col_2\n0     X  27.50\n1     Y -12.50\n2     Z   5.73 \n```", "```py\nIn [653]: pd.read_sql_table(\"data\", engine, parse_dates=[\"Date\"])\nOut[653]: \n index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True \n```", "```py\npd.read_sql_table(\"data\", engine, parse_dates={\"Date\": \"%Y-%m-%d\"})\npd.read_sql_table(\n    \"data\",\n    engine,\n    parse_dates={\"Date\": {\"format\": \"%Y-%m-%d %H:%M:%S\"}},\n) \n```", "```py\ndf.to_sql(name=\"table\", con=engine, schema=\"other_schema\")\npd.read_sql_table(\"table\", engine, schema=\"other_schema\") \n```", "```py\nIn [654]: pd.read_sql_query(\"SELECT * FROM data\", engine)\nOut[654]: \n index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X  27.50      1\n1      1  42  2010-10-19 00:00:00.000000     Y -12.50      0\n2      2  63  2010-10-20 00:00:00.000000     Z   5.73      1 \n```", "```py\nIn [655]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[655]: \n id Col_1  Col_2\n0  42     Y  -12.5 \n```", "```py\nIn [656]: df = pd.DataFrame(np.random.randn(20, 3), columns=list(\"abc\"))\n\nIn [657]: df.to_sql(name=\"data_chunks\", con=engine, index=False)\nOut[657]: 20 \n```", "```py\nIn [658]: for chunk in pd.read_sql_query(\"SELECT * FROM data_chunks\", engine, chunksize=5):\n .....:    print(chunk)\n .....: \n a         b         c\n0 -0.395347 -0.822726 -0.363777\n1  1.676124 -0.908102 -1.391346\n2 -1.094269  0.278380  1.205899\n3  1.503443  0.932171 -0.709459\n4 -0.645944 -1.351389  0.132023\n a         b         c\n0  0.210427  0.192202  0.661949\n1  1.690629 -1.046044  0.618697\n2 -0.013863  1.314289  1.951611\n3 -1.485026  0.304662  1.194757\n4 -0.446717  0.528496 -0.657575\n a         b         c\n0 -0.876654  0.336252  0.172668\n1  0.337684 -0.411202 -0.828394\n2 -0.244413  1.094948  0.087183\n3  1.125934 -1.480095  1.205944\n4 -0.451849  0.452214 -2.208192\n a         b         c\n0 -2.061019  0.044184 -0.017118\n1  1.248959 -0.675595 -1.908296\n2 -0.125934  1.491974  0.648726\n3  0.391214  0.438609  1.634248\n4  1.208707 -1.535740  1.620399 \n```", "```py\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\"postgresql://scott:tiger@localhost:5432/mydatabase\")\n\nengine = create_engine(\"mysql+mysqldb://scott:tiger@localhost/foo\")\n\nengine = create_engine(\"oracle://scott:[[email\u00a0protected]](/cdn-cgi/l/email-protection):1521/sidname\")\n\nengine = create_engine(\"mssql+pyodbc://mydsn\")\n\n# sqlite://<nohostname>/<path>\n# where <path> is relative:\nengine = create_engine(\"sqlite:///foo.db\")\n\n# or absolute, starting with a slash:\nengine = create_engine(\"sqlite:////absolute/path/to/foo.db\") \n```", "```py\nIn [659]: import sqlalchemy as sa\n\nIn [660]: pd.read_sql(\n .....:    sa.text(\"SELECT * FROM data where Col_1=:col1\"), engine, params={\"col1\": \"X\"}\n .....: )\n .....: \nOut[660]: \n index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X   27.5      1 \n```", "```py\nIn [661]: metadata = sa.MetaData()\n\nIn [662]: data_table = sa.Table(\n .....:    \"data\",\n .....:    metadata,\n .....:    sa.Column(\"index\", sa.Integer),\n .....:    sa.Column(\"Date\", sa.DateTime),\n .....:    sa.Column(\"Col_1\", sa.String),\n .....:    sa.Column(\"Col_2\", sa.Float),\n .....:    sa.Column(\"Col_3\", sa.Boolean),\n .....: )\n .....: \n\nIn [663]: pd.read_sql(sa.select(data_table).where(data_table.c.Col_3 is True), engine)\nOut[663]: \nEmpty DataFrame\nColumns: [index, Date, Col_1, Col_2, Col_3]\nIndex: [] \n```", "```py\nIn [664]: import datetime as dt\n\nIn [665]: expr = sa.select(data_table).where(data_table.c.Date > sa.bindparam(\"date\"))\n\nIn [666]: pd.read_sql(expr, engine, params={\"date\": dt.datetime(2010, 10, 18)})\nOut[666]: \n index       Date Col_1  Col_2  Col_3\n0      1 2010-10-19     Y -12.50  False\n1      2 2010-10-20     Z   5.73   True \n```", "```py\nimport sqlite3\n\ncon = sqlite3.connect(\":memory:\") \n```", "```py\ndata.to_sql(\"data\", con)\npd.read_sql_query(\"SELECT * FROM data\", con) \n```", "```py\nIn [667]: df = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [668]: df.to_stata(\"stata.dta\") \n```", "```py\nIn [669]: pd.read_stata(\"stata.dta\")\nOut[669]: \n index         A         B\n0      0 -0.165614  0.490482\n1      1 -0.637829  0.067091\n2      2 -0.242577  1.348038\n3      3  0.647699 -0.644937\n4      4  0.625771  0.918376\n5      5  0.401781 -1.488919\n6      6 -0.981845 -0.046882\n7      7 -0.306796  0.877025\n8      8 -0.336606  0.624747\n9      9 -1.582600  0.806340 \n```", "```py\nIn [670]: with pd.read_stata(\"stata.dta\", chunksize=3) as reader:\n .....:    for df in reader:\n .....:        print(df.shape)\n .....: \n(3, 3)\n(3, 3)\n(3, 3)\n(1, 3) \n```", "```py\nIn [671]: with pd.read_stata(\"stata.dta\", iterator=True) as reader:\n .....:    chunk1 = reader.read(5)\n .....:    chunk2 = reader.read(5)\n .....: \n```", "```py\ndf = pd.read_sas(\"sas_data.sas7bdat\") \n```", "```py\ndef do_something(chunk):\n    pass\n\nwith pd.read_sas(\"sas_xport.xpt\", chunk=100000) as rdr:\n    for chunk in rdr:\n        do_something(chunk) \n```", "```py\ndf = pd.read_spss(\"spss_data.sav\") \n```", "```py\ndf = pd.read_spss(\n    \"spss_data.sav\",\n    usecols=[\"foo\", \"bar\"],\n    convert_categoricals=False,\n) \n```", "```py\nIn [1]: sz = 1000000\nIn [2]: df = pd.DataFrame({'A': np.random.randn(sz), 'B': [1] * sz})\n\nIn [3]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 2 columns):\nA    1000000 non-null float64\nB    1000000 non-null int64\ndtypes: float64(1), int64(1)\nmemory usage: 15.3 MB \n```", "```py\nimport numpy as np\n\nimport os\n\nsz = 1000000\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\nsz = 1000000\nnp.random.seed(42)\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\ndef test_sql_write(df):\n    if os.path.exists(\"test.sql\"):\n        os.remove(\"test.sql\")\n    sql_db = sqlite3.connect(\"test.sql\")\n    df.to_sql(name=\"test_table\", con=sql_db)\n    sql_db.close()\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\"test.sql\")\n    pd.read_sql_query(\"select * from test_table\", sql_db)\n    sql_db.close()\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\"test_fixed.hdf\", key=\"test\", mode=\"w\")\n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\"test_fixed.hdf\", \"test\")\n\ndef test_hdf_fixed_write_compress(df):\n    df.to_hdf(\"test_fixed_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\")\n\ndef test_hdf_fixed_read_compress():\n    pd.read_hdf(\"test_fixed_compress.hdf\", \"test\")\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\"test_table.hdf\", key=\"test\", mode=\"w\", format=\"table\")\n\ndef test_hdf_table_read():\n    pd.read_hdf(\"test_table.hdf\", \"test\")\n\ndef test_hdf_table_write_compress(df):\n    df.to_hdf(\n        \"test_table_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\", format=\"table\"\n    )\n\ndef test_hdf_table_read_compress():\n    pd.read_hdf(\"test_table_compress.hdf\", \"test\")\n\ndef test_csv_write(df):\n    df.to_csv(\"test.csv\", mode=\"w\")\n\ndef test_csv_read():\n    pd.read_csv(\"test.csv\", index_col=0)\n\ndef test_feather_write(df):\n    df.to_feather(\"test.feather\")\n\ndef test_feather_read():\n    pd.read_feather(\"test.feather\")\n\ndef test_pickle_write(df):\n    df.to_pickle(\"test.pkl\")\n\ndef test_pickle_read():\n    pd.read_pickle(\"test.pkl\")\n\ndef test_pickle_write_compress(df):\n    df.to_pickle(\"test.pkl.compress\", compression=\"xz\")\n\ndef test_pickle_read_compress():\n    pd.read_pickle(\"test.pkl.compress\", compression=\"xz\")\n\ndef test_parquet_write(df):\n    df.to_parquet(\"test.parquet\")\n\ndef test_parquet_read():\n    pd.read_parquet(\"test.parquet\") \n```", "```py\nIn [4]: %timeit test_sql_write(df)\n3.29 s \u00b1 43.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [5]: %timeit test_hdf_fixed_write(df)\n19.4 ms \u00b1 560 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [6]: %timeit test_hdf_fixed_write_compress(df)\n19.6 ms \u00b1 308 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [7]: %timeit test_hdf_table_write(df)\n449 ms \u00b1 5.61 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [8]: %timeit test_hdf_table_write_compress(df)\n448 ms \u00b1 11.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [9]: %timeit test_csv_write(df)\n3.66 s \u00b1 26.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [10]: %timeit test_feather_write(df)\n9.75 ms \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [11]: %timeit test_pickle_write(df)\n30.1 ms \u00b1 229 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [12]: %timeit test_pickle_write_compress(df)\n4.29 s \u00b1 15.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [13]: %timeit test_parquet_write(df)\n67.6 ms \u00b1 706 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) \n```", "```py\nIn [14]: %timeit test_sql_read()\n1.77 s \u00b1 17.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [15]: %timeit test_hdf_fixed_read()\n19.4 ms \u00b1 436 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [16]: %timeit test_hdf_fixed_read_compress()\n19.5 ms \u00b1 222 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [17]: %timeit test_hdf_table_read()\n38.6 ms \u00b1 857 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [18]: %timeit test_hdf_table_read_compress()\n38.8 ms \u00b1 1.49 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [19]: %timeit test_csv_read()\n452 ms \u00b1 9.04 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [20]: %timeit test_feather_read()\n12.4 ms \u00b1 99.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [21]: %timeit test_pickle_read()\n18.4 ms \u00b1 191 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [22]: %timeit test_pickle_read_compress()\n915 ms \u00b1 7.48 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [23]: %timeit test_parquet_read()\n24.4 ms \u00b1 146 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) \n```", "```py\n29519500 Oct 10 06:45 test.csv\n16000248 Oct 10 06:45 test.feather\n8281983  Oct 10 06:49 test.parquet\n16000857 Oct 10 06:47 test.pkl\n7552144  Oct 10 06:48 test.pkl.compress\n34816000 Oct 10 06:42 test.sql\n24009288 Oct 10 06:43 test_fixed.hdf\n24009288 Oct 10 06:43 test_fixed_compress.hdf\n24458940 Oct 10 06:44 test_table.hdf\n24458940 Oct 10 06:44 test_table_compress.hdf \n```", "```py\nIn [1]: import pandas as pd\n\nIn [2]: from io import StringIO\n\nIn [3]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [4]: pd.read_csv(StringIO(data))\nOut[4]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [5]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"COL1\", \"COL3\"])\nOut[5]: \n col1  col3\n0    a     1\n1    a     2\n2    c     3 \n```", "```py\nIn [6]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [7]: pd.read_csv(StringIO(data))\nOut[7]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [8]: pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)\nOut[8]: \n col1 col2  col3\n0    a    b     2 \n```", "```py\nIn [9]: import numpy as np\n\nIn [10]: data = \"a,b,c,d\\n1,2,3,4\\n5,6,7,8\\n9,10,11\"\n\nIn [11]: print(data)\na,b,c,d\n1,2,3,4\n5,6,7,8\n9,10,11\n\nIn [12]: df = pd.read_csv(StringIO(data), dtype=object)\n\nIn [13]: df\nOut[13]: \n a   b   c    d\n0  1   2   3    4\n1  5   6   7    8\n2  9  10  11  NaN\n\nIn [14]: df[\"a\"][0]\nOut[14]: '1'\n\nIn [15]: df = pd.read_csv(StringIO(data), dtype={\"b\": object, \"c\": np.float64, \"d\": \"Int64\"})\n\nIn [16]: df.dtypes\nOut[16]: \na      int64\nb     object\nc    float64\nd      Int64\ndtype: object \n```", "```py\nIn [17]: data = \"col_1\\n1\\n2\\n'A'\\n4.22\"\n\nIn [18]: df = pd.read_csv(StringIO(data), converters={\"col_1\": str})\n\nIn [19]: df\nOut[19]: \n col_1\n0     1\n1     2\n2   'A'\n3  4.22\n\nIn [20]: df[\"col_1\"].apply(type).value_counts()\nOut[20]: \ncol_1\n<class 'str'>    4\nName: count, dtype: int64 \n```", "```py\nIn [21]: df2 = pd.read_csv(StringIO(data))\n\nIn [22]: df2[\"col_1\"] = pd.to_numeric(df2[\"col_1\"], errors=\"coerce\")\n\nIn [23]: df2\nOut[23]: \n col_1\n0   1.00\n1   2.00\n2    NaN\n3   4.22\n\nIn [24]: df2[\"col_1\"].apply(type).value_counts()\nOut[24]: \ncol_1\n<class 'float'>    4\nName: count, dtype: int64 \n```", "```py\nIn [25]: col_1 = list(range(500000)) + [\"a\", \"b\"] + list(range(500000))\n\nIn [26]: df = pd.DataFrame({\"col_1\": col_1})\n\nIn [27]: df.to_csv(\"foo.csv\")\n\nIn [28]: mixed_df = pd.read_csv(\"foo.csv\")\n\nIn [29]: mixed_df[\"col_1\"].apply(type).value_counts()\nOut[29]: \ncol_1\n<class 'int'>    737858\n<class 'str'>    262144\nName: count, dtype: int64\n\nIn [30]: mixed_df[\"col_1\"].dtype\nOut[30]: dtype('O') \n```", "```py\nIn [31]: data = \"\"\"a,b,c,d,e,f,g,h,i,j\n ....: 1,2.5,True,a,,,,,12-31-2019,\n ....: 3,4.5,False,b,6,7.5,True,a,12-31-2019,\n ....: \"\"\"\n ....: \n\nIn [32]: df = pd.read_csv(StringIO(data), dtype_backend=\"numpy_nullable\", parse_dates=[\"i\"])\n\nIn [33]: df\nOut[33]: \n a    b      c  d     e     f     g     h          i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA> 2019-12-31  <NA>\n1  3  4.5  False  b     6   7.5  True     a 2019-12-31  <NA>\n\nIn [34]: df.dtypes\nOut[34]: \na             Int64\nb           Float64\nc           boolean\nd    string[python]\ne             Int64\nf           Float64\ng           boolean\nh    string[python]\ni    datetime64[ns]\nj             Int64\ndtype: object \n```", "```py\nIn [35]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [36]: pd.read_csv(StringIO(data))\nOut[36]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [37]: pd.read_csv(StringIO(data)).dtypes\nOut[37]: \ncol1    object\ncol2    object\ncol3     int64\ndtype: object\n\nIn [38]: pd.read_csv(StringIO(data), dtype=\"category\").dtypes\nOut[38]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object \n```", "```py\nIn [39]: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\nOut[39]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object \n```", "```py\nIn [40]: from pandas.api.types import CategoricalDtype\n\nIn [41]: dtype = CategoricalDtype([\"d\", \"c\", \"b\", \"a\"], ordered=True)\n\nIn [42]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).dtypes\nOut[42]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object \n```", "```py\nIn [43]: dtype = CategoricalDtype([\"a\", \"b\", \"d\"])  # No 'c'\n\nIn [44]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).col1\nOut[44]: \n0      a\n1      a\n2    NaN\nName: col1, dtype: category\nCategories (3, object): ['a', 'b', 'd'] \n```", "```py\nIn [45]: df = pd.read_csv(StringIO(data), dtype=\"category\")\n\nIn [46]: df.dtypes\nOut[46]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object\n\nIn [47]: df[\"col3\"]\nOut[47]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, object): ['1', '2', '3']\n\nIn [48]: new_categories = pd.to_numeric(df[\"col3\"].cat.categories)\n\nIn [49]: df[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\n\nIn [50]: df[\"col3\"]\nOut[50]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, int64): [1, 2, 3] \n```", "```py\nIn [51]: data = \"a,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [52]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [53]: pd.read_csv(StringIO(data))\nOut[53]: \n a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9 \n```", "```py\nIn [54]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [55]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=0)\nOut[55]: \n foo  bar  baz\n0    1    2    3\n1    4    5    6\n2    7    8    9\n\nIn [56]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=None)\nOut[56]: \n foo bar baz\n0   a   b   c\n1   1   2   3\n2   4   5   6\n3   7   8   9 \n```", "```py\nIn [57]: data = \"skip this skip it\\na,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [58]: pd.read_csv(StringIO(data), header=1)\nOut[58]: \n a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9 \n```", "```py\nIn [59]: data = \"a,b,a\\n0,1,2\\n3,4,5\"\n\nIn [60]: pd.read_csv(StringIO(data))\nOut[60]: \n a  b  a.1\n0  0  1    2\n1  3  4    5 \n```", "```py\nIn [61]: data = \"a,b,c,d\\n1,2,3,foo\\n4,5,6,bar\\n7,8,9,baz\"\n\nIn [62]: pd.read_csv(StringIO(data))\nOut[62]: \n a  b  c    d\n0  1  2  3  foo\n1  4  5  6  bar\n2  7  8  9  baz\n\nIn [63]: pd.read_csv(StringIO(data), usecols=[\"b\", \"d\"])\nOut[63]: \n b    d\n0  2  foo\n1  5  bar\n2  8  baz\n\nIn [64]: pd.read_csv(StringIO(data), usecols=[0, 2, 3])\nOut[64]: \n a  c    d\n0  1  3  foo\n1  4  6  bar\n2  7  9  baz\n\nIn [65]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"A\", \"C\"])\nOut[65]: \n a  c\n0  1  3\n1  4  6\n2  7  9 \n```", "```py\nIn [66]: pd.read_csv(StringIO(data), usecols=lambda x: x not in [\"a\", \"c\"])\nOut[66]: \n b    d\n0  2  foo\n1  5  bar\n2  8  baz \n```", "```py\nIn [67]: data = \"\\na,b,c\\n  \\n# commented line\\n1,2,3\\n\\n4,5,6\"\n\nIn [68]: print(data)\n\na,b,c\n\n# commented line\n1,2,3\n\n4,5,6\n\nIn [69]: pd.read_csv(StringIO(data), comment=\"#\")\nOut[69]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [70]: data = \"a,b,c\\n\\n1,2,3\\n\\n\\n4,5,6\"\n\nIn [71]: pd.read_csv(StringIO(data), skip_blank_lines=False)\nOut[71]: \n a    b    c\n0  NaN  NaN  NaN\n1  1.0  2.0  3.0\n2  NaN  NaN  NaN\n3  NaN  NaN  NaN\n4  4.0  5.0  6.0 \n```", "```py\nIn [72]: data = \"#comment\\na,b,c\\nA,B,C\\n1,2,3\"\n\nIn [73]: pd.read_csv(StringIO(data), comment=\"#\", header=1)\nOut[73]: \n A  B  C\n0  1  2  3\n\nIn [74]: data = \"A,B,C\\n#comment\\na,b,c\\n1,2,3\"\n\nIn [75]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=2)\nOut[75]: \n a  b  c\n0  1  2  3 \n```", "```py\nIn [76]: data = (\n ....:    \"# empty\\n\"\n ....:    \"# second empty line\\n\"\n ....:    \"# third emptyline\\n\"\n ....:    \"X,Y,Z\\n\"\n ....:    \"1,2,3\\n\"\n ....:    \"A,B,C\\n\"\n ....:    \"1,2.,4.\\n\"\n ....:    \"5.,NaN,10.0\\n\"\n ....: )\n ....: \n\nIn [77]: print(data)\n# empty\n# second empty line\n# third emptyline\nX,Y,Z\n1,2,3\nA,B,C\n1,2.,4.\n5.,NaN,10.0\n\nIn [78]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=4, header=1)\nOut[78]: \n A    B     C\n0  1.0  2.0   4.0\n1  5.0  NaN  10.0 \n```", "```py\nIn [79]: data = (\n ....:    \"ID,level,category\\n\"\n ....:    \"Patient1,123000,x # really unpleasant\\n\"\n ....:    \"Patient2,23000,y # wouldn't take his medicine\\n\"\n ....:    \"Patient3,1234018,z # awesome\"\n ....: )\n ....: \n\nIn [80]: with open(\"tmp.csv\", \"w\") as fh:\n ....:    fh.write(data)\n ....: \n\nIn [81]: print(open(\"tmp.csv\").read())\nID,level,category\nPatient1,123000,x # really unpleasant\nPatient2,23000,y # wouldn't take his medicine\nPatient3,1234018,z # awesome \n```", "```py\nIn [82]: df = pd.read_csv(\"tmp.csv\")\n\nIn [83]: df\nOut[83]: \n ID    level                        category\n0  Patient1   123000           x # really unpleasant\n1  Patient2    23000  y # wouldn't take his medicine\n2  Patient3  1234018                     z # awesome \n```", "```py\nIn [84]: df = pd.read_csv(\"tmp.csv\", comment=\"#\")\n\nIn [85]: df\nOut[85]: \n ID    level category\n0  Patient1   123000       x \n1  Patient2    23000       y \n2  Patient3  1234018       z \n```", "```py\nIn [86]: from io import BytesIO\n\nIn [87]: data = b\"word,length\\n\" b\"Tr\\xc3\\xa4umen,7\\n\" b\"Gr\\xc3\\xbc\\xc3\\x9fe,5\"\n\nIn [88]: data = data.decode(\"utf8\").encode(\"latin-1\")\n\nIn [89]: df = pd.read_csv(BytesIO(data), encoding=\"latin-1\")\n\nIn [90]: df\nOut[90]: \n word  length\n0  Tr\u00e4umen       7\n1    Gr\u00fc\u00dfe       5\n\nIn [91]: df[\"word\"][1]\nOut[91]: 'Gr\u00fc\u00dfe' \n```", "```py\nIn [92]: data = \"a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [93]: pd.read_csv(StringIO(data))\nOut[93]: \n a    b     c\n4   apple  bat   5.7\n8  orange  cow  10.0 \n```", "```py\nIn [94]: data = \"index,a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [95]: pd.read_csv(StringIO(data), index_col=0)\nOut[95]: \n a    b     c\nindex \n4       apple  bat   5.7\n8      orange  cow  10.0 \n```", "```py\nIn [96]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [97]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [98]: pd.read_csv(StringIO(data))\nOut[98]: \n a    b   c\n4   apple  bat NaN\n8  orange  cow NaN\n\nIn [99]: pd.read_csv(StringIO(data), index_col=False)\nOut[99]: \n a       b    c\n0  4   apple  bat\n1  8  orange  cow \n```", "```py\nIn [100]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [101]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [102]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"])\nOut[102]: \n b   c\n4  bat NaN\n8  cow NaN\n\nIn [103]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"], index_col=0)\nOut[103]: \n b   c\n4  bat NaN\n8  cow NaN \n```", "```py\nIn [104]: with open(\"foo.csv\", mode=\"w\") as f:\n .....:    f.write(\"date,A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\")\n .....: \n\n# Use a column as an index, and parse it as dates.\nIn [105]: df = pd.read_csv(\"foo.csv\", index_col=0, parse_dates=True)\n\nIn [106]: df\nOut[106]: \n A  B  C\ndate \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5\n\n# These are Python datetime objects\nIn [107]: df.index\nOut[107]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', name='date', freq=None) \n```", "```py\nIn [108]: data = (\n .....:    \"KORD,19990127, 19:00:00, 18:56:00, 0.8100\\n\"\n .....:    \"KORD,19990127, 20:00:00, 19:56:00, 0.0100\\n\"\n .....:    \"KORD,19990127, 21:00:00, 20:56:00, -0.5900\\n\"\n .....:    \"KORD,19990127, 21:00:00, 21:18:00, -0.9900\\n\"\n .....:    \"KORD,19990127, 22:00:00, 21:56:00, -0.5900\\n\"\n .....:    \"KORD,19990127, 23:00:00, 22:56:00, -0.5900\"\n .....: )\n .....: \n\nIn [109]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [110]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]])\n\nIn [111]: df\nOut[111]: \n 1_2                 1_3     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [112]: df = pd.read_csv(\n .....:    \"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]], keep_date_col=True\n .....: )\n .....: \n\nIn [113]: df\nOut[113]: \n 1_2                 1_3     0  ...          2          3     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  ...   19:00:00   18:56:00  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  ...   20:00:00   19:56:00  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD  ...   21:00:00   20:56:00 -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD  ...   21:00:00   21:18:00 -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD  ...   22:00:00   21:56:00 -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD  ...   23:00:00   22:56:00 -0.59\n\n[6 rows x 7 columns] \n```", "```py\nIn [114]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [115]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=date_spec)\n\nIn [116]: df\nOut[116]: \n nominal              actual     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [117]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [118]: df = pd.read_csv(\n .....:    \"tmp.csv\", header=None, parse_dates=date_spec, index_col=0\n .....: )  # index is the nominal column\n .....: \n\nIn [119]: df\nOut[119]: \n actual     0     4\nnominal \n1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [120]: content = \"\"\"\\\n .....: a\n .....: 2000-01-01T00:00:00+05:00\n .....: 2000-01-01T00:00:00+06:00\"\"\"\n .....: \n\nIn [121]: df = pd.read_csv(StringIO(content))\n\nIn [122]: df[\"a\"] = pd.to_datetime(df[\"a\"], utc=True)\n\nIn [123]: df[\"a\"]\nOut[123]: \n0   1999-12-31 19:00:00+00:00\n1   1999-12-31 18:00:00+00:00\nName: a, dtype: datetime64[ns, UTC] \n```", "```py\nIn [124]: df = pd.read_csv(\n .....:    \"foo.csv\",\n .....:    index_col=0,\n .....:    parse_dates=True,\n .....: )\n .....: \n\nIn [125]: df\nOut[125]: \n A  B  C\ndate \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5 \n```", "```py\nIn [126]: data = StringIO(\"date\\n12 Jan 2000\\n2000-01-13\\n\")\n\nIn [127]: df = pd.read_csv(data)\n\nIn [128]: df['date'] = pd.to_datetime(df['date'], format='mixed')\n\nIn [129]: df\nOut[129]: \n date\n0 2000-01-12\n1 2000-01-13 \n```", "```py\nIn [130]: data = StringIO(\"date\\n2020-01-01\\n2020-01-01 03:00\\n\")\n\nIn [131]: df = pd.read_csv(data)\n\nIn [132]: df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n\nIn [133]: df\nOut[133]: \n date\n0 2020-01-01 00:00:00\n1 2020-01-01 03:00:00 \n```", "```py\nIn [134]: data = \"date,value,cat\\n1/6/2000,5,a\\n2/6/2000,10,b\\n3/6/2000,15,c\"\n\nIn [135]: print(data)\ndate,value,cat\n1/6/2000,5,a\n2/6/2000,10,b\n3/6/2000,15,c\n\nIn [136]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [137]: pd.read_csv(\"tmp.csv\", parse_dates=[0])\nOut[137]: \n date  value cat\n0 2000-01-06      5   a\n1 2000-02-06     10   b\n2 2000-03-06     15   c\n\nIn [138]: pd.read_csv(\"tmp.csv\", dayfirst=True, parse_dates=[0])\nOut[138]: \n date  value cat\n0 2000-06-01      5   a\n1 2000-06-02     10   b\n2 2000-06-03     15   c \n```", "```py\nIn [139]: import io\n\nIn [140]: data = pd.DataFrame([0, 1, 2])\n\nIn [141]: buffer = io.BytesIO()\n\nIn [142]: data.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\") \n```", "```py\nIn [143]: val = \"0.3066101993807095471566981359501369297504425048828125\"\n\nIn [144]: data = \"a,b,c\\n1,2,{0}\".format(val)\n\nIn [145]: abs(\n .....:    pd.read_csv(\n .....:        StringIO(data),\n .....:        engine=\"c\",\n .....:        float_precision=None,\n .....:    )[\"c\"][0] - float(val)\n .....: )\n .....: \nOut[145]: 5.551115123125783e-17\n\nIn [146]: abs(\n .....:    pd.read_csv(\n .....:        StringIO(data),\n .....:        engine=\"c\",\n .....:        float_precision=\"high\",\n .....:    )[\"c\"][0] - float(val)\n .....: )\n .....: \nOut[146]: 5.551115123125783e-17\n\nIn [147]: abs(\n .....:    pd.read_csv(StringIO(data), engine=\"c\", float_precision=\"round_trip\")[\"c\"][0]\n .....:    - float(val)\n .....: )\n .....: \nOut[147]: 0.0 \n```", "```py\nIn [148]: data = (\n .....:    \"ID|level|category\\n\"\n .....:    \"Patient1|123,000|x\\n\"\n .....:    \"Patient2|23,000|y\\n\"\n .....:    \"Patient3|1,234,018|z\"\n .....: )\n .....: \n\nIn [149]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [150]: df = pd.read_csv(\"tmp.csv\", sep=\"|\")\n\nIn [151]: df\nOut[151]: \n ID      level category\n0  Patient1    123,000        x\n1  Patient2     23,000        y\n2  Patient3  1,234,018        z\n\nIn [152]: df.level.dtype\nOut[152]: dtype('O') \n```", "```py\nIn [153]: df = pd.read_csv(\"tmp.csv\", sep=\"|\", thousands=\",\")\n\nIn [154]: df\nOut[154]: \n ID    level category\n0  Patient1   123000        x\n1  Patient2    23000        y\n2  Patient3  1234018        z\n\nIn [155]: df.level.dtype\nOut[155]: dtype('int64') \n```", "```py\npd.read_csv(\"path_to_file.csv\", na_values=[5]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"\"]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"NA\", \"0\"]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", na_values=[\"Nope\"]) \n```", "```py\nIn [156]: data = \"a,b,c\\n1,Yes,2\\n3,No,4\"\n\nIn [157]: print(data)\na,b,c\n1,Yes,2\n3,No,4\n\nIn [158]: pd.read_csv(StringIO(data))\nOut[158]: \n a    b  c\n0  1  Yes  2\n1  3   No  4\n\nIn [159]: pd.read_csv(StringIO(data), true_values=[\"Yes\"], false_values=[\"No\"])\nOut[159]: \n a      b  c\n0  1   True  2\n1  3  False  4 \n```", "```py\nIn [160]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [161]: pd.read_csv(StringIO(data))\n---------------------------------------------------------------------------\nParserError  Traceback (most recent call last)\nCell In[161], line 1\n----> 1 pd.read_csv(StringIO(data))\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:626, in _read(filepath_or_buffer, kwds)\n  623     return parser\n  625 with parser:\n--> 626     return parser.read(nrows)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1923, in TextFileReader.read(self, nrows)\n  1916 nrows = validate_integer(\"nrows\", nrows)\n  1917 try:\n  1918     # error: \"ParserBase\" has no attribute \"read\"\n  1919     (\n  1920         index,\n  1921         columns,\n  1922         col_dict,\n-> 1923     ) = self._engine.read(  # type: ignore[attr-defined]\n  1924         nrows\n  1925     )\n  1926 except Exception:\n  1927     self.close()\n\nFile ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:234, in CParserWrapper.read(self, nrows)\n  232 try:\n  233     if self.low_memory:\n--> 234         chunks = self._reader.read_low_memory(nrows)\n  235         # destructive to chunks\n  236         data = _concatenate_chunks(chunks)\n\nFile parsers.pyx:838, in pandas._libs.parsers.TextReader.read_low_memory()\n\nFile parsers.pyx:905, in pandas._libs.parsers.TextReader._read_rows()\n\nFile parsers.pyx:874, in pandas._libs.parsers.TextReader._tokenize_rows()\n\nFile parsers.pyx:891, in pandas._libs.parsers.TextReader._check_tokenize_status()\n\nFile parsers.pyx:2061, in pandas._libs.parsers.raise_parser_error()\n\nParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4 \n```", "```py\nIn [162]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [163]: pd.read_csv(StringIO(data), on_bad_lines=\"skip\")\nOut[163]: \n a  b   c\n0  1  2   3\n1  8  9  10 \n```", "```py\nIn [164]: external_list = []\n\nIn [165]: def bad_lines_func(line):\n .....:    external_list.append(line)\n .....:    return line[-3:]\n .....: \n\nIn [166]: external_list\nOut[166]: [] \n```", "```py\nIn [167]: bad_lines_func = lambda line: print(line)\n\nIn [168]: data = 'name,type\\nname a,a is of type a\\nname b,\"b\\\" is of type b\"'\n\nIn [169]: data\nOut[169]: 'name,type\\nname a,a is of type a\\nname b,\"b\" is of type b\"'\n\nIn [170]: pd.read_csv(StringIO(data), on_bad_lines=bad_lines_func, engine=\"python\")\nOut[170]: \n name            type\n0  name a  a is of type a \n```", "```py\nIn [171]: pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n---------------------------------------------------------------------------\nValueError  Traceback (most recent call last)\nCell In[171], line 1\n----> 1 pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n  617 _validate_names(kwds.get(\"names\", None))\n  619 # Create the parser.\n--> 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n  622 if chunksize or iterator:\n  623     return parser\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n  1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n  1619 self.handles: IOHandles | None = None\n-> 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1898, in TextFileReader._make_engine(self, f, engine)\n  1895     raise ValueError(msg)\n  1897 try:\n-> 1898     return mapping[engine](f, **self.options)\n  1899 except Exception:\n  1900     if self.handles is not None:\n\nFile ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:155, in CParserWrapper.__init__(self, src, **kwds)\n  152     # error: Cannot determine type of 'names'\n  153     if len(self.names) < len(usecols):  # type: ignore[has-type]\n  154         # error: Cannot determine type of 'names'\n--> 155         self._validate_usecols_names(\n  156             usecols,\n  157             self.names,  # type: ignore[has-type]\n  158         )\n  160 # error: Cannot determine type of 'names'\n  161 self._validate_parse_dates_presence(self.names)  # type: ignore[has-type]\n\nFile ~/work/pandas/pandas/pandas/io/parsers/base_parser.py:979, in ParserBase._validate_usecols_names(self, usecols, names)\n  977 missing = [c for c in usecols if c not in names]\n  978 if len(missing) > 0:\n--> 979     raise ValueError(\n  980         f\"Usecols do not match columns, columns expected but not found: \"\n  981         f\"{missing}\"\n  982     )\n  984 return usecols\n\nValueError: Usecols do not match columns, columns expected but not found: [0, 1, 2] \n```", "```py\nIn [172]: pd.read_csv(StringIO(data), names=['a', 'b', 'c', 'd'])\nOut[172]: \n a                b   c   d\n0    name             type NaN NaN\n1  name a   a is of type a NaN NaN\n2  name b  b is of type b\" NaN NaN \n```", "```py\nIn [173]: data = \"label1,label2,label3\\n\" 'index1,\"a,c,e\\n' \"index2,b,d,f\"\n\nIn [174]: print(data)\nlabel1,label2,label3\nindex1,\"a,c,e\nindex2,b,d,f \n```", "```py\nIn [175]: import csv\n\nIn [176]: dia = csv.excel()\n\nIn [177]: dia.quoting = csv.QUOTE_NONE\n\nIn [178]: pd.read_csv(StringIO(data), dialect=dia)\nOut[178]: \n label1 label2 label3\nindex1     \"a      c      e\nindex2      b      d      f \n```", "```py\nIn [179]: data = \"a,b,c~1,2,3~4,5,6\"\n\nIn [180]: pd.read_csv(StringIO(data), lineterminator=\"~\")\nOut[180]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [181]: data = \"a, b, c\\n1, 2, 3\\n4, 5, 6\"\n\nIn [182]: print(data)\na, b, c\n1, 2, 3\n4, 5, 6\n\nIn [183]: pd.read_csv(StringIO(data), skipinitialspace=True)\nOut[183]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [184]: data = 'a,b\\n\"hello, \\\\\"Bob\\\\\", nice to see you\",5'\n\nIn [185]: print(data)\na,b\n\"hello, \\\"Bob\\\", nice to see you\",5\n\nIn [186]: pd.read_csv(StringIO(data), escapechar=\"\\\\\")\nOut[186]: \n a  b\n0  hello, \"Bob\", nice to see you  5 \n```", "```py\nIn [187]: data1 = (\n .....:    \"id8141    360.242940   149.910199   11950.7\\n\"\n .....:    \"id1594    444.953632   166.985655   11788.4\\n\"\n .....:    \"id1849    364.136849   183.628767   11806.2\\n\"\n .....:    \"id1230    413.836124   184.375703   11916.8\\n\"\n .....:    \"id1948    502.953953   173.237159   12468.3\"\n .....: )\n .....: \n\nIn [188]: with open(\"bar.csv\", \"w\") as f:\n .....:    f.write(data1)\n .....: \n```", "```py\n# Column specifications are a list of half-intervals\nIn [189]: colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]\n\nIn [190]: df = pd.read_fwf(\"bar.csv\", colspecs=colspecs, header=None, index_col=0)\n\nIn [191]: df\nOut[191]: \n 1           2        3\n0 \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3 \n```", "```py\n# Widths are a list of integers\nIn [192]: widths = [6, 14, 13, 10]\n\nIn [193]: df = pd.read_fwf(\"bar.csv\", widths=widths, header=None)\n\nIn [194]: df\nOut[194]: \n 0           1           2        3\n0  id8141  360.242940  149.910199  11950.7\n1  id1594  444.953632  166.985655  11788.4\n2  id1849  364.136849  183.628767  11806.2\n3  id1230  413.836124  184.375703  11916.8\n4  id1948  502.953953  173.237159  12468.3 \n```", "```py\nIn [195]: df = pd.read_fwf(\"bar.csv\", header=None, index_col=0)\n\nIn [196]: df\nOut[196]: \n 1           2        3\n0 \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3 \n```", "```py\nIn [197]: pd.read_fwf(\"bar.csv\", header=None, index_col=0).dtypes\nOut[197]: \n1    float64\n2    float64\n3    float64\ndtype: object\n\nIn [198]: pd.read_fwf(\"bar.csv\", header=None, dtype={2: \"object\"}).dtypes\nOut[198]: \n0     object\n1    float64\n2     object\n3    float64\ndtype: object \n```", "```py\nIn [199]: data = \"A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\"\n\nIn [200]: print(data)\nA,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5\n\nIn [201]: with open(\"foo.csv\", \"w\") as f:\n .....:    f.write(data)\n .....: \n```", "```py\nIn [202]: pd.read_csv(\"foo.csv\")\nOut[202]: \n A  B  C\n20090101  a  1  2\n20090102  b  3  4\n20090103  c  4  5 \n```", "```py\nIn [203]: df = pd.read_csv(\"foo.csv\", parse_dates=True)\n\nIn [204]: df.index\nOut[204]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [205]: data = 'year,indiv,zit,xit\\n1977,\"A\",1.2,.6\\n1977,\"B\",1.5,.5'\n\nIn [206]: print(data)\nyear,indiv,zit,xit\n1977,\"A\",1.2,.6\n1977,\"B\",1.5,.5\n\nIn [207]: with open(\"mindex_ex.csv\", mode=\"w\") as f:\n .....:    f.write(data)\n .....: \n```", "```py\nIn [208]: df = pd.read_csv(\"mindex_ex.csv\", index_col=[0, 1])\n\nIn [209]: df\nOut[209]: \n zit  xit\nyear indiv \n1977 A      1.2  0.6\n B      1.5  0.5\n\nIn [210]: df.loc[1977]\nOut[210]: \n zit  xit\nindiv \nA      1.2  0.6\nB      1.5  0.5 \n```", "```py\nIn [211]: mi_idx = pd.MultiIndex.from_arrays([[1, 2, 3, 4], list(\"abcd\")], names=list(\"ab\"))\n\nIn [212]: mi_col = pd.MultiIndex.from_arrays([[1, 2], list(\"ab\")], names=list(\"cd\"))\n\nIn [213]: df = pd.DataFrame(np.ones((4, 2)), index=mi_idx, columns=mi_col)\n\nIn [214]: df.to_csv(\"mi.csv\")\n\nIn [215]: print(open(\"mi.csv\").read())\nc,,1,2\nd,,a,b\na,b,,\n1,a,1.0,1.0\n2,b,1.0,1.0\n3,c,1.0,1.0\n4,d,1.0,1.0\n\nIn [216]: pd.read_csv(\"mi.csv\", header=[0, 1, 2, 3], index_col=[0, 1])\nOut[216]: \nc                    1                  2\nd                    a                  b\na   Unnamed: 2_level_2 Unnamed: 3_level_2\n1                  1.0                1.0\n2 b                1.0                1.0\n3 c                1.0                1.0\n4 d                1.0                1.0 \n```", "```py\nIn [217]: data = \",a,a,a,b,c,c\\n,q,r,s,t,u,v\\none,1,2,3,4,5,6\\ntwo,7,8,9,10,11,12\"\n\nIn [218]: print(data)\n,a,a,a,b,c,c\n,q,r,s,t,u,v\none,1,2,3,4,5,6\ntwo,7,8,9,10,11,12\n\nIn [219]: with open(\"mi2.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [220]: pd.read_csv(\"mi2.csv\", header=[0, 1], index_col=0)\nOut[220]: \n a         b   c \n q  r  s   t   u   v\none  1  2  3   4   5   6\ntwo  7  8  9  10  11  12 \n```", "```py\nIn [221]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [222]: df.to_csv(\"tmp2.csv\", sep=\":\", index=False)\n\nIn [223]: pd.read_csv(\"tmp2.csv\", sep=None, engine=\"python\")\nOut[223]: \n 0         1         2         3\n0  0.469112 -0.282863 -1.509059 -1.135632\n1  1.212112 -0.173215  0.119209 -1.044236\n2 -0.861849 -2.104569 -0.494929  1.071804\n3  0.721555 -0.706771 -1.039575  0.271860\n4 -0.424972  0.567020  0.276232 -1.087401\n5 -0.673690  0.113648 -1.478427  0.524988\n6  0.404705  0.577046 -1.715002 -1.039268\n7 -0.370647 -1.157892 -1.344312  0.844885\n8  1.075770 -0.109050  1.643563 -1.469388\n9  0.357021 -0.674600 -1.776904 -0.968914 \n```", "```py\nIn [224]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [225]: df.to_csv(\"tmp.csv\", index=False)\n\nIn [226]: table = pd.read_csv(\"tmp.csv\")\n\nIn [227]: table\nOut[227]: \n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n4  0.410835  0.813850  0.132003 -0.827317\n5 -0.076467 -1.187678  1.130127 -1.436737\n6 -1.413681  1.607920  1.024180  0.569605\n7  0.875906 -2.211372  0.974466 -2.006747\n8 -0.410001 -0.078638  0.545952 -1.219217\n9 -1.226825  0.769804 -1.281247 -0.727707 \n```", "```py\nIn [228]: with pd.read_csv(\"tmp.csv\", chunksize=4) as reader:\n .....:    print(reader)\n .....:    for chunk in reader:\n .....:        print(chunk)\n .....: \n<pandas.io.parsers.readers.TextFileReader object at 0x7ff2e5421db0>\n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n 0         1         2         3\n4  0.410835  0.813850  0.132003 -0.827317\n5 -0.076467 -1.187678  1.130127 -1.436737\n6 -1.413681  1.607920  1.024180  0.569605\n7  0.875906 -2.211372  0.974466 -2.006747\n 0         1         2         3\n8 -0.410001 -0.078638  0.545952 -1.219217\n9 -1.226825  0.769804 -1.281247 -0.727707 \n```", "```py\nIn [229]: with pd.read_csv(\"tmp.csv\", iterator=True) as reader:\n .....:    print(reader.get_chunk(5))\n .....: \n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n4  0.410835  0.813850  0.132003 -0.827317 \n```", "```py\ndf = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\") \n```", "```py\nheaders = {\"User-Agent\": \"pandas\"}\ndf = pd.read_csv(\n    \"https://download.bls.gov/pub/time.series/cu/cu.item\",\n    sep=\"\\t\",\n    storage_options=headers\n) \n```", "```py\ndf = pd.read_json(\"s3://pandas-test/adatafile.json\") \n```", "```py\nstorage_options = {\"client_kwargs\": {\"endpoint_url\": \"http://127.0.0.1:5555\"}}}\ndf = pd.read_json(\"s3://pandas-test/test-1\", storage_options=storage_options) \n```", "```py\npd.read_csv(\n    \"s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013\"\n    \"-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"anon\": True},\n) \n```", "```py\npd.read_csv(\n    \"simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/\"\n    \"SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"s3\": {\"anon\": True}},\n) \n```", "```py\nIn [1]: import pandas as pd\n\nIn [2]: from io import StringIO\n\nIn [3]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [4]: pd.read_csv(StringIO(data))\nOut[4]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [5]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"COL1\", \"COL3\"])\nOut[5]: \n col1  col3\n0    a     1\n1    a     2\n2    c     3 \n```", "```py\nIn [6]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [7]: pd.read_csv(StringIO(data))\nOut[7]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [8]: pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)\nOut[8]: \n col1 col2  col3\n0    a    b     2 \n```", "```py\nIn [1]: import pandas as pd\n\nIn [2]: from io import StringIO\n\nIn [3]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [4]: pd.read_csv(StringIO(data))\nOut[4]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [5]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"COL1\", \"COL3\"])\nOut[5]: \n col1  col3\n0    a     1\n1    a     2\n2    c     3 \n```", "```py\nIn [6]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [7]: pd.read_csv(StringIO(data))\nOut[7]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [8]: pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)\nOut[8]: \n col1 col2  col3\n0    a    b     2 \n```", "```py\nIn [9]: import numpy as np\n\nIn [10]: data = \"a,b,c,d\\n1,2,3,4\\n5,6,7,8\\n9,10,11\"\n\nIn [11]: print(data)\na,b,c,d\n1,2,3,4\n5,6,7,8\n9,10,11\n\nIn [12]: df = pd.read_csv(StringIO(data), dtype=object)\n\nIn [13]: df\nOut[13]: \n a   b   c    d\n0  1   2   3    4\n1  5   6   7    8\n2  9  10  11  NaN\n\nIn [14]: df[\"a\"][0]\nOut[14]: '1'\n\nIn [15]: df = pd.read_csv(StringIO(data), dtype={\"b\": object, \"c\": np.float64, \"d\": \"Int64\"})\n\nIn [16]: df.dtypes\nOut[16]: \na      int64\nb     object\nc    float64\nd      Int64\ndtype: object \n```", "```py\nIn [17]: data = \"col_1\\n1\\n2\\n'A'\\n4.22\"\n\nIn [18]: df = pd.read_csv(StringIO(data), converters={\"col_1\": str})\n\nIn [19]: df\nOut[19]: \n col_1\n0     1\n1     2\n2   'A'\n3  4.22\n\nIn [20]: df[\"col_1\"].apply(type).value_counts()\nOut[20]: \ncol_1\n<class 'str'>    4\nName: count, dtype: int64 \n```", "```py\nIn [21]: df2 = pd.read_csv(StringIO(data))\n\nIn [22]: df2[\"col_1\"] = pd.to_numeric(df2[\"col_1\"], errors=\"coerce\")\n\nIn [23]: df2\nOut[23]: \n col_1\n0   1.00\n1   2.00\n2    NaN\n3   4.22\n\nIn [24]: df2[\"col_1\"].apply(type).value_counts()\nOut[24]: \ncol_1\n<class 'float'>    4\nName: count, dtype: int64 \n```", "```py\nIn [25]: col_1 = list(range(500000)) + [\"a\", \"b\"] + list(range(500000))\n\nIn [26]: df = pd.DataFrame({\"col_1\": col_1})\n\nIn [27]: df.to_csv(\"foo.csv\")\n\nIn [28]: mixed_df = pd.read_csv(\"foo.csv\")\n\nIn [29]: mixed_df[\"col_1\"].apply(type).value_counts()\nOut[29]: \ncol_1\n<class 'int'>    737858\n<class 'str'>    262144\nName: count, dtype: int64\n\nIn [30]: mixed_df[\"col_1\"].dtype\nOut[30]: dtype('O') \n```", "```py\nIn [31]: data = \"\"\"a,b,c,d,e,f,g,h,i,j\n ....: 1,2.5,True,a,,,,,12-31-2019,\n ....: 3,4.5,False,b,6,7.5,True,a,12-31-2019,\n ....: \"\"\"\n ....: \n\nIn [32]: df = pd.read_csv(StringIO(data), dtype_backend=\"numpy_nullable\", parse_dates=[\"i\"])\n\nIn [33]: df\nOut[33]: \n a    b      c  d     e     f     g     h          i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA> 2019-12-31  <NA>\n1  3  4.5  False  b     6   7.5  True     a 2019-12-31  <NA>\n\nIn [34]: df.dtypes\nOut[34]: \na             Int64\nb           Float64\nc           boolean\nd    string[python]\ne             Int64\nf           Float64\ng           boolean\nh    string[python]\ni    datetime64[ns]\nj             Int64\ndtype: object \n```", "```py\nIn [35]: data = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\nIn [36]: pd.read_csv(StringIO(data))\nOut[36]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nIn [37]: pd.read_csv(StringIO(data)).dtypes\nOut[37]: \ncol1    object\ncol2    object\ncol3     int64\ndtype: object\n\nIn [38]: pd.read_csv(StringIO(data), dtype=\"category\").dtypes\nOut[38]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object \n```", "```py\nIn [39]: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\nOut[39]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object \n```", "```py\nIn [40]: from pandas.api.types import CategoricalDtype\n\nIn [41]: dtype = CategoricalDtype([\"d\", \"c\", \"b\", \"a\"], ordered=True)\n\nIn [42]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).dtypes\nOut[42]: \ncol1    category\ncol2      object\ncol3       int64\ndtype: object \n```", "```py\nIn [43]: dtype = CategoricalDtype([\"a\", \"b\", \"d\"])  # No 'c'\n\nIn [44]: pd.read_csv(StringIO(data), dtype={\"col1\": dtype}).col1\nOut[44]: \n0      a\n1      a\n2    NaN\nName: col1, dtype: category\nCategories (3, object): ['a', 'b', 'd'] \n```", "```py\nIn [45]: df = pd.read_csv(StringIO(data), dtype=\"category\")\n\nIn [46]: df.dtypes\nOut[46]: \ncol1    category\ncol2    category\ncol3    category\ndtype: object\n\nIn [47]: df[\"col3\"]\nOut[47]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, object): ['1', '2', '3']\n\nIn [48]: new_categories = pd.to_numeric(df[\"col3\"].cat.categories)\n\nIn [49]: df[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\n\nIn [50]: df[\"col3\"]\nOut[50]: \n0    1\n1    2\n2    3\nName: col3, dtype: category\nCategories (3, int64): [1, 2, 3] \n```", "```py\nIn [51]: data = \"a,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [52]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [53]: pd.read_csv(StringIO(data))\nOut[53]: \n a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9 \n```", "```py\nIn [54]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [55]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=0)\nOut[55]: \n foo  bar  baz\n0    1    2    3\n1    4    5    6\n2    7    8    9\n\nIn [56]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=None)\nOut[56]: \n foo bar baz\n0   a   b   c\n1   1   2   3\n2   4   5   6\n3   7   8   9 \n```", "```py\nIn [57]: data = \"skip this skip it\\na,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [58]: pd.read_csv(StringIO(data), header=1)\nOut[58]: \n a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9 \n```", "```py\nIn [51]: data = \"a,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [52]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [53]: pd.read_csv(StringIO(data))\nOut[53]: \n a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9 \n```", "```py\nIn [54]: print(data)\na,b,c\n1,2,3\n4,5,6\n7,8,9\n\nIn [55]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=0)\nOut[55]: \n foo  bar  baz\n0    1    2    3\n1    4    5    6\n2    7    8    9\n\nIn [56]: pd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=None)\nOut[56]: \n foo bar baz\n0   a   b   c\n1   1   2   3\n2   4   5   6\n3   7   8   9 \n```", "```py\nIn [57]: data = \"skip this skip it\\na,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\n\nIn [58]: pd.read_csv(StringIO(data), header=1)\nOut[58]: \n a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9 \n```", "```py\nIn [59]: data = \"a,b,a\\n0,1,2\\n3,4,5\"\n\nIn [60]: pd.read_csv(StringIO(data))\nOut[60]: \n a  b  a.1\n0  0  1    2\n1  3  4    5 \n```", "```py\nIn [61]: data = \"a,b,c,d\\n1,2,3,foo\\n4,5,6,bar\\n7,8,9,baz\"\n\nIn [62]: pd.read_csv(StringIO(data))\nOut[62]: \n a  b  c    d\n0  1  2  3  foo\n1  4  5  6  bar\n2  7  8  9  baz\n\nIn [63]: pd.read_csv(StringIO(data), usecols=[\"b\", \"d\"])\nOut[63]: \n b    d\n0  2  foo\n1  5  bar\n2  8  baz\n\nIn [64]: pd.read_csv(StringIO(data), usecols=[0, 2, 3])\nOut[64]: \n a  c    d\n0  1  3  foo\n1  4  6  bar\n2  7  9  baz\n\nIn [65]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"A\", \"C\"])\nOut[65]: \n a  c\n0  1  3\n1  4  6\n2  7  9 \n```", "```py\nIn [66]: pd.read_csv(StringIO(data), usecols=lambda x: x not in [\"a\", \"c\"])\nOut[66]: \n b    d\n0  2  foo\n1  5  bar\n2  8  baz \n```", "```py\nIn [61]: data = \"a,b,c,d\\n1,2,3,foo\\n4,5,6,bar\\n7,8,9,baz\"\n\nIn [62]: pd.read_csv(StringIO(data))\nOut[62]: \n a  b  c    d\n0  1  2  3  foo\n1  4  5  6  bar\n2  7  8  9  baz\n\nIn [63]: pd.read_csv(StringIO(data), usecols=[\"b\", \"d\"])\nOut[63]: \n b    d\n0  2  foo\n1  5  bar\n2  8  baz\n\nIn [64]: pd.read_csv(StringIO(data), usecols=[0, 2, 3])\nOut[64]: \n a  c    d\n0  1  3  foo\n1  4  6  bar\n2  7  9  baz\n\nIn [65]: pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"A\", \"C\"])\nOut[65]: \n a  c\n0  1  3\n1  4  6\n2  7  9 \n```", "```py\nIn [66]: pd.read_csv(StringIO(data), usecols=lambda x: x not in [\"a\", \"c\"])\nOut[66]: \n b    d\n0  2  foo\n1  5  bar\n2  8  baz \n```", "```py\nIn [67]: data = \"\\na,b,c\\n  \\n# commented line\\n1,2,3\\n\\n4,5,6\"\n\nIn [68]: print(data)\n\na,b,c\n\n# commented line\n1,2,3\n\n4,5,6\n\nIn [69]: pd.read_csv(StringIO(data), comment=\"#\")\nOut[69]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [70]: data = \"a,b,c\\n\\n1,2,3\\n\\n\\n4,5,6\"\n\nIn [71]: pd.read_csv(StringIO(data), skip_blank_lines=False)\nOut[71]: \n a    b    c\n0  NaN  NaN  NaN\n1  1.0  2.0  3.0\n2  NaN  NaN  NaN\n3  NaN  NaN  NaN\n4  4.0  5.0  6.0 \n```", "```py\nIn [72]: data = \"#comment\\na,b,c\\nA,B,C\\n1,2,3\"\n\nIn [73]: pd.read_csv(StringIO(data), comment=\"#\", header=1)\nOut[73]: \n A  B  C\n0  1  2  3\n\nIn [74]: data = \"A,B,C\\n#comment\\na,b,c\\n1,2,3\"\n\nIn [75]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=2)\nOut[75]: \n a  b  c\n0  1  2  3 \n```", "```py\nIn [76]: data = (\n ....:    \"# empty\\n\"\n ....:    \"# second empty line\\n\"\n ....:    \"# third emptyline\\n\"\n ....:    \"X,Y,Z\\n\"\n ....:    \"1,2,3\\n\"\n ....:    \"A,B,C\\n\"\n ....:    \"1,2.,4.\\n\"\n ....:    \"5.,NaN,10.0\\n\"\n ....: )\n ....: \n\nIn [77]: print(data)\n# empty\n# second empty line\n# third emptyline\nX,Y,Z\n1,2,3\nA,B,C\n1,2.,4.\n5.,NaN,10.0\n\nIn [78]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=4, header=1)\nOut[78]: \n A    B     C\n0  1.0  2.0   4.0\n1  5.0  NaN  10.0 \n```", "```py\nIn [79]: data = (\n ....:    \"ID,level,category\\n\"\n ....:    \"Patient1,123000,x # really unpleasant\\n\"\n ....:    \"Patient2,23000,y # wouldn't take his medicine\\n\"\n ....:    \"Patient3,1234018,z # awesome\"\n ....: )\n ....: \n\nIn [80]: with open(\"tmp.csv\", \"w\") as fh:\n ....:    fh.write(data)\n ....: \n\nIn [81]: print(open(\"tmp.csv\").read())\nID,level,category\nPatient1,123000,x # really unpleasant\nPatient2,23000,y # wouldn't take his medicine\nPatient3,1234018,z # awesome \n```", "```py\nIn [82]: df = pd.read_csv(\"tmp.csv\")\n\nIn [83]: df\nOut[83]: \n ID    level                        category\n0  Patient1   123000           x # really unpleasant\n1  Patient2    23000  y # wouldn't take his medicine\n2  Patient3  1234018                     z # awesome \n```", "```py\nIn [84]: df = pd.read_csv(\"tmp.csv\", comment=\"#\")\n\nIn [85]: df\nOut[85]: \n ID    level category\n0  Patient1   123000       x \n1  Patient2    23000       y \n2  Patient3  1234018       z \n```", "```py\nIn [67]: data = \"\\na,b,c\\n  \\n# commented line\\n1,2,3\\n\\n4,5,6\"\n\nIn [68]: print(data)\n\na,b,c\n\n# commented line\n1,2,3\n\n4,5,6\n\nIn [69]: pd.read_csv(StringIO(data), comment=\"#\")\nOut[69]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [70]: data = \"a,b,c\\n\\n1,2,3\\n\\n\\n4,5,6\"\n\nIn [71]: pd.read_csv(StringIO(data), skip_blank_lines=False)\nOut[71]: \n a    b    c\n0  NaN  NaN  NaN\n1  1.0  2.0  3.0\n2  NaN  NaN  NaN\n3  NaN  NaN  NaN\n4  4.0  5.0  6.0 \n```", "```py\nIn [72]: data = \"#comment\\na,b,c\\nA,B,C\\n1,2,3\"\n\nIn [73]: pd.read_csv(StringIO(data), comment=\"#\", header=1)\nOut[73]: \n A  B  C\n0  1  2  3\n\nIn [74]: data = \"A,B,C\\n#comment\\na,b,c\\n1,2,3\"\n\nIn [75]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=2)\nOut[75]: \n a  b  c\n0  1  2  3 \n```", "```py\nIn [76]: data = (\n ....:    \"# empty\\n\"\n ....:    \"# second empty line\\n\"\n ....:    \"# third emptyline\\n\"\n ....:    \"X,Y,Z\\n\"\n ....:    \"1,2,3\\n\"\n ....:    \"A,B,C\\n\"\n ....:    \"1,2.,4.\\n\"\n ....:    \"5.,NaN,10.0\\n\"\n ....: )\n ....: \n\nIn [77]: print(data)\n# empty\n# second empty line\n# third emptyline\nX,Y,Z\n1,2,3\nA,B,C\n1,2.,4.\n5.,NaN,10.0\n\nIn [78]: pd.read_csv(StringIO(data), comment=\"#\", skiprows=4, header=1)\nOut[78]: \n A    B     C\n0  1.0  2.0   4.0\n1  5.0  NaN  10.0 \n```", "```py\nIn [79]: data = (\n ....:    \"ID,level,category\\n\"\n ....:    \"Patient1,123000,x # really unpleasant\\n\"\n ....:    \"Patient2,23000,y # wouldn't take his medicine\\n\"\n ....:    \"Patient3,1234018,z # awesome\"\n ....: )\n ....: \n\nIn [80]: with open(\"tmp.csv\", \"w\") as fh:\n ....:    fh.write(data)\n ....: \n\nIn [81]: print(open(\"tmp.csv\").read())\nID,level,category\nPatient1,123000,x # really unpleasant\nPatient2,23000,y # wouldn't take his medicine\nPatient3,1234018,z # awesome \n```", "```py\nIn [82]: df = pd.read_csv(\"tmp.csv\")\n\nIn [83]: df\nOut[83]: \n ID    level                        category\n0  Patient1   123000           x # really unpleasant\n1  Patient2    23000  y # wouldn't take his medicine\n2  Patient3  1234018                     z # awesome \n```", "```py\nIn [84]: df = pd.read_csv(\"tmp.csv\", comment=\"#\")\n\nIn [85]: df\nOut[85]: \n ID    level category\n0  Patient1   123000       x \n1  Patient2    23000       y \n2  Patient3  1234018       z \n```", "```py\nIn [86]: from io import BytesIO\n\nIn [87]: data = b\"word,length\\n\" b\"Tr\\xc3\\xa4umen,7\\n\" b\"Gr\\xc3\\xbc\\xc3\\x9fe,5\"\n\nIn [88]: data = data.decode(\"utf8\").encode(\"latin-1\")\n\nIn [89]: df = pd.read_csv(BytesIO(data), encoding=\"latin-1\")\n\nIn [90]: df\nOut[90]: \n word  length\n0  Tr\u00e4umen       7\n1    Gr\u00fc\u00dfe       5\n\nIn [91]: df[\"word\"][1]\nOut[91]: 'Gr\u00fc\u00dfe' \n```", "```py\nIn [92]: data = \"a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [93]: pd.read_csv(StringIO(data))\nOut[93]: \n a    b     c\n4   apple  bat   5.7\n8  orange  cow  10.0 \n```", "```py\nIn [94]: data = \"index,a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\n\nIn [95]: pd.read_csv(StringIO(data), index_col=0)\nOut[95]: \n a    b     c\nindex \n4       apple  bat   5.7\n8      orange  cow  10.0 \n```", "```py\nIn [96]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [97]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [98]: pd.read_csv(StringIO(data))\nOut[98]: \n a    b   c\n4   apple  bat NaN\n8  orange  cow NaN\n\nIn [99]: pd.read_csv(StringIO(data), index_col=False)\nOut[99]: \n a       b    c\n0  4   apple  bat\n1  8  orange  cow \n```", "```py\nIn [100]: data = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\n\nIn [101]: print(data)\na,b,c\n4,apple,bat,\n8,orange,cow,\n\nIn [102]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"])\nOut[102]: \n b   c\n4  bat NaN\n8  cow NaN\n\nIn [103]: pd.read_csv(StringIO(data), usecols=[\"b\", \"c\"], index_col=0)\nOut[103]: \n b   c\n4  bat NaN\n8  cow NaN \n```", "```py\nIn [104]: with open(\"foo.csv\", mode=\"w\") as f:\n .....:    f.write(\"date,A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\")\n .....: \n\n# Use a column as an index, and parse it as dates.\nIn [105]: df = pd.read_csv(\"foo.csv\", index_col=0, parse_dates=True)\n\nIn [106]: df\nOut[106]: \n A  B  C\ndate \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5\n\n# These are Python datetime objects\nIn [107]: df.index\nOut[107]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', name='date', freq=None) \n```", "```py\nIn [108]: data = (\n .....:    \"KORD,19990127, 19:00:00, 18:56:00, 0.8100\\n\"\n .....:    \"KORD,19990127, 20:00:00, 19:56:00, 0.0100\\n\"\n .....:    \"KORD,19990127, 21:00:00, 20:56:00, -0.5900\\n\"\n .....:    \"KORD,19990127, 21:00:00, 21:18:00, -0.9900\\n\"\n .....:    \"KORD,19990127, 22:00:00, 21:56:00, -0.5900\\n\"\n .....:    \"KORD,19990127, 23:00:00, 22:56:00, -0.5900\"\n .....: )\n .....: \n\nIn [109]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [110]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]])\n\nIn [111]: df\nOut[111]: \n 1_2                 1_3     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [112]: df = pd.read_csv(\n .....:    \"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]], keep_date_col=True\n .....: )\n .....: \n\nIn [113]: df\nOut[113]: \n 1_2                 1_3     0  ...          2          3     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  ...   19:00:00   18:56:00  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  ...   20:00:00   19:56:00  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD  ...   21:00:00   20:56:00 -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD  ...   21:00:00   21:18:00 -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD  ...   22:00:00   21:56:00 -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD  ...   23:00:00   22:56:00 -0.59\n\n[6 rows x 7 columns] \n```", "```py\nIn [114]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [115]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=date_spec)\n\nIn [116]: df\nOut[116]: \n nominal              actual     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [117]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [118]: df = pd.read_csv(\n .....:    \"tmp.csv\", header=None, parse_dates=date_spec, index_col=0\n .....: )  # index is the nominal column\n .....: \n\nIn [119]: df\nOut[119]: \n actual     0     4\nnominal \n1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [120]: content = \"\"\"\\\n .....: a\n .....: 2000-01-01T00:00:00+05:00\n .....: 2000-01-01T00:00:00+06:00\"\"\"\n .....: \n\nIn [121]: df = pd.read_csv(StringIO(content))\n\nIn [122]: df[\"a\"] = pd.to_datetime(df[\"a\"], utc=True)\n\nIn [123]: df[\"a\"]\nOut[123]: \n0   1999-12-31 19:00:00+00:00\n1   1999-12-31 18:00:00+00:00\nName: a, dtype: datetime64[ns, UTC] \n```", "```py\nIn [124]: df = pd.read_csv(\n .....:    \"foo.csv\",\n .....:    index_col=0,\n .....:    parse_dates=True,\n .....: )\n .....: \n\nIn [125]: df\nOut[125]: \n A  B  C\ndate \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5 \n```", "```py\nIn [126]: data = StringIO(\"date\\n12 Jan 2000\\n2000-01-13\\n\")\n\nIn [127]: df = pd.read_csv(data)\n\nIn [128]: df['date'] = pd.to_datetime(df['date'], format='mixed')\n\nIn [129]: df\nOut[129]: \n date\n0 2000-01-12\n1 2000-01-13 \n```", "```py\nIn [130]: data = StringIO(\"date\\n2020-01-01\\n2020-01-01 03:00\\n\")\n\nIn [131]: df = pd.read_csv(data)\n\nIn [132]: df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n\nIn [133]: df\nOut[133]: \n date\n0 2020-01-01 00:00:00\n1 2020-01-01 03:00:00 \n```", "```py\nIn [134]: data = \"date,value,cat\\n1/6/2000,5,a\\n2/6/2000,10,b\\n3/6/2000,15,c\"\n\nIn [135]: print(data)\ndate,value,cat\n1/6/2000,5,a\n2/6/2000,10,b\n3/6/2000,15,c\n\nIn [136]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [137]: pd.read_csv(\"tmp.csv\", parse_dates=[0])\nOut[137]: \n date  value cat\n0 2000-01-06      5   a\n1 2000-02-06     10   b\n2 2000-03-06     15   c\n\nIn [138]: pd.read_csv(\"tmp.csv\", dayfirst=True, parse_dates=[0])\nOut[138]: \n date  value cat\n0 2000-06-01      5   a\n1 2000-06-02     10   b\n2 2000-06-03     15   c \n```", "```py\nIn [139]: import io\n\nIn [140]: data = pd.DataFrame([0, 1, 2])\n\nIn [141]: buffer = io.BytesIO()\n\nIn [142]: data.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\") \n```", "```py\nIn [104]: with open(\"foo.csv\", mode=\"w\") as f:\n .....:    f.write(\"date,A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\")\n .....: \n\n# Use a column as an index, and parse it as dates.\nIn [105]: df = pd.read_csv(\"foo.csv\", index_col=0, parse_dates=True)\n\nIn [106]: df\nOut[106]: \n A  B  C\ndate \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5\n\n# These are Python datetime objects\nIn [107]: df.index\nOut[107]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', name='date', freq=None) \n```", "```py\nIn [108]: data = (\n .....:    \"KORD,19990127, 19:00:00, 18:56:00, 0.8100\\n\"\n .....:    \"KORD,19990127, 20:00:00, 19:56:00, 0.0100\\n\"\n .....:    \"KORD,19990127, 21:00:00, 20:56:00, -0.5900\\n\"\n .....:    \"KORD,19990127, 21:00:00, 21:18:00, -0.9900\\n\"\n .....:    \"KORD,19990127, 22:00:00, 21:56:00, -0.5900\\n\"\n .....:    \"KORD,19990127, 23:00:00, 22:56:00, -0.5900\"\n .....: )\n .....: \n\nIn [109]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [110]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]])\n\nIn [111]: df\nOut[111]: \n 1_2                 1_3     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [112]: df = pd.read_csv(\n .....:    \"tmp.csv\", header=None, parse_dates=[[1, 2], [1, 3]], keep_date_col=True\n .....: )\n .....: \n\nIn [113]: df\nOut[113]: \n 1_2                 1_3     0  ...          2          3     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  ...   19:00:00   18:56:00  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  ...   20:00:00   19:56:00  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD  ...   21:00:00   20:56:00 -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD  ...   21:00:00   21:18:00 -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD  ...   22:00:00   21:56:00 -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD  ...   23:00:00   22:56:00 -0.59\n\n[6 rows x 7 columns] \n```", "```py\nIn [114]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [115]: df = pd.read_csv(\"tmp.csv\", header=None, parse_dates=date_spec)\n\nIn [116]: df\nOut[116]: \n nominal              actual     0     4\n0 1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1 1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n2 1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n3 1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n4 1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n5 1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [117]: date_spec = {\"nominal\": [1, 2], \"actual\": [1, 3]}\n\nIn [118]: df = pd.read_csv(\n .....:    \"tmp.csv\", header=None, parse_dates=date_spec, index_col=0\n .....: )  # index is the nominal column\n .....: \n\nIn [119]: df\nOut[119]: \n actual     0     4\nnominal \n1999-01-27 19:00:00 1999-01-27 18:56:00  KORD  0.81\n1999-01-27 20:00:00 1999-01-27 19:56:00  KORD  0.01\n1999-01-27 21:00:00 1999-01-27 20:56:00  KORD -0.59\n1999-01-27 21:00:00 1999-01-27 21:18:00  KORD -0.99\n1999-01-27 22:00:00 1999-01-27 21:56:00  KORD -0.59\n1999-01-27 23:00:00 1999-01-27 22:56:00  KORD -0.59 \n```", "```py\nIn [120]: content = \"\"\"\\\n .....: a\n .....: 2000-01-01T00:00:00+05:00\n .....: 2000-01-01T00:00:00+06:00\"\"\"\n .....: \n\nIn [121]: df = pd.read_csv(StringIO(content))\n\nIn [122]: df[\"a\"] = pd.to_datetime(df[\"a\"], utc=True)\n\nIn [123]: df[\"a\"]\nOut[123]: \n0   1999-12-31 19:00:00+00:00\n1   1999-12-31 18:00:00+00:00\nName: a, dtype: datetime64[ns, UTC] \n```", "```py\nIn [124]: df = pd.read_csv(\n .....:    \"foo.csv\",\n .....:    index_col=0,\n .....:    parse_dates=True,\n .....: )\n .....: \n\nIn [125]: df\nOut[125]: \n A  B  C\ndate \n2009-01-01  a  1  2\n2009-01-02  b  3  4\n2009-01-03  c  4  5 \n```", "```py\nIn [126]: data = StringIO(\"date\\n12 Jan 2000\\n2000-01-13\\n\")\n\nIn [127]: df = pd.read_csv(data)\n\nIn [128]: df['date'] = pd.to_datetime(df['date'], format='mixed')\n\nIn [129]: df\nOut[129]: \n date\n0 2000-01-12\n1 2000-01-13 \n```", "```py\nIn [130]: data = StringIO(\"date\\n2020-01-01\\n2020-01-01 03:00\\n\")\n\nIn [131]: df = pd.read_csv(data)\n\nIn [132]: df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n\nIn [133]: df\nOut[133]: \n date\n0 2020-01-01 00:00:00\n1 2020-01-01 03:00:00 \n```", "```py\nIn [134]: data = \"date,value,cat\\n1/6/2000,5,a\\n2/6/2000,10,b\\n3/6/2000,15,c\"\n\nIn [135]: print(data)\ndate,value,cat\n1/6/2000,5,a\n2/6/2000,10,b\n3/6/2000,15,c\n\nIn [136]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [137]: pd.read_csv(\"tmp.csv\", parse_dates=[0])\nOut[137]: \n date  value cat\n0 2000-01-06      5   a\n1 2000-02-06     10   b\n2 2000-03-06     15   c\n\nIn [138]: pd.read_csv(\"tmp.csv\", dayfirst=True, parse_dates=[0])\nOut[138]: \n date  value cat\n0 2000-06-01      5   a\n1 2000-06-02     10   b\n2 2000-06-03     15   c \n```", "```py\nIn [139]: import io\n\nIn [140]: data = pd.DataFrame([0, 1, 2])\n\nIn [141]: buffer = io.BytesIO()\n\nIn [142]: data.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\") \n```", "```py\nIn [143]: val = \"0.3066101993807095471566981359501369297504425048828125\"\n\nIn [144]: data = \"a,b,c\\n1,2,{0}\".format(val)\n\nIn [145]: abs(\n .....:    pd.read_csv(\n .....:        StringIO(data),\n .....:        engine=\"c\",\n .....:        float_precision=None,\n .....:    )[\"c\"][0] - float(val)\n .....: )\n .....: \nOut[145]: 5.551115123125783e-17\n\nIn [146]: abs(\n .....:    pd.read_csv(\n .....:        StringIO(data),\n .....:        engine=\"c\",\n .....:        float_precision=\"high\",\n .....:    )[\"c\"][0] - float(val)\n .....: )\n .....: \nOut[146]: 5.551115123125783e-17\n\nIn [147]: abs(\n .....:    pd.read_csv(StringIO(data), engine=\"c\", float_precision=\"round_trip\")[\"c\"][0]\n .....:    - float(val)\n .....: )\n .....: \nOut[147]: 0.0 \n```", "```py\nIn [148]: data = (\n .....:    \"ID|level|category\\n\"\n .....:    \"Patient1|123,000|x\\n\"\n .....:    \"Patient2|23,000|y\\n\"\n .....:    \"Patient3|1,234,018|z\"\n .....: )\n .....: \n\nIn [149]: with open(\"tmp.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [150]: df = pd.read_csv(\"tmp.csv\", sep=\"|\")\n\nIn [151]: df\nOut[151]: \n ID      level category\n0  Patient1    123,000        x\n1  Patient2     23,000        y\n2  Patient3  1,234,018        z\n\nIn [152]: df.level.dtype\nOut[152]: dtype('O') \n```", "```py\nIn [153]: df = pd.read_csv(\"tmp.csv\", sep=\"|\", thousands=\",\")\n\nIn [154]: df\nOut[154]: \n ID    level category\n0  Patient1   123000        x\n1  Patient2    23000        y\n2  Patient3  1234018        z\n\nIn [155]: df.level.dtype\nOut[155]: dtype('int64') \n```", "```py\npd.read_csv(\"path_to_file.csv\", na_values=[5]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"\"]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"NA\", \"0\"]) \n```", "```py\npd.read_csv(\"path_to_file.csv\", na_values=[\"Nope\"]) \n```", "```py\nIn [156]: data = \"a,b,c\\n1,Yes,2\\n3,No,4\"\n\nIn [157]: print(data)\na,b,c\n1,Yes,2\n3,No,4\n\nIn [158]: pd.read_csv(StringIO(data))\nOut[158]: \n a    b  c\n0  1  Yes  2\n1  3   No  4\n\nIn [159]: pd.read_csv(StringIO(data), true_values=[\"Yes\"], false_values=[\"No\"])\nOut[159]: \n a      b  c\n0  1   True  2\n1  3  False  4 \n```", "```py\nIn [160]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [161]: pd.read_csv(StringIO(data))\n---------------------------------------------------------------------------\nParserError  Traceback (most recent call last)\nCell In[161], line 1\n----> 1 pd.read_csv(StringIO(data))\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:626, in _read(filepath_or_buffer, kwds)\n  623     return parser\n  625 with parser:\n--> 626     return parser.read(nrows)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1923, in TextFileReader.read(self, nrows)\n  1916 nrows = validate_integer(\"nrows\", nrows)\n  1917 try:\n  1918     # error: \"ParserBase\" has no attribute \"read\"\n  1919     (\n  1920         index,\n  1921         columns,\n  1922         col_dict,\n-> 1923     ) = self._engine.read(  # type: ignore[attr-defined]\n  1924         nrows\n  1925     )\n  1926 except Exception:\n  1927     self.close()\n\nFile ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:234, in CParserWrapper.read(self, nrows)\n  232 try:\n  233     if self.low_memory:\n--> 234         chunks = self._reader.read_low_memory(nrows)\n  235         # destructive to chunks\n  236         data = _concatenate_chunks(chunks)\n\nFile parsers.pyx:838, in pandas._libs.parsers.TextReader.read_low_memory()\n\nFile parsers.pyx:905, in pandas._libs.parsers.TextReader._read_rows()\n\nFile parsers.pyx:874, in pandas._libs.parsers.TextReader._tokenize_rows()\n\nFile parsers.pyx:891, in pandas._libs.parsers.TextReader._check_tokenize_status()\n\nFile parsers.pyx:2061, in pandas._libs.parsers.raise_parser_error()\n\nParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4 \n```", "```py\nIn [162]: data = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\n\nIn [163]: pd.read_csv(StringIO(data), on_bad_lines=\"skip\")\nOut[163]: \n a  b   c\n0  1  2   3\n1  8  9  10 \n```", "```py\nIn [164]: external_list = []\n\nIn [165]: def bad_lines_func(line):\n .....:    external_list.append(line)\n .....:    return line[-3:]\n .....: \n\nIn [166]: external_list\nOut[166]: [] \n```", "```py\nIn [167]: bad_lines_func = lambda line: print(line)\n\nIn [168]: data = 'name,type\\nname a,a is of type a\\nname b,\"b\\\" is of type b\"'\n\nIn [169]: data\nOut[169]: 'name,type\\nname a,a is of type a\\nname b,\"b\" is of type b\"'\n\nIn [170]: pd.read_csv(StringIO(data), on_bad_lines=bad_lines_func, engine=\"python\")\nOut[170]: \n name            type\n0  name a  a is of type a \n```", "```py\nIn [171]: pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n---------------------------------------------------------------------------\nValueError  Traceback (most recent call last)\nCell In[171], line 1\n----> 1 pd.read_csv(StringIO(data), usecols=[0, 1, 2])\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n  617 _validate_names(kwds.get(\"names\", None))\n  619 # Create the parser.\n--> 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n  622 if chunksize or iterator:\n  623     return parser\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n  1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n  1619 self.handles: IOHandles | None = None\n-> 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1898, in TextFileReader._make_engine(self, f, engine)\n  1895     raise ValueError(msg)\n  1897 try:\n-> 1898     return mapping[engine](f, **self.options)\n  1899 except Exception:\n  1900     if self.handles is not None:\n\nFile ~/work/pandas/pandas/pandas/io/parsers/c_parser_wrapper.py:155, in CParserWrapper.__init__(self, src, **kwds)\n  152     # error: Cannot determine type of 'names'\n  153     if len(self.names) < len(usecols):  # type: ignore[has-type]\n  154         # error: Cannot determine type of 'names'\n--> 155         self._validate_usecols_names(\n  156             usecols,\n  157             self.names,  # type: ignore[has-type]\n  158         )\n  160 # error: Cannot determine type of 'names'\n  161 self._validate_parse_dates_presence(self.names)  # type: ignore[has-type]\n\nFile ~/work/pandas/pandas/pandas/io/parsers/base_parser.py:979, in ParserBase._validate_usecols_names(self, usecols, names)\n  977 missing = [c for c in usecols if c not in names]\n  978 if len(missing) > 0:\n--> 979     raise ValueError(\n  980         f\"Usecols do not match columns, columns expected but not found: \"\n  981         f\"{missing}\"\n  982     )\n  984 return usecols\n\nValueError: Usecols do not match columns, columns expected but not found: [0, 1, 2] \n```", "```py\nIn [172]: pd.read_csv(StringIO(data), names=['a', 'b', 'c', 'd'])\nOut[172]: \n a                b   c   d\n0    name             type NaN NaN\n1  name a   a is of type a NaN NaN\n2  name b  b is of type b\" NaN NaN \n```", "```py\nIn [173]: data = \"label1,label2,label3\\n\" 'index1,\"a,c,e\\n' \"index2,b,d,f\"\n\nIn [174]: print(data)\nlabel1,label2,label3\nindex1,\"a,c,e\nindex2,b,d,f \n```", "```py\nIn [175]: import csv\n\nIn [176]: dia = csv.excel()\n\nIn [177]: dia.quoting = csv.QUOTE_NONE\n\nIn [178]: pd.read_csv(StringIO(data), dialect=dia)\nOut[178]: \n label1 label2 label3\nindex1     \"a      c      e\nindex2      b      d      f \n```", "```py\nIn [179]: data = \"a,b,c~1,2,3~4,5,6\"\n\nIn [180]: pd.read_csv(StringIO(data), lineterminator=\"~\")\nOut[180]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [181]: data = \"a, b, c\\n1, 2, 3\\n4, 5, 6\"\n\nIn [182]: print(data)\na, b, c\n1, 2, 3\n4, 5, 6\n\nIn [183]: pd.read_csv(StringIO(data), skipinitialspace=True)\nOut[183]: \n a  b  c\n0  1  2  3\n1  4  5  6 \n```", "```py\nIn [184]: data = 'a,b\\n\"hello, \\\\\"Bob\\\\\", nice to see you\",5'\n\nIn [185]: print(data)\na,b\n\"hello, \\\"Bob\\\", nice to see you\",5\n\nIn [186]: pd.read_csv(StringIO(data), escapechar=\"\\\\\")\nOut[186]: \n a  b\n0  hello, \"Bob\", nice to see you  5 \n```", "```py\nIn [187]: data1 = (\n .....:    \"id8141    360.242940   149.910199   11950.7\\n\"\n .....:    \"id1594    444.953632   166.985655   11788.4\\n\"\n .....:    \"id1849    364.136849   183.628767   11806.2\\n\"\n .....:    \"id1230    413.836124   184.375703   11916.8\\n\"\n .....:    \"id1948    502.953953   173.237159   12468.3\"\n .....: )\n .....: \n\nIn [188]: with open(\"bar.csv\", \"w\") as f:\n .....:    f.write(data1)\n .....: \n```", "```py\n# Column specifications are a list of half-intervals\nIn [189]: colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]\n\nIn [190]: df = pd.read_fwf(\"bar.csv\", colspecs=colspecs, header=None, index_col=0)\n\nIn [191]: df\nOut[191]: \n 1           2        3\n0 \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3 \n```", "```py\n# Widths are a list of integers\nIn [192]: widths = [6, 14, 13, 10]\n\nIn [193]: df = pd.read_fwf(\"bar.csv\", widths=widths, header=None)\n\nIn [194]: df\nOut[194]: \n 0           1           2        3\n0  id8141  360.242940  149.910199  11950.7\n1  id1594  444.953632  166.985655  11788.4\n2  id1849  364.136849  183.628767  11806.2\n3  id1230  413.836124  184.375703  11916.8\n4  id1948  502.953953  173.237159  12468.3 \n```", "```py\nIn [195]: df = pd.read_fwf(\"bar.csv\", header=None, index_col=0)\n\nIn [196]: df\nOut[196]: \n 1           2        3\n0 \nid8141  360.242940  149.910199  11950.7\nid1594  444.953632  166.985655  11788.4\nid1849  364.136849  183.628767  11806.2\nid1230  413.836124  184.375703  11916.8\nid1948  502.953953  173.237159  12468.3 \n```", "```py\nIn [197]: pd.read_fwf(\"bar.csv\", header=None, index_col=0).dtypes\nOut[197]: \n1    float64\n2    float64\n3    float64\ndtype: object\n\nIn [198]: pd.read_fwf(\"bar.csv\", header=None, dtype={2: \"object\"}).dtypes\nOut[198]: \n0     object\n1    float64\n2     object\n3    float64\ndtype: object \n```", "```py\nIn [199]: data = \"A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\"\n\nIn [200]: print(data)\nA,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5\n\nIn [201]: with open(\"foo.csv\", \"w\") as f:\n .....:    f.write(data)\n .....: \n```", "```py\nIn [202]: pd.read_csv(\"foo.csv\")\nOut[202]: \n A  B  C\n20090101  a  1  2\n20090102  b  3  4\n20090103  c  4  5 \n```", "```py\nIn [203]: df = pd.read_csv(\"foo.csv\", parse_dates=True)\n\nIn [204]: df.index\nOut[204]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [205]: data = 'year,indiv,zit,xit\\n1977,\"A\",1.2,.6\\n1977,\"B\",1.5,.5'\n\nIn [206]: print(data)\nyear,indiv,zit,xit\n1977,\"A\",1.2,.6\n1977,\"B\",1.5,.5\n\nIn [207]: with open(\"mindex_ex.csv\", mode=\"w\") as f:\n .....:    f.write(data)\n .....: \n```", "```py\nIn [208]: df = pd.read_csv(\"mindex_ex.csv\", index_col=[0, 1])\n\nIn [209]: df\nOut[209]: \n zit  xit\nyear indiv \n1977 A      1.2  0.6\n B      1.5  0.5\n\nIn [210]: df.loc[1977]\nOut[210]: \n zit  xit\nindiv \nA      1.2  0.6\nB      1.5  0.5 \n```", "```py\nIn [211]: mi_idx = pd.MultiIndex.from_arrays([[1, 2, 3, 4], list(\"abcd\")], names=list(\"ab\"))\n\nIn [212]: mi_col = pd.MultiIndex.from_arrays([[1, 2], list(\"ab\")], names=list(\"cd\"))\n\nIn [213]: df = pd.DataFrame(np.ones((4, 2)), index=mi_idx, columns=mi_col)\n\nIn [214]: df.to_csv(\"mi.csv\")\n\nIn [215]: print(open(\"mi.csv\").read())\nc,,1,2\nd,,a,b\na,b,,\n1,a,1.0,1.0\n2,b,1.0,1.0\n3,c,1.0,1.0\n4,d,1.0,1.0\n\nIn [216]: pd.read_csv(\"mi.csv\", header=[0, 1, 2, 3], index_col=[0, 1])\nOut[216]: \nc                    1                  2\nd                    a                  b\na   Unnamed: 2_level_2 Unnamed: 3_level_2\n1                  1.0                1.0\n2 b                1.0                1.0\n3 c                1.0                1.0\n4 d                1.0                1.0 \n```", "```py\nIn [217]: data = \",a,a,a,b,c,c\\n,q,r,s,t,u,v\\none,1,2,3,4,5,6\\ntwo,7,8,9,10,11,12\"\n\nIn [218]: print(data)\n,a,a,a,b,c,c\n,q,r,s,t,u,v\none,1,2,3,4,5,6\ntwo,7,8,9,10,11,12\n\nIn [219]: with open(\"mi2.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [220]: pd.read_csv(\"mi2.csv\", header=[0, 1], index_col=0)\nOut[220]: \n a         b   c \n q  r  s   t   u   v\none  1  2  3   4   5   6\ntwo  7  8  9  10  11  12 \n```", "```py\nIn [199]: data = \"A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\"\n\nIn [200]: print(data)\nA,B,C\n20090101,a,1,2\n20090102,b,3,4\n20090103,c,4,5\n\nIn [201]: with open(\"foo.csv\", \"w\") as f:\n .....:    f.write(data)\n .....: \n```", "```py\nIn [202]: pd.read_csv(\"foo.csv\")\nOut[202]: \n A  B  C\n20090101  a  1  2\n20090102  b  3  4\n20090103  c  4  5 \n```", "```py\nIn [203]: df = pd.read_csv(\"foo.csv\", parse_dates=True)\n\nIn [204]: df.index\nOut[204]: DatetimeIndex(['2009-01-01', '2009-01-02', '2009-01-03'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [205]: data = 'year,indiv,zit,xit\\n1977,\"A\",1.2,.6\\n1977,\"B\",1.5,.5'\n\nIn [206]: print(data)\nyear,indiv,zit,xit\n1977,\"A\",1.2,.6\n1977,\"B\",1.5,.5\n\nIn [207]: with open(\"mindex_ex.csv\", mode=\"w\") as f:\n .....:    f.write(data)\n .....: \n```", "```py\nIn [208]: df = pd.read_csv(\"mindex_ex.csv\", index_col=[0, 1])\n\nIn [209]: df\nOut[209]: \n zit  xit\nyear indiv \n1977 A      1.2  0.6\n B      1.5  0.5\n\nIn [210]: df.loc[1977]\nOut[210]: \n zit  xit\nindiv \nA      1.2  0.6\nB      1.5  0.5 \n```", "```py\nIn [211]: mi_idx = pd.MultiIndex.from_arrays([[1, 2, 3, 4], list(\"abcd\")], names=list(\"ab\"))\n\nIn [212]: mi_col = pd.MultiIndex.from_arrays([[1, 2], list(\"ab\")], names=list(\"cd\"))\n\nIn [213]: df = pd.DataFrame(np.ones((4, 2)), index=mi_idx, columns=mi_col)\n\nIn [214]: df.to_csv(\"mi.csv\")\n\nIn [215]: print(open(\"mi.csv\").read())\nc,,1,2\nd,,a,b\na,b,,\n1,a,1.0,1.0\n2,b,1.0,1.0\n3,c,1.0,1.0\n4,d,1.0,1.0\n\nIn [216]: pd.read_csv(\"mi.csv\", header=[0, 1, 2, 3], index_col=[0, 1])\nOut[216]: \nc                    1                  2\nd                    a                  b\na   Unnamed: 2_level_2 Unnamed: 3_level_2\n1                  1.0                1.0\n2 b                1.0                1.0\n3 c                1.0                1.0\n4 d                1.0                1.0 \n```", "```py\nIn [217]: data = \",a,a,a,b,c,c\\n,q,r,s,t,u,v\\none,1,2,3,4,5,6\\ntwo,7,8,9,10,11,12\"\n\nIn [218]: print(data)\n,a,a,a,b,c,c\n,q,r,s,t,u,v\none,1,2,3,4,5,6\ntwo,7,8,9,10,11,12\n\nIn [219]: with open(\"mi2.csv\", \"w\") as fh:\n .....:    fh.write(data)\n .....: \n\nIn [220]: pd.read_csv(\"mi2.csv\", header=[0, 1], index_col=0)\nOut[220]: \n a         b   c \n q  r  s   t   u   v\none  1  2  3   4   5   6\ntwo  7  8  9  10  11  12 \n```", "```py\nIn [221]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [222]: df.to_csv(\"tmp2.csv\", sep=\":\", index=False)\n\nIn [223]: pd.read_csv(\"tmp2.csv\", sep=None, engine=\"python\")\nOut[223]: \n 0         1         2         3\n0  0.469112 -0.282863 -1.509059 -1.135632\n1  1.212112 -0.173215  0.119209 -1.044236\n2 -0.861849 -2.104569 -0.494929  1.071804\n3  0.721555 -0.706771 -1.039575  0.271860\n4 -0.424972  0.567020  0.276232 -1.087401\n5 -0.673690  0.113648 -1.478427  0.524988\n6  0.404705  0.577046 -1.715002 -1.039268\n7 -0.370647 -1.157892 -1.344312  0.844885\n8  1.075770 -0.109050  1.643563 -1.469388\n9  0.357021 -0.674600 -1.776904 -0.968914 \n```", "```py\nIn [224]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [225]: df.to_csv(\"tmp.csv\", index=False)\n\nIn [226]: table = pd.read_csv(\"tmp.csv\")\n\nIn [227]: table\nOut[227]: \n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n4  0.410835  0.813850  0.132003 -0.827317\n5 -0.076467 -1.187678  1.130127 -1.436737\n6 -1.413681  1.607920  1.024180  0.569605\n7  0.875906 -2.211372  0.974466 -2.006747\n8 -0.410001 -0.078638  0.545952 -1.219217\n9 -1.226825  0.769804 -1.281247 -0.727707 \n```", "```py\nIn [228]: with pd.read_csv(\"tmp.csv\", chunksize=4) as reader:\n .....:    print(reader)\n .....:    for chunk in reader:\n .....:        print(chunk)\n .....: \n<pandas.io.parsers.readers.TextFileReader object at 0x7ff2e5421db0>\n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n 0         1         2         3\n4  0.410835  0.813850  0.132003 -0.827317\n5 -0.076467 -1.187678  1.130127 -1.436737\n6 -1.413681  1.607920  1.024180  0.569605\n7  0.875906 -2.211372  0.974466 -2.006747\n 0         1         2         3\n8 -0.410001 -0.078638  0.545952 -1.219217\n9 -1.226825  0.769804 -1.281247 -0.727707 \n```", "```py\nIn [229]: with pd.read_csv(\"tmp.csv\", iterator=True) as reader:\n .....:    print(reader.get_chunk(5))\n .....: \n 0         1         2         3\n0 -1.294524  0.413738  0.276662 -0.472035\n1 -0.013960 -0.362543 -0.006154 -0.923061\n2  0.895717  0.805244 -1.206412  2.565646\n3  1.431256  1.340309 -1.170299 -0.226169\n4  0.410835  0.813850  0.132003 -0.827317 \n```", "```py\ndf = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\") \n```", "```py\nheaders = {\"User-Agent\": \"pandas\"}\ndf = pd.read_csv(\n    \"https://download.bls.gov/pub/time.series/cu/cu.item\",\n    sep=\"\\t\",\n    storage_options=headers\n) \n```", "```py\ndf = pd.read_json(\"s3://pandas-test/adatafile.json\") \n```", "```py\nstorage_options = {\"client_kwargs\": {\"endpoint_url\": \"http://127.0.0.1:5555\"}}}\ndf = pd.read_json(\"s3://pandas-test/test-1\", storage_options=storage_options) \n```", "```py\npd.read_csv(\n    \"s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013\"\n    \"-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"anon\": True},\n) \n```", "```py\npd.read_csv(\n    \"simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/\"\n    \"SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"s3\": {\"anon\": True}},\n) \n```", "```py\nIn [230]: dfj = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [231]: json = dfj.to_json()\n\nIn [232]: json\nOut[232]: '{\"A\":{\"0\":-0.1213062281,\"1\":0.6957746499,\"2\":0.9597255933,\"3\":-0.6199759194,\"4\":-0.7323393705},\"B\":{\"0\":-0.0978826728,\"1\":0.3417343559,\"2\":-1.1103361029,\"3\":0.1497483186,\"4\":0.6877383895}}' \n```", "```py\nIn [233]: dfjo = pd.DataFrame(\n .....:    dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),\n .....:    columns=list(\"ABC\"),\n .....:    index=list(\"xyz\"),\n .....: )\n .....: \n\nIn [234]: dfjo\nOut[234]: \n A  B  C\nx  1  4  7\ny  2  5  8\nz  3  6  9\n\nIn [235]: sjo = pd.Series(dict(x=15, y=16, z=17), name=\"D\")\n\nIn [236]: sjo\nOut[236]: \nx    15\ny    16\nz    17\nName: D, dtype: int64 \n```", "```py\nIn [237]: dfjo.to_json(orient=\"columns\")\nOut[237]: '{\"A\":{\"x\":1,\"y\":2,\"z\":3},\"B\":{\"x\":4,\"y\":5,\"z\":6},\"C\":{\"x\":7,\"y\":8,\"z\":9}}'\n\n# Not available for Series \n```", "```py\nIn [238]: dfjo.to_json(orient=\"index\")\nOut[238]: '{\"x\":{\"A\":1,\"B\":4,\"C\":7},\"y\":{\"A\":2,\"B\":5,\"C\":8},\"z\":{\"A\":3,\"B\":6,\"C\":9}}'\n\nIn [239]: sjo.to_json(orient=\"index\")\nOut[239]: '{\"x\":15,\"y\":16,\"z\":17}' \n```", "```py\nIn [240]: dfjo.to_json(orient=\"records\")\nOut[240]: '[{\"A\":1,\"B\":4,\"C\":7},{\"A\":2,\"B\":5,\"C\":8},{\"A\":3,\"B\":6,\"C\":9}]'\n\nIn [241]: sjo.to_json(orient=\"records\")\nOut[241]: '[15,16,17]' \n```", "```py\nIn [242]: dfjo.to_json(orient=\"values\")\nOut[242]: '[[1,4,7],[2,5,8],[3,6,9]]'\n\n# Not available for Series \n```", "```py\nIn [243]: dfjo.to_json(orient=\"split\")\nOut[243]: '{\"columns\":[\"A\",\"B\",\"C\"],\"index\":[\"x\",\"y\",\"z\"],\"data\":[[1,4,7],[2,5,8],[3,6,9]]}'\n\nIn [244]: sjo.to_json(orient=\"split\")\nOut[244]: '{\"name\":\"D\",\"index\":[\"x\",\"y\",\"z\"],\"data\":[15,16,17]}' \n```", "```py\nIn [245]: dfd = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [246]: dfd[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [247]: dfd = dfd.sort_index(axis=1, ascending=False)\n\nIn [248]: json = dfd.to_json(date_format=\"iso\")\n\nIn [249]: json\nOut[249]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000\",\"1\":\"2013-01-01T00:00:00.000\",\"2\":\"2013-01-01T00:00:00.000\",\"3\":\"2013-01-01T00:00:00.000\",\"4\":\"2013-01-01T00:00:00.000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [250]: json = dfd.to_json(date_format=\"iso\", date_unit=\"us\")\n\nIn [251]: json\nOut[251]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000000\",\"1\":\"2013-01-01T00:00:00.000000\",\"2\":\"2013-01-01T00:00:00.000000\",\"3\":\"2013-01-01T00:00:00.000000\",\"4\":\"2013-01-01T00:00:00.000000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [252]: json = dfd.to_json(date_format=\"epoch\", date_unit=\"s\")\n\nIn [253]: json\nOut[253]: '{\"date\":{\"0\":1,\"1\":1,\"2\":1,\"3\":1,\"4\":1},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [254]: dfj2 = dfj.copy()\n\nIn [255]: dfj2[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [256]: dfj2[\"ints\"] = list(range(5))\n\nIn [257]: dfj2[\"bools\"] = True\n\nIn [258]: dfj2.index = pd.date_range(\"20130101\", periods=5)\n\nIn [259]: dfj2.to_json(\"test.json\")\n\nIn [260]: with open(\"test.json\") as fh:\n .....:    print(fh.read())\n .....: \n{\"A\":{\"1356998400000\":-0.1213062281,\"1357084800000\":0.6957746499,\"1357171200000\":0.9597255933,\"1357257600000\":-0.6199759194,\"1357344000000\":-0.7323393705},\"B\":{\"1356998400000\":-0.0978826728,\"1357084800000\":0.3417343559,\"1357171200000\":-1.1103361029,\"1357257600000\":0.1497483186,\"1357344000000\":0.6877383895},\"date\":{\"1356998400000\":1356,\"1357084800000\":1356,\"1357171200000\":1356,\"1357257600000\":1356,\"1357344000000\":1356},\"ints\":{\"1356998400000\":0,\"1357084800000\":1,\"1357171200000\":2,\"1357257600000\":3,\"1357344000000\":4},\"bools\":{\"1356998400000\":true,\"1357084800000\":true,\"1357171200000\":true,\"1357257600000\":true,\"1357344000000\":true}} \n```", "```py\n>>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json()  # raises\nRuntimeError: Unhandled numpy dtype 15 \n```", "```py\nIn [261]: pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)\nOut[261]: '{\"0\":{\"0\":\"(1+0j)\",\"1\":\"(2+0j)\",\"2\":\"(1+2j)\"}}' \n```", "```py\nIn [262]: from io import StringIO\n\nIn [263]: pd.read_json(StringIO(json))\nOut[263]: \n date         B         A\n0     1  0.403310  0.176444\n1     1  0.301624 -0.154951\n2     1 -1.369849 -2.179861\n3     1  1.462696 -0.954208\n4     1 -0.826591 -1.743161 \n```", "```py\nIn [264]: pd.read_json(\"test.json\")\nOut[264]: \n A         B  date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356     0   True\n2013-01-02  0.695775  0.341734  1356     1   True\n2013-01-03  0.959726 -1.110336  1356     2   True\n2013-01-04 -0.619976  0.149748  1356     3   True\n2013-01-05 -0.732339  0.687738  1356     4   True \n```", "```py\nIn [265]: pd.read_json(\"test.json\", dtype=object).dtypes\nOut[265]: \nA        object\nB        object\ndate     object\nints     object\nbools    object\ndtype: object \n```", "```py\nIn [266]: pd.read_json(\"test.json\", dtype={\"A\": \"float32\", \"bools\": \"int8\"}).dtypes\nOut[266]: \nA        float32\nB        float64\ndate       int64\nints       int64\nbools       int8\ndtype: object \n```", "```py\nIn [267]: from io import StringIO\n\nIn [268]: si = pd.DataFrame(\n .....:    np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)]\n .....: )\n .....: \n\nIn [269]: si\nOut[269]: \n 0    1    2    3\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n\nIn [270]: si.index\nOut[270]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [271]: si.columns\nOut[271]: Index([0, 1, 2, 3], dtype='int64')\n\nIn [272]: json = si.to_json()\n\nIn [273]: sij = pd.read_json(StringIO(json), convert_axes=False)\n\nIn [274]: sij\nOut[274]: \n 0  1  2  3\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n3  0  0  0  0\n\nIn [275]: sij.index\nOut[275]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [276]: sij.columns\nOut[276]: Index(['0', '1', '2', '3'], dtype='object') \n```", "```py\nIn [277]: from io import StringIO\n\nIn [278]: json = dfj2.to_json(date_unit=\"ns\")\n\n# Try to parse timestamps as milliseconds -> Won't Work\nIn [279]: dfju = pd.read_json(StringIO(json), date_unit=\"ms\")\n\nIn [280]: dfju\nOut[280]: \n A         B        date  ints  bools\n1356998400000000000 -0.121306 -0.097883  1356998400     0   True\n1357084800000000000  0.695775  0.341734  1356998400     1   True\n1357171200000000000  0.959726 -1.110336  1356998400     2   True\n1357257600000000000 -0.619976  0.149748  1356998400     3   True\n1357344000000000000 -0.732339  0.687738  1356998400     4   True\n\n# Let pandas detect the correct precision\nIn [281]: dfju = pd.read_json(StringIO(json))\n\nIn [282]: dfju\nOut[282]: \n A         B       date  ints  bools\n2013-01-01 -0.121306 -0.097883 2013-01-01     0   True\n2013-01-02  0.695775  0.341734 2013-01-01     1   True\n2013-01-03  0.959726 -1.110336 2013-01-01     2   True\n2013-01-04 -0.619976  0.149748 2013-01-01     3   True\n2013-01-05 -0.732339  0.687738 2013-01-01     4   True\n\n# Or specify that all timestamps are in nanoseconds\nIn [283]: dfju = pd.read_json(StringIO(json), date_unit=\"ns\")\n\nIn [284]: dfju\nOut[284]: \n A         B        date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356998400     0   True\n2013-01-02  0.695775  0.341734  1356998400     1   True\n2013-01-03  0.959726 -1.110336  1356998400     2   True\n2013-01-04 -0.619976  0.149748  1356998400     3   True\n2013-01-05 -0.732339  0.687738  1356998400     4   True \n```", "```py\nIn [285]: data = (\n .....: '{\"a\":{\"0\":1,\"1\":3},\"b\":{\"0\":2.5,\"1\":4.5},\"c\":{\"0\":true,\"1\":false},\"d\":{\"0\":\"a\",\"1\":\"b\"},'\n .....: '\"e\":{\"0\":null,\"1\":6.0},\"f\":{\"0\":null,\"1\":7.5},\"g\":{\"0\":null,\"1\":true},\"h\":{\"0\":null,\"1\":\"a\"},'\n .....: '\"i\":{\"0\":\"12-31-2019\",\"1\":\"12-31-2019\"},\"j\":{\"0\":null,\"1\":null}}'\n .....: )\n .....: \n\nIn [286]: df = pd.read_json(StringIO(data), dtype_backend=\"pyarrow\")\n\nIn [287]: df\nOut[287]: \n a    b      c  d     e     f     g     h           i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA>  12-31-2019  None\n1  3  4.5  False  b     6   7.5  True     a  12-31-2019  None\n\nIn [288]: df.dtypes\nOut[288]: \na     int64[pyarrow]\nb    double[pyarrow]\nc      bool[pyarrow]\nd    string[pyarrow]\ne     int64[pyarrow]\nf    double[pyarrow]\ng      bool[pyarrow]\nh    string[pyarrow]\ni    string[pyarrow]\nj      null[pyarrow]\ndtype: object \n```", "```py\nIn [289]: data = [\n .....:    {\"id\": 1, \"name\": {\"first\": \"Coleen\", \"last\": \"Volk\"}},\n .....:    {\"name\": {\"given\": \"Mark\", \"family\": \"Regner\"}},\n .....:    {\"id\": 2, \"name\": \"Faye Raker\"},\n .....: ]\n .....: \n\nIn [290]: pd.json_normalize(data)\nOut[290]: \n id name.first name.last name.given name.family        name\n0  1.0     Coleen      Volk        NaN         NaN         NaN\n1  NaN        NaN       NaN       Mark      Regner         NaN\n2  2.0        NaN       NaN        NaN         NaN  Faye Raker \n```", "```py\nIn [291]: data = [\n .....:    {\n .....:        \"state\": \"Florida\",\n .....:        \"shortname\": \"FL\",\n .....:        \"info\": {\"governor\": \"Rick Scott\"},\n .....:        \"county\": [\n .....:            {\"name\": \"Dade\", \"population\": 12345},\n .....:            {\"name\": \"Broward\", \"population\": 40000},\n .....:            {\"name\": \"Palm Beach\", \"population\": 60000},\n .....:        ],\n .....:    },\n .....:    {\n .....:        \"state\": \"Ohio\",\n .....:        \"shortname\": \"OH\",\n .....:        \"info\": {\"governor\": \"John Kasich\"},\n .....:        \"county\": [\n .....:            {\"name\": \"Summit\", \"population\": 1234},\n .....:            {\"name\": \"Cuyahoga\", \"population\": 1337},\n .....:        ],\n .....:    },\n .....: ]\n .....: \n\nIn [292]: pd.json_normalize(data, \"county\", [\"state\", \"shortname\", [\"info\", \"governor\"]])\nOut[292]: \n name  population    state shortname info.governor\n0        Dade       12345  Florida        FL    Rick Scott\n1     Broward       40000  Florida        FL    Rick Scott\n2  Palm Beach       60000  Florida        FL    Rick Scott\n3      Summit        1234     Ohio        OH   John Kasich\n4    Cuyahoga        1337     Ohio        OH   John Kasich \n```", "```py\nIn [293]: data = [\n .....:    {\n .....:        \"CreatedBy\": {\"Name\": \"User001\"},\n .....:        \"Lookup\": {\n .....:            \"TextField\": \"Some text\",\n .....:            \"UserField\": {\"Id\": \"ID001\", \"Name\": \"Name001\"},\n .....:        },\n .....:        \"Image\": {\"a\": \"b\"},\n .....:    }\n .....: ]\n .....: \n\nIn [294]: pd.json_normalize(data, max_level=1)\nOut[294]: \n CreatedBy.Name Lookup.TextField                    Lookup.UserField Image.a\n0        User001        Some text  {'Id': 'ID001', 'Name': 'Name001'}       b \n```", "```py\nIn [295]: from io import StringIO\n\nIn [296]: jsonl = \"\"\"\n .....:    {\"a\": 1, \"b\": 2}\n .....:    {\"a\": 3, \"b\": 4}\n .....: \"\"\"\n .....: \n\nIn [297]: df = pd.read_json(StringIO(jsonl), lines=True)\n\nIn [298]: df\nOut[298]: \n a  b\n0  1  2\n1  3  4\n\nIn [299]: df.to_json(orient=\"records\", lines=True)\nOut[299]: '{\"a\":1,\"b\":2}\\n{\"a\":3,\"b\":4}\\n'\n\n# reader is an iterator that returns ``chunksize`` lines each iteration\nIn [300]: with pd.read_json(StringIO(jsonl), lines=True, chunksize=1) as reader:\n .....:    reader\n .....:    for chunk in reader:\n .....:        print(chunk)\n .....: \nEmpty DataFrame\nColumns: []\nIndex: []\n a  b\n0  1  2\n a  b\n1  3  4 \n```", "```py\nIn [301]: from io import BytesIO\n\nIn [302]: df = pd.read_json(BytesIO(jsonl.encode()), lines=True, engine=\"pyarrow\")\n\nIn [303]: df\nOut[303]: \n a  b\n0  1  2\n1  3  4 \n```", "```py\nIn [304]: df = pd.DataFrame(\n .....:    {\n .....:        \"A\": [1, 2, 3],\n .....:        \"B\": [\"a\", \"b\", \"c\"],\n .....:        \"C\": pd.date_range(\"2016-01-01\", freq=\"d\", periods=3),\n .....:    },\n .....:    index=pd.Index(range(3), name=\"idx\"),\n .....: )\n .....: \n\nIn [305]: df\nOut[305]: \n A  B          C\nidx \n0    1  a 2016-01-01\n1    2  b 2016-01-02\n2    3  c 2016-01-03\n\nIn [306]: df.to_json(orient=\"table\", date_format=\"iso\")\nOut[306]: '{\"schema\":{\"fields\":[{\"name\":\"idx\",\"type\":\"integer\"},{\"name\":\"A\",\"type\":\"integer\"},{\"name\":\"B\",\"type\":\"string\"},{\"name\":\"C\",\"type\":\"datetime\"}],\"primaryKey\":[\"idx\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"idx\":0,\"A\":1,\"B\":\"a\",\"C\":\"2016-01-01T00:00:00.000\"},{\"idx\":1,\"A\":2,\"B\":\"b\",\"C\":\"2016-01-02T00:00:00.000\"},{\"idx\":2,\"A\":3,\"B\":\"c\",\"C\":\"2016-01-03T00:00:00.000\"}]}' \n```", "```py\n    In [307]: from pandas.io.json import build_table_schema\n\n    In [308]: s = pd.Series(pd.date_range(\"2016\", periods=4))\n\n    In [309]: build_table_schema(s)\n    Out[309]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'datetime'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [310]: s_tz = pd.Series(pd.date_range(\"2016\", periods=12, tz=\"US/Central\"))\n\n    In [311]: build_table_schema(s_tz)\n    Out[311]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'datetime', 'tz': 'US/Central'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [312]: s_per = pd.Series(1, index=pd.period_range(\"2016\", freq=\"Y-DEC\", periods=4))\n\n    In [313]: build_table_schema(s_per)\n    Out[313]: \n    {'fields': [{'name': 'index', 'type': 'datetime', 'freq': 'YE-DEC'},\n     {'name': 'values', 'type': 'integer'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [314]: s_cat = pd.Series(pd.Categorical([\"a\", \"b\", \"a\"]))\n\n    In [315]: build_table_schema(s_cat)\n    Out[315]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values',\n     'type': 'any',\n     'constraints': {'enum': ['a', 'b']},\n     'ordered': False}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [316]: s_dupe = pd.Series([1, 2], index=[1, 1])\n\n    In [317]: build_table_schema(s_dupe)\n    Out[317]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'integer'}],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [318]: s_multi = pd.Series(1, index=pd.MultiIndex.from_product([(\"a\", \"b\"), (0, 1)]))\n\n    In [319]: build_table_schema(s_multi)\n    Out[319]: \n    {'fields': [{'name': 'level_0', 'type': 'string'},\n     {'name': 'level_1', 'type': 'integer'},\n     {'name': 'values', 'type': 'integer'}],\n     'primaryKey': FrozenList(['level_0', 'level_1']),\n     'pandas_version': '1.4.0'} \n    ```", "```py\nIn [320]: df = pd.DataFrame(\n .....:    {\n .....:        \"foo\": [1, 2, 3, 4],\n .....:        \"bar\": [\"a\", \"b\", \"c\", \"d\"],\n .....:        \"baz\": pd.date_range(\"2018-01-01\", freq=\"d\", periods=4),\n .....:        \"qux\": pd.Categorical([\"a\", \"b\", \"c\", \"c\"]),\n .....:    },\n .....:    index=pd.Index(range(4), name=\"idx\"),\n .....: )\n .....: \n\nIn [321]: df\nOut[321]: \n foo bar        baz qux\nidx \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [322]: df.dtypes\nOut[322]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object\n\nIn [323]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [324]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [325]: new_df\nOut[325]: \n foo bar        baz qux\nidx \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [326]: new_df.dtypes\nOut[326]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object \n```", "```py\nIn [327]: df.index.name = \"index\"\n\nIn [328]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [329]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [330]: print(new_df.index.name)\nNone \n```", "```py\nIn [230]: dfj = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [231]: json = dfj.to_json()\n\nIn [232]: json\nOut[232]: '{\"A\":{\"0\":-0.1213062281,\"1\":0.6957746499,\"2\":0.9597255933,\"3\":-0.6199759194,\"4\":-0.7323393705},\"B\":{\"0\":-0.0978826728,\"1\":0.3417343559,\"2\":-1.1103361029,\"3\":0.1497483186,\"4\":0.6877383895}}' \n```", "```py\nIn [233]: dfjo = pd.DataFrame(\n .....:    dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),\n .....:    columns=list(\"ABC\"),\n .....:    index=list(\"xyz\"),\n .....: )\n .....: \n\nIn [234]: dfjo\nOut[234]: \n A  B  C\nx  1  4  7\ny  2  5  8\nz  3  6  9\n\nIn [235]: sjo = pd.Series(dict(x=15, y=16, z=17), name=\"D\")\n\nIn [236]: sjo\nOut[236]: \nx    15\ny    16\nz    17\nName: D, dtype: int64 \n```", "```py\nIn [237]: dfjo.to_json(orient=\"columns\")\nOut[237]: '{\"A\":{\"x\":1,\"y\":2,\"z\":3},\"B\":{\"x\":4,\"y\":5,\"z\":6},\"C\":{\"x\":7,\"y\":8,\"z\":9}}'\n\n# Not available for Series \n```", "```py\nIn [238]: dfjo.to_json(orient=\"index\")\nOut[238]: '{\"x\":{\"A\":1,\"B\":4,\"C\":7},\"y\":{\"A\":2,\"B\":5,\"C\":8},\"z\":{\"A\":3,\"B\":6,\"C\":9}}'\n\nIn [239]: sjo.to_json(orient=\"index\")\nOut[239]: '{\"x\":15,\"y\":16,\"z\":17}' \n```", "```py\nIn [240]: dfjo.to_json(orient=\"records\")\nOut[240]: '[{\"A\":1,\"B\":4,\"C\":7},{\"A\":2,\"B\":5,\"C\":8},{\"A\":3,\"B\":6,\"C\":9}]'\n\nIn [241]: sjo.to_json(orient=\"records\")\nOut[241]: '[15,16,17]' \n```", "```py\nIn [242]: dfjo.to_json(orient=\"values\")\nOut[242]: '[[1,4,7],[2,5,8],[3,6,9]]'\n\n# Not available for Series \n```", "```py\nIn [243]: dfjo.to_json(orient=\"split\")\nOut[243]: '{\"columns\":[\"A\",\"B\",\"C\"],\"index\":[\"x\",\"y\",\"z\"],\"data\":[[1,4,7],[2,5,8],[3,6,9]]}'\n\nIn [244]: sjo.to_json(orient=\"split\")\nOut[244]: '{\"name\":\"D\",\"index\":[\"x\",\"y\",\"z\"],\"data\":[15,16,17]}' \n```", "```py\nIn [245]: dfd = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [246]: dfd[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [247]: dfd = dfd.sort_index(axis=1, ascending=False)\n\nIn [248]: json = dfd.to_json(date_format=\"iso\")\n\nIn [249]: json\nOut[249]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000\",\"1\":\"2013-01-01T00:00:00.000\",\"2\":\"2013-01-01T00:00:00.000\",\"3\":\"2013-01-01T00:00:00.000\",\"4\":\"2013-01-01T00:00:00.000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [250]: json = dfd.to_json(date_format=\"iso\", date_unit=\"us\")\n\nIn [251]: json\nOut[251]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000000\",\"1\":\"2013-01-01T00:00:00.000000\",\"2\":\"2013-01-01T00:00:00.000000\",\"3\":\"2013-01-01T00:00:00.000000\",\"4\":\"2013-01-01T00:00:00.000000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [252]: json = dfd.to_json(date_format=\"epoch\", date_unit=\"s\")\n\nIn [253]: json\nOut[253]: '{\"date\":{\"0\":1,\"1\":1,\"2\":1,\"3\":1,\"4\":1},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [254]: dfj2 = dfj.copy()\n\nIn [255]: dfj2[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [256]: dfj2[\"ints\"] = list(range(5))\n\nIn [257]: dfj2[\"bools\"] = True\n\nIn [258]: dfj2.index = pd.date_range(\"20130101\", periods=5)\n\nIn [259]: dfj2.to_json(\"test.json\")\n\nIn [260]: with open(\"test.json\") as fh:\n .....:    print(fh.read())\n .....: \n{\"A\":{\"1356998400000\":-0.1213062281,\"1357084800000\":0.6957746499,\"1357171200000\":0.9597255933,\"1357257600000\":-0.6199759194,\"1357344000000\":-0.7323393705},\"B\":{\"1356998400000\":-0.0978826728,\"1357084800000\":0.3417343559,\"1357171200000\":-1.1103361029,\"1357257600000\":0.1497483186,\"1357344000000\":0.6877383895},\"date\":{\"1356998400000\":1356,\"1357084800000\":1356,\"1357171200000\":1356,\"1357257600000\":1356,\"1357344000000\":1356},\"ints\":{\"1356998400000\":0,\"1357084800000\":1,\"1357171200000\":2,\"1357257600000\":3,\"1357344000000\":4},\"bools\":{\"1356998400000\":true,\"1357084800000\":true,\"1357171200000\":true,\"1357257600000\":true,\"1357344000000\":true}} \n```", "```py\n>>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json()  # raises\nRuntimeError: Unhandled numpy dtype 15 \n```", "```py\nIn [261]: pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)\nOut[261]: '{\"0\":{\"0\":\"(1+0j)\",\"1\":\"(2+0j)\",\"2\":\"(1+2j)\"}}' \n```", "```py\nIn [233]: dfjo = pd.DataFrame(\n .....:    dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),\n .....:    columns=list(\"ABC\"),\n .....:    index=list(\"xyz\"),\n .....: )\n .....: \n\nIn [234]: dfjo\nOut[234]: \n A  B  C\nx  1  4  7\ny  2  5  8\nz  3  6  9\n\nIn [235]: sjo = pd.Series(dict(x=15, y=16, z=17), name=\"D\")\n\nIn [236]: sjo\nOut[236]: \nx    15\ny    16\nz    17\nName: D, dtype: int64 \n```", "```py\nIn [237]: dfjo.to_json(orient=\"columns\")\nOut[237]: '{\"A\":{\"x\":1,\"y\":2,\"z\":3},\"B\":{\"x\":4,\"y\":5,\"z\":6},\"C\":{\"x\":7,\"y\":8,\"z\":9}}'\n\n# Not available for Series \n```", "```py\nIn [238]: dfjo.to_json(orient=\"index\")\nOut[238]: '{\"x\":{\"A\":1,\"B\":4,\"C\":7},\"y\":{\"A\":2,\"B\":5,\"C\":8},\"z\":{\"A\":3,\"B\":6,\"C\":9}}'\n\nIn [239]: sjo.to_json(orient=\"index\")\nOut[239]: '{\"x\":15,\"y\":16,\"z\":17}' \n```", "```py\nIn [240]: dfjo.to_json(orient=\"records\")\nOut[240]: '[{\"A\":1,\"B\":4,\"C\":7},{\"A\":2,\"B\":5,\"C\":8},{\"A\":3,\"B\":6,\"C\":9}]'\n\nIn [241]: sjo.to_json(orient=\"records\")\nOut[241]: '[15,16,17]' \n```", "```py\nIn [242]: dfjo.to_json(orient=\"values\")\nOut[242]: '[[1,4,7],[2,5,8],[3,6,9]]'\n\n# Not available for Series \n```", "```py\nIn [243]: dfjo.to_json(orient=\"split\")\nOut[243]: '{\"columns\":[\"A\",\"B\",\"C\"],\"index\":[\"x\",\"y\",\"z\"],\"data\":[[1,4,7],[2,5,8],[3,6,9]]}'\n\nIn [244]: sjo.to_json(orient=\"split\")\nOut[244]: '{\"name\":\"D\",\"index\":[\"x\",\"y\",\"z\"],\"data\":[15,16,17]}' \n```", "```py\nIn [245]: dfd = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\n\nIn [246]: dfd[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [247]: dfd = dfd.sort_index(axis=1, ascending=False)\n\nIn [248]: json = dfd.to_json(date_format=\"iso\")\n\nIn [249]: json\nOut[249]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000\",\"1\":\"2013-01-01T00:00:00.000\",\"2\":\"2013-01-01T00:00:00.000\",\"3\":\"2013-01-01T00:00:00.000\",\"4\":\"2013-01-01T00:00:00.000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [250]: json = dfd.to_json(date_format=\"iso\", date_unit=\"us\")\n\nIn [251]: json\nOut[251]: '{\"date\":{\"0\":\"2013-01-01T00:00:00.000000\",\"1\":\"2013-01-01T00:00:00.000000\",\"2\":\"2013-01-01T00:00:00.000000\",\"3\":\"2013-01-01T00:00:00.000000\",\"4\":\"2013-01-01T00:00:00.000000\"},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [252]: json = dfd.to_json(date_format=\"epoch\", date_unit=\"s\")\n\nIn [253]: json\nOut[253]: '{\"date\":{\"0\":1,\"1\":1,\"2\":1,\"3\":1,\"4\":1},\"B\":{\"0\":0.403309524,\"1\":0.3016244523,\"2\":-1.3698493577,\"3\":1.4626960492,\"4\":-0.8265909164},\"A\":{\"0\":0.1764443426,\"1\":-0.1549507744,\"2\":-2.1798606054,\"3\":-0.9542078401,\"4\":-1.7431609117}}' \n```", "```py\nIn [254]: dfj2 = dfj.copy()\n\nIn [255]: dfj2[\"date\"] = pd.Timestamp(\"20130101\")\n\nIn [256]: dfj2[\"ints\"] = list(range(5))\n\nIn [257]: dfj2[\"bools\"] = True\n\nIn [258]: dfj2.index = pd.date_range(\"20130101\", periods=5)\n\nIn [259]: dfj2.to_json(\"test.json\")\n\nIn [260]: with open(\"test.json\") as fh:\n .....:    print(fh.read())\n .....: \n{\"A\":{\"1356998400000\":-0.1213062281,\"1357084800000\":0.6957746499,\"1357171200000\":0.9597255933,\"1357257600000\":-0.6199759194,\"1357344000000\":-0.7323393705},\"B\":{\"1356998400000\":-0.0978826728,\"1357084800000\":0.3417343559,\"1357171200000\":-1.1103361029,\"1357257600000\":0.1497483186,\"1357344000000\":0.6877383895},\"date\":{\"1356998400000\":1356,\"1357084800000\":1356,\"1357171200000\":1356,\"1357257600000\":1356,\"1357344000000\":1356},\"ints\":{\"1356998400000\":0,\"1357084800000\":1,\"1357171200000\":2,\"1357257600000\":3,\"1357344000000\":4},\"bools\":{\"1356998400000\":true,\"1357084800000\":true,\"1357171200000\":true,\"1357257600000\":true,\"1357344000000\":true}} \n```", "```py\n>>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json()  # raises\nRuntimeError: Unhandled numpy dtype 15 \n```", "```py\nIn [261]: pd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)\nOut[261]: '{\"0\":{\"0\":\"(1+0j)\",\"1\":\"(2+0j)\",\"2\":\"(1+2j)\"}}' \n```", "```py\nIn [262]: from io import StringIO\n\nIn [263]: pd.read_json(StringIO(json))\nOut[263]: \n date         B         A\n0     1  0.403310  0.176444\n1     1  0.301624 -0.154951\n2     1 -1.369849 -2.179861\n3     1  1.462696 -0.954208\n4     1 -0.826591 -1.743161 \n```", "```py\nIn [264]: pd.read_json(\"test.json\")\nOut[264]: \n A         B  date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356     0   True\n2013-01-02  0.695775  0.341734  1356     1   True\n2013-01-03  0.959726 -1.110336  1356     2   True\n2013-01-04 -0.619976  0.149748  1356     3   True\n2013-01-05 -0.732339  0.687738  1356     4   True \n```", "```py\nIn [265]: pd.read_json(\"test.json\", dtype=object).dtypes\nOut[265]: \nA        object\nB        object\ndate     object\nints     object\nbools    object\ndtype: object \n```", "```py\nIn [266]: pd.read_json(\"test.json\", dtype={\"A\": \"float32\", \"bools\": \"int8\"}).dtypes\nOut[266]: \nA        float32\nB        float64\ndate       int64\nints       int64\nbools       int8\ndtype: object \n```", "```py\nIn [267]: from io import StringIO\n\nIn [268]: si = pd.DataFrame(\n .....:    np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)]\n .....: )\n .....: \n\nIn [269]: si\nOut[269]: \n 0    1    2    3\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n\nIn [270]: si.index\nOut[270]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [271]: si.columns\nOut[271]: Index([0, 1, 2, 3], dtype='int64')\n\nIn [272]: json = si.to_json()\n\nIn [273]: sij = pd.read_json(StringIO(json), convert_axes=False)\n\nIn [274]: sij\nOut[274]: \n 0  1  2  3\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n3  0  0  0  0\n\nIn [275]: sij.index\nOut[275]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [276]: sij.columns\nOut[276]: Index(['0', '1', '2', '3'], dtype='object') \n```", "```py\nIn [277]: from io import StringIO\n\nIn [278]: json = dfj2.to_json(date_unit=\"ns\")\n\n# Try to parse timestamps as milliseconds -> Won't Work\nIn [279]: dfju = pd.read_json(StringIO(json), date_unit=\"ms\")\n\nIn [280]: dfju\nOut[280]: \n A         B        date  ints  bools\n1356998400000000000 -0.121306 -0.097883  1356998400     0   True\n1357084800000000000  0.695775  0.341734  1356998400     1   True\n1357171200000000000  0.959726 -1.110336  1356998400     2   True\n1357257600000000000 -0.619976  0.149748  1356998400     3   True\n1357344000000000000 -0.732339  0.687738  1356998400     4   True\n\n# Let pandas detect the correct precision\nIn [281]: dfju = pd.read_json(StringIO(json))\n\nIn [282]: dfju\nOut[282]: \n A         B       date  ints  bools\n2013-01-01 -0.121306 -0.097883 2013-01-01     0   True\n2013-01-02  0.695775  0.341734 2013-01-01     1   True\n2013-01-03  0.959726 -1.110336 2013-01-01     2   True\n2013-01-04 -0.619976  0.149748 2013-01-01     3   True\n2013-01-05 -0.732339  0.687738 2013-01-01     4   True\n\n# Or specify that all timestamps are in nanoseconds\nIn [283]: dfju = pd.read_json(StringIO(json), date_unit=\"ns\")\n\nIn [284]: dfju\nOut[284]: \n A         B        date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356998400     0   True\n2013-01-02  0.695775  0.341734  1356998400     1   True\n2013-01-03  0.959726 -1.110336  1356998400     2   True\n2013-01-04 -0.619976  0.149748  1356998400     3   True\n2013-01-05 -0.732339  0.687738  1356998400     4   True \n```", "```py\nIn [285]: data = (\n .....: '{\"a\":{\"0\":1,\"1\":3},\"b\":{\"0\":2.5,\"1\":4.5},\"c\":{\"0\":true,\"1\":false},\"d\":{\"0\":\"a\",\"1\":\"b\"},'\n .....: '\"e\":{\"0\":null,\"1\":6.0},\"f\":{\"0\":null,\"1\":7.5},\"g\":{\"0\":null,\"1\":true},\"h\":{\"0\":null,\"1\":\"a\"},'\n .....: '\"i\":{\"0\":\"12-31-2019\",\"1\":\"12-31-2019\"},\"j\":{\"0\":null,\"1\":null}}'\n .....: )\n .....: \n\nIn [286]: df = pd.read_json(StringIO(data), dtype_backend=\"pyarrow\")\n\nIn [287]: df\nOut[287]: \n a    b      c  d     e     f     g     h           i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA>  12-31-2019  None\n1  3  4.5  False  b     6   7.5  True     a  12-31-2019  None\n\nIn [288]: df.dtypes\nOut[288]: \na     int64[pyarrow]\nb    double[pyarrow]\nc      bool[pyarrow]\nd    string[pyarrow]\ne     int64[pyarrow]\nf    double[pyarrow]\ng      bool[pyarrow]\nh    string[pyarrow]\ni    string[pyarrow]\nj      null[pyarrow]\ndtype: object \n```", "```py\nIn [262]: from io import StringIO\n\nIn [263]: pd.read_json(StringIO(json))\nOut[263]: \n date         B         A\n0     1  0.403310  0.176444\n1     1  0.301624 -0.154951\n2     1 -1.369849 -2.179861\n3     1  1.462696 -0.954208\n4     1 -0.826591 -1.743161 \n```", "```py\nIn [264]: pd.read_json(\"test.json\")\nOut[264]: \n A         B  date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356     0   True\n2013-01-02  0.695775  0.341734  1356     1   True\n2013-01-03  0.959726 -1.110336  1356     2   True\n2013-01-04 -0.619976  0.149748  1356     3   True\n2013-01-05 -0.732339  0.687738  1356     4   True \n```", "```py\nIn [265]: pd.read_json(\"test.json\", dtype=object).dtypes\nOut[265]: \nA        object\nB        object\ndate     object\nints     object\nbools    object\ndtype: object \n```", "```py\nIn [266]: pd.read_json(\"test.json\", dtype={\"A\": \"float32\", \"bools\": \"int8\"}).dtypes\nOut[266]: \nA        float32\nB        float64\ndate       int64\nints       int64\nbools       int8\ndtype: object \n```", "```py\nIn [267]: from io import StringIO\n\nIn [268]: si = pd.DataFrame(\n .....:    np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)]\n .....: )\n .....: \n\nIn [269]: si\nOut[269]: \n 0    1    2    3\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n\nIn [270]: si.index\nOut[270]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [271]: si.columns\nOut[271]: Index([0, 1, 2, 3], dtype='int64')\n\nIn [272]: json = si.to_json()\n\nIn [273]: sij = pd.read_json(StringIO(json), convert_axes=False)\n\nIn [274]: sij\nOut[274]: \n 0  1  2  3\n0  0  0  0  0\n1  0  0  0  0\n2  0  0  0  0\n3  0  0  0  0\n\nIn [275]: sij.index\nOut[275]: Index(['0', '1', '2', '3'], dtype='object')\n\nIn [276]: sij.columns\nOut[276]: Index(['0', '1', '2', '3'], dtype='object') \n```", "```py\nIn [277]: from io import StringIO\n\nIn [278]: json = dfj2.to_json(date_unit=\"ns\")\n\n# Try to parse timestamps as milliseconds -> Won't Work\nIn [279]: dfju = pd.read_json(StringIO(json), date_unit=\"ms\")\n\nIn [280]: dfju\nOut[280]: \n A         B        date  ints  bools\n1356998400000000000 -0.121306 -0.097883  1356998400     0   True\n1357084800000000000  0.695775  0.341734  1356998400     1   True\n1357171200000000000  0.959726 -1.110336  1356998400     2   True\n1357257600000000000 -0.619976  0.149748  1356998400     3   True\n1357344000000000000 -0.732339  0.687738  1356998400     4   True\n\n# Let pandas detect the correct precision\nIn [281]: dfju = pd.read_json(StringIO(json))\n\nIn [282]: dfju\nOut[282]: \n A         B       date  ints  bools\n2013-01-01 -0.121306 -0.097883 2013-01-01     0   True\n2013-01-02  0.695775  0.341734 2013-01-01     1   True\n2013-01-03  0.959726 -1.110336 2013-01-01     2   True\n2013-01-04 -0.619976  0.149748 2013-01-01     3   True\n2013-01-05 -0.732339  0.687738 2013-01-01     4   True\n\n# Or specify that all timestamps are in nanoseconds\nIn [283]: dfju = pd.read_json(StringIO(json), date_unit=\"ns\")\n\nIn [284]: dfju\nOut[284]: \n A         B        date  ints  bools\n2013-01-01 -0.121306 -0.097883  1356998400     0   True\n2013-01-02  0.695775  0.341734  1356998400     1   True\n2013-01-03  0.959726 -1.110336  1356998400     2   True\n2013-01-04 -0.619976  0.149748  1356998400     3   True\n2013-01-05 -0.732339  0.687738  1356998400     4   True \n```", "```py\nIn [285]: data = (\n .....: '{\"a\":{\"0\":1,\"1\":3},\"b\":{\"0\":2.5,\"1\":4.5},\"c\":{\"0\":true,\"1\":false},\"d\":{\"0\":\"a\",\"1\":\"b\"},'\n .....: '\"e\":{\"0\":null,\"1\":6.0},\"f\":{\"0\":null,\"1\":7.5},\"g\":{\"0\":null,\"1\":true},\"h\":{\"0\":null,\"1\":\"a\"},'\n .....: '\"i\":{\"0\":\"12-31-2019\",\"1\":\"12-31-2019\"},\"j\":{\"0\":null,\"1\":null}}'\n .....: )\n .....: \n\nIn [286]: df = pd.read_json(StringIO(data), dtype_backend=\"pyarrow\")\n\nIn [287]: df\nOut[287]: \n a    b      c  d     e     f     g     h           i     j\n0  1  2.5   True  a  <NA>  <NA>  <NA>  <NA>  12-31-2019  None\n1  3  4.5  False  b     6   7.5  True     a  12-31-2019  None\n\nIn [288]: df.dtypes\nOut[288]: \na     int64[pyarrow]\nb    double[pyarrow]\nc      bool[pyarrow]\nd    string[pyarrow]\ne     int64[pyarrow]\nf    double[pyarrow]\ng      bool[pyarrow]\nh    string[pyarrow]\ni    string[pyarrow]\nj      null[pyarrow]\ndtype: object \n```", "```py\nIn [289]: data = [\n .....:    {\"id\": 1, \"name\": {\"first\": \"Coleen\", \"last\": \"Volk\"}},\n .....:    {\"name\": {\"given\": \"Mark\", \"family\": \"Regner\"}},\n .....:    {\"id\": 2, \"name\": \"Faye Raker\"},\n .....: ]\n .....: \n\nIn [290]: pd.json_normalize(data)\nOut[290]: \n id name.first name.last name.given name.family        name\n0  1.0     Coleen      Volk        NaN         NaN         NaN\n1  NaN        NaN       NaN       Mark      Regner         NaN\n2  2.0        NaN       NaN        NaN         NaN  Faye Raker \n```", "```py\nIn [291]: data = [\n .....:    {\n .....:        \"state\": \"Florida\",\n .....:        \"shortname\": \"FL\",\n .....:        \"info\": {\"governor\": \"Rick Scott\"},\n .....:        \"county\": [\n .....:            {\"name\": \"Dade\", \"population\": 12345},\n .....:            {\"name\": \"Broward\", \"population\": 40000},\n .....:            {\"name\": \"Palm Beach\", \"population\": 60000},\n .....:        ],\n .....:    },\n .....:    {\n .....:        \"state\": \"Ohio\",\n .....:        \"shortname\": \"OH\",\n .....:        \"info\": {\"governor\": \"John Kasich\"},\n .....:        \"county\": [\n .....:            {\"name\": \"Summit\", \"population\": 1234},\n .....:            {\"name\": \"Cuyahoga\", \"population\": 1337},\n .....:        ],\n .....:    },\n .....: ]\n .....: \n\nIn [292]: pd.json_normalize(data, \"county\", [\"state\", \"shortname\", [\"info\", \"governor\"]])\nOut[292]: \n name  population    state shortname info.governor\n0        Dade       12345  Florida        FL    Rick Scott\n1     Broward       40000  Florida        FL    Rick Scott\n2  Palm Beach       60000  Florida        FL    Rick Scott\n3      Summit        1234     Ohio        OH   John Kasich\n4    Cuyahoga        1337     Ohio        OH   John Kasich \n```", "```py\nIn [293]: data = [\n .....:    {\n .....:        \"CreatedBy\": {\"Name\": \"User001\"},\n .....:        \"Lookup\": {\n .....:            \"TextField\": \"Some text\",\n .....:            \"UserField\": {\"Id\": \"ID001\", \"Name\": \"Name001\"},\n .....:        },\n .....:        \"Image\": {\"a\": \"b\"},\n .....:    }\n .....: ]\n .....: \n\nIn [294]: pd.json_normalize(data, max_level=1)\nOut[294]: \n CreatedBy.Name Lookup.TextField                    Lookup.UserField Image.a\n0        User001        Some text  {'Id': 'ID001', 'Name': 'Name001'}       b \n```", "```py\nIn [295]: from io import StringIO\n\nIn [296]: jsonl = \"\"\"\n .....:    {\"a\": 1, \"b\": 2}\n .....:    {\"a\": 3, \"b\": 4}\n .....: \"\"\"\n .....: \n\nIn [297]: df = pd.read_json(StringIO(jsonl), lines=True)\n\nIn [298]: df\nOut[298]: \n a  b\n0  1  2\n1  3  4\n\nIn [299]: df.to_json(orient=\"records\", lines=True)\nOut[299]: '{\"a\":1,\"b\":2}\\n{\"a\":3,\"b\":4}\\n'\n\n# reader is an iterator that returns ``chunksize`` lines each iteration\nIn [300]: with pd.read_json(StringIO(jsonl), lines=True, chunksize=1) as reader:\n .....:    reader\n .....:    for chunk in reader:\n .....:        print(chunk)\n .....: \nEmpty DataFrame\nColumns: []\nIndex: []\n a  b\n0  1  2\n a  b\n1  3  4 \n```", "```py\nIn [301]: from io import BytesIO\n\nIn [302]: df = pd.read_json(BytesIO(jsonl.encode()), lines=True, engine=\"pyarrow\")\n\nIn [303]: df\nOut[303]: \n a  b\n0  1  2\n1  3  4 \n```", "```py\nIn [304]: df = pd.DataFrame(\n .....:    {\n .....:        \"A\": [1, 2, 3],\n .....:        \"B\": [\"a\", \"b\", \"c\"],\n .....:        \"C\": pd.date_range(\"2016-01-01\", freq=\"d\", periods=3),\n .....:    },\n .....:    index=pd.Index(range(3), name=\"idx\"),\n .....: )\n .....: \n\nIn [305]: df\nOut[305]: \n A  B          C\nidx \n0    1  a 2016-01-01\n1    2  b 2016-01-02\n2    3  c 2016-01-03\n\nIn [306]: df.to_json(orient=\"table\", date_format=\"iso\")\nOut[306]: '{\"schema\":{\"fields\":[{\"name\":\"idx\",\"type\":\"integer\"},{\"name\":\"A\",\"type\":\"integer\"},{\"name\":\"B\",\"type\":\"string\"},{\"name\":\"C\",\"type\":\"datetime\"}],\"primaryKey\":[\"idx\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"idx\":0,\"A\":1,\"B\":\"a\",\"C\":\"2016-01-01T00:00:00.000\"},{\"idx\":1,\"A\":2,\"B\":\"b\",\"C\":\"2016-01-02T00:00:00.000\"},{\"idx\":2,\"A\":3,\"B\":\"c\",\"C\":\"2016-01-03T00:00:00.000\"}]}' \n```", "```py\n    In [307]: from pandas.io.json import build_table_schema\n\n    In [308]: s = pd.Series(pd.date_range(\"2016\", periods=4))\n\n    In [309]: build_table_schema(s)\n    Out[309]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'datetime'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [310]: s_tz = pd.Series(pd.date_range(\"2016\", periods=12, tz=\"US/Central\"))\n\n    In [311]: build_table_schema(s_tz)\n    Out[311]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'datetime', 'tz': 'US/Central'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [312]: s_per = pd.Series(1, index=pd.period_range(\"2016\", freq=\"Y-DEC\", periods=4))\n\n    In [313]: build_table_schema(s_per)\n    Out[313]: \n    {'fields': [{'name': 'index', 'type': 'datetime', 'freq': 'YE-DEC'},\n     {'name': 'values', 'type': 'integer'}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [314]: s_cat = pd.Series(pd.Categorical([\"a\", \"b\", \"a\"]))\n\n    In [315]: build_table_schema(s_cat)\n    Out[315]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values',\n     'type': 'any',\n     'constraints': {'enum': ['a', 'b']},\n     'ordered': False}],\n     'primaryKey': ['index'],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [316]: s_dupe = pd.Series([1, 2], index=[1, 1])\n\n    In [317]: build_table_schema(s_dupe)\n    Out[317]: \n    {'fields': [{'name': 'index', 'type': 'integer'},\n     {'name': 'values', 'type': 'integer'}],\n     'pandas_version': '1.4.0'} \n    ```", "```py\n    In [318]: s_multi = pd.Series(1, index=pd.MultiIndex.from_product([(\"a\", \"b\"), (0, 1)]))\n\n    In [319]: build_table_schema(s_multi)\n    Out[319]: \n    {'fields': [{'name': 'level_0', 'type': 'string'},\n     {'name': 'level_1', 'type': 'integer'},\n     {'name': 'values', 'type': 'integer'}],\n     'primaryKey': FrozenList(['level_0', 'level_1']),\n     'pandas_version': '1.4.0'} \n    ```", "```py\nIn [320]: df = pd.DataFrame(\n .....:    {\n .....:        \"foo\": [1, 2, 3, 4],\n .....:        \"bar\": [\"a\", \"b\", \"c\", \"d\"],\n .....:        \"baz\": pd.date_range(\"2018-01-01\", freq=\"d\", periods=4),\n .....:        \"qux\": pd.Categorical([\"a\", \"b\", \"c\", \"c\"]),\n .....:    },\n .....:    index=pd.Index(range(4), name=\"idx\"),\n .....: )\n .....: \n\nIn [321]: df\nOut[321]: \n foo bar        baz qux\nidx \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [322]: df.dtypes\nOut[322]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object\n\nIn [323]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [324]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [325]: new_df\nOut[325]: \n foo bar        baz qux\nidx \n0      1   a 2018-01-01   a\n1      2   b 2018-01-02   b\n2      3   c 2018-01-03   c\n3      4   d 2018-01-04   c\n\nIn [326]: new_df.dtypes\nOut[326]: \nfoo             int64\nbar            object\nbaz    datetime64[ns]\nqux          category\ndtype: object \n```", "```py\nIn [327]: df.index.name = \"index\"\n\nIn [328]: df.to_json(\"test.json\", orient=\"table\")\n\nIn [329]: new_df = pd.read_json(\"test.json\", orient=\"table\")\n\nIn [330]: print(new_df.index.name)\nNone \n```", "```py\nIn [320]: url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\nIn [321]: pd.read_html(url)\nOut[321]:\n[                         Bank NameBank           CityCity StateSt  ...              Acquiring InstitutionAI Closing DateClosing FundFund\n 0                    Almena State Bank             Almena      KS  ...                          Equity Bank    October 23, 2020    10538\n 1           First City Bank of Florida  Fort Walton Beach      FL  ...            United Fidelity Bank, fsb    October 16, 2020    10537\n 2                 The First State Bank      Barboursville      WV  ...                       MVB Bank, Inc.       April 3, 2020    10536\n 3                   Ericson State Bank            Ericson      NE  ...           Farmers and Merchants Bank   February 14, 2020    10535\n 4     City National Bank of New Jersey             Newark      NJ  ...                      Industrial Bank    November 1, 2019    10534\n ..                                 ...                ...     ...  ...                                  ...                 ...      ...\n 558                 Superior Bank, FSB           Hinsdale      IL  ...                Superior Federal, FSB       July 27, 2001     6004\n 559                Malta National Bank              Malta      OH  ...                    North Valley Bank         May 3, 2001     4648\n 560    First Alliance Bank & Trust Co.         Manchester      NH  ...  Southern New Hampshire Bank & Trust    February 2, 2001     4647\n 561  National State Bank of Metropolis         Metropolis      IL  ...              Banterra Bank of Marion   December 14, 2000     4646\n 562                   Bank of Honolulu           Honolulu      HI  ...                   Bank of the Orient    October 13, 2000     4645\n\n [563 rows x 7 columns]] \n```", "```py\nIn [322]: url = 'https://www.sump.org/notes/request/' # HTTP request reflector\nIn [323]: pd.read_html(url)\nOut[323]:\n[                   0                    1\n 0     Remote Socket:  51.15.105.256:51760\n 1  Protocol Version:             HTTP/1.1\n 2    Request Method:                  GET\n 3       Request URI:      /notes/request/\n 4     Request Query:                  NaN,\n 0   Accept-Encoding:             identity\n 1              Host:         www.sump.org\n 2        User-Agent:    Python-urllib/3.8\n 3        Connection:                close]\nIn [324]: headers = {\nIn [325]:    'User-Agent':'Mozilla Firefox v14.0',\nIn [326]:    'Accept':'application/json',\nIn [327]:    'Connection':'keep-alive',\nIn [328]:    'Auth':'Bearer 2*/f3+fe68df*4'\nIn [329]: }\nIn [340]: pd.read_html(url, storage_options=headers)\nOut[340]:\n[                   0                    1\n 0     Remote Socket:  51.15.105.256:51760\n 1  Protocol Version:             HTTP/1.1\n 2    Request Method:                  GET\n 3       Request URI:      /notes/request/\n 4     Request Query:                  NaN,\n 0        User-Agent: Mozilla Firefox v14.0\n 1    AcceptEncoding:   gzip,  deflate,  br\n 2            Accept:      application/json\n 3        Connection:             keep-alive\n 4              Auth:  Bearer 2*/f3+fe68df*4] \n```", "```py\nIn [331]: html_str = \"\"\"\n .....:         <table>\n .....:             <tr>\n .....:                 <th>A</th>\n .....:                 <th colspan=\"1\">B</th>\n .....:                 <th rowspan=\"1\">C</th>\n .....:             </tr>\n .....:             <tr>\n .....:                 <td>a</td>\n .....:                 <td>b</td>\n .....:                 <td>c</td>\n .....:             </tr>\n .....:         </table>\n .....:     \"\"\"\n .....: \n\nIn [332]: with open(\"tmp.html\", \"w\") as f:\n .....:    f.write(html_str)\n .....: \n\nIn [333]: df = pd.read_html(\"tmp.html\")\n\nIn [334]: df[0]\nOut[334]: \n A  B  C\n0  a  b  c \n```", "```py\nIn [335]: dfs = pd.read_html(StringIO(html_str))\n\nIn [336]: dfs[0]\nOut[336]: \n A  B  C\n0  a  b  c \n```", "```py\nmatch = \"Metcalf Bank\"\ndf_list = pd.read_html(url, match=match) \n```", "```py\ndfs = pd.read_html(url, header=0) \n```", "```py\ndfs = pd.read_html(url, index_col=0) \n```", "```py\ndfs = pd.read_html(url, skiprows=0) \n```", "```py\ndfs = pd.read_html(url, skiprows=range(2)) \n```", "```py\ndfs1 = pd.read_html(url, attrs={\"id\": \"table\"})\ndfs2 = pd.read_html(url, attrs={\"class\": \"sortable\"})\nprint(np.array_equal(dfs1[0], dfs2[0]))  # Should be True \n```", "```py\ndfs = pd.read_html(url, na_values=[\"No Acquirer\"]) \n```", "```py\ndfs = pd.read_html(url, keep_default_na=False) \n```", "```py\nurl_mcc = \"https://en.wikipedia.org/wiki/Mobile_country_code?oldid=899173761\"\ndfs = pd.read_html(\n    url_mcc,\n    match=\"Telekom Albania\",\n    header=0,\n    converters={\"MNC\": str},\n) \n```", "```py\ndfs = pd.read_html(url, match=\"Metcalf Bank\", index_col=0) \n```", "```py\ndf = pd.DataFrame(np.random.randn(2, 2))\ns = df.to_html(float_format=\"{0:.40g}\".format)\ndfin = pd.read_html(s, index_col=0) \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\"]) \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=\"lxml\") \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\", \"bs4\"]) \n```", "```py\nIn [337]: html_table = \"\"\"\n .....: <table>\n .....:  <tr>\n .....:    <th>GitHub</th>\n .....:  </tr>\n .....:  <tr>\n .....:    <td><a href=\"https://github.com/pandas-dev/pandas\">pandas</a></td>\n .....:  </tr>\n .....: </table>\n .....: \"\"\"\n .....: \n\nIn [338]: df = pd.read_html(\n .....:    StringIO(html_table),\n .....:    extract_links=\"all\"\n .....: )[0]\n .....: \n\nIn [339]: df\nOut[339]: \n (GitHub, None)\n0  (pandas, https://github.com/pandas-dev/pandas)\n\nIn [340]: df[(\"GitHub\", None)]\nOut[340]: \n0    (pandas, https://github.com/pandas-dev/pandas)\nName: (GitHub, None), dtype: object\n\nIn [341]: df[(\"GitHub\", None)].str[1]\nOut[341]: \n0    https://github.com/pandas-dev/pandas\nName: (GitHub, None), dtype: object \n```", "```py\nIn [342]: from IPython.display import display, HTML\n\nIn [343]: df = pd.DataFrame(np.random.randn(2, 2))\n\nIn [344]: df\nOut[344]: \n 0         1\n0 -0.345352  1.314232\n1  0.690579  0.995761\n\nIn [345]: html = df.to_html()\n\nIn [346]: print(html)  # raw html\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table>\n\nIn [347]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [348]: html = df.to_html(columns=[0])\n\nIn [349]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n </tr>\n </tbody>\n</table>\n\nIn [350]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [351]: html = df.to_html(float_format=\"{0:.10f}\".format)\n\nIn [352]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.3453521949</td>\n <td>1.3142323796</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.6905793352</td>\n <td>0.9957609037</td>\n </tr>\n </tbody>\n</table>\n\nIn [353]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [354]: html = df.to_html(bold_rows=False)\n\nIn [355]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <td>0</td>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <td>1</td>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table>\n\nIn [356]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [357]: print(df.to_html(classes=[\"awesome_table_class\", \"even_more_awesome_class\"]))\n<table border=\"1\" class=\"dataframe awesome_table_class even_more_awesome_class\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table> \n```", "```py\nIn [358]: url_df = pd.DataFrame(\n .....:    {\n .....:        \"name\": [\"Python\", \"pandas\"],\n .....:        \"url\": [\"https://www.python.org/\", \"https://pandas.pydata.org\"],\n .....:    }\n .....: )\n .....: \n\nIn [359]: html = url_df.to_html(render_links=True)\n\nIn [360]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>name</th>\n <th>url</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>Python</td>\n <td><a href=\"https://www.python.org/\" target=\"_blank\">https://www.python.org/</a></td>\n </tr>\n <tr>\n <th>1</th>\n <td>pandas</td>\n <td><a href=\"https://pandas.pydata.org\" target=\"_blank\">https://pandas.pydata.org</a></td>\n </tr>\n </tbody>\n</table>\n\nIn [361]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [362]: df = pd.DataFrame({\"a\": list(\"&<>\"), \"b\": np.random.randn(3)}) \n```", "```py\nIn [363]: html = df.to_html()\n\nIn [364]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>a</th>\n <th>b</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>&amp;</td>\n <td>2.396780</td>\n </tr>\n <tr>\n <th>1</th>\n <td>&lt;</td>\n <td>0.014871</td>\n </tr>\n <tr>\n <th>2</th>\n <td>&gt;</td>\n <td>3.357427</td>\n </tr>\n </tbody>\n</table>\n\nIn [365]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [366]: html = df.to_html(escape=False)\n\nIn [367]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>a</th>\n <th>b</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>&</td>\n <td>2.396780</td>\n </tr>\n <tr>\n <th>1</th>\n <td><</td>\n <td>0.014871</td>\n </tr>\n <tr>\n <th>2</th>\n <td>></td>\n <td>3.357427</td>\n </tr>\n </tbody>\n</table>\n\nIn [368]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [320]: url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\nIn [321]: pd.read_html(url)\nOut[321]:\n[                         Bank NameBank           CityCity StateSt  ...              Acquiring InstitutionAI Closing DateClosing FundFund\n 0                    Almena State Bank             Almena      KS  ...                          Equity Bank    October 23, 2020    10538\n 1           First City Bank of Florida  Fort Walton Beach      FL  ...            United Fidelity Bank, fsb    October 16, 2020    10537\n 2                 The First State Bank      Barboursville      WV  ...                       MVB Bank, Inc.       April 3, 2020    10536\n 3                   Ericson State Bank            Ericson      NE  ...           Farmers and Merchants Bank   February 14, 2020    10535\n 4     City National Bank of New Jersey             Newark      NJ  ...                      Industrial Bank    November 1, 2019    10534\n ..                                 ...                ...     ...  ...                                  ...                 ...      ...\n 558                 Superior Bank, FSB           Hinsdale      IL  ...                Superior Federal, FSB       July 27, 2001     6004\n 559                Malta National Bank              Malta      OH  ...                    North Valley Bank         May 3, 2001     4648\n 560    First Alliance Bank & Trust Co.         Manchester      NH  ...  Southern New Hampshire Bank & Trust    February 2, 2001     4647\n 561  National State Bank of Metropolis         Metropolis      IL  ...              Banterra Bank of Marion   December 14, 2000     4646\n 562                   Bank of Honolulu           Honolulu      HI  ...                   Bank of the Orient    October 13, 2000     4645\n\n [563 rows x 7 columns]] \n```", "```py\nIn [322]: url = 'https://www.sump.org/notes/request/' # HTTP request reflector\nIn [323]: pd.read_html(url)\nOut[323]:\n[                   0                    1\n 0     Remote Socket:  51.15.105.256:51760\n 1  Protocol Version:             HTTP/1.1\n 2    Request Method:                  GET\n 3       Request URI:      /notes/request/\n 4     Request Query:                  NaN,\n 0   Accept-Encoding:             identity\n 1              Host:         www.sump.org\n 2        User-Agent:    Python-urllib/3.8\n 3        Connection:                close]\nIn [324]: headers = {\nIn [325]:    'User-Agent':'Mozilla Firefox v14.0',\nIn [326]:    'Accept':'application/json',\nIn [327]:    'Connection':'keep-alive',\nIn [328]:    'Auth':'Bearer 2*/f3+fe68df*4'\nIn [329]: }\nIn [340]: pd.read_html(url, storage_options=headers)\nOut[340]:\n[                   0                    1\n 0     Remote Socket:  51.15.105.256:51760\n 1  Protocol Version:             HTTP/1.1\n 2    Request Method:                  GET\n 3       Request URI:      /notes/request/\n 4     Request Query:                  NaN,\n 0        User-Agent: Mozilla Firefox v14.0\n 1    AcceptEncoding:   gzip,  deflate,  br\n 2            Accept:      application/json\n 3        Connection:             keep-alive\n 4              Auth:  Bearer 2*/f3+fe68df*4] \n```", "```py\nIn [331]: html_str = \"\"\"\n .....:         <table>\n .....:             <tr>\n .....:                 <th>A</th>\n .....:                 <th colspan=\"1\">B</th>\n .....:                 <th rowspan=\"1\">C</th>\n .....:             </tr>\n .....:             <tr>\n .....:                 <td>a</td>\n .....:                 <td>b</td>\n .....:                 <td>c</td>\n .....:             </tr>\n .....:         </table>\n .....:     \"\"\"\n .....: \n\nIn [332]: with open(\"tmp.html\", \"w\") as f:\n .....:    f.write(html_str)\n .....: \n\nIn [333]: df = pd.read_html(\"tmp.html\")\n\nIn [334]: df[0]\nOut[334]: \n A  B  C\n0  a  b  c \n```", "```py\nIn [335]: dfs = pd.read_html(StringIO(html_str))\n\nIn [336]: dfs[0]\nOut[336]: \n A  B  C\n0  a  b  c \n```", "```py\nmatch = \"Metcalf Bank\"\ndf_list = pd.read_html(url, match=match) \n```", "```py\ndfs = pd.read_html(url, header=0) \n```", "```py\ndfs = pd.read_html(url, index_col=0) \n```", "```py\ndfs = pd.read_html(url, skiprows=0) \n```", "```py\ndfs = pd.read_html(url, skiprows=range(2)) \n```", "```py\ndfs1 = pd.read_html(url, attrs={\"id\": \"table\"})\ndfs2 = pd.read_html(url, attrs={\"class\": \"sortable\"})\nprint(np.array_equal(dfs1[0], dfs2[0]))  # Should be True \n```", "```py\ndfs = pd.read_html(url, na_values=[\"No Acquirer\"]) \n```", "```py\ndfs = pd.read_html(url, keep_default_na=False) \n```", "```py\nurl_mcc = \"https://en.wikipedia.org/wiki/Mobile_country_code?oldid=899173761\"\ndfs = pd.read_html(\n    url_mcc,\n    match=\"Telekom Albania\",\n    header=0,\n    converters={\"MNC\": str},\n) \n```", "```py\ndfs = pd.read_html(url, match=\"Metcalf Bank\", index_col=0) \n```", "```py\ndf = pd.DataFrame(np.random.randn(2, 2))\ns = df.to_html(float_format=\"{0:.40g}\".format)\ndfin = pd.read_html(s, index_col=0) \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\"]) \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=\"lxml\") \n```", "```py\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\", \"bs4\"]) \n```", "```py\nIn [337]: html_table = \"\"\"\n .....: <table>\n .....:  <tr>\n .....:    <th>GitHub</th>\n .....:  </tr>\n .....:  <tr>\n .....:    <td><a href=\"https://github.com/pandas-dev/pandas\">pandas</a></td>\n .....:  </tr>\n .....: </table>\n .....: \"\"\"\n .....: \n\nIn [338]: df = pd.read_html(\n .....:    StringIO(html_table),\n .....:    extract_links=\"all\"\n .....: )[0]\n .....: \n\nIn [339]: df\nOut[339]: \n (GitHub, None)\n0  (pandas, https://github.com/pandas-dev/pandas)\n\nIn [340]: df[(\"GitHub\", None)]\nOut[340]: \n0    (pandas, https://github.com/pandas-dev/pandas)\nName: (GitHub, None), dtype: object\n\nIn [341]: df[(\"GitHub\", None)].str[1]\nOut[341]: \n0    https://github.com/pandas-dev/pandas\nName: (GitHub, None), dtype: object \n```", "```py\nIn [342]: from IPython.display import display, HTML\n\nIn [343]: df = pd.DataFrame(np.random.randn(2, 2))\n\nIn [344]: df\nOut[344]: \n 0         1\n0 -0.345352  1.314232\n1  0.690579  0.995761\n\nIn [345]: html = df.to_html()\n\nIn [346]: print(html)  # raw html\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table>\n\nIn [347]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [348]: html = df.to_html(columns=[0])\n\nIn [349]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n </tr>\n </tbody>\n</table>\n\nIn [350]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [351]: html = df.to_html(float_format=\"{0:.10f}\".format)\n\nIn [352]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.3453521949</td>\n <td>1.3142323796</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.6905793352</td>\n <td>0.9957609037</td>\n </tr>\n </tbody>\n</table>\n\nIn [353]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [354]: html = df.to_html(bold_rows=False)\n\nIn [355]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <td>0</td>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <td>1</td>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table>\n\nIn [356]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [357]: print(df.to_html(classes=[\"awesome_table_class\", \"even_more_awesome_class\"]))\n<table border=\"1\" class=\"dataframe awesome_table_class even_more_awesome_class\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>0</th>\n <th>1</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>-0.345352</td>\n <td>1.314232</td>\n </tr>\n <tr>\n <th>1</th>\n <td>0.690579</td>\n <td>0.995761</td>\n </tr>\n </tbody>\n</table> \n```", "```py\nIn [358]: url_df = pd.DataFrame(\n .....:    {\n .....:        \"name\": [\"Python\", \"pandas\"],\n .....:        \"url\": [\"https://www.python.org/\", \"https://pandas.pydata.org\"],\n .....:    }\n .....: )\n .....: \n\nIn [359]: html = url_df.to_html(render_links=True)\n\nIn [360]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>name</th>\n <th>url</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>Python</td>\n <td><a href=\"https://www.python.org/\" target=\"_blank\">https://www.python.org/</a></td>\n </tr>\n <tr>\n <th>1</th>\n <td>pandas</td>\n <td><a href=\"https://pandas.pydata.org\" target=\"_blank\">https://pandas.pydata.org</a></td>\n </tr>\n </tbody>\n</table>\n\nIn [361]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [362]: df = pd.DataFrame({\"a\": list(\"&<>\"), \"b\": np.random.randn(3)}) \n```", "```py\nIn [363]: html = df.to_html()\n\nIn [364]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>a</th>\n <th>b</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>&amp;</td>\n <td>2.396780</td>\n </tr>\n <tr>\n <th>1</th>\n <td>&lt;</td>\n <td>0.014871</td>\n </tr>\n <tr>\n <th>2</th>\n <td>&gt;</td>\n <td>3.357427</td>\n </tr>\n </tbody>\n</table>\n\nIn [365]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [366]: html = df.to_html(escape=False)\n\nIn [367]: print(html)\n<table border=\"1\" class=\"dataframe\">\n <thead>\n <tr style=\"text-align: right;\">\n <th></th>\n <th>a</th>\n <th>b</th>\n </tr>\n </thead>\n <tbody>\n <tr>\n <th>0</th>\n <td>&</td>\n <td>2.396780</td>\n </tr>\n <tr>\n <th>1</th>\n <td><</td>\n <td>0.014871</td>\n </tr>\n <tr>\n <th>2</th>\n <td>></td>\n <td>3.357427</td>\n </tr>\n </tbody>\n</table>\n\nIn [368]: display(HTML(html))\n<IPython.core.display.HTML object> \n```", "```py\nIn [369]: df = pd.DataFrame([[1, 2], [3, 4]], index=[\"a\", \"b\"], columns=[\"c\", \"d\"])\n\nIn [370]: print(df.style.to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & 1 & 2 \\\\\nb & 3 & 4 \\\\\n\\end{tabular} \n```", "```py\nIn [371]: print(df.style.format(\"\u20ac {}\").to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & \u20ac 1 & \u20ac 2 \\\\\nb & \u20ac 3 & \u20ac 4 \\\\\n\\end{tabular} \n```", "```py\nIn [369]: df = pd.DataFrame([[1, 2], [3, 4]], index=[\"a\", \"b\"], columns=[\"c\", \"d\"])\n\nIn [370]: print(df.style.to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & 1 & 2 \\\\\nb & 3 & 4 \\\\\n\\end{tabular} \n```", "```py\nIn [371]: print(df.style.format(\"\u20ac {}\").to_latex())\n\\begin{tabular}{lrr}\n & c & d \\\\\na & \u20ac 1 & \u20ac 2 \\\\\nb & \u20ac 3 & \u20ac 4 \\\\\n\\end{tabular} \n```", "```py\nIn [372]: from io import StringIO\n\nIn [373]: xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n .....: <bookstore>\n .....:  <book category=\"cooking\">\n .....:    <title lang=\"en\">Everyday Italian</title>\n .....:    <author>Giada De Laurentiis</author>\n .....:    <year>2005</year>\n .....:    <price>30.00</price>\n .....:  </book>\n .....:  <book category=\"children\">\n .....:    <title lang=\"en\">Harry Potter</title>\n .....:    <author>J K. Rowling</author>\n .....:    <year>2005</year>\n .....:    <price>29.99</price>\n .....:  </book>\n .....:  <book category=\"web\">\n .....:    <title lang=\"en\">Learning XML</title>\n .....:    <author>Erik T. Ray</author>\n .....:    <year>2003</year>\n .....:    <price>39.95</price>\n .....:  </book>\n .....: </bookstore>\"\"\"\n .....: \n\nIn [374]: df = pd.read_xml(StringIO(xml))\n\nIn [375]: df\nOut[375]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [376]: df = pd.read_xml(\"https://www.w3schools.com/xml/books.xml\")\n\nIn [377]: df\nOut[377]: \n category              title                  author  year  price      cover\n0   cooking   Everyday Italian     Giada De Laurentiis  2005  30.00       None\n1  children       Harry Potter            J K. Rowling  2005  29.99       None\n2       web  XQuery Kick Start  Vaidyanathan Nagarajan  2003  49.99       None\n3       web       Learning XML             Erik T. Ray  2003  39.95  paperback \n```", "```py\nIn [378]: file_path = \"books.xml\"\n\nIn [379]: with open(file_path, \"w\") as f:\n .....:    f.write(xml)\n .....: \n\nIn [380]: with open(file_path, \"r\") as f:\n .....:    df = pd.read_xml(StringIO(f.read()))\n .....: \n\nIn [381]: df\nOut[381]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [382]: with open(file_path, \"r\") as f:\n .....:    sio = StringIO(f.read())\n .....: \n\nIn [383]: df = pd.read_xml(sio)\n\nIn [384]: df\nOut[384]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [385]: with open(file_path, \"rb\") as f:\n .....:    bio = BytesIO(f.read())\n .....: \n\nIn [386]: df = pd.read_xml(bio)\n\nIn [387]: df\nOut[387]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [388]: df = pd.read_xml(\n .....:    \"s3://pmc-oa-opendata/oa_comm/xml/all/PMC1236943.xml\",\n .....:    xpath=\".//journal-meta\",\n .....: )\n .....: \n\nIn [389]: df\nOut[389]: \n journal-id              journal-title       issn  publisher\n0  Cardiovasc Ultrasound  Cardiovascular Ultrasound  1476-7120        NaN \n```", "```py\nIn [390]: df = pd.read_xml(file_path, xpath=\"//book[year=2005]\")\n\nIn [391]: df\nOut[391]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99 \n```", "```py\nIn [392]: df = pd.read_xml(file_path, elems_only=True)\n\nIn [393]: df\nOut[393]: \n title               author  year  price\n0  Everyday Italian  Giada De Laurentiis  2005  30.00\n1      Harry Potter         J K. Rowling  2005  29.99\n2      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [394]: df = pd.read_xml(file_path, attrs_only=True)\n\nIn [395]: df\nOut[395]: \n category\n0   cooking\n1  children\n2       web \n```", "```py\nIn [396]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <doc:data >\n .....:  <doc:row>\n .....:    <doc:shape>square</doc:shape>\n .....:    <doc:degrees>360</doc:degrees>\n .....:    <doc:sides>4.0</doc:sides>\n .....:  </doc:row>\n .....:  <doc:row>\n .....:    <doc:shape>circle</doc:shape>\n .....:    <doc:degrees>360</doc:degrees>\n .....:    <doc:sides/>\n .....:  </doc:row>\n .....:  <doc:row>\n .....:    <doc:shape>triangle</doc:shape>\n .....:    <doc:degrees>180</doc:degrees>\n .....:    <doc:sides>3.0</doc:sides>\n .....:  </doc:row>\n .....: </doc:data>\"\"\"\n .....: \n\nIn [397]: df = pd.read_xml(StringIO(xml),\n .....:                 xpath=\"//doc:row\",\n .....:                 namespaces={\"doc\": \"https://example.com\"})\n .....: \n\nIn [398]: df\nOut[398]: \n shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0 \n```", "```py\nIn [399]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <data >\n .....: <row>\n .....:   <shape>square</shape>\n .....:   <degrees>360</degrees>\n .....:   <sides>4.0</sides>\n .....: </row>\n .....: <row>\n .....:   <shape>circle</shape>\n .....:   <degrees>360</degrees>\n .....:   <sides/>\n .....: </row>\n .....: <row>\n .....:   <shape>triangle</shape>\n .....:   <degrees>180</degrees>\n .....:   <sides>3.0</sides>\n .....: </row>\n .....: </data>\"\"\"\n .....: \n\nIn [400]: df = pd.read_xml(StringIO(xml),\n .....:                 xpath=\"//pandas:row\",\n .....:                 namespaces={\"pandas\": \"https://example.com\"})\n .....: \n\nIn [401]: df\nOut[401]: \n shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0 \n```", "```py\nIn [402]: xml = \"\"\"\n .....: <data>\n .....:  <row>\n .....:    <shape sides=\"4\">square</shape>\n .....:    <degrees>360</degrees>\n .....:  </row>\n .....:  <row>\n .....:    <shape sides=\"0\">circle</shape>\n .....:    <degrees>360</degrees>\n .....:  </row>\n .....:  <row>\n .....:    <shape sides=\"3\">triangle</shape>\n .....:    <degrees>180</degrees>\n .....:  </row>\n .....: </data>\"\"\"\n .....: \n\nIn [403]: df = pd.read_xml(StringIO(xml), xpath=\"./row\")\n\nIn [404]: df\nOut[404]: \n shape  degrees\n0    square      360\n1    circle      360\n2  triangle      180 \n```", "```py\nIn [405]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <response>\n .....:  <row>\n .....:    <station id=\"40850\" name=\"Library\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>864.2</avg_weekday_rides>\n .....:      <avg_saturday_rides>534</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....:  <row>\n .....:    <station id=\"41700\" name=\"Washington/Wabash\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>2707.4</avg_weekday_rides>\n .....:      <avg_saturday_rides>1909.8</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....:  <row>\n .....:    <station id=\"40380\" name=\"Clark/Lake\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>2949.6</avg_weekday_rides>\n .....:      <avg_saturday_rides>1657</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....: </response>\"\"\"\n .....: \n\nIn [406]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" >\n .....:   <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n .....:   <xsl:strip-space elements=\"*\"/>\n .....:   <xsl:template match=\"/response\">\n .....:      <xsl:copy>\n .....:        <xsl:apply-templates select=\"row\"/>\n .....:      </xsl:copy>\n .....:   </xsl:template>\n .....:   <xsl:template match=\"row\">\n .....:      <xsl:copy>\n .....:        <station_id><xsl:value-of select=\"station/@id\"/></station_id>\n .....:        <station_name><xsl:value-of select=\"station/@name\"/></station_name>\n .....:        <xsl:copy-of select=\"month|rides/*\"/>\n .....:      </xsl:copy>\n .....:   </xsl:template>\n .....: </xsl:stylesheet>\"\"\"\n .....: \n\nIn [407]: output = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <response>\n .....:   <row>\n .....:      <station_id>40850</station_id>\n .....:      <station_name>Library</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>864.2</avg_weekday_rides>\n .....:      <avg_saturday_rides>534</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n .....:   </row>\n .....:   <row>\n .....:      <station_id>41700</station_id>\n .....:      <station_name>Washington/Wabash</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>2707.4</avg_weekday_rides>\n .....:      <avg_saturday_rides>1909.8</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n .....:   </row>\n .....:   <row>\n .....:      <station_id>40380</station_id>\n .....:      <station_name>Clark/Lake</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>2949.6</avg_weekday_rides>\n .....:      <avg_saturday_rides>1657</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n .....:   </row>\n .....: </response>\"\"\"\n .....: \n\nIn [408]: df = pd.read_xml(StringIO(xml), stylesheet=xsl)\n\nIn [409]: df\nOut[409]: \n station_id       station_name  ... avg_saturday_rides  avg_sunday_holiday_rides\n0       40850            Library  ...              534.0                     417.2\n1       41700  Washington/Wabash  ...             1909.8                    1438.6\n2       40380         Clark/Lake  ...             1657.0                    1453.8\n\n[3 rows x 6 columns] \n```", "```py\nIn [1]: df = pd.read_xml(\n...         \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n...         iterparse = {\"page\": [\"title\", \"ns\", \"id\"]}\n...     )\n...     df\nOut[2]:\n title   ns        id\n0                                       Gettysburg Address    0     21450\n1                                                Main Page    0     42950\n2                            Declaration by United Nations    0      8435\n3             Constitution of the United States of America    0      8435\n4                     Declaration of Independence (Israel)    0     17858\n...                                                    ...  ...       ...\n3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649\n3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649\n3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649\n3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291\n3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450\n\n[3578765 rows x 3 columns] \n```", "```py\nIn [410]: geom_df = pd.DataFrame(\n .....:    {\n .....:        \"shape\": [\"square\", \"circle\", \"triangle\"],\n .....:        \"degrees\": [360, 360, 180],\n .....:        \"sides\": [4, np.nan, 3],\n .....:    }\n .....: )\n .....: \n\nIn [411]: print(geom_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </row>\n <row>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [412]: print(geom_df.to_xml(root_name=\"geometry\", row_name=\"objects\"))\n<?xml version='1.0' encoding='utf-8'?>\n<geometry>\n <objects>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </objects>\n <objects>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </objects>\n <objects>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </objects>\n</geometry> \n```", "```py\nIn [413]: print(geom_df.to_xml(attr_cols=geom_df.columns.tolist()))\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/>\n <row index=\"1\" shape=\"circle\" degrees=\"360\"/>\n <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/>\n</data> \n```", "```py\nIn [414]: print(\n .....:    geom_df.to_xml(\n .....:        index=False,\n .....:        attr_cols=['shape'],\n .....:        elem_cols=['degrees', 'sides'])\n .....: )\n .....: \n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row shape=\"square\">\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row shape=\"circle\">\n <degrees>360</degrees>\n <sides/>\n </row>\n <row shape=\"triangle\">\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [415]: ext_geom_df = pd.DataFrame(\n .....:    {\n .....:        \"type\": [\"polygon\", \"other\", \"polygon\"],\n .....:        \"shape\": [\"square\", \"circle\", \"triangle\"],\n .....:        \"degrees\": [360, 360, 180],\n .....:        \"sides\": [4, np.nan, 3],\n .....:    }\n .....: )\n .....: \n\nIn [416]: pvt_df = ext_geom_df.pivot_table(index='shape',\n .....:                                 columns='type',\n .....:                                 values=['degrees', 'sides'],\n .....:                                 aggfunc='sum')\n .....: \n\nIn [417]: pvt_df\nOut[417]: \n degrees         sides \ntype       other polygon other polygon\nshape \ncircle     360.0     NaN   0.0     NaN\nsquare       NaN   360.0   NaN     4.0\ntriangle     NaN   180.0   NaN     3.0\n\nIn [418]: print(pvt_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row>\n <shape>circle</shape>\n <degrees_other>360.0</degrees_other>\n <degrees_polygon/>\n <sides_other>0.0</sides_other>\n <sides_polygon/>\n </row>\n <row>\n <shape>square</shape>\n <degrees_other/>\n <degrees_polygon>360.0</degrees_polygon>\n <sides_other/>\n <sides_polygon>4.0</sides_polygon>\n </row>\n <row>\n <shape>triangle</shape>\n <degrees_other/>\n <degrees_polygon>180.0</degrees_polygon>\n <sides_other/>\n <sides_polygon>3.0</sides_polygon>\n </row>\n</data> \n```", "```py\nIn [419]: print(geom_df.to_xml(namespaces={\"\": \"https://example.com\"}))\n<?xml version='1.0' encoding='utf-8'?>\n<data >\n <row>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </row>\n <row>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [420]: print(\n .....:    geom_df.to_xml(namespaces={\"doc\": \"https://example.com\"},\n .....:                   prefix=\"doc\")\n .....: )\n .....: \n<?xml version='1.0' encoding='utf-8'?>\n<doc:data >\n <doc:row>\n <doc:index>0</doc:index>\n <doc:shape>square</doc:shape>\n <doc:degrees>360</doc:degrees>\n <doc:sides>4.0</doc:sides>\n </doc:row>\n <doc:row>\n <doc:index>1</doc:index>\n <doc:shape>circle</doc:shape>\n <doc:degrees>360</doc:degrees>\n <doc:sides/>\n </doc:row>\n <doc:row>\n <doc:index>2</doc:index>\n <doc:shape>triangle</doc:shape>\n <doc:degrees>180</doc:degrees>\n <doc:sides>3.0</doc:sides>\n </doc:row>\n</doc:data> \n```", "```py\nIn [421]: print(\n .....:    geom_df.to_xml(xml_declaration=False,\n .....:                   pretty_print=False)\n .....: )\n .....: \n<data><row><index>0</index><shape>square</shape><degrees>360</degrees><sides>4.0</sides></row><row><index>1</index><shape>circle</shape><degrees>360</degrees><sides/></row><row><index>2</index><shape>triangle</shape><degrees>180</degrees><sides>3.0</sides></row></data> \n```", "```py\nIn [422]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" >\n .....:   <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n .....:   <xsl:strip-space elements=\"*\"/>\n .....:   <xsl:template match=\"/data\">\n .....:     <geometry>\n .....:       <xsl:apply-templates select=\"row\"/>\n .....:     </geometry>\n .....:   </xsl:template>\n .....:   <xsl:template match=\"row\">\n .....:     <object index=\"{index}\">\n .....:       <xsl:if test=\"shape!='circle'\">\n .....:           <xsl:attribute name=\"type\">polygon</xsl:attribute>\n .....:       </xsl:if>\n .....:       <xsl:copy-of select=\"shape\"/>\n .....:       <property>\n .....:           <xsl:copy-of select=\"degrees|sides\"/>\n .....:       </property>\n .....:     </object>\n .....:   </xsl:template>\n .....: </xsl:stylesheet>\"\"\"\n .....: \n\nIn [423]: print(geom_df.to_xml(stylesheet=xsl))\n<?xml version=\"1.0\"?>\n<geometry>\n <object index=\"0\" type=\"polygon\">\n <shape>square</shape>\n <property>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </property>\n </object>\n <object index=\"1\">\n <shape>circle</shape>\n <property>\n <degrees>360</degrees>\n <sides/>\n </property>\n </object>\n <object index=\"2\" type=\"polygon\">\n <shape>triangle</shape>\n <property>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </property>\n </object>\n</geometry> \n```", "```py\nIn [372]: from io import StringIO\n\nIn [373]: xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n .....: <bookstore>\n .....:  <book category=\"cooking\">\n .....:    <title lang=\"en\">Everyday Italian</title>\n .....:    <author>Giada De Laurentiis</author>\n .....:    <year>2005</year>\n .....:    <price>30.00</price>\n .....:  </book>\n .....:  <book category=\"children\">\n .....:    <title lang=\"en\">Harry Potter</title>\n .....:    <author>J K. Rowling</author>\n .....:    <year>2005</year>\n .....:    <price>29.99</price>\n .....:  </book>\n .....:  <book category=\"web\">\n .....:    <title lang=\"en\">Learning XML</title>\n .....:    <author>Erik T. Ray</author>\n .....:    <year>2003</year>\n .....:    <price>39.95</price>\n .....:  </book>\n .....: </bookstore>\"\"\"\n .....: \n\nIn [374]: df = pd.read_xml(StringIO(xml))\n\nIn [375]: df\nOut[375]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [376]: df = pd.read_xml(\"https://www.w3schools.com/xml/books.xml\")\n\nIn [377]: df\nOut[377]: \n category              title                  author  year  price      cover\n0   cooking   Everyday Italian     Giada De Laurentiis  2005  30.00       None\n1  children       Harry Potter            J K. Rowling  2005  29.99       None\n2       web  XQuery Kick Start  Vaidyanathan Nagarajan  2003  49.99       None\n3       web       Learning XML             Erik T. Ray  2003  39.95  paperback \n```", "```py\nIn [378]: file_path = \"books.xml\"\n\nIn [379]: with open(file_path, \"w\") as f:\n .....:    f.write(xml)\n .....: \n\nIn [380]: with open(file_path, \"r\") as f:\n .....:    df = pd.read_xml(StringIO(f.read()))\n .....: \n\nIn [381]: df\nOut[381]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [382]: with open(file_path, \"r\") as f:\n .....:    sio = StringIO(f.read())\n .....: \n\nIn [383]: df = pd.read_xml(sio)\n\nIn [384]: df\nOut[384]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [385]: with open(file_path, \"rb\") as f:\n .....:    bio = BytesIO(f.read())\n .....: \n\nIn [386]: df = pd.read_xml(bio)\n\nIn [387]: df\nOut[387]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99\n2       web      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [388]: df = pd.read_xml(\n .....:    \"s3://pmc-oa-opendata/oa_comm/xml/all/PMC1236943.xml\",\n .....:    xpath=\".//journal-meta\",\n .....: )\n .....: \n\nIn [389]: df\nOut[389]: \n journal-id              journal-title       issn  publisher\n0  Cardiovasc Ultrasound  Cardiovascular Ultrasound  1476-7120        NaN \n```", "```py\nIn [390]: df = pd.read_xml(file_path, xpath=\"//book[year=2005]\")\n\nIn [391]: df\nOut[391]: \n category             title               author  year  price\n0   cooking  Everyday Italian  Giada De Laurentiis  2005  30.00\n1  children      Harry Potter         J K. Rowling  2005  29.99 \n```", "```py\nIn [392]: df = pd.read_xml(file_path, elems_only=True)\n\nIn [393]: df\nOut[393]: \n title               author  year  price\n0  Everyday Italian  Giada De Laurentiis  2005  30.00\n1      Harry Potter         J K. Rowling  2005  29.99\n2      Learning XML          Erik T. Ray  2003  39.95 \n```", "```py\nIn [394]: df = pd.read_xml(file_path, attrs_only=True)\n\nIn [395]: df\nOut[395]: \n category\n0   cooking\n1  children\n2       web \n```", "```py\nIn [396]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <doc:data >\n .....:  <doc:row>\n .....:    <doc:shape>square</doc:shape>\n .....:    <doc:degrees>360</doc:degrees>\n .....:    <doc:sides>4.0</doc:sides>\n .....:  </doc:row>\n .....:  <doc:row>\n .....:    <doc:shape>circle</doc:shape>\n .....:    <doc:degrees>360</doc:degrees>\n .....:    <doc:sides/>\n .....:  </doc:row>\n .....:  <doc:row>\n .....:    <doc:shape>triangle</doc:shape>\n .....:    <doc:degrees>180</doc:degrees>\n .....:    <doc:sides>3.0</doc:sides>\n .....:  </doc:row>\n .....: </doc:data>\"\"\"\n .....: \n\nIn [397]: df = pd.read_xml(StringIO(xml),\n .....:                 xpath=\"//doc:row\",\n .....:                 namespaces={\"doc\": \"https://example.com\"})\n .....: \n\nIn [398]: df\nOut[398]: \n shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0 \n```", "```py\nIn [399]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <data >\n .....: <row>\n .....:   <shape>square</shape>\n .....:   <degrees>360</degrees>\n .....:   <sides>4.0</sides>\n .....: </row>\n .....: <row>\n .....:   <shape>circle</shape>\n .....:   <degrees>360</degrees>\n .....:   <sides/>\n .....: </row>\n .....: <row>\n .....:   <shape>triangle</shape>\n .....:   <degrees>180</degrees>\n .....:   <sides>3.0</sides>\n .....: </row>\n .....: </data>\"\"\"\n .....: \n\nIn [400]: df = pd.read_xml(StringIO(xml),\n .....:                 xpath=\"//pandas:row\",\n .....:                 namespaces={\"pandas\": \"https://example.com\"})\n .....: \n\nIn [401]: df\nOut[401]: \n shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0 \n```", "```py\nIn [402]: xml = \"\"\"\n .....: <data>\n .....:  <row>\n .....:    <shape sides=\"4\">square</shape>\n .....:    <degrees>360</degrees>\n .....:  </row>\n .....:  <row>\n .....:    <shape sides=\"0\">circle</shape>\n .....:    <degrees>360</degrees>\n .....:  </row>\n .....:  <row>\n .....:    <shape sides=\"3\">triangle</shape>\n .....:    <degrees>180</degrees>\n .....:  </row>\n .....: </data>\"\"\"\n .....: \n\nIn [403]: df = pd.read_xml(StringIO(xml), xpath=\"./row\")\n\nIn [404]: df\nOut[404]: \n shape  degrees\n0    square      360\n1    circle      360\n2  triangle      180 \n```", "```py\nIn [405]: xml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <response>\n .....:  <row>\n .....:    <station id=\"40850\" name=\"Library\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>864.2</avg_weekday_rides>\n .....:      <avg_saturday_rides>534</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....:  <row>\n .....:    <station id=\"41700\" name=\"Washington/Wabash\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>2707.4</avg_weekday_rides>\n .....:      <avg_saturday_rides>1909.8</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....:  <row>\n .....:    <station id=\"40380\" name=\"Clark/Lake\"/>\n .....:    <month>2020-09-01T00:00:00</month>\n .....:    <rides>\n .....:      <avg_weekday_rides>2949.6</avg_weekday_rides>\n .....:      <avg_saturday_rides>1657</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n .....:    </rides>\n .....:  </row>\n .....: </response>\"\"\"\n .....: \n\nIn [406]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" >\n .....:   <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n .....:   <xsl:strip-space elements=\"*\"/>\n .....:   <xsl:template match=\"/response\">\n .....:      <xsl:copy>\n .....:        <xsl:apply-templates select=\"row\"/>\n .....:      </xsl:copy>\n .....:   </xsl:template>\n .....:   <xsl:template match=\"row\">\n .....:      <xsl:copy>\n .....:        <station_id><xsl:value-of select=\"station/@id\"/></station_id>\n .....:        <station_name><xsl:value-of select=\"station/@name\"/></station_name>\n .....:        <xsl:copy-of select=\"month|rides/*\"/>\n .....:      </xsl:copy>\n .....:   </xsl:template>\n .....: </xsl:stylesheet>\"\"\"\n .....: \n\nIn [407]: output = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n .....: <response>\n .....:   <row>\n .....:      <station_id>40850</station_id>\n .....:      <station_name>Library</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>864.2</avg_weekday_rides>\n .....:      <avg_saturday_rides>534</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n .....:   </row>\n .....:   <row>\n .....:      <station_id>41700</station_id>\n .....:      <station_name>Washington/Wabash</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>2707.4</avg_weekday_rides>\n .....:      <avg_saturday_rides>1909.8</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n .....:   </row>\n .....:   <row>\n .....:      <station_id>40380</station_id>\n .....:      <station_name>Clark/Lake</station_name>\n .....:      <month>2020-09-01T00:00:00</month>\n .....:      <avg_weekday_rides>2949.6</avg_weekday_rides>\n .....:      <avg_saturday_rides>1657</avg_saturday_rides>\n .....:      <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n .....:   </row>\n .....: </response>\"\"\"\n .....: \n\nIn [408]: df = pd.read_xml(StringIO(xml), stylesheet=xsl)\n\nIn [409]: df\nOut[409]: \n station_id       station_name  ... avg_saturday_rides  avg_sunday_holiday_rides\n0       40850            Library  ...              534.0                     417.2\n1       41700  Washington/Wabash  ...             1909.8                    1438.6\n2       40380         Clark/Lake  ...             1657.0                    1453.8\n\n[3 rows x 6 columns] \n```", "```py\nIn [1]: df = pd.read_xml(\n...         \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n...         iterparse = {\"page\": [\"title\", \"ns\", \"id\"]}\n...     )\n...     df\nOut[2]:\n title   ns        id\n0                                       Gettysburg Address    0     21450\n1                                                Main Page    0     42950\n2                            Declaration by United Nations    0      8435\n3             Constitution of the United States of America    0      8435\n4                     Declaration of Independence (Israel)    0     17858\n...                                                    ...  ...       ...\n3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649\n3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649\n3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649\n3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291\n3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450\n\n[3578765 rows x 3 columns] \n```", "```py\nIn [410]: geom_df = pd.DataFrame(\n .....:    {\n .....:        \"shape\": [\"square\", \"circle\", \"triangle\"],\n .....:        \"degrees\": [360, 360, 180],\n .....:        \"sides\": [4, np.nan, 3],\n .....:    }\n .....: )\n .....: \n\nIn [411]: print(geom_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </row>\n <row>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [412]: print(geom_df.to_xml(root_name=\"geometry\", row_name=\"objects\"))\n<?xml version='1.0' encoding='utf-8'?>\n<geometry>\n <objects>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </objects>\n <objects>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </objects>\n <objects>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </objects>\n</geometry> \n```", "```py\nIn [413]: print(geom_df.to_xml(attr_cols=geom_df.columns.tolist()))\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row index=\"0\" shape=\"square\" degrees=\"360\" sides=\"4.0\"/>\n <row index=\"1\" shape=\"circle\" degrees=\"360\"/>\n <row index=\"2\" shape=\"triangle\" degrees=\"180\" sides=\"3.0\"/>\n</data> \n```", "```py\nIn [414]: print(\n .....:    geom_df.to_xml(\n .....:        index=False,\n .....:        attr_cols=['shape'],\n .....:        elem_cols=['degrees', 'sides'])\n .....: )\n .....: \n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row shape=\"square\">\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row shape=\"circle\">\n <degrees>360</degrees>\n <sides/>\n </row>\n <row shape=\"triangle\">\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [415]: ext_geom_df = pd.DataFrame(\n .....:    {\n .....:        \"type\": [\"polygon\", \"other\", \"polygon\"],\n .....:        \"shape\": [\"square\", \"circle\", \"triangle\"],\n .....:        \"degrees\": [360, 360, 180],\n .....:        \"sides\": [4, np.nan, 3],\n .....:    }\n .....: )\n .....: \n\nIn [416]: pvt_df = ext_geom_df.pivot_table(index='shape',\n .....:                                 columns='type',\n .....:                                 values=['degrees', 'sides'],\n .....:                                 aggfunc='sum')\n .....: \n\nIn [417]: pvt_df\nOut[417]: \n degrees         sides \ntype       other polygon other polygon\nshape \ncircle     360.0     NaN   0.0     NaN\nsquare       NaN   360.0   NaN     4.0\ntriangle     NaN   180.0   NaN     3.0\n\nIn [418]: print(pvt_df.to_xml())\n<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row>\n <shape>circle</shape>\n <degrees_other>360.0</degrees_other>\n <degrees_polygon/>\n <sides_other>0.0</sides_other>\n <sides_polygon/>\n </row>\n <row>\n <shape>square</shape>\n <degrees_other/>\n <degrees_polygon>360.0</degrees_polygon>\n <sides_other/>\n <sides_polygon>4.0</sides_polygon>\n </row>\n <row>\n <shape>triangle</shape>\n <degrees_other/>\n <degrees_polygon>180.0</degrees_polygon>\n <sides_other/>\n <sides_polygon>3.0</sides_polygon>\n </row>\n</data> \n```", "```py\nIn [419]: print(geom_df.to_xml(namespaces={\"\": \"https://example.com\"}))\n<?xml version='1.0' encoding='utf-8'?>\n<data >\n <row>\n <index>0</index>\n <shape>square</shape>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </row>\n <row>\n <index>1</index>\n <shape>circle</shape>\n <degrees>360</degrees>\n <sides/>\n </row>\n <row>\n <index>2</index>\n <shape>triangle</shape>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </row>\n</data> \n```", "```py\nIn [420]: print(\n .....:    geom_df.to_xml(namespaces={\"doc\": \"https://example.com\"},\n .....:                   prefix=\"doc\")\n .....: )\n .....: \n<?xml version='1.0' encoding='utf-8'?>\n<doc:data >\n <doc:row>\n <doc:index>0</doc:index>\n <doc:shape>square</doc:shape>\n <doc:degrees>360</doc:degrees>\n <doc:sides>4.0</doc:sides>\n </doc:row>\n <doc:row>\n <doc:index>1</doc:index>\n <doc:shape>circle</doc:shape>\n <doc:degrees>360</doc:degrees>\n <doc:sides/>\n </doc:row>\n <doc:row>\n <doc:index>2</doc:index>\n <doc:shape>triangle</doc:shape>\n <doc:degrees>180</doc:degrees>\n <doc:sides>3.0</doc:sides>\n </doc:row>\n</doc:data> \n```", "```py\nIn [421]: print(\n .....:    geom_df.to_xml(xml_declaration=False,\n .....:                   pretty_print=False)\n .....: )\n .....: \n<data><row><index>0</index><shape>square</shape><degrees>360</degrees><sides>4.0</sides></row><row><index>1</index><shape>circle</shape><degrees>360</degrees><sides/></row><row><index>2</index><shape>triangle</shape><degrees>180</degrees><sides>3.0</sides></row></data> \n```", "```py\nIn [422]: xsl = \"\"\"<xsl:stylesheet version=\"1.0\" >\n .....:   <xsl:output method=\"xml\" omit-xml-declaration=\"no\" indent=\"yes\"/>\n .....:   <xsl:strip-space elements=\"*\"/>\n .....:   <xsl:template match=\"/data\">\n .....:     <geometry>\n .....:       <xsl:apply-templates select=\"row\"/>\n .....:     </geometry>\n .....:   </xsl:template>\n .....:   <xsl:template match=\"row\">\n .....:     <object index=\"{index}\">\n .....:       <xsl:if test=\"shape!='circle'\">\n .....:           <xsl:attribute name=\"type\">polygon</xsl:attribute>\n .....:       </xsl:if>\n .....:       <xsl:copy-of select=\"shape\"/>\n .....:       <property>\n .....:           <xsl:copy-of select=\"degrees|sides\"/>\n .....:       </property>\n .....:     </object>\n .....:   </xsl:template>\n .....: </xsl:stylesheet>\"\"\"\n .....: \n\nIn [423]: print(geom_df.to_xml(stylesheet=xsl))\n<?xml version=\"1.0\"?>\n<geometry>\n <object index=\"0\" type=\"polygon\">\n <shape>square</shape>\n <property>\n <degrees>360</degrees>\n <sides>4.0</sides>\n </property>\n </object>\n <object index=\"1\">\n <shape>circle</shape>\n <property>\n <degrees>360</degrees>\n <sides/>\n </property>\n </object>\n <object index=\"2\" type=\"polygon\">\n <shape>triangle</shape>\n <property>\n <degrees>180</degrees>\n <sides>3.0</sides>\n </property>\n </object>\n</geometry> \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", sheet_name=\"Sheet1\") \n```", "```py\nxlsx = pd.ExcelFile(\"path_to_file.xls\")\ndf = pd.read_excel(xlsx, \"Sheet1\") \n```", "```py\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\") \n```", "```py\ndata = {}\n# For when Sheet1's format differs from Sheet2\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=1) \n```", "```py\n# using the ExcelFile class\ndata = {}\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=None, na_values=[\"NA\"])\n\n# equivalent using the read_excel function\ndata = pd.read_excel(\n    \"path_to_file.xls\", [\"Sheet1\", \"Sheet2\"], index_col=None, na_values=[\"NA\"]\n) \n```", "```py\nimport xlrd\n\nxlrd_book = xlrd.open_workbook(\"path_to_file.xls\", on_demand=True)\nwith pd.ExcelFile(xlrd_book) as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", index_col=None, na_values=[\"NA\"]) \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", 0, index_col=None, na_values=[\"NA\"]) \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\") \n```", "```py\n# Returns a dictionary of DataFrames\npd.read_excel(\"path_to_file.xls\", sheet_name=None) \n```", "```py\n# Returns the 1st and 4th sheet, as a dictionary of DataFrames.\npd.read_excel(\"path_to_file.xls\", sheet_name=[\"Sheet1\", 3]) \n```", "```py\nIn [424]: df = pd.DataFrame(\n .....:    {\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]},\n .....:    index=pd.MultiIndex.from_product([[\"a\", \"b\"], [\"c\", \"d\"]]),\n .....: )\n .....: \n\nIn [425]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [426]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [427]: df\nOut[427]: \n a  b\na c  1  5\n d  2  6\nb c  3  7\n d  4  8 \n```", "```py\nIn [428]: df.index = df.index.set_names([\"lvl1\", \"lvl2\"])\n\nIn [429]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [430]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [431]: df\nOut[431]: \n a  b\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\nIn [432]: df.columns = pd.MultiIndex.from_product([[\"a\"], [\"b\", \"d\"]], names=[\"c1\", \"c2\"])\n\nIn [433]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [434]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1], header=[0, 1])\n\nIn [435]: df\nOut[435]: \nc1         a \nc2         b  d\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=\"A,C:E\") \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[0, 2, 3]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[\"foo\", \"bar\"]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=lambda x: x.isalpha()) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", parse_dates=[\"date_strings\"]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyBools\": bool}) \n```", "```py\ndef cfun(x):\n    return int(x) if x else -1\n\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyInts\": cfun}) \n```", "```py\npd.read_excel(\"path_to_file.xls\", dtype={\"MyInts\": \"int64\", \"MyText\": str}) \n```", "```py\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\") \n```", "```py\ndf.to_excel(\"path_to_file.xlsx\", index_label=\"label\", merge_cells=False) \n```", "```py\nwith pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Sheet1\")\n    df2.to_excel(writer, sheet_name=\"Sheet2\") \n```", "```py\nfrom io import BytesIO\n\nbio = BytesIO()\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(bio, engine=\"xlsxwriter\")\ndf.to_excel(writer, sheet_name=\"Sheet1\")\n\n# Save the workbook\nwriter.save()\n\n# Seek to the beginning and read to copy the workbook to a variable in memory\nbio.seek(0)\nworkbook = bio.read() \n```", "```py\n# By setting the 'engine' in the DataFrame 'to_excel()' methods.\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\", engine=\"xlsxwriter\")\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(\"path_to_file.xlsx\", engine=\"xlsxwriter\")\n\n# Or via pandas configuration.\nfrom pandas import options  # noqa: E402\n\noptions.io.excel.xlsx.writer = \"xlsxwriter\"\n\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", sheet_name=\"Sheet1\") \n```", "```py\nxlsx = pd.ExcelFile(\"path_to_file.xls\")\ndf = pd.read_excel(xlsx, \"Sheet1\") \n```", "```py\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\") \n```", "```py\ndata = {}\n# For when Sheet1's format differs from Sheet2\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=1) \n```", "```py\n# using the ExcelFile class\ndata = {}\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=None, na_values=[\"NA\"])\n\n# equivalent using the read_excel function\ndata = pd.read_excel(\n    \"path_to_file.xls\", [\"Sheet1\", \"Sheet2\"], index_col=None, na_values=[\"NA\"]\n) \n```", "```py\nimport xlrd\n\nxlrd_book = xlrd.open_workbook(\"path_to_file.xls\", on_demand=True)\nwith pd.ExcelFile(xlrd_book) as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", index_col=None, na_values=[\"NA\"]) \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", 0, index_col=None, na_values=[\"NA\"]) \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\") \n```", "```py\n# Returns a dictionary of DataFrames\npd.read_excel(\"path_to_file.xls\", sheet_name=None) \n```", "```py\n# Returns the 1st and 4th sheet, as a dictionary of DataFrames.\npd.read_excel(\"path_to_file.xls\", sheet_name=[\"Sheet1\", 3]) \n```", "```py\nIn [424]: df = pd.DataFrame(\n .....:    {\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]},\n .....:    index=pd.MultiIndex.from_product([[\"a\", \"b\"], [\"c\", \"d\"]]),\n .....: )\n .....: \n\nIn [425]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [426]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [427]: df\nOut[427]: \n a  b\na c  1  5\n d  2  6\nb c  3  7\n d  4  8 \n```", "```py\nIn [428]: df.index = df.index.set_names([\"lvl1\", \"lvl2\"])\n\nIn [429]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [430]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [431]: df\nOut[431]: \n a  b\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\nIn [432]: df.columns = pd.MultiIndex.from_product([[\"a\"], [\"b\", \"d\"]], names=[\"c1\", \"c2\"])\n\nIn [433]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [434]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1], header=[0, 1])\n\nIn [435]: df\nOut[435]: \nc1         a \nc2         b  d\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=\"A,C:E\") \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[0, 2, 3]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[\"foo\", \"bar\"]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=lambda x: x.isalpha()) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", parse_dates=[\"date_strings\"]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyBools\": bool}) \n```", "```py\ndef cfun(x):\n    return int(x) if x else -1\n\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyInts\": cfun}) \n```", "```py\npd.read_excel(\"path_to_file.xls\", dtype={\"MyInts\": \"int64\", \"MyText\": str}) \n```", "```py\nxlsx = pd.ExcelFile(\"path_to_file.xls\")\ndf = pd.read_excel(xlsx, \"Sheet1\") \n```", "```py\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\") \n```", "```py\ndata = {}\n# For when Sheet1's format differs from Sheet2\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=1) \n```", "```py\n# using the ExcelFile class\ndata = {}\nwith pd.ExcelFile(\"path_to_file.xls\") as xls:\n    data[\"Sheet1\"] = pd.read_excel(xls, \"Sheet1\", index_col=None, na_values=[\"NA\"])\n    data[\"Sheet2\"] = pd.read_excel(xls, \"Sheet2\", index_col=None, na_values=[\"NA\"])\n\n# equivalent using the read_excel function\ndata = pd.read_excel(\n    \"path_to_file.xls\", [\"Sheet1\", \"Sheet2\"], index_col=None, na_values=[\"NA\"]\n) \n```", "```py\nimport xlrd\n\nxlrd_book = xlrd.open_workbook(\"path_to_file.xls\", on_demand=True)\nwith pd.ExcelFile(xlrd_book) as xls:\n    df1 = pd.read_excel(xls, \"Sheet1\")\n    df2 = pd.read_excel(xls, \"Sheet2\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", index_col=None, na_values=[\"NA\"]) \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\", 0, index_col=None, na_values=[\"NA\"]) \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xls\") \n```", "```py\n# Returns a dictionary of DataFrames\npd.read_excel(\"path_to_file.xls\", sheet_name=None) \n```", "```py\n# Returns the 1st and 4th sheet, as a dictionary of DataFrames.\npd.read_excel(\"path_to_file.xls\", sheet_name=[\"Sheet1\", 3]) \n```", "```py\nIn [424]: df = pd.DataFrame(\n .....:    {\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]},\n .....:    index=pd.MultiIndex.from_product([[\"a\", \"b\"], [\"c\", \"d\"]]),\n .....: )\n .....: \n\nIn [425]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [426]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [427]: df\nOut[427]: \n a  b\na c  1  5\n d  2  6\nb c  3  7\n d  4  8 \n```", "```py\nIn [428]: df.index = df.index.set_names([\"lvl1\", \"lvl2\"])\n\nIn [429]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [430]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1])\n\nIn [431]: df\nOut[431]: \n a  b\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\nIn [432]: df.columns = pd.MultiIndex.from_product([[\"a\"], [\"b\", \"d\"]], names=[\"c1\", \"c2\"])\n\nIn [433]: df.to_excel(\"path_to_file.xlsx\")\n\nIn [434]: df = pd.read_excel(\"path_to_file.xlsx\", index_col=[0, 1], header=[0, 1])\n\nIn [435]: df\nOut[435]: \nc1         a \nc2         b  d\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=\"A,C:E\") \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[0, 2, 3]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[\"foo\", \"bar\"]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=lambda x: x.isalpha()) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", parse_dates=[\"date_strings\"]) \n```", "```py\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyBools\": bool}) \n```", "```py\ndef cfun(x):\n    return int(x) if x else -1\n\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyInts\": cfun}) \n```", "```py\npd.read_excel(\"path_to_file.xls\", dtype={\"MyInts\": \"int64\", \"MyText\": str}) \n```", "```py\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\") \n```", "```py\ndf.to_excel(\"path_to_file.xlsx\", index_label=\"label\", merge_cells=False) \n```", "```py\nwith pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Sheet1\")\n    df2.to_excel(writer, sheet_name=\"Sheet2\") \n```", "```py\nfrom io import BytesIO\n\nbio = BytesIO()\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(bio, engine=\"xlsxwriter\")\ndf.to_excel(writer, sheet_name=\"Sheet1\")\n\n# Save the workbook\nwriter.save()\n\n# Seek to the beginning and read to copy the workbook to a variable in memory\nbio.seek(0)\nworkbook = bio.read() \n```", "```py\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\") \n```", "```py\ndf.to_excel(\"path_to_file.xlsx\", index_label=\"label\", merge_cells=False) \n```", "```py\nwith pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Sheet1\")\n    df2.to_excel(writer, sheet_name=\"Sheet2\") \n```", "```py\nfrom io import BytesIO\n\nbio = BytesIO()\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(bio, engine=\"xlsxwriter\")\ndf.to_excel(writer, sheet_name=\"Sheet1\")\n\n# Save the workbook\nwriter.save()\n\n# Seek to the beginning and read to copy the workbook to a variable in memory\nbio.seek(0)\nworkbook = bio.read() \n```", "```py\n# By setting the 'engine' in the DataFrame 'to_excel()' methods.\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\", engine=\"xlsxwriter\")\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(\"path_to_file.xlsx\", engine=\"xlsxwriter\")\n\n# Or via pandas configuration.\nfrom pandas import options  # noqa: E402\n\noptions.io.excel.xlsx.writer = \"xlsxwriter\"\n\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.ods\", engine=\"odf\") \n```", "```py\n# Writes DataFrame to a .ods file\ndf.to_excel(\"path_to_file.ods\", engine=\"odf\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"pyxlsb\") \n```", "```py\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"calamine\") \n```", "```py\n A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r \n```", "```py\n>>> clipdf = pd.read_clipboard()\n>>> clipdf\n A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r \n```", "```py\n>>> df = pd.DataFrame(\n...     {\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [\"p\", \"q\", \"r\"]}, index=[\"x\", \"y\", \"z\"]\n... )\n\n>>> df\n A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n>>> df.to_clipboard()\n>>> pd.read_clipboard()\n A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r \n```", "```py\nIn [436]: df\nOut[436]: \nc1         a \nc2         b  d\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8\n\nIn [437]: df.to_pickle(\"foo.pkl\") \n```", "```py\nIn [438]: pd.read_pickle(\"foo.pkl\")\nOut[438]: \nc1         a \nc2         b  d\nlvl1 lvl2 \na    c     1  5\n d     2  6\nb    c     3  7\n d     4  8 \n```", "```py\nIn [439]: df = pd.DataFrame(\n .....:    {\n .....:        \"A\": np.random.randn(1000),\n .....:        \"B\": \"foo\",\n .....:        \"C\": pd.date_range(\"20130101\", periods=1000, freq=\"s\"),\n .....:    }\n .....: )\n .....: \n\nIn [440]: df\nOut[440]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [441]: df.to_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [442]: rt = pd.read_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [443]: rt\nOut[443]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [444]: df.to_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [445]: rt = pd.read_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [446]: rt\nOut[446]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [447]: df.to_pickle(\"data.pkl.gz\")\n\nIn [448]: rt = pd.read_pickle(\"data.pkl.gz\")\n\nIn [449]: rt\nOut[449]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]\n\nIn [450]: df[\"A\"].to_pickle(\"s1.pkl.bz2\")\n\nIn [451]: rt = pd.read_pickle(\"s1.pkl.bz2\")\n\nIn [452]: rt\nOut[452]: \n0     -0.317441\n1     -1.236269\n2      0.896171\n3     -0.487602\n4     -0.082240\n ... \n995   -0.171092\n996    1.786173\n997   -0.575189\n998    0.820750\n999   -1.256530\nName: A, Length: 1000, dtype: float64 \n```", "```py\nIn [453]: df.to_pickle(\"data.pkl.gz\", compression={\"method\": \"gzip\", \"compresslevel\": 1}) \n```", "```py\nIn [439]: df = pd.DataFrame(\n .....:    {\n .....:        \"A\": np.random.randn(1000),\n .....:        \"B\": \"foo\",\n .....:        \"C\": pd.date_range(\"20130101\", periods=1000, freq=\"s\"),\n .....:    }\n .....: )\n .....: \n\nIn [440]: df\nOut[440]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [441]: df.to_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [442]: rt = pd.read_pickle(\"data.pkl.compress\", compression=\"gzip\")\n\nIn [443]: rt\nOut[443]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [444]: df.to_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [445]: rt = pd.read_pickle(\"data.pkl.xz\", compression=\"infer\")\n\nIn [446]: rt\nOut[446]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns] \n```", "```py\nIn [447]: df.to_pickle(\"data.pkl.gz\")\n\nIn [448]: rt = pd.read_pickle(\"data.pkl.gz\")\n\nIn [449]: rt\nOut[449]: \n A    B                   C\n0   -0.317441  foo 2013-01-01 00:00:00\n1   -1.236269  foo 2013-01-01 00:00:01\n2    0.896171  foo 2013-01-01 00:00:02\n3   -0.487602  foo 2013-01-01 00:00:03\n4   -0.082240  foo 2013-01-01 00:00:04\n..        ...  ...                 ...\n995 -0.171092  foo 2013-01-01 00:16:35\n996  1.786173  foo 2013-01-01 00:16:36\n997 -0.575189  foo 2013-01-01 00:16:37\n998  0.820750  foo 2013-01-01 00:16:38\n999 -1.256530  foo 2013-01-01 00:16:39\n\n[1000 rows x 3 columns]\n\nIn [450]: df[\"A\"].to_pickle(\"s1.pkl.bz2\")\n\nIn [451]: rt = pd.read_pickle(\"s1.pkl.bz2\")\n\nIn [452]: rt\nOut[452]: \n0     -0.317441\n1     -1.236269\n2      0.896171\n3     -0.487602\n4     -0.082240\n ... \n995   -0.171092\n996    1.786173\n997   -0.575189\n998    0.820750\n999   -1.256530\nName: A, Length: 1000, dtype: float64 \n```", "```py\nIn [453]: df.to_pickle(\"data.pkl.gz\", compression={\"method\": \"gzip\", \"compresslevel\": 1}) \n```", "```py\nIn [454]: store = pd.HDFStore(\"store.h5\")\n\nIn [455]: print(store)\n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\nIn [456]: index = pd.date_range(\"1/1/2000\", periods=8)\n\nIn [457]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n\nIn [458]: df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\n# store.put('s', s) is an equivalent method\nIn [459]: store[\"s\"] = s\n\nIn [460]: store[\"df\"] = df\n\nIn [461]: store\nOut[461]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\n# store.get('df') is an equivalent method\nIn [462]: store[\"df\"]\nOut[462]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\n\n# dotted (attribute) access provides get as well\nIn [463]: store.df\nOut[463]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\n# store.remove('df') is an equivalent method\nIn [464]: del store[\"df\"]\n\nIn [465]: store\nOut[465]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\nIn [466]: store.close()\n\nIn [467]: store\nOut[467]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\nIn [468]: store.is_open\nOut[468]: False\n\n# Working with, and automatically closing the store using a context manager\nIn [469]: with pd.HDFStore(\"store.h5\") as store:\n .....:    store.keys()\n .....: \n```", "```py\nIn [470]: df_tl = pd.DataFrame({\"A\": list(range(5)), \"B\": list(range(5))})\n\nIn [471]: df_tl.to_hdf(\"store_tl.h5\", key=\"table\", append=True)\n\nIn [472]: pd.read_hdf(\"store_tl.h5\", \"table\", where=[\"index>2\"])\nOut[472]: \n A  B\n3  3  3\n4  4  4 \n```", "```py\nIn [473]: df_with_missing = pd.DataFrame(\n .....:    {\n .....:        \"col1\": [0, np.nan, 2],\n .....:        \"col2\": [1, np.nan, np.nan],\n .....:    }\n .....: )\n .....: \n\nIn [474]: df_with_missing\nOut[474]: \n col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [475]: df_with_missing.to_hdf(\"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\")\n\nIn [476]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[476]: \n col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [477]: df_with_missing.to_hdf(\n .....:    \"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\", dropna=True\n .....: )\n .....: \n\nIn [478]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[478]: \n col1  col2\n0   0.0   1.0\n2   2.0   NaN \n```", "```py\nIn [479]: pd.DataFrame(np.random.randn(10, 2)).to_hdf(\"test_fixed.h5\", key=\"df\")\n\nIn [480]: pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n---------------------------------------------------------------------------\nTypeError  Traceback (most recent call last)\nCell In[480], line 1\n----> 1 pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:452, in read_hdf(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\n  447                 raise ValueError(\n  448                     \"key must be provided when HDF5 \"\n  449                     \"file contains multiple datasets.\"\n  450                 )\n  451         key = candidate_only_group._v_pathname\n--> 452     return store.select(\n  453         key,\n  454         where=where,\n  455         start=start,\n  456         stop=stop,\n  457         columns=columns,\n  458         iterator=iterator,\n  459         chunksize=chunksize,\n  460         auto_close=auto_close,\n  461     )\n  462 except (ValueError, TypeError, LookupError):\n  463     if not isinstance(path_or_buf, HDFStore):\n  464         # if there is an error, close the store if we opened it.\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:906, in HDFStore.select(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\n  892 # create the iterator\n  893 it = TableIterator(\n  894     self,\n  895     s,\n   (...)\n  903     auto_close=auto_close,\n  904 )\n--> 906 return it.get_result()\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:2029, in TableIterator.get_result(self, coordinates)\n  2026     where = self.where\n  2028 # directly return the result\n-> 2029 results = self.func(self.start, self.stop, where)\n  2030 self.close()\n  2031 return results\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:890, in HDFStore.select.<locals>.func(_start, _stop, _where)\n  889 def func(_start, _stop, _where):\n--> 890     return s.read(start=_start, stop=_stop, where=_where, columns=columns)\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:3278, in BlockManagerFixed.read(self, where, columns, start, stop)\n  3270 def read(\n  3271     self,\n  3272     where=None,\n   (...)\n  3276 ) -> DataFrame:\n  3277     # start, stop applied to rows, so 0th axis only\n-> 3278     self.validate_read(columns, where)\n  3279     select_axis = self.obj_type()._get_block_manager_axis(0)\n  3281     axes = []\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:2922, in GenericFixed.validate_read(self, columns, where)\n  2917     raise TypeError(\n  2918         \"cannot pass a column specification when reading \"\n  2919         \"a Fixed format store. this store must be selected in its entirety\"\n  2920     )\n  2921 if where is not None:\n-> 2922     raise TypeError(\n  2923         \"cannot pass a where specification when reading \"\n  2924         \"from a Fixed format store. this store must be selected in its entirety\"\n  2925     )\n\nTypeError: cannot pass a where specification when reading from a Fixed format store. this store must be selected in its entirety \n```", "```py\nIn [481]: store = pd.HDFStore(\"store.h5\")\n\nIn [482]: df1 = df[0:4]\n\nIn [483]: df2 = df[4:]\n\n# append data (creates a table automatically)\nIn [484]: store.append(\"df\", df1)\n\nIn [485]: store.append(\"df\", df2)\n\nIn [486]: store\nOut[486]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# select the entire object\nIn [487]: store.select(\"df\")\nOut[487]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\n\n# the type of stored data\nIn [488]: store.root.df._v_attrs.pandas_type\nOut[488]: 'frame_table' \n```", "```py\nIn [489]: store.put(\"foo/bar/bah\", df)\n\nIn [490]: store.append(\"food/orange\", df)\n\nIn [491]: store.append(\"food/apple\", df)\n\nIn [492]: store\nOut[492]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# a list of keys are returned\nIn [493]: store.keys()\nOut[493]: ['/df', '/food/apple', '/food/orange', '/foo/bar/bah']\n\n# remove all nodes under this level\nIn [494]: store.remove(\"food\")\n\nIn [495]: store\nOut[495]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\nIn [496]: for (path, subgroups, subkeys) in store.walk():\n .....:    for subgroup in subgroups:\n .....:        print(\"GROUP: {}/{}\".format(path, subgroup))\n .....:    for subkey in subkeys:\n .....:        key = \"/\".join([path, subkey])\n .....:        print(\"KEY: {}\".format(key))\n .....:        print(store.get(key))\n .....: \nGROUP: /foo\nKEY: /df\n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\nGROUP: /foo/bar\nKEY: /foo/bar/bah\n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nIn [497]: store.foo.bar.bah\n---------------------------------------------------------------------------\nTypeError  Traceback (most recent call last)\nCell In[497], line 1\n----> 1 store.foo.bar.bah\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:613, in HDFStore.__getattr__(self, name)\n  611  \"\"\"allow attribute access to get stores\"\"\"\n  612 try:\n--> 613     return self.get(name)\n  614 except (KeyError, ClosedFileError):\n  615     pass\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:813, in HDFStore.get(self, key)\n  811 if group is None:\n  812     raise KeyError(f\"No object named {key} in the file\")\n--> 813 return self._read_group(group)\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:1878, in HDFStore._read_group(self, group)\n  1877 def _read_group(self, group: Node):\n-> 1878     s = self._create_storer(group)\n  1879     s.infer_axes()\n  1880     return s.read()\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:1752, in HDFStore._create_storer(self, group, format, value, encoding, errors)\n  1750         tt = \"generic_table\"\n  1751     else:\n-> 1752         raise TypeError(\n  1753             \"cannot create a storer if the object is not existing \"\n  1754             \"nor a value are passed\"\n  1755         )\n  1756 else:\n  1757     if isinstance(value, Series):\n\nTypeError: cannot create a storer if the object is not existing nor a value are passed \n```", "```py\n# you can directly access the actual PyTables node but using the root node\nIn [498]: store.root.foo.bar.bah\nOut[498]: \n/foo/bar/bah (Group) ''\n children := ['axis0' (Array), 'axis1' (Array), 'block0_items' (Array), 'block0_values' (Array)] \n```", "```py\nIn [499]: store[\"foo/bar/bah\"]\nOut[499]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nIn [500]: df_mixed = pd.DataFrame(\n .....:    {\n .....:        \"A\": np.random.randn(8),\n .....:        \"B\": np.random.randn(8),\n .....:        \"C\": np.array(np.random.randn(8), dtype=\"float32\"),\n .....:        \"string\": \"string\",\n .....:        \"int\": 1,\n .....:        \"bool\": True,\n .....:        \"datetime64\": pd.Timestamp(\"20010102\"),\n .....:    },\n .....:    index=list(range(8)),\n .....: )\n .....: \n\nIn [501]: df_mixed.loc[df_mixed.index[3:5], [\"A\", \"B\", \"string\", \"datetime64\"]] = np.nan\n\nIn [502]: store.append(\"df_mixed\", df_mixed, min_itemsize={\"values\": 50})\n\nIn [503]: df_mixed1 = store.select(\"df_mixed\")\n\nIn [504]: df_mixed1\nOut[504]: \n A         B         C  ... int  bool                    datetime64\n0  0.013747 -1.166078 -1.292080  ...   1  True 1970-01-01 00:00:00.978393600\n1 -0.712009  0.247572  1.526911  ...   1  True 1970-01-01 00:00:00.978393600\n2 -0.645096  1.687406  0.288504  ...   1  True 1970-01-01 00:00:00.978393600\n3       NaN       NaN  0.097771  ...   1  True                           NaT\n4       NaN       NaN  1.536408  ...   1  True                           NaT\n5 -0.023202  0.043702  0.926790  ...   1  True 1970-01-01 00:00:00.978393600\n6  2.359782  0.088224 -0.676448  ...   1  True 1970-01-01 00:00:00.978393600\n7 -0.143428 -0.813360 -0.179724  ...   1  True 1970-01-01 00:00:00.978393600\n\n[8 rows x 7 columns]\n\nIn [505]: df_mixed1.dtypes.value_counts()\nOut[505]: \nfloat64           2\nfloat32           1\nobject            1\nint64             1\nbool              1\ndatetime64[ns]    1\nName: count, dtype: int64\n\n# we have provided a minimum string column size\nIn [506]: store.root.df_mixed.table\nOut[506]: \n/df_mixed/table (Table(8,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1),\n \"values_block_1\": Float32Col(shape=(1,), dflt=0.0, pos=2),\n \"values_block_2\": StringCol(itemsize=50, shape=(1,), dflt=b'', pos=3),\n \"values_block_3\": Int64Col(shape=(1,), dflt=0, pos=4),\n \"values_block_4\": BoolCol(shape=(1,), dflt=False, pos=5),\n \"values_block_5\": Int64Col(shape=(1,), dflt=0, pos=6)}\n byteorder := 'little'\n chunkshape := (689,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [507]: index = pd.MultiIndex(\n .....:   levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n .....:   codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n .....:   names=[\"foo\", \"bar\"],\n .....: )\n .....: \n\nIn [508]: df_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [509]: df_mi\nOut[509]: \n A         B         C\nfoo bar \nfoo one   -1.303456 -0.642994 -0.649456\n two    1.012694  0.414147  1.950460\n three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n two   -1.660522  0.929553 -1.298649\n three  3.565769  0.682402  1.041927\n\nIn [510]: store.append(\"df_mi\", df_mi)\n\nIn [511]: store.select(\"df_mi\")\nOut[511]: \n A         B         C\nfoo bar \nfoo one   -1.303456 -0.642994 -0.649456\n two    1.012694  0.414147  1.950460\n three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n two   -1.660522  0.929553 -1.298649\n three  3.565769  0.682402  1.041927\n\n# the levels are automatically included as data columns\nIn [512]: store.select(\"df_mi\", \"foo=bar\")\nOut[512]: \n A         B         C\nfoo bar \nbar one  0.410395  0.618321  0.560398\n two  1.434027 -0.033270  0.343197 \n```", "```py\nstring = \"HolyMoly'\"\nstore.select(\"df\", \"index == string\") \n```", "```py\nstring = \"HolyMoly'\"\nstore.select('df', f'index == {string}') \n```", "```py\nstore.select(\"df\", \"index == %r\" % string) \n```", "```py\nIn [513]: dfq = pd.DataFrame(\n .....:    np.random.randn(10, 4),\n .....:    columns=list(\"ABCD\"),\n .....:    index=pd.date_range(\"20130101\", periods=10),\n .....: )\n .....: \n\nIn [514]: store.append(\"dfq\", dfq, format=\"table\", data_columns=True) \n```", "```py\nIn [515]: store.select(\"dfq\", \"index>pd.Timestamp('20130104') & columns=['A', 'B']\")\nOut[515]: \n A         B\n2013-01-05 -0.830545 -0.457071\n2013-01-06  0.431186  1.049421\n2013-01-07  0.617509 -0.811230\n2013-01-08  0.947422 -0.671233\n2013-01-09 -0.183798 -1.211230\n2013-01-10  0.361428  0.887304 \n```", "```py\nIn [516]: store.select(\"dfq\", where=\"A>0 or C>0\")\nOut[516]: \n A         B         C         D\n2013-01-02  0.658179  0.362814 -0.917897  0.010165\n2013-01-03  0.905122  1.848731 -1.184241  0.932053\n2013-01-05 -0.830545 -0.457071  1.565581  1.148032\n2013-01-06  0.431186  1.049421  0.383309  0.595013\n2013-01-07  0.617509 -0.811230 -2.088563 -1.393500\n2013-01-08  0.947422 -0.671233 -0.847097 -1.187785\n2013-01-10  0.361428  0.887304  0.266457 -0.399641 \n```", "```py\nIn [517]: store.select(\"df\", \"columns=['A', 'B']\")\nOut[517]: \n A         B\n2000-01-01  0.858644 -0.851236\n2000-01-02 -0.080372 -1.268121\n2000-01-03  0.816983  1.965656\n2000-01-04  0.712795 -0.062433\n2000-01-05 -0.298721 -1.988045\n2000-01-06  1.103675  1.382242\n2000-01-07 -0.729161 -0.142928\n2000-01-08 -1.005977  0.465222 \n```", "```py\nIn [518]: from datetime import timedelta\n\nIn [519]: dftd = pd.DataFrame(\n .....:    {\n .....:        \"A\": pd.Timestamp(\"20130101\"),\n .....:        \"B\": [\n .....:            pd.Timestamp(\"20130101\") + timedelta(days=i, seconds=10)\n .....:            for i in range(10)\n .....:        ],\n .....:    }\n .....: )\n .....: \n\nIn [520]: dftd[\"C\"] = dftd[\"A\"] - dftd[\"B\"]\n\nIn [521]: dftd\nOut[521]: \n A                   B                  C\n0 2013-01-01 2013-01-01 00:00:10  -1 days +23:59:50\n1 2013-01-01 2013-01-02 00:00:10  -2 days +23:59:50\n2 2013-01-01 2013-01-03 00:00:10  -3 days +23:59:50\n3 2013-01-01 2013-01-04 00:00:10  -4 days +23:59:50\n4 2013-01-01 2013-01-05 00:00:10  -5 days +23:59:50\n5 2013-01-01 2013-01-06 00:00:10  -6 days +23:59:50\n6 2013-01-01 2013-01-07 00:00:10  -7 days +23:59:50\n7 2013-01-01 2013-01-08 00:00:10  -8 days +23:59:50\n8 2013-01-01 2013-01-09 00:00:10  -9 days +23:59:50\n9 2013-01-01 2013-01-10 00:00:10 -10 days +23:59:50\n\nIn [522]: store.append(\"dftd\", dftd, data_columns=True)\n\nIn [523]: store.select(\"dftd\", \"C<'-3.5D'\")\nOut[523]: \n A                   B                  C\n4 1970-01-01 00:00:01.356998400 2013-01-05 00:00:10  -5 days +23:59:50\n5 1970-01-01 00:00:01.356998400 2013-01-06 00:00:10  -6 days +23:59:50\n6 1970-01-01 00:00:01.356998400 2013-01-07 00:00:10  -7 days +23:59:50\n7 1970-01-01 00:00:01.356998400 2013-01-08 00:00:10  -8 days +23:59:50\n8 1970-01-01 00:00:01.356998400 2013-01-09 00:00:10  -9 days +23:59:50\n9 1970-01-01 00:00:01.356998400 2013-01-10 00:00:10 -10 days +23:59:50 \n```", "```py\nIn [524]: df_mi.index.names\nOut[524]: FrozenList(['foo', 'bar'])\n\nIn [525]: store.select(\"df_mi\", \"foo=baz and bar=two\")\nOut[525]: \n A         B         C\nfoo bar \nbaz two -1.646063 -0.695847 -0.429156 \n```", "```py\nIn [526]: index = pd.MultiIndex(\n .....:    levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n .....:    codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n .....: )\n .....: \n\nIn [527]: df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [528]: df_mi_2\nOut[528]: \n A         B         C\nfoo one   -0.219582  1.186860 -1.437189\n two    0.053768  1.872644 -1.469813\n three -0.564201  0.876341  0.407749\nbar one   -0.232583  0.179812  0.922152\n two   -1.820952 -0.641360  2.133239\nbaz two   -0.941248 -0.136307 -1.271305\n three -0.099774 -0.061438 -0.845172\nqux one    0.465793  0.756995 -0.541690\n two   -0.802241  0.877657 -2.553831\n three  0.094899 -2.319519  0.293601\n\nIn [529]: store.append(\"df_mi_2\", df_mi_2)\n\n# the levels are automatically included as data columns with keyword level_n\nIn [530]: store.select(\"df_mi_2\", \"level_0=foo and level_1=two\")\nOut[530]: \n A         B         C\nfoo two  0.053768  1.872644 -1.469813 \n```", "```py\n# we have automagically already created an index (in the first section)\nIn [531]: i = store.root.df.table.cols.index.index\n\nIn [532]: i.optlevel, i.kind\nOut[532]: (6, 'medium')\n\n# change an index by passing new parameters\nIn [533]: store.create_table_index(\"df\", optlevel=9, kind=\"full\")\n\nIn [534]: i = store.root.df.table.cols.index.index\n\nIn [535]: i.optlevel, i.kind\nOut[535]: (9, 'full') \n```", "```py\nIn [536]: df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [537]: df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [538]: st = pd.HDFStore(\"appends.h5\", mode=\"w\")\n\nIn [539]: st.append(\"df\", df_1, data_columns=[\"B\"], index=False)\n\nIn [540]: st.append(\"df\", df_2, data_columns=[\"B\"], index=False)\n\nIn [541]: st.get_storer(\"df\").table\nOut[541]: \n/df/table (Table(20,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n byteorder := 'little'\n chunkshape := (2730,) \n```", "```py\nIn [542]: st.create_table_index(\"df\", columns=[\"B\"], optlevel=9, kind=\"full\")\n\nIn [543]: st.get_storer(\"df\").table\nOut[543]: \n/df/table (Table(20,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n byteorder := 'little'\n chunkshape := (2730,)\n autoindex := True\n colindexes := {\n \"B\": Index(9, fullshuffle, zlib(1)).is_csi=True}\n\nIn [544]: st.close() \n```", "```py\nIn [545]: df_dc = df.copy()\n\nIn [546]: df_dc[\"string\"] = \"foo\"\n\nIn [547]: df_dc.loc[df_dc.index[4:6], \"string\"] = np.nan\n\nIn [548]: df_dc.loc[df_dc.index[7:9], \"string\"] = \"bar\"\n\nIn [549]: df_dc[\"string2\"] = \"cool\"\n\nIn [550]: df_dc.loc[df_dc.index[1:3], [\"B\", \"C\"]] = 1.0\n\nIn [551]: df_dc\nOut[551]: \n A         B         C string string2\n2000-01-01  0.858644 -0.851236  1.058006    foo    cool\n2000-01-02 -0.080372  1.000000  1.000000    foo    cool\n2000-01-03  0.816983  1.000000  1.000000    foo    cool\n2000-01-04  0.712795 -0.062433  0.736755    foo    cool\n2000-01-05 -0.298721 -1.988045  1.475308    NaN    cool\n2000-01-06  1.103675  1.382242 -0.650762    NaN    cool\n2000-01-07 -0.729161 -0.142928 -1.063038    foo    cool\n2000-01-08 -1.005977  0.465222 -0.094517    bar    cool\n\n# on-disk operations\nIn [552]: store.append(\"df_dc\", df_dc, data_columns=[\"B\", \"C\", \"string\", \"string2\"])\n\nIn [553]: store.select(\"df_dc\", where=\"B > 0\")\nOut[553]: \n A         B         C string string2\n2000-01-02 -0.080372  1.000000  1.000000    foo    cool\n2000-01-03  0.816983  1.000000  1.000000    foo    cool\n2000-01-06  1.103675  1.382242 -0.650762    NaN    cool\n2000-01-08 -1.005977  0.465222 -0.094517    bar    cool\n\n# getting creative\nIn [554]: store.select(\"df_dc\", \"B > 0 & C > 0 & string == foo\")\nOut[554]: \n A    B    C string string2\n2000-01-02 -0.080372  1.0  1.0    foo    cool\n2000-01-03  0.816983  1.0  1.0    foo    cool\n\n# this is in-memory version of this type of selection\nIn [555]: df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == \"foo\")]\nOut[555]: \n A    B    C string string2\n2000-01-02 -0.080372  1.0  1.0    foo    cool\n2000-01-03  0.816983  1.0  1.0    foo    cool\n\n# we have automagically created this index and the B/C/string/string2\n# columns are stored separately as ``PyTables`` columns\nIn [556]: store.root.df_dc.table\nOut[556]: \n/df_dc/table (Table(8,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2),\n \"C\": Float64Col(shape=(), dflt=0.0, pos=3),\n \"string\": StringCol(itemsize=3, shape=(), dflt=b'', pos=4),\n \"string2\": StringCol(itemsize=4, shape=(), dflt=b'', pos=5)}\n byteorder := 'little'\n chunkshape := (1680,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"B\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"C\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"string\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"string2\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [557]: for df in store.select(\"df\", chunksize=3):\n .....:    print(df)\n .....: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n A         B         C\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n A         B         C\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nfor df in pd.read_hdf(\"store.h5\", \"df\", chunksize=3):\n    print(df) \n```", "```py\nIn [558]: dfeq = pd.DataFrame({\"number\": np.arange(1, 11)})\n\nIn [559]: dfeq\nOut[559]: \n number\n0       1\n1       2\n2       3\n3       4\n4       5\n5       6\n6       7\n7       8\n8       9\n9      10\n\nIn [560]: store.append(\"dfeq\", dfeq, data_columns=[\"number\"])\n\nIn [561]: def chunks(l, n):\n .....:    return [l[i: i + n] for i in range(0, len(l), n)]\n .....: \n\nIn [562]: evens = [2, 4, 6, 8, 10]\n\nIn [563]: coordinates = store.select_as_coordinates(\"dfeq\", \"number=evens\")\n\nIn [564]: for c in chunks(coordinates, 2):\n .....:    print(store.select(\"dfeq\", where=c))\n .....: \n number\n1       2\n3       4\n number\n5       6\n7       8\n number\n9      10 \n```", "```py\nIn [565]: store.select_column(\"df_dc\", \"index\")\nOut[565]: \n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\n3   2000-01-04\n4   2000-01-05\n5   2000-01-06\n6   2000-01-07\n7   2000-01-08\nName: index, dtype: datetime64[ns]\n\nIn [566]: store.select_column(\"df_dc\", \"string\")\nOut[566]: \n0    foo\n1    foo\n2    foo\n3    foo\n4    NaN\n5    NaN\n6    foo\n7    bar\nName: string, dtype: object \n```", "```py\nIn [567]: df_coord = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [568]: store.append(\"df_coord\", df_coord)\n\nIn [569]: c = store.select_as_coordinates(\"df_coord\", \"index > 20020101\")\n\nIn [570]: c\nOut[570]: \nIndex([732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n ...\n 990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n dtype='int64', length=268)\n\nIn [571]: store.select(\"df_coord\", where=c)\nOut[571]: \n 0         1\n2002-01-02  0.007717  1.168386\n2002-01-03  0.759328 -0.638934\n2002-01-04 -1.154018 -0.324071\n2002-01-05 -0.804551 -1.280593\n2002-01-06 -0.047208  1.260503\n...              ...       ...\n2002-09-22 -1.139583  0.344316\n2002-09-23 -0.760643 -1.306704\n2002-09-24  0.059018  1.775482\n2002-09-25  1.242255 -0.055457\n2002-09-26  0.410317  2.194489\n\n[268 rows x 2 columns] \n```", "```py\nIn [572]: df_mask = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [573]: store.append(\"df_mask\", df_mask)\n\nIn [574]: c = store.select_column(\"df_mask\", \"index\")\n\nIn [575]: where = c[pd.DatetimeIndex(c).month == 5].index\n\nIn [576]: store.select(\"df_mask\", where=where)\nOut[576]: \n 0         1\n2000-05-01  1.479511  0.516433\n2000-05-02 -0.334984 -1.493537\n2000-05-03  0.900321  0.049695\n2000-05-04  0.614266 -1.077151\n2000-05-05  0.233881  0.493246\n...              ...       ...\n2002-05-27  0.294122  0.457407\n2002-05-28 -1.102535  1.215650\n2002-05-29 -0.432911  0.753606\n2002-05-30 -1.105212  2.311877\n2002-05-31  2.567296  2.610691\n\n[93 rows x 2 columns] \n```", "```py\nIn [577]: store.get_storer(\"df_dc\").nrows\nOut[577]: 8 \n```", "```py\nIn [578]: df_mt = pd.DataFrame(\n .....:    np.random.randn(8, 6),\n .....:    index=pd.date_range(\"1/1/2000\", periods=8),\n .....:    columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n .....: )\n .....: \n\nIn [579]: df_mt[\"foo\"] = \"bar\"\n\nIn [580]: df_mt.loc[df_mt.index[1], (\"A\", \"B\")] = np.nan\n\n# you can also create the tables individually\nIn [581]: store.append_to_multiple(\n .....:    {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\"\n .....: )\n .....: \n\nIn [582]: store\nOut[582]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# individual tables were created\nIn [583]: store.select(\"df1_mt\")\nOut[583]: \n A         B\n2000-01-01  0.162291 -0.430489\n2000-01-02       NaN       NaN\n2000-01-03  0.429207 -1.099274\n2000-01-04  1.869081 -1.466039\n2000-01-05  0.092130 -1.726280\n2000-01-06  0.266901 -0.036854\n2000-01-07 -0.517871 -0.990317\n2000-01-08 -0.231342  0.557402\n\nIn [584]: store.select(\"df2_mt\")\nOut[584]: \n C         D         E         F  foo\n2000-01-01 -2.502042  0.668149  0.460708  1.834518  bar\n2000-01-02  0.130441 -0.608465  0.439872  0.506364  bar\n2000-01-03 -1.069546  1.236277  0.116634 -1.772519  bar\n2000-01-04  0.137462  0.313939  0.748471 -0.943009  bar\n2000-01-05  0.836517  2.049798  0.562167  0.189952  bar\n2000-01-06  1.112750 -0.151596  1.503311  0.939470  bar\n2000-01-07 -0.294348  0.335844 -0.794159  1.495614  bar\n2000-01-08  0.860312 -0.538674 -0.541986 -1.759606  bar\n\n# as a multiple\nIn [585]: store.select_as_multiple(\n .....:    [\"df1_mt\", \"df2_mt\"],\n .....:    where=[\"A>0\", \"B>0\"],\n .....:    selector=\"df1_mt\",\n .....: )\n .....: \nOut[585]: \nEmpty DataFrame\nColumns: [A, B, C, D, E, F, foo]\nIndex: [] \n```", "```py\nstore_compressed = pd.HDFStore(\n    \"store_compressed.h5\", complevel=9, complib=\"blosc:blosclz\"\n) \n```", "```py\nstore.append(\"df\", df, complib=\"zlib\", complevel=5) \n```", "```py\nptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5 \n```", "```py\nIn [586]: dfcat = pd.DataFrame(\n .....:    {\"A\": pd.Series(list(\"aabbcdba\")).astype(\"category\"), \"B\": np.random.randn(8)}\n .....: )\n .....: \n\nIn [587]: dfcat\nOut[587]: \n A         B\n0  a -1.520478\n1  a -1.069391\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n5  d  0.301911\n6  b -0.640843\n7  a -2.253022\n\nIn [588]: dfcat.dtypes\nOut[588]: \nA    category\nB     float64\ndtype: object\n\nIn [589]: cstore = pd.HDFStore(\"cats.h5\", mode=\"w\")\n\nIn [590]: cstore.append(\"dfcat\", dfcat, format=\"table\", data_columns=[\"A\"])\n\nIn [591]: result = cstore.select(\"dfcat\", where=\"A in ['b', 'c']\")\n\nIn [592]: result\nOut[592]: \n A         B\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n6  b -0.640843\n\nIn [593]: result.dtypes\nOut[593]: \nA    category\nB     float64\ndtype: object \n```", "```py\nIn [594]: dfs = pd.DataFrame({\"A\": \"foo\", \"B\": \"bar\"}, index=list(range(5)))\n\nIn [595]: dfs\nOut[595]: \n A    B\n0  foo  bar\n1  foo  bar\n2  foo  bar\n3  foo  bar\n4  foo  bar\n\n# A and B have a size of 30\nIn [596]: store.append(\"dfs\", dfs, min_itemsize=30)\n\nIn [597]: store.get_storer(\"dfs\").table\nOut[597]: \n/dfs/table (Table(5,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": StringCol(itemsize=30, shape=(2,), dflt=b'', pos=1)}\n byteorder := 'little'\n chunkshape := (963,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False}\n\n# A is created as a data_column with a size of 30\n# B is size is calculated\nIn [598]: store.append(\"dfs2\", dfs, min_itemsize={\"A\": 30})\n\nIn [599]: store.get_storer(\"dfs2\").table\nOut[599]: \n/dfs2/table (Table(5,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": StringCol(itemsize=3, shape=(1,), dflt=b'', pos=1),\n \"A\": StringCol(itemsize=30, shape=(), dflt=b'', pos=2)}\n byteorder := 'little'\n chunkshape := (1598,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"A\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [600]: dfss = pd.DataFrame({\"A\": [\"foo\", \"bar\", \"nan\"]})\n\nIn [601]: dfss\nOut[601]: \n A\n0  foo\n1  bar\n2  nan\n\nIn [602]: store.append(\"dfss\", dfss)\n\nIn [603]: store.select(\"dfss\")\nOut[603]: \n A\n0  foo\n1  bar\n2  NaN\n\n# here you need to specify a different nan rep\nIn [604]: store.append(\"dfss2\", dfss, nan_rep=\"_nan_\")\n\nIn [605]: store.select(\"dfss2\")\nOut[605]: \n A\n0  foo\n1  bar\n2  nan \n```", "```py\nIn [470]: df_tl = pd.DataFrame({\"A\": list(range(5)), \"B\": list(range(5))})\n\nIn [471]: df_tl.to_hdf(\"store_tl.h5\", key=\"table\", append=True)\n\nIn [472]: pd.read_hdf(\"store_tl.h5\", \"table\", where=[\"index>2\"])\nOut[472]: \n A  B\n3  3  3\n4  4  4 \n```", "```py\nIn [473]: df_with_missing = pd.DataFrame(\n .....:    {\n .....:        \"col1\": [0, np.nan, 2],\n .....:        \"col2\": [1, np.nan, np.nan],\n .....:    }\n .....: )\n .....: \n\nIn [474]: df_with_missing\nOut[474]: \n col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [475]: df_with_missing.to_hdf(\"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\")\n\nIn [476]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[476]: \n col1  col2\n0   0.0   1.0\n1   NaN   NaN\n2   2.0   NaN\n\nIn [477]: df_with_missing.to_hdf(\n .....:    \"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\", dropna=True\n .....: )\n .....: \n\nIn [478]: pd.read_hdf(\"file.h5\", \"df_with_missing\")\nOut[478]: \n col1  col2\n0   0.0   1.0\n2   2.0   NaN \n```", "```py\nIn [479]: pd.DataFrame(np.random.randn(10, 2)).to_hdf(\"test_fixed.h5\", key=\"df\")\n\nIn [480]: pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n---------------------------------------------------------------------------\nTypeError  Traceback (most recent call last)\nCell In[480], line 1\n----> 1 pd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:452, in read_hdf(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\n  447                 raise ValueError(\n  448                     \"key must be provided when HDF5 \"\n  449                     \"file contains multiple datasets.\"\n  450                 )\n  451         key = candidate_only_group._v_pathname\n--> 452     return store.select(\n  453         key,\n  454         where=where,\n  455         start=start,\n  456         stop=stop,\n  457         columns=columns,\n  458         iterator=iterator,\n  459         chunksize=chunksize,\n  460         auto_close=auto_close,\n  461     )\n  462 except (ValueError, TypeError, LookupError):\n  463     if not isinstance(path_or_buf, HDFStore):\n  464         # if there is an error, close the store if we opened it.\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:906, in HDFStore.select(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\n  892 # create the iterator\n  893 it = TableIterator(\n  894     self,\n  895     s,\n   (...)\n  903     auto_close=auto_close,\n  904 )\n--> 906 return it.get_result()\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:2029, in TableIterator.get_result(self, coordinates)\n  2026     where = self.where\n  2028 # directly return the result\n-> 2029 results = self.func(self.start, self.stop, where)\n  2030 self.close()\n  2031 return results\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:890, in HDFStore.select.<locals>.func(_start, _stop, _where)\n  889 def func(_start, _stop, _where):\n--> 890     return s.read(start=_start, stop=_stop, where=_where, columns=columns)\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:3278, in BlockManagerFixed.read(self, where, columns, start, stop)\n  3270 def read(\n  3271     self,\n  3272     where=None,\n   (...)\n  3276 ) -> DataFrame:\n  3277     # start, stop applied to rows, so 0th axis only\n-> 3278     self.validate_read(columns, where)\n  3279     select_axis = self.obj_type()._get_block_manager_axis(0)\n  3281     axes = []\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:2922, in GenericFixed.validate_read(self, columns, where)\n  2917     raise TypeError(\n  2918         \"cannot pass a column specification when reading \"\n  2919         \"a Fixed format store. this store must be selected in its entirety\"\n  2920     )\n  2921 if where is not None:\n-> 2922     raise TypeError(\n  2923         \"cannot pass a where specification when reading \"\n  2924         \"from a Fixed format store. this store must be selected in its entirety\"\n  2925     )\n\nTypeError: cannot pass a where specification when reading from a Fixed format store. this store must be selected in its entirety \n```", "```py\nIn [481]: store = pd.HDFStore(\"store.h5\")\n\nIn [482]: df1 = df[0:4]\n\nIn [483]: df2 = df[4:]\n\n# append data (creates a table automatically)\nIn [484]: store.append(\"df\", df1)\n\nIn [485]: store.append(\"df\", df2)\n\nIn [486]: store\nOut[486]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# select the entire object\nIn [487]: store.select(\"df\")\nOut[487]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\n\n# the type of stored data\nIn [488]: store.root.df._v_attrs.pandas_type\nOut[488]: 'frame_table' \n```", "```py\nIn [489]: store.put(\"foo/bar/bah\", df)\n\nIn [490]: store.append(\"food/orange\", df)\n\nIn [491]: store.append(\"food/apple\", df)\n\nIn [492]: store\nOut[492]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# a list of keys are returned\nIn [493]: store.keys()\nOut[493]: ['/df', '/food/apple', '/food/orange', '/foo/bar/bah']\n\n# remove all nodes under this level\nIn [494]: store.remove(\"food\")\n\nIn [495]: store\nOut[495]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5 \n```", "```py\nIn [496]: for (path, subgroups, subkeys) in store.walk():\n .....:    for subgroup in subgroups:\n .....:        print(\"GROUP: {}/{}\".format(path, subgroup))\n .....:    for subkey in subkeys:\n .....:        key = \"/\".join([path, subkey])\n .....:        print(\"KEY: {}\".format(key))\n .....:        print(store.get(key))\n .....: \nGROUP: /foo\nKEY: /df\n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517\nGROUP: /foo/bar\nKEY: /foo/bar/bah\n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nIn [497]: store.foo.bar.bah\n---------------------------------------------------------------------------\nTypeError  Traceback (most recent call last)\nCell In[497], line 1\n----> 1 store.foo.bar.bah\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:613, in HDFStore.__getattr__(self, name)\n  611  \"\"\"allow attribute access to get stores\"\"\"\n  612 try:\n--> 613     return self.get(name)\n  614 except (KeyError, ClosedFileError):\n  615     pass\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:813, in HDFStore.get(self, key)\n  811 if group is None:\n  812     raise KeyError(f\"No object named {key} in the file\")\n--> 813 return self._read_group(group)\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:1878, in HDFStore._read_group(self, group)\n  1877 def _read_group(self, group: Node):\n-> 1878     s = self._create_storer(group)\n  1879     s.infer_axes()\n  1880     return s.read()\n\nFile ~/work/pandas/pandas/pandas/io/pytables.py:1752, in HDFStore._create_storer(self, group, format, value, encoding, errors)\n  1750         tt = \"generic_table\"\n  1751     else:\n-> 1752         raise TypeError(\n  1753             \"cannot create a storer if the object is not existing \"\n  1754             \"nor a value are passed\"\n  1755         )\n  1756 else:\n  1757     if isinstance(value, Series):\n\nTypeError: cannot create a storer if the object is not existing nor a value are passed \n```", "```py\n# you can directly access the actual PyTables node but using the root node\nIn [498]: store.root.foo.bar.bah\nOut[498]: \n/foo/bar/bah (Group) ''\n children := ['axis0' (Array), 'axis1' (Array), 'block0_items' (Array), 'block0_values' (Array)] \n```", "```py\nIn [499]: store[\"foo/bar/bah\"]\nOut[499]: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nIn [500]: df_mixed = pd.DataFrame(\n .....:    {\n .....:        \"A\": np.random.randn(8),\n .....:        \"B\": np.random.randn(8),\n .....:        \"C\": np.array(np.random.randn(8), dtype=\"float32\"),\n .....:        \"string\": \"string\",\n .....:        \"int\": 1,\n .....:        \"bool\": True,\n .....:        \"datetime64\": pd.Timestamp(\"20010102\"),\n .....:    },\n .....:    index=list(range(8)),\n .....: )\n .....: \n\nIn [501]: df_mixed.loc[df_mixed.index[3:5], [\"A\", \"B\", \"string\", \"datetime64\"]] = np.nan\n\nIn [502]: store.append(\"df_mixed\", df_mixed, min_itemsize={\"values\": 50})\n\nIn [503]: df_mixed1 = store.select(\"df_mixed\")\n\nIn [504]: df_mixed1\nOut[504]: \n A         B         C  ... int  bool                    datetime64\n0  0.013747 -1.166078 -1.292080  ...   1  True 1970-01-01 00:00:00.978393600\n1 -0.712009  0.247572  1.526911  ...   1  True 1970-01-01 00:00:00.978393600\n2 -0.645096  1.687406  0.288504  ...   1  True 1970-01-01 00:00:00.978393600\n3       NaN       NaN  0.097771  ...   1  True                           NaT\n4       NaN       NaN  1.536408  ...   1  True                           NaT\n5 -0.023202  0.043702  0.926790  ...   1  True 1970-01-01 00:00:00.978393600\n6  2.359782  0.088224 -0.676448  ...   1  True 1970-01-01 00:00:00.978393600\n7 -0.143428 -0.813360 -0.179724  ...   1  True 1970-01-01 00:00:00.978393600\n\n[8 rows x 7 columns]\n\nIn [505]: df_mixed1.dtypes.value_counts()\nOut[505]: \nfloat64           2\nfloat32           1\nobject            1\nint64             1\nbool              1\ndatetime64[ns]    1\nName: count, dtype: int64\n\n# we have provided a minimum string column size\nIn [506]: store.root.df_mixed.table\nOut[506]: \n/df_mixed/table (Table(8,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1),\n \"values_block_1\": Float32Col(shape=(1,), dflt=0.0, pos=2),\n \"values_block_2\": StringCol(itemsize=50, shape=(1,), dflt=b'', pos=3),\n \"values_block_3\": Int64Col(shape=(1,), dflt=0, pos=4),\n \"values_block_4\": BoolCol(shape=(1,), dflt=False, pos=5),\n \"values_block_5\": Int64Col(shape=(1,), dflt=0, pos=6)}\n byteorder := 'little'\n chunkshape := (689,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [507]: index = pd.MultiIndex(\n .....:   levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n .....:   codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n .....:   names=[\"foo\", \"bar\"],\n .....: )\n .....: \n\nIn [508]: df_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [509]: df_mi\nOut[509]: \n A         B         C\nfoo bar \nfoo one   -1.303456 -0.642994 -0.649456\n two    1.012694  0.414147  1.950460\n three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n two   -1.660522  0.929553 -1.298649\n three  3.565769  0.682402  1.041927\n\nIn [510]: store.append(\"df_mi\", df_mi)\n\nIn [511]: store.select(\"df_mi\")\nOut[511]: \n A         B         C\nfoo bar \nfoo one   -1.303456 -0.642994 -0.649456\n two    1.012694  0.414147  1.950460\n three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n two   -1.660522  0.929553 -1.298649\n three  3.565769  0.682402  1.041927\n\n# the levels are automatically included as data columns\nIn [512]: store.select(\"df_mi\", \"foo=bar\")\nOut[512]: \n A         B         C\nfoo bar \nbar one  0.410395  0.618321  0.560398\n two  1.434027 -0.033270  0.343197 \n```", "```py\nIn [500]: df_mixed = pd.DataFrame(\n .....:    {\n .....:        \"A\": np.random.randn(8),\n .....:        \"B\": np.random.randn(8),\n .....:        \"C\": np.array(np.random.randn(8), dtype=\"float32\"),\n .....:        \"string\": \"string\",\n .....:        \"int\": 1,\n .....:        \"bool\": True,\n .....:        \"datetime64\": pd.Timestamp(\"20010102\"),\n .....:    },\n .....:    index=list(range(8)),\n .....: )\n .....: \n\nIn [501]: df_mixed.loc[df_mixed.index[3:5], [\"A\", \"B\", \"string\", \"datetime64\"]] = np.nan\n\nIn [502]: store.append(\"df_mixed\", df_mixed, min_itemsize={\"values\": 50})\n\nIn [503]: df_mixed1 = store.select(\"df_mixed\")\n\nIn [504]: df_mixed1\nOut[504]: \n A         B         C  ... int  bool                    datetime64\n0  0.013747 -1.166078 -1.292080  ...   1  True 1970-01-01 00:00:00.978393600\n1 -0.712009  0.247572  1.526911  ...   1  True 1970-01-01 00:00:00.978393600\n2 -0.645096  1.687406  0.288504  ...   1  True 1970-01-01 00:00:00.978393600\n3       NaN       NaN  0.097771  ...   1  True                           NaT\n4       NaN       NaN  1.536408  ...   1  True                           NaT\n5 -0.023202  0.043702  0.926790  ...   1  True 1970-01-01 00:00:00.978393600\n6  2.359782  0.088224 -0.676448  ...   1  True 1970-01-01 00:00:00.978393600\n7 -0.143428 -0.813360 -0.179724  ...   1  True 1970-01-01 00:00:00.978393600\n\n[8 rows x 7 columns]\n\nIn [505]: df_mixed1.dtypes.value_counts()\nOut[505]: \nfloat64           2\nfloat32           1\nobject            1\nint64             1\nbool              1\ndatetime64[ns]    1\nName: count, dtype: int64\n\n# we have provided a minimum string column size\nIn [506]: store.root.df_mixed.table\nOut[506]: \n/df_mixed/table (Table(8,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(2,), dflt=0.0, pos=1),\n \"values_block_1\": Float32Col(shape=(1,), dflt=0.0, pos=2),\n \"values_block_2\": StringCol(itemsize=50, shape=(1,), dflt=b'', pos=3),\n \"values_block_3\": Int64Col(shape=(1,), dflt=0, pos=4),\n \"values_block_4\": BoolCol(shape=(1,), dflt=False, pos=5),\n \"values_block_5\": Int64Col(shape=(1,), dflt=0, pos=6)}\n byteorder := 'little'\n chunkshape := (689,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [507]: index = pd.MultiIndex(\n .....:   levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n .....:   codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n .....:   names=[\"foo\", \"bar\"],\n .....: )\n .....: \n\nIn [508]: df_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [509]: df_mi\nOut[509]: \n A         B         C\nfoo bar \nfoo one   -1.303456 -0.642994 -0.649456\n two    1.012694  0.414147  1.950460\n three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n two   -1.660522  0.929553 -1.298649\n three  3.565769  0.682402  1.041927\n\nIn [510]: store.append(\"df_mi\", df_mi)\n\nIn [511]: store.select(\"df_mi\")\nOut[511]: \n A         B         C\nfoo bar \nfoo one   -1.303456 -0.642994 -0.649456\n two    1.012694  0.414147  1.950460\n three  1.094544 -0.802899 -0.583343\nbar one    0.410395  0.618321  0.560398\n two    1.434027 -0.033270  0.343197\nbaz two   -1.646063 -0.695847 -0.429156\n three -0.244688 -1.428229 -0.138691\nqux one    1.866184 -1.446617  0.036660\n two   -1.660522  0.929553 -1.298649\n three  3.565769  0.682402  1.041927\n\n# the levels are automatically included as data columns\nIn [512]: store.select(\"df_mi\", \"foo=bar\")\nOut[512]: \n A         B         C\nfoo bar \nbar one  0.410395  0.618321  0.560398\n two  1.434027 -0.033270  0.343197 \n```", "```py\nstring = \"HolyMoly'\"\nstore.select(\"df\", \"index == string\") \n```", "```py\nstring = \"HolyMoly'\"\nstore.select('df', f'index == {string}') \n```", "```py\nstore.select(\"df\", \"index == %r\" % string) \n```", "```py\nIn [513]: dfq = pd.DataFrame(\n .....:    np.random.randn(10, 4),\n .....:    columns=list(\"ABCD\"),\n .....:    index=pd.date_range(\"20130101\", periods=10),\n .....: )\n .....: \n\nIn [514]: store.append(\"dfq\", dfq, format=\"table\", data_columns=True) \n```", "```py\nIn [515]: store.select(\"dfq\", \"index>pd.Timestamp('20130104') & columns=['A', 'B']\")\nOut[515]: \n A         B\n2013-01-05 -0.830545 -0.457071\n2013-01-06  0.431186  1.049421\n2013-01-07  0.617509 -0.811230\n2013-01-08  0.947422 -0.671233\n2013-01-09 -0.183798 -1.211230\n2013-01-10  0.361428  0.887304 \n```", "```py\nIn [516]: store.select(\"dfq\", where=\"A>0 or C>0\")\nOut[516]: \n A         B         C         D\n2013-01-02  0.658179  0.362814 -0.917897  0.010165\n2013-01-03  0.905122  1.848731 -1.184241  0.932053\n2013-01-05 -0.830545 -0.457071  1.565581  1.148032\n2013-01-06  0.431186  1.049421  0.383309  0.595013\n2013-01-07  0.617509 -0.811230 -2.088563 -1.393500\n2013-01-08  0.947422 -0.671233 -0.847097 -1.187785\n2013-01-10  0.361428  0.887304  0.266457 -0.399641 \n```", "```py\nIn [517]: store.select(\"df\", \"columns=['A', 'B']\")\nOut[517]: \n A         B\n2000-01-01  0.858644 -0.851236\n2000-01-02 -0.080372 -1.268121\n2000-01-03  0.816983  1.965656\n2000-01-04  0.712795 -0.062433\n2000-01-05 -0.298721 -1.988045\n2000-01-06  1.103675  1.382242\n2000-01-07 -0.729161 -0.142928\n2000-01-08 -1.005977  0.465222 \n```", "```py\nIn [518]: from datetime import timedelta\n\nIn [519]: dftd = pd.DataFrame(\n .....:    {\n .....:        \"A\": pd.Timestamp(\"20130101\"),\n .....:        \"B\": [\n .....:            pd.Timestamp(\"20130101\") + timedelta(days=i, seconds=10)\n .....:            for i in range(10)\n .....:        ],\n .....:    }\n .....: )\n .....: \n\nIn [520]: dftd[\"C\"] = dftd[\"A\"] - dftd[\"B\"]\n\nIn [521]: dftd\nOut[521]: \n A                   B                  C\n0 2013-01-01 2013-01-01 00:00:10  -1 days +23:59:50\n1 2013-01-01 2013-01-02 00:00:10  -2 days +23:59:50\n2 2013-01-01 2013-01-03 00:00:10  -3 days +23:59:50\n3 2013-01-01 2013-01-04 00:00:10  -4 days +23:59:50\n4 2013-01-01 2013-01-05 00:00:10  -5 days +23:59:50\n5 2013-01-01 2013-01-06 00:00:10  -6 days +23:59:50\n6 2013-01-01 2013-01-07 00:00:10  -7 days +23:59:50\n7 2013-01-01 2013-01-08 00:00:10  -8 days +23:59:50\n8 2013-01-01 2013-01-09 00:00:10  -9 days +23:59:50\n9 2013-01-01 2013-01-10 00:00:10 -10 days +23:59:50\n\nIn [522]: store.append(\"dftd\", dftd, data_columns=True)\n\nIn [523]: store.select(\"dftd\", \"C<'-3.5D'\")\nOut[523]: \n A                   B                  C\n4 1970-01-01 00:00:01.356998400 2013-01-05 00:00:10  -5 days +23:59:50\n5 1970-01-01 00:00:01.356998400 2013-01-06 00:00:10  -6 days +23:59:50\n6 1970-01-01 00:00:01.356998400 2013-01-07 00:00:10  -7 days +23:59:50\n7 1970-01-01 00:00:01.356998400 2013-01-08 00:00:10  -8 days +23:59:50\n8 1970-01-01 00:00:01.356998400 2013-01-09 00:00:10  -9 days +23:59:50\n9 1970-01-01 00:00:01.356998400 2013-01-10 00:00:10 -10 days +23:59:50 \n```", "```py\nIn [524]: df_mi.index.names\nOut[524]: FrozenList(['foo', 'bar'])\n\nIn [525]: store.select(\"df_mi\", \"foo=baz and bar=two\")\nOut[525]: \n A         B         C\nfoo bar \nbaz two -1.646063 -0.695847 -0.429156 \n```", "```py\nIn [526]: index = pd.MultiIndex(\n .....:    levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n .....:    codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n .....: )\n .....: \n\nIn [527]: df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [528]: df_mi_2\nOut[528]: \n A         B         C\nfoo one   -0.219582  1.186860 -1.437189\n two    0.053768  1.872644 -1.469813\n three -0.564201  0.876341  0.407749\nbar one   -0.232583  0.179812  0.922152\n two   -1.820952 -0.641360  2.133239\nbaz two   -0.941248 -0.136307 -1.271305\n three -0.099774 -0.061438 -0.845172\nqux one    0.465793  0.756995 -0.541690\n two   -0.802241  0.877657 -2.553831\n three  0.094899 -2.319519  0.293601\n\nIn [529]: store.append(\"df_mi_2\", df_mi_2)\n\n# the levels are automatically included as data columns with keyword level_n\nIn [530]: store.select(\"df_mi_2\", \"level_0=foo and level_1=two\")\nOut[530]: \n A         B         C\nfoo two  0.053768  1.872644 -1.469813 \n```", "```py\n# we have automagically already created an index (in the first section)\nIn [531]: i = store.root.df.table.cols.index.index\n\nIn [532]: i.optlevel, i.kind\nOut[532]: (6, 'medium')\n\n# change an index by passing new parameters\nIn [533]: store.create_table_index(\"df\", optlevel=9, kind=\"full\")\n\nIn [534]: i = store.root.df.table.cols.index.index\n\nIn [535]: i.optlevel, i.kind\nOut[535]: (9, 'full') \n```", "```py\nIn [536]: df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [537]: df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [538]: st = pd.HDFStore(\"appends.h5\", mode=\"w\")\n\nIn [539]: st.append(\"df\", df_1, data_columns=[\"B\"], index=False)\n\nIn [540]: st.append(\"df\", df_2, data_columns=[\"B\"], index=False)\n\nIn [541]: st.get_storer(\"df\").table\nOut[541]: \n/df/table (Table(20,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n byteorder := 'little'\n chunkshape := (2730,) \n```", "```py\nIn [542]: st.create_table_index(\"df\", columns=[\"B\"], optlevel=9, kind=\"full\")\n\nIn [543]: st.get_storer(\"df\").table\nOut[543]: \n/df/table (Table(20,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n byteorder := 'little'\n chunkshape := (2730,)\n autoindex := True\n colindexes := {\n \"B\": Index(9, fullshuffle, zlib(1)).is_csi=True}\n\nIn [544]: st.close() \n```", "```py\nIn [545]: df_dc = df.copy()\n\nIn [546]: df_dc[\"string\"] = \"foo\"\n\nIn [547]: df_dc.loc[df_dc.index[4:6], \"string\"] = np.nan\n\nIn [548]: df_dc.loc[df_dc.index[7:9], \"string\"] = \"bar\"\n\nIn [549]: df_dc[\"string2\"] = \"cool\"\n\nIn [550]: df_dc.loc[df_dc.index[1:3], [\"B\", \"C\"]] = 1.0\n\nIn [551]: df_dc\nOut[551]: \n A         B         C string string2\n2000-01-01  0.858644 -0.851236  1.058006    foo    cool\n2000-01-02 -0.080372  1.000000  1.000000    foo    cool\n2000-01-03  0.816983  1.000000  1.000000    foo    cool\n2000-01-04  0.712795 -0.062433  0.736755    foo    cool\n2000-01-05 -0.298721 -1.988045  1.475308    NaN    cool\n2000-01-06  1.103675  1.382242 -0.650762    NaN    cool\n2000-01-07 -0.729161 -0.142928 -1.063038    foo    cool\n2000-01-08 -1.005977  0.465222 -0.094517    bar    cool\n\n# on-disk operations\nIn [552]: store.append(\"df_dc\", df_dc, data_columns=[\"B\", \"C\", \"string\", \"string2\"])\n\nIn [553]: store.select(\"df_dc\", where=\"B > 0\")\nOut[553]: \n A         B         C string string2\n2000-01-02 -0.080372  1.000000  1.000000    foo    cool\n2000-01-03  0.816983  1.000000  1.000000    foo    cool\n2000-01-06  1.103675  1.382242 -0.650762    NaN    cool\n2000-01-08 -1.005977  0.465222 -0.094517    bar    cool\n\n# getting creative\nIn [554]: store.select(\"df_dc\", \"B > 0 & C > 0 & string == foo\")\nOut[554]: \n A    B    C string string2\n2000-01-02 -0.080372  1.0  1.0    foo    cool\n2000-01-03  0.816983  1.0  1.0    foo    cool\n\n# this is in-memory version of this type of selection\nIn [555]: df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == \"foo\")]\nOut[555]: \n A    B    C string string2\n2000-01-02 -0.080372  1.0  1.0    foo    cool\n2000-01-03  0.816983  1.0  1.0    foo    cool\n\n# we have automagically created this index and the B/C/string/string2\n# columns are stored separately as ``PyTables`` columns\nIn [556]: store.root.df_dc.table\nOut[556]: \n/df_dc/table (Table(8,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2),\n \"C\": Float64Col(shape=(), dflt=0.0, pos=3),\n \"string\": StringCol(itemsize=3, shape=(), dflt=b'', pos=4),\n \"string2\": StringCol(itemsize=4, shape=(), dflt=b'', pos=5)}\n byteorder := 'little'\n chunkshape := (1680,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"B\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"C\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"string\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"string2\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [557]: for df in store.select(\"df\", chunksize=3):\n .....:    print(df)\n .....: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n A         B         C\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n A         B         C\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nfor df in pd.read_hdf(\"store.h5\", \"df\", chunksize=3):\n    print(df) \n```", "```py\nIn [558]: dfeq = pd.DataFrame({\"number\": np.arange(1, 11)})\n\nIn [559]: dfeq\nOut[559]: \n number\n0       1\n1       2\n2       3\n3       4\n4       5\n5       6\n6       7\n7       8\n8       9\n9      10\n\nIn [560]: store.append(\"dfeq\", dfeq, data_columns=[\"number\"])\n\nIn [561]: def chunks(l, n):\n .....:    return [l[i: i + n] for i in range(0, len(l), n)]\n .....: \n\nIn [562]: evens = [2, 4, 6, 8, 10]\n\nIn [563]: coordinates = store.select_as_coordinates(\"dfeq\", \"number=evens\")\n\nIn [564]: for c in chunks(coordinates, 2):\n .....:    print(store.select(\"dfeq\", where=c))\n .....: \n number\n1       2\n3       4\n number\n5       6\n7       8\n number\n9      10 \n```", "```py\nIn [565]: store.select_column(\"df_dc\", \"index\")\nOut[565]: \n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\n3   2000-01-04\n4   2000-01-05\n5   2000-01-06\n6   2000-01-07\n7   2000-01-08\nName: index, dtype: datetime64[ns]\n\nIn [566]: store.select_column(\"df_dc\", \"string\")\nOut[566]: \n0    foo\n1    foo\n2    foo\n3    foo\n4    NaN\n5    NaN\n6    foo\n7    bar\nName: string, dtype: object \n```", "```py\nIn [567]: df_coord = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [568]: store.append(\"df_coord\", df_coord)\n\nIn [569]: c = store.select_as_coordinates(\"df_coord\", \"index > 20020101\")\n\nIn [570]: c\nOut[570]: \nIndex([732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n ...\n 990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n dtype='int64', length=268)\n\nIn [571]: store.select(\"df_coord\", where=c)\nOut[571]: \n 0         1\n2002-01-02  0.007717  1.168386\n2002-01-03  0.759328 -0.638934\n2002-01-04 -1.154018 -0.324071\n2002-01-05 -0.804551 -1.280593\n2002-01-06 -0.047208  1.260503\n...              ...       ...\n2002-09-22 -1.139583  0.344316\n2002-09-23 -0.760643 -1.306704\n2002-09-24  0.059018  1.775482\n2002-09-25  1.242255 -0.055457\n2002-09-26  0.410317  2.194489\n\n[268 rows x 2 columns] \n```", "```py\nIn [572]: df_mask = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [573]: store.append(\"df_mask\", df_mask)\n\nIn [574]: c = store.select_column(\"df_mask\", \"index\")\n\nIn [575]: where = c[pd.DatetimeIndex(c).month == 5].index\n\nIn [576]: store.select(\"df_mask\", where=where)\nOut[576]: \n 0         1\n2000-05-01  1.479511  0.516433\n2000-05-02 -0.334984 -1.493537\n2000-05-03  0.900321  0.049695\n2000-05-04  0.614266 -1.077151\n2000-05-05  0.233881  0.493246\n...              ...       ...\n2002-05-27  0.294122  0.457407\n2002-05-28 -1.102535  1.215650\n2002-05-29 -0.432911  0.753606\n2002-05-30 -1.105212  2.311877\n2002-05-31  2.567296  2.610691\n\n[93 rows x 2 columns] \n```", "```py\nIn [577]: store.get_storer(\"df_dc\").nrows\nOut[577]: 8 \n```", "```py\nIn [578]: df_mt = pd.DataFrame(\n .....:    np.random.randn(8, 6),\n .....:    index=pd.date_range(\"1/1/2000\", periods=8),\n .....:    columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n .....: )\n .....: \n\nIn [579]: df_mt[\"foo\"] = \"bar\"\n\nIn [580]: df_mt.loc[df_mt.index[1], (\"A\", \"B\")] = np.nan\n\n# you can also create the tables individually\nIn [581]: store.append_to_multiple(\n .....:    {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\"\n .....: )\n .....: \n\nIn [582]: store\nOut[582]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# individual tables were created\nIn [583]: store.select(\"df1_mt\")\nOut[583]: \n A         B\n2000-01-01  0.162291 -0.430489\n2000-01-02       NaN       NaN\n2000-01-03  0.429207 -1.099274\n2000-01-04  1.869081 -1.466039\n2000-01-05  0.092130 -1.726280\n2000-01-06  0.266901 -0.036854\n2000-01-07 -0.517871 -0.990317\n2000-01-08 -0.231342  0.557402\n\nIn [584]: store.select(\"df2_mt\")\nOut[584]: \n C         D         E         F  foo\n2000-01-01 -2.502042  0.668149  0.460708  1.834518  bar\n2000-01-02  0.130441 -0.608465  0.439872  0.506364  bar\n2000-01-03 -1.069546  1.236277  0.116634 -1.772519  bar\n2000-01-04  0.137462  0.313939  0.748471 -0.943009  bar\n2000-01-05  0.836517  2.049798  0.562167  0.189952  bar\n2000-01-06  1.112750 -0.151596  1.503311  0.939470  bar\n2000-01-07 -0.294348  0.335844 -0.794159  1.495614  bar\n2000-01-08  0.860312 -0.538674 -0.541986 -1.759606  bar\n\n# as a multiple\nIn [585]: store.select_as_multiple(\n .....:    [\"df1_mt\", \"df2_mt\"],\n .....:    where=[\"A>0\", \"B>0\"],\n .....:    selector=\"df1_mt\",\n .....: )\n .....: \nOut[585]: \nEmpty DataFrame\nColumns: [A, B, C, D, E, F, foo]\nIndex: [] \n```", "```py\nstring = \"HolyMoly'\"\nstore.select(\"df\", \"index == string\") \n```", "```py\nstring = \"HolyMoly'\"\nstore.select('df', f'index == {string}') \n```", "```py\nstore.select(\"df\", \"index == %r\" % string) \n```", "```py\nIn [513]: dfq = pd.DataFrame(\n .....:    np.random.randn(10, 4),\n .....:    columns=list(\"ABCD\"),\n .....:    index=pd.date_range(\"20130101\", periods=10),\n .....: )\n .....: \n\nIn [514]: store.append(\"dfq\", dfq, format=\"table\", data_columns=True) \n```", "```py\nIn [515]: store.select(\"dfq\", \"index>pd.Timestamp('20130104') & columns=['A', 'B']\")\nOut[515]: \n A         B\n2013-01-05 -0.830545 -0.457071\n2013-01-06  0.431186  1.049421\n2013-01-07  0.617509 -0.811230\n2013-01-08  0.947422 -0.671233\n2013-01-09 -0.183798 -1.211230\n2013-01-10  0.361428  0.887304 \n```", "```py\nIn [516]: store.select(\"dfq\", where=\"A>0 or C>0\")\nOut[516]: \n A         B         C         D\n2013-01-02  0.658179  0.362814 -0.917897  0.010165\n2013-01-03  0.905122  1.848731 -1.184241  0.932053\n2013-01-05 -0.830545 -0.457071  1.565581  1.148032\n2013-01-06  0.431186  1.049421  0.383309  0.595013\n2013-01-07  0.617509 -0.811230 -2.088563 -1.393500\n2013-01-08  0.947422 -0.671233 -0.847097 -1.187785\n2013-01-10  0.361428  0.887304  0.266457 -0.399641 \n```", "```py\nIn [517]: store.select(\"df\", \"columns=['A', 'B']\")\nOut[517]: \n A         B\n2000-01-01  0.858644 -0.851236\n2000-01-02 -0.080372 -1.268121\n2000-01-03  0.816983  1.965656\n2000-01-04  0.712795 -0.062433\n2000-01-05 -0.298721 -1.988045\n2000-01-06  1.103675  1.382242\n2000-01-07 -0.729161 -0.142928\n2000-01-08 -1.005977  0.465222 \n```", "```py\nIn [518]: from datetime import timedelta\n\nIn [519]: dftd = pd.DataFrame(\n .....:    {\n .....:        \"A\": pd.Timestamp(\"20130101\"),\n .....:        \"B\": [\n .....:            pd.Timestamp(\"20130101\") + timedelta(days=i, seconds=10)\n .....:            for i in range(10)\n .....:        ],\n .....:    }\n .....: )\n .....: \n\nIn [520]: dftd[\"C\"] = dftd[\"A\"] - dftd[\"B\"]\n\nIn [521]: dftd\nOut[521]: \n A                   B                  C\n0 2013-01-01 2013-01-01 00:00:10  -1 days +23:59:50\n1 2013-01-01 2013-01-02 00:00:10  -2 days +23:59:50\n2 2013-01-01 2013-01-03 00:00:10  -3 days +23:59:50\n3 2013-01-01 2013-01-04 00:00:10  -4 days +23:59:50\n4 2013-01-01 2013-01-05 00:00:10  -5 days +23:59:50\n5 2013-01-01 2013-01-06 00:00:10  -6 days +23:59:50\n6 2013-01-01 2013-01-07 00:00:10  -7 days +23:59:50\n7 2013-01-01 2013-01-08 00:00:10  -8 days +23:59:50\n8 2013-01-01 2013-01-09 00:00:10  -9 days +23:59:50\n9 2013-01-01 2013-01-10 00:00:10 -10 days +23:59:50\n\nIn [522]: store.append(\"dftd\", dftd, data_columns=True)\n\nIn [523]: store.select(\"dftd\", \"C<'-3.5D'\")\nOut[523]: \n A                   B                  C\n4 1970-01-01 00:00:01.356998400 2013-01-05 00:00:10  -5 days +23:59:50\n5 1970-01-01 00:00:01.356998400 2013-01-06 00:00:10  -6 days +23:59:50\n6 1970-01-01 00:00:01.356998400 2013-01-07 00:00:10  -7 days +23:59:50\n7 1970-01-01 00:00:01.356998400 2013-01-08 00:00:10  -8 days +23:59:50\n8 1970-01-01 00:00:01.356998400 2013-01-09 00:00:10  -9 days +23:59:50\n9 1970-01-01 00:00:01.356998400 2013-01-10 00:00:10 -10 days +23:59:50 \n```", "```py\nIn [524]: df_mi.index.names\nOut[524]: FrozenList(['foo', 'bar'])\n\nIn [525]: store.select(\"df_mi\", \"foo=baz and bar=two\")\nOut[525]: \n A         B         C\nfoo bar \nbaz two -1.646063 -0.695847 -0.429156 \n```", "```py\nIn [526]: index = pd.MultiIndex(\n .....:    levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n .....:    codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n .....: )\n .....: \n\nIn [527]: df_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\nIn [528]: df_mi_2\nOut[528]: \n A         B         C\nfoo one   -0.219582  1.186860 -1.437189\n two    0.053768  1.872644 -1.469813\n three -0.564201  0.876341  0.407749\nbar one   -0.232583  0.179812  0.922152\n two   -1.820952 -0.641360  2.133239\nbaz two   -0.941248 -0.136307 -1.271305\n three -0.099774 -0.061438 -0.845172\nqux one    0.465793  0.756995 -0.541690\n two   -0.802241  0.877657 -2.553831\n three  0.094899 -2.319519  0.293601\n\nIn [529]: store.append(\"df_mi_2\", df_mi_2)\n\n# the levels are automatically included as data columns with keyword level_n\nIn [530]: store.select(\"df_mi_2\", \"level_0=foo and level_1=two\")\nOut[530]: \n A         B         C\nfoo two  0.053768  1.872644 -1.469813 \n```", "```py\n# we have automagically already created an index (in the first section)\nIn [531]: i = store.root.df.table.cols.index.index\n\nIn [532]: i.optlevel, i.kind\nOut[532]: (6, 'medium')\n\n# change an index by passing new parameters\nIn [533]: store.create_table_index(\"df\", optlevel=9, kind=\"full\")\n\nIn [534]: i = store.root.df.table.cols.index.index\n\nIn [535]: i.optlevel, i.kind\nOut[535]: (9, 'full') \n```", "```py\nIn [536]: df_1 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [537]: df_2 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [538]: st = pd.HDFStore(\"appends.h5\", mode=\"w\")\n\nIn [539]: st.append(\"df\", df_1, data_columns=[\"B\"], index=False)\n\nIn [540]: st.append(\"df\", df_2, data_columns=[\"B\"], index=False)\n\nIn [541]: st.get_storer(\"df\").table\nOut[541]: \n/df/table (Table(20,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n byteorder := 'little'\n chunkshape := (2730,) \n```", "```py\nIn [542]: st.create_table_index(\"df\", columns=[\"B\"], optlevel=9, kind=\"full\")\n\nIn [543]: st.get_storer(\"df\").table\nOut[543]: \n/df/table (Table(20,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2)}\n byteorder := 'little'\n chunkshape := (2730,)\n autoindex := True\n colindexes := {\n \"B\": Index(9, fullshuffle, zlib(1)).is_csi=True}\n\nIn [544]: st.close() \n```", "```py\nIn [545]: df_dc = df.copy()\n\nIn [546]: df_dc[\"string\"] = \"foo\"\n\nIn [547]: df_dc.loc[df_dc.index[4:6], \"string\"] = np.nan\n\nIn [548]: df_dc.loc[df_dc.index[7:9], \"string\"] = \"bar\"\n\nIn [549]: df_dc[\"string2\"] = \"cool\"\n\nIn [550]: df_dc.loc[df_dc.index[1:3], [\"B\", \"C\"]] = 1.0\n\nIn [551]: df_dc\nOut[551]: \n A         B         C string string2\n2000-01-01  0.858644 -0.851236  1.058006    foo    cool\n2000-01-02 -0.080372  1.000000  1.000000    foo    cool\n2000-01-03  0.816983  1.000000  1.000000    foo    cool\n2000-01-04  0.712795 -0.062433  0.736755    foo    cool\n2000-01-05 -0.298721 -1.988045  1.475308    NaN    cool\n2000-01-06  1.103675  1.382242 -0.650762    NaN    cool\n2000-01-07 -0.729161 -0.142928 -1.063038    foo    cool\n2000-01-08 -1.005977  0.465222 -0.094517    bar    cool\n\n# on-disk operations\nIn [552]: store.append(\"df_dc\", df_dc, data_columns=[\"B\", \"C\", \"string\", \"string2\"])\n\nIn [553]: store.select(\"df_dc\", where=\"B > 0\")\nOut[553]: \n A         B         C string string2\n2000-01-02 -0.080372  1.000000  1.000000    foo    cool\n2000-01-03  0.816983  1.000000  1.000000    foo    cool\n2000-01-06  1.103675  1.382242 -0.650762    NaN    cool\n2000-01-08 -1.005977  0.465222 -0.094517    bar    cool\n\n# getting creative\nIn [554]: store.select(\"df_dc\", \"B > 0 & C > 0 & string == foo\")\nOut[554]: \n A    B    C string string2\n2000-01-02 -0.080372  1.0  1.0    foo    cool\n2000-01-03  0.816983  1.0  1.0    foo    cool\n\n# this is in-memory version of this type of selection\nIn [555]: df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == \"foo\")]\nOut[555]: \n A    B    C string string2\n2000-01-02 -0.080372  1.0  1.0    foo    cool\n2000-01-03  0.816983  1.0  1.0    foo    cool\n\n# we have automagically created this index and the B/C/string/string2\n# columns are stored separately as ``PyTables`` columns\nIn [556]: store.root.df_dc.table\nOut[556]: \n/df_dc/table (Table(8,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": Float64Col(shape=(1,), dflt=0.0, pos=1),\n \"B\": Float64Col(shape=(), dflt=0.0, pos=2),\n \"C\": Float64Col(shape=(), dflt=0.0, pos=3),\n \"string\": StringCol(itemsize=3, shape=(), dflt=b'', pos=4),\n \"string2\": StringCol(itemsize=4, shape=(), dflt=b'', pos=5)}\n byteorder := 'little'\n chunkshape := (1680,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"B\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"C\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"string\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"string2\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [557]: for df in store.select(\"df\", chunksize=3):\n .....:    print(df)\n .....: \n A         B         C\n2000-01-01  0.858644 -0.851236  1.058006\n2000-01-02 -0.080372 -1.268121  1.561967\n2000-01-03  0.816983  1.965656 -1.169408\n A         B         C\n2000-01-04  0.712795 -0.062433  0.736755\n2000-01-05 -0.298721 -1.988045  1.475308\n2000-01-06  1.103675  1.382242 -0.650762\n A         B         C\n2000-01-07 -0.729161 -0.142928 -1.063038\n2000-01-08 -1.005977  0.465222 -0.094517 \n```", "```py\nfor df in pd.read_hdf(\"store.h5\", \"df\", chunksize=3):\n    print(df) \n```", "```py\nIn [558]: dfeq = pd.DataFrame({\"number\": np.arange(1, 11)})\n\nIn [559]: dfeq\nOut[559]: \n number\n0       1\n1       2\n2       3\n3       4\n4       5\n5       6\n6       7\n7       8\n8       9\n9      10\n\nIn [560]: store.append(\"dfeq\", dfeq, data_columns=[\"number\"])\n\nIn [561]: def chunks(l, n):\n .....:    return [l[i: i + n] for i in range(0, len(l), n)]\n .....: \n\nIn [562]: evens = [2, 4, 6, 8, 10]\n\nIn [563]: coordinates = store.select_as_coordinates(\"dfeq\", \"number=evens\")\n\nIn [564]: for c in chunks(coordinates, 2):\n .....:    print(store.select(\"dfeq\", where=c))\n .....: \n number\n1       2\n3       4\n number\n5       6\n7       8\n number\n9      10 \n```", "```py\nIn [565]: store.select_column(\"df_dc\", \"index\")\nOut[565]: \n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\n3   2000-01-04\n4   2000-01-05\n5   2000-01-06\n6   2000-01-07\n7   2000-01-08\nName: index, dtype: datetime64[ns]\n\nIn [566]: store.select_column(\"df_dc\", \"string\")\nOut[566]: \n0    foo\n1    foo\n2    foo\n3    foo\n4    NaN\n5    NaN\n6    foo\n7    bar\nName: string, dtype: object \n```", "```py\nIn [567]: df_coord = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [568]: store.append(\"df_coord\", df_coord)\n\nIn [569]: c = store.select_as_coordinates(\"df_coord\", \"index > 20020101\")\n\nIn [570]: c\nOut[570]: \nIndex([732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n ...\n 990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n dtype='int64', length=268)\n\nIn [571]: store.select(\"df_coord\", where=c)\nOut[571]: \n 0         1\n2002-01-02  0.007717  1.168386\n2002-01-03  0.759328 -0.638934\n2002-01-04 -1.154018 -0.324071\n2002-01-05 -0.804551 -1.280593\n2002-01-06 -0.047208  1.260503\n...              ...       ...\n2002-09-22 -1.139583  0.344316\n2002-09-23 -0.760643 -1.306704\n2002-09-24  0.059018  1.775482\n2002-09-25  1.242255 -0.055457\n2002-09-26  0.410317  2.194489\n\n[268 rows x 2 columns] \n```", "```py\nIn [572]: df_mask = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [573]: store.append(\"df_mask\", df_mask)\n\nIn [574]: c = store.select_column(\"df_mask\", \"index\")\n\nIn [575]: where = c[pd.DatetimeIndex(c).month == 5].index\n\nIn [576]: store.select(\"df_mask\", where=where)\nOut[576]: \n 0         1\n2000-05-01  1.479511  0.516433\n2000-05-02 -0.334984 -1.493537\n2000-05-03  0.900321  0.049695\n2000-05-04  0.614266 -1.077151\n2000-05-05  0.233881  0.493246\n...              ...       ...\n2002-05-27  0.294122  0.457407\n2002-05-28 -1.102535  1.215650\n2002-05-29 -0.432911  0.753606\n2002-05-30 -1.105212  2.311877\n2002-05-31  2.567296  2.610691\n\n[93 rows x 2 columns] \n```", "```py\nIn [577]: store.get_storer(\"df_dc\").nrows\nOut[577]: 8 \n```", "```py\nIn [565]: store.select_column(\"df_dc\", \"index\")\nOut[565]: \n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\n3   2000-01-04\n4   2000-01-05\n5   2000-01-06\n6   2000-01-07\n7   2000-01-08\nName: index, dtype: datetime64[ns]\n\nIn [566]: store.select_column(\"df_dc\", \"string\")\nOut[566]: \n0    foo\n1    foo\n2    foo\n3    foo\n4    NaN\n5    NaN\n6    foo\n7    bar\nName: string, dtype: object \n```", "```py\nIn [567]: df_coord = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [568]: store.append(\"df_coord\", df_coord)\n\nIn [569]: c = store.select_as_coordinates(\"df_coord\", \"index > 20020101\")\n\nIn [570]: c\nOut[570]: \nIndex([732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n ...\n 990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n dtype='int64', length=268)\n\nIn [571]: store.select(\"df_coord\", where=c)\nOut[571]: \n 0         1\n2002-01-02  0.007717  1.168386\n2002-01-03  0.759328 -0.638934\n2002-01-04 -1.154018 -0.324071\n2002-01-05 -0.804551 -1.280593\n2002-01-06 -0.047208  1.260503\n...              ...       ...\n2002-09-22 -1.139583  0.344316\n2002-09-23 -0.760643 -1.306704\n2002-09-24  0.059018  1.775482\n2002-09-25  1.242255 -0.055457\n2002-09-26  0.410317  2.194489\n\n[268 rows x 2 columns] \n```", "```py\nIn [572]: df_mask = pd.DataFrame(\n .....:    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n .....: )\n .....: \n\nIn [573]: store.append(\"df_mask\", df_mask)\n\nIn [574]: c = store.select_column(\"df_mask\", \"index\")\n\nIn [575]: where = c[pd.DatetimeIndex(c).month == 5].index\n\nIn [576]: store.select(\"df_mask\", where=where)\nOut[576]: \n 0         1\n2000-05-01  1.479511  0.516433\n2000-05-02 -0.334984 -1.493537\n2000-05-03  0.900321  0.049695\n2000-05-04  0.614266 -1.077151\n2000-05-05  0.233881  0.493246\n...              ...       ...\n2002-05-27  0.294122  0.457407\n2002-05-28 -1.102535  1.215650\n2002-05-29 -0.432911  0.753606\n2002-05-30 -1.105212  2.311877\n2002-05-31  2.567296  2.610691\n\n[93 rows x 2 columns] \n```", "```py\nIn [577]: store.get_storer(\"df_dc\").nrows\nOut[577]: 8 \n```", "```py\nIn [578]: df_mt = pd.DataFrame(\n .....:    np.random.randn(8, 6),\n .....:    index=pd.date_range(\"1/1/2000\", periods=8),\n .....:    columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n .....: )\n .....: \n\nIn [579]: df_mt[\"foo\"] = \"bar\"\n\nIn [580]: df_mt.loc[df_mt.index[1], (\"A\", \"B\")] = np.nan\n\n# you can also create the tables individually\nIn [581]: store.append_to_multiple(\n .....:    {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\"\n .....: )\n .....: \n\nIn [582]: store\nOut[582]: \n<class 'pandas.io.pytables.HDFStore'>\nFile path: store.h5\n\n# individual tables were created\nIn [583]: store.select(\"df1_mt\")\nOut[583]: \n A         B\n2000-01-01  0.162291 -0.430489\n2000-01-02       NaN       NaN\n2000-01-03  0.429207 -1.099274\n2000-01-04  1.869081 -1.466039\n2000-01-05  0.092130 -1.726280\n2000-01-06  0.266901 -0.036854\n2000-01-07 -0.517871 -0.990317\n2000-01-08 -0.231342  0.557402\n\nIn [584]: store.select(\"df2_mt\")\nOut[584]: \n C         D         E         F  foo\n2000-01-01 -2.502042  0.668149  0.460708  1.834518  bar\n2000-01-02  0.130441 -0.608465  0.439872  0.506364  bar\n2000-01-03 -1.069546  1.236277  0.116634 -1.772519  bar\n2000-01-04  0.137462  0.313939  0.748471 -0.943009  bar\n2000-01-05  0.836517  2.049798  0.562167  0.189952  bar\n2000-01-06  1.112750 -0.151596  1.503311  0.939470  bar\n2000-01-07 -0.294348  0.335844 -0.794159  1.495614  bar\n2000-01-08  0.860312 -0.538674 -0.541986 -1.759606  bar\n\n# as a multiple\nIn [585]: store.select_as_multiple(\n .....:    [\"df1_mt\", \"df2_mt\"],\n .....:    where=[\"A>0\", \"B>0\"],\n .....:    selector=\"df1_mt\",\n .....: )\n .....: \nOut[585]: \nEmpty DataFrame\nColumns: [A, B, C, D, E, F, foo]\nIndex: [] \n```", "```py\nstore_compressed = pd.HDFStore(\n    \"store_compressed.h5\", complevel=9, complib=\"blosc:blosclz\"\n) \n```", "```py\nstore.append(\"df\", df, complib=\"zlib\", complevel=5) \n```", "```py\nptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5 \n```", "```py\nstore_compressed = pd.HDFStore(\n    \"store_compressed.h5\", complevel=9, complib=\"blosc:blosclz\"\n) \n```", "```py\nstore.append(\"df\", df, complib=\"zlib\", complevel=5) \n```", "```py\nptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5 \n```", "```py\nIn [586]: dfcat = pd.DataFrame(\n .....:    {\"A\": pd.Series(list(\"aabbcdba\")).astype(\"category\"), \"B\": np.random.randn(8)}\n .....: )\n .....: \n\nIn [587]: dfcat\nOut[587]: \n A         B\n0  a -1.520478\n1  a -1.069391\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n5  d  0.301911\n6  b -0.640843\n7  a -2.253022\n\nIn [588]: dfcat.dtypes\nOut[588]: \nA    category\nB     float64\ndtype: object\n\nIn [589]: cstore = pd.HDFStore(\"cats.h5\", mode=\"w\")\n\nIn [590]: cstore.append(\"dfcat\", dfcat, format=\"table\", data_columns=[\"A\"])\n\nIn [591]: result = cstore.select(\"dfcat\", where=\"A in ['b', 'c']\")\n\nIn [592]: result\nOut[592]: \n A         B\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n6  b -0.640843\n\nIn [593]: result.dtypes\nOut[593]: \nA    category\nB     float64\ndtype: object \n```", "```py\nIn [594]: dfs = pd.DataFrame({\"A\": \"foo\", \"B\": \"bar\"}, index=list(range(5)))\n\nIn [595]: dfs\nOut[595]: \n A    B\n0  foo  bar\n1  foo  bar\n2  foo  bar\n3  foo  bar\n4  foo  bar\n\n# A and B have a size of 30\nIn [596]: store.append(\"dfs\", dfs, min_itemsize=30)\n\nIn [597]: store.get_storer(\"dfs\").table\nOut[597]: \n/dfs/table (Table(5,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": StringCol(itemsize=30, shape=(2,), dflt=b'', pos=1)}\n byteorder := 'little'\n chunkshape := (963,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False}\n\n# A is created as a data_column with a size of 30\n# B is size is calculated\nIn [598]: store.append(\"dfs2\", dfs, min_itemsize={\"A\": 30})\n\nIn [599]: store.get_storer(\"dfs2\").table\nOut[599]: \n/dfs2/table (Table(5,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": StringCol(itemsize=3, shape=(1,), dflt=b'', pos=1),\n \"A\": StringCol(itemsize=30, shape=(), dflt=b'', pos=2)}\n byteorder := 'little'\n chunkshape := (1598,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"A\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [600]: dfss = pd.DataFrame({\"A\": [\"foo\", \"bar\", \"nan\"]})\n\nIn [601]: dfss\nOut[601]: \n A\n0  foo\n1  bar\n2  nan\n\nIn [602]: store.append(\"dfss\", dfss)\n\nIn [603]: store.select(\"dfss\")\nOut[603]: \n A\n0  foo\n1  bar\n2  NaN\n\n# here you need to specify a different nan rep\nIn [604]: store.append(\"dfss2\", dfss, nan_rep=\"_nan_\")\n\nIn [605]: store.select(\"dfss2\")\nOut[605]: \n A\n0  foo\n1  bar\n2  nan \n```", "```py\nIn [586]: dfcat = pd.DataFrame(\n .....:    {\"A\": pd.Series(list(\"aabbcdba\")).astype(\"category\"), \"B\": np.random.randn(8)}\n .....: )\n .....: \n\nIn [587]: dfcat\nOut[587]: \n A         B\n0  a -1.520478\n1  a -1.069391\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n5  d  0.301911\n6  b -0.640843\n7  a -2.253022\n\nIn [588]: dfcat.dtypes\nOut[588]: \nA    category\nB     float64\ndtype: object\n\nIn [589]: cstore = pd.HDFStore(\"cats.h5\", mode=\"w\")\n\nIn [590]: cstore.append(\"dfcat\", dfcat, format=\"table\", data_columns=[\"A\"])\n\nIn [591]: result = cstore.select(\"dfcat\", where=\"A in ['b', 'c']\")\n\nIn [592]: result\nOut[592]: \n A         B\n2  b -0.551981\n3  b  0.452407\n4  c  0.409257\n6  b -0.640843\n\nIn [593]: result.dtypes\nOut[593]: \nA    category\nB     float64\ndtype: object \n```", "```py\nIn [594]: dfs = pd.DataFrame({\"A\": \"foo\", \"B\": \"bar\"}, index=list(range(5)))\n\nIn [595]: dfs\nOut[595]: \n A    B\n0  foo  bar\n1  foo  bar\n2  foo  bar\n3  foo  bar\n4  foo  bar\n\n# A and B have a size of 30\nIn [596]: store.append(\"dfs\", dfs, min_itemsize=30)\n\nIn [597]: store.get_storer(\"dfs\").table\nOut[597]: \n/dfs/table (Table(5,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": StringCol(itemsize=30, shape=(2,), dflt=b'', pos=1)}\n byteorder := 'little'\n chunkshape := (963,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False}\n\n# A is created as a data_column with a size of 30\n# B is size is calculated\nIn [598]: store.append(\"dfs2\", dfs, min_itemsize={\"A\": 30})\n\nIn [599]: store.get_storer(\"dfs2\").table\nOut[599]: \n/dfs2/table (Table(5,)) ''\n description := {\n \"index\": Int64Col(shape=(), dflt=0, pos=0),\n \"values_block_0\": StringCol(itemsize=3, shape=(1,), dflt=b'', pos=1),\n \"A\": StringCol(itemsize=30, shape=(), dflt=b'', pos=2)}\n byteorder := 'little'\n chunkshape := (1598,)\n autoindex := True\n colindexes := {\n \"index\": Index(6, mediumshuffle, zlib(1)).is_csi=False,\n \"A\": Index(6, mediumshuffle, zlib(1)).is_csi=False} \n```", "```py\nIn [600]: dfss = pd.DataFrame({\"A\": [\"foo\", \"bar\", \"nan\"]})\n\nIn [601]: dfss\nOut[601]: \n A\n0  foo\n1  bar\n2  nan\n\nIn [602]: store.append(\"dfss\", dfss)\n\nIn [603]: store.select(\"dfss\")\nOut[603]: \n A\n0  foo\n1  bar\n2  NaN\n\n# here you need to specify a different nan rep\nIn [604]: store.append(\"dfss2\", dfss, nan_rep=\"_nan_\")\n\nIn [605]: store.select(\"dfss2\")\nOut[605]: \n A\n0  foo\n1  bar\n2  nan \n```", "```py\nIn [606]: df = pd.DataFrame(\n .....:    {\n .....:        \"a\": list(\"abc\"),\n .....:        \"b\": list(range(1, 4)),\n .....:        \"c\": np.arange(3, 6).astype(\"u1\"),\n .....:        \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n .....:        \"e\": [True, False, True],\n .....:        \"f\": pd.Categorical(list(\"abc\")),\n .....:        \"g\": pd.date_range(\"20130101\", periods=3),\n .....:        \"h\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n .....:        \"i\": pd.date_range(\"20130101\", periods=3, freq=\"ns\"),\n .....:    }\n .....: )\n .....: \n\nIn [607]: df\nOut[607]: \n a  b  c  ...          g                         h                             i\n0  a  1  3  ... 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000\n1  b  2  4  ... 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001\n2  c  3  5  ... 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002\n\n[3 rows x 9 columns]\n\nIn [608]: df.dtypes\nOut[608]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                      category\ng                datetime64[ns]\nh    datetime64[ns, US/Eastern]\ni                datetime64[ns]\ndtype: object \n```", "```py\nIn [609]: df.to_feather(\"example.feather\") \n```", "```py\nIn [610]: result = pd.read_feather(\"example.feather\")\n\nIn [611]: result\nOut[611]: \n a  b  c  ...          g                         h                             i\n0  a  1  3  ... 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00.000000000\n1  b  2  4  ... 2013-01-02 2013-01-02 00:00:00-05:00 2013-01-01 00:00:00.000000001\n2  c  3  5  ... 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-01 00:00:00.000000002\n\n[3 rows x 9 columns]\n\n# we preserve dtypes\nIn [612]: result.dtypes\nOut[612]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                      category\ng                datetime64[ns]\nh    datetime64[ns, US/Eastern]\ni                datetime64[ns]\ndtype: object \n```", "```py\nIn [613]: df = pd.DataFrame(\n .....:    {\n .....:        \"a\": list(\"abc\"),\n .....:        \"b\": list(range(1, 4)),\n .....:        \"c\": np.arange(3, 6).astype(\"u1\"),\n .....:        \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n .....:        \"e\": [True, False, True],\n .....:        \"f\": pd.date_range(\"20130101\", periods=3),\n .....:        \"g\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n .....:        \"h\": pd.Categorical(list(\"abc\")),\n .....:        \"i\": pd.Categorical(list(\"abc\"), ordered=True),\n .....:    }\n .....: )\n .....: \n\nIn [614]: df\nOut[614]: \n a  b  c    d      e          f                         g  h  i\n0  a  1  3  4.0   True 2013-01-01 2013-01-01 00:00:00-05:00  a  a\n1  b  2  4  5.0  False 2013-01-02 2013-01-02 00:00:00-05:00  b  b\n2  c  3  5  6.0   True 2013-01-03 2013-01-03 00:00:00-05:00  c  c\n\nIn [615]: df.dtypes\nOut[615]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                datetime64[ns]\ng    datetime64[ns, US/Eastern]\nh                      category\ni                      category\ndtype: object \n```", "```py\nIn [616]: df.to_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\n\nIn [617]: df.to_parquet(\"example_fp.parquet\", engine=\"fastparquet\") \n```", "```py\nIn [618]: result = pd.read_parquet(\"example_fp.parquet\", engine=\"fastparquet\")\n\nIn [619]: result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\n\nIn [620]: result.dtypes\nOut[620]: \na                        object\nb                         int64\nc                         uint8\nd                       float64\ne                          bool\nf                datetime64[ns]\ng    datetime64[ns, US/Eastern]\nh                      category\ni                      category\ndtype: object \n```", "```py\nIn [621]: result = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n\nIn [622]: result.dtypes\nOut[622]: \na                                      string[pyarrow]\nb                                       int64[pyarrow]\nc                                       uint8[pyarrow]\nd                                      double[pyarrow]\ne                                        bool[pyarrow]\nf                               timestamp[ns][pyarrow]\ng                timestamp[ns, tz=US/Eastern][pyarrow]\nh    dictionary<values=string, indices=int32, order...\ni    dictionary<values=string, indices=int32, order...\ndtype: object \n```", "```py\nIn [623]: result = pd.read_parquet(\n .....:    \"example_fp.parquet\",\n .....:    engine=\"fastparquet\",\n .....:    columns=[\"a\", \"b\"],\n .....: )\n .....: \n\nIn [624]: result = pd.read_parquet(\n .....:    \"example_pa.parquet\",\n .....:    engine=\"pyarrow\",\n .....:    columns=[\"a\", \"b\"],\n .....: )\n .....: \n\nIn [625]: result.dtypes\nOut[625]: \na    object\nb     int64\ndtype: object \n```", "```py\nIn [626]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n\nIn [627]: df.to_parquet(\"test.parquet\", engine=\"pyarrow\") \n```", "```py\nIn [628]: df.to_parquet(\"test.parquet\", index=False) \n```", "```py\nIn [629]: df = pd.DataFrame({\"a\": [0, 0, 1, 1], \"b\": [0, 1, 0, 1]})\n\nIn [630]: df.to_parquet(path=\"test\", engine=\"pyarrow\", partition_cols=[\"a\"], compression=None) \n```", "```py\ntest\n\u251c\u2500\u2500 a=0\n\u2502   \u251c\u2500\u2500 0bac803e32dc42ae83fddfd029cbdebc.parquet\n\u2502   \u2514\u2500\u2500  ...\n\u2514\u2500\u2500 a=1\n    \u251c\u2500\u2500 e6ab24a4f45147b49b54a662f0c412a3.parquet\n    \u2514\u2500\u2500 ... \n```", "```py\nIn [626]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n\nIn [627]: df.to_parquet(\"test.parquet\", engine=\"pyarrow\") \n```", "```py\nIn [628]: df.to_parquet(\"test.parquet\", index=False) \n```", "```py\nIn [629]: df = pd.DataFrame({\"a\": [0, 0, 1, 1], \"b\": [0, 1, 0, 1]})\n\nIn [630]: df.to_parquet(path=\"test\", engine=\"pyarrow\", partition_cols=[\"a\"], compression=None) \n```", "```py\ntest\n\u251c\u2500\u2500 a=0\n\u2502   \u251c\u2500\u2500 0bac803e32dc42ae83fddfd029cbdebc.parquet\n\u2502   \u2514\u2500\u2500  ...\n\u2514\u2500\u2500 a=1\n    \u251c\u2500\u2500 e6ab24a4f45147b49b54a662f0c412a3.parquet\n    \u2514\u2500\u2500 ... \n```", "```py\nIn [631]: df = pd.DataFrame(\n .....:    {\n .....:        \"a\": list(\"abc\"),\n .....:        \"b\": list(range(1, 4)),\n .....:        \"c\": np.arange(4.0, 7.0, dtype=\"float64\"),\n .....:        \"d\": [True, False, True],\n .....:        \"e\": pd.date_range(\"20130101\", periods=3),\n .....:    }\n .....: )\n .....: \n\nIn [632]: df\nOut[632]: \n a  b    c      d          e\n0  a  1  4.0   True 2013-01-01\n1  b  2  5.0  False 2013-01-02\n2  c  3  6.0   True 2013-01-03\n\nIn [633]: df.dtypes\nOut[633]: \na            object\nb             int64\nc           float64\nd              bool\ne    datetime64[ns]\ndtype: object \n```", "```py\nIn [634]: df.to_orc(\"example_pa.orc\", engine=\"pyarrow\") \n```", "```py\nIn [635]: result = pd.read_orc(\"example_pa.orc\")\n\nIn [636]: result.dtypes\nOut[636]: \na            object\nb             int64\nc           float64\nd              bool\ne    datetime64[ns]\ndtype: object \n```", "```py\nIn [637]: result = pd.read_orc(\n .....:    \"example_pa.orc\",\n .....:    columns=[\"a\", \"b\"],\n .....: )\n .....: \n\nIn [638]: result.dtypes\nOut[638]: \na    object\nb     int64\ndtype: object \n```", "```py\nimport adbc_driver_sqlite.dbapi as sqlite_dbapi\n\n# Create the connection\nwith sqlite_dbapi.connect(\"sqlite:///:memory:\") as conn:\n     df = pd.read_sql_table(\"data\", conn) \n```", "```py\nIn [639]: from sqlalchemy import create_engine\n\n# Create your engine.\nIn [640]: engine = create_engine(\"sqlite:///:memory:\") \n```", "```py\nwith engine.connect() as conn, conn.begin():\n    data = pd.read_sql_table(\"data\", conn) \n```", "```py\nIn [641]: import datetime\n\nIn [642]: c = [\"id\", \"Date\", \"Col_1\", \"Col_2\", \"Col_3\"]\n\nIn [643]: d = [\n .....:    (26, datetime.datetime(2010, 10, 18), \"X\", 27.5, True),\n .....:    (42, datetime.datetime(2010, 10, 19), \"Y\", -12.5, False),\n .....:    (63, datetime.datetime(2010, 10, 20), \"Z\", 5.73, True),\n .....: ]\n .....: \n\nIn [644]: data = pd.DataFrame(d, columns=c)\n\nIn [645]: data\nOut[645]: \n id       Date Col_1  Col_2  Col_3\n0  26 2010-10-18     X  27.50   True\n1  42 2010-10-19     Y -12.50  False\n2  63 2010-10-20     Z   5.73   True\n\nIn [646]: data.to_sql(\"data\", con=engine)\nOut[646]: 3 \n```", "```py\nIn [647]: data.to_sql(\"data_chunked\", con=engine, chunksize=1000)\nOut[647]: 3 \n```", "```py\n# for roundtripping\nwith pg_dbapi.connect(uri) as conn:\n    df2 = pd.read_sql(\"pandas_table\", conn, dtype_backend=\"pyarrow\") \n```", "```py\nIn [648]: from sqlalchemy.types import String\n\nIn [649]: data.to_sql(\"data_dtype\", con=engine, dtype={\"Col_1\": String})\nOut[649]: 3 \n```", "```py\n# Alternative to_sql() *method* for DBs that support COPY FROM\nimport csv\nfrom io import StringIO\n\ndef psql_insert_copy(table, conn, keys, data_iter):\n  \"\"\"\n Execute SQL statement inserting data\n\n Parameters\n ----------\n table : pandas.io.sql.SQLTable\n conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n keys : list of str\n Column names\n data_iter : Iterable that iterates the values to be inserted\n \"\"\"\n    # gets a DBAPI connection that can provide a cursor\n    dbapi_conn = conn.connection\n    with dbapi_conn.cursor() as cur:\n        s_buf = StringIO()\n        writer = csv.writer(s_buf)\n        writer.writerows(data_iter)\n        s_buf.seek(0)\n\n        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n        if table.schema:\n            table_name = '{}.{}'.format(table.schema, table.name)\n        else:\n            table_name = table.name\n\n        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n            table_name, columns)\n        cur.copy_expert(sql=sql, file=s_buf) \n```", "```py\nIn [650]: pd.read_sql_table(\"data\", engine)\nOut[650]: \n index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True \n```", "```py\nIn [651]: pd.read_sql_table(\"data\", engine, index_col=\"id\")\nOut[651]: \n index       Date Col_1  Col_2  Col_3\nid \n26      0 2010-10-18     X  27.50   True\n42      1 2010-10-19     Y -12.50  False\n63      2 2010-10-20     Z   5.73   True\n\nIn [652]: pd.read_sql_table(\"data\", engine, columns=[\"Col_1\", \"Col_2\"])\nOut[652]: \n Col_1  Col_2\n0     X  27.50\n1     Y -12.50\n2     Z   5.73 \n```", "```py\nIn [653]: pd.read_sql_table(\"data\", engine, parse_dates=[\"Date\"])\nOut[653]: \n index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True \n```", "```py\npd.read_sql_table(\"data\", engine, parse_dates={\"Date\": \"%Y-%m-%d\"})\npd.read_sql_table(\n    \"data\",\n    engine,\n    parse_dates={\"Date\": {\"format\": \"%Y-%m-%d %H:%M:%S\"}},\n) \n```", "```py\ndf.to_sql(name=\"table\", con=engine, schema=\"other_schema\")\npd.read_sql_table(\"table\", engine, schema=\"other_schema\") \n```", "```py\nIn [654]: pd.read_sql_query(\"SELECT * FROM data\", engine)\nOut[654]: \n index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X  27.50      1\n1      1  42  2010-10-19 00:00:00.000000     Y -12.50      0\n2      2  63  2010-10-20 00:00:00.000000     Z   5.73      1 \n```", "```py\nIn [655]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[655]: \n id Col_1  Col_2\n0  42     Y  -12.5 \n```", "```py\nIn [656]: df = pd.DataFrame(np.random.randn(20, 3), columns=list(\"abc\"))\n\nIn [657]: df.to_sql(name=\"data_chunks\", con=engine, index=False)\nOut[657]: 20 \n```", "```py\nIn [658]: for chunk in pd.read_sql_query(\"SELECT * FROM data_chunks\", engine, chunksize=5):\n .....:    print(chunk)\n .....: \n a         b         c\n0 -0.395347 -0.822726 -0.363777\n1  1.676124 -0.908102 -1.391346\n2 -1.094269  0.278380  1.205899\n3  1.503443  0.932171 -0.709459\n4 -0.645944 -1.351389  0.132023\n a         b         c\n0  0.210427  0.192202  0.661949\n1  1.690629 -1.046044  0.618697\n2 -0.013863  1.314289  1.951611\n3 -1.485026  0.304662  1.194757\n4 -0.446717  0.528496 -0.657575\n a         b         c\n0 -0.876654  0.336252  0.172668\n1  0.337684 -0.411202 -0.828394\n2 -0.244413  1.094948  0.087183\n3  1.125934 -1.480095  1.205944\n4 -0.451849  0.452214 -2.208192\n a         b         c\n0 -2.061019  0.044184 -0.017118\n1  1.248959 -0.675595 -1.908296\n2 -0.125934  1.491974  0.648726\n3  0.391214  0.438609  1.634248\n4  1.208707 -1.535740  1.620399 \n```", "```py\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\"postgresql://scott:tiger@localhost:5432/mydatabase\")\n\nengine = create_engine(\"mysql+mysqldb://scott:tiger@localhost/foo\")\n\nengine = create_engine(\"oracle://scott:[[email\u00a0protected]](/cdn-cgi/l/email-protection):1521/sidname\")\n\nengine = create_engine(\"mssql+pyodbc://mydsn\")\n\n# sqlite://<nohostname>/<path>\n# where <path> is relative:\nengine = create_engine(\"sqlite:///foo.db\")\n\n# or absolute, starting with a slash:\nengine = create_engine(\"sqlite:////absolute/path/to/foo.db\") \n```", "```py\nIn [659]: import sqlalchemy as sa\n\nIn [660]: pd.read_sql(\n .....:    sa.text(\"SELECT * FROM data where Col_1=:col1\"), engine, params={\"col1\": \"X\"}\n .....: )\n .....: \nOut[660]: \n index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X   27.5      1 \n```", "```py\nIn [661]: metadata = sa.MetaData()\n\nIn [662]: data_table = sa.Table(\n .....:    \"data\",\n .....:    metadata,\n .....:    sa.Column(\"index\", sa.Integer),\n .....:    sa.Column(\"Date\", sa.DateTime),\n .....:    sa.Column(\"Col_1\", sa.String),\n .....:    sa.Column(\"Col_2\", sa.Float),\n .....:    sa.Column(\"Col_3\", sa.Boolean),\n .....: )\n .....: \n\nIn [663]: pd.read_sql(sa.select(data_table).where(data_table.c.Col_3 is True), engine)\nOut[663]: \nEmpty DataFrame\nColumns: [index, Date, Col_1, Col_2, Col_3]\nIndex: [] \n```", "```py\nIn [664]: import datetime as dt\n\nIn [665]: expr = sa.select(data_table).where(data_table.c.Date > sa.bindparam(\"date\"))\n\nIn [666]: pd.read_sql(expr, engine, params={\"date\": dt.datetime(2010, 10, 18)})\nOut[666]: \n index       Date Col_1  Col_2  Col_3\n0      1 2010-10-19     Y -12.50  False\n1      2 2010-10-20     Z   5.73   True \n```", "```py\nimport sqlite3\n\ncon = sqlite3.connect(\":memory:\") \n```", "```py\ndata.to_sql(\"data\", con)\npd.read_sql_query(\"SELECT * FROM data\", con) \n```", "```py\nIn [641]: import datetime\n\nIn [642]: c = [\"id\", \"Date\", \"Col_1\", \"Col_2\", \"Col_3\"]\n\nIn [643]: d = [\n .....:    (26, datetime.datetime(2010, 10, 18), \"X\", 27.5, True),\n .....:    (42, datetime.datetime(2010, 10, 19), \"Y\", -12.5, False),\n .....:    (63, datetime.datetime(2010, 10, 20), \"Z\", 5.73, True),\n .....: ]\n .....: \n\nIn [644]: data = pd.DataFrame(d, columns=c)\n\nIn [645]: data\nOut[645]: \n id       Date Col_1  Col_2  Col_3\n0  26 2010-10-18     X  27.50   True\n1  42 2010-10-19     Y -12.50  False\n2  63 2010-10-20     Z   5.73   True\n\nIn [646]: data.to_sql(\"data\", con=engine)\nOut[646]: 3 \n```", "```py\nIn [647]: data.to_sql(\"data_chunked\", con=engine, chunksize=1000)\nOut[647]: 3 \n```", "```py\n# for roundtripping\nwith pg_dbapi.connect(uri) as conn:\n    df2 = pd.read_sql(\"pandas_table\", conn, dtype_backend=\"pyarrow\") \n```", "```py\nIn [648]: from sqlalchemy.types import String\n\nIn [649]: data.to_sql(\"data_dtype\", con=engine, dtype={\"Col_1\": String})\nOut[649]: 3 \n```", "```py\n# for roundtripping\nwith pg_dbapi.connect(uri) as conn:\n    df2 = pd.read_sql(\"pandas_table\", conn, dtype_backend=\"pyarrow\") \n```", "```py\nIn [648]: from sqlalchemy.types import String\n\nIn [649]: data.to_sql(\"data_dtype\", con=engine, dtype={\"Col_1\": String})\nOut[649]: 3 \n```", "```py\n# Alternative to_sql() *method* for DBs that support COPY FROM\nimport csv\nfrom io import StringIO\n\ndef psql_insert_copy(table, conn, keys, data_iter):\n  \"\"\"\n Execute SQL statement inserting data\n\n Parameters\n ----------\n table : pandas.io.sql.SQLTable\n conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n keys : list of str\n Column names\n data_iter : Iterable that iterates the values to be inserted\n \"\"\"\n    # gets a DBAPI connection that can provide a cursor\n    dbapi_conn = conn.connection\n    with dbapi_conn.cursor() as cur:\n        s_buf = StringIO()\n        writer = csv.writer(s_buf)\n        writer.writerows(data_iter)\n        s_buf.seek(0)\n\n        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n        if table.schema:\n            table_name = '{}.{}'.format(table.schema, table.name)\n        else:\n            table_name = table.name\n\n        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n            table_name, columns)\n        cur.copy_expert(sql=sql, file=s_buf) \n```", "```py\n# Alternative to_sql() *method* for DBs that support COPY FROM\nimport csv\nfrom io import StringIO\n\ndef psql_insert_copy(table, conn, keys, data_iter):\n  \"\"\"\n Execute SQL statement inserting data\n\n Parameters\n ----------\n table : pandas.io.sql.SQLTable\n conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n keys : list of str\n Column names\n data_iter : Iterable that iterates the values to be inserted\n \"\"\"\n    # gets a DBAPI connection that can provide a cursor\n    dbapi_conn = conn.connection\n    with dbapi_conn.cursor() as cur:\n        s_buf = StringIO()\n        writer = csv.writer(s_buf)\n        writer.writerows(data_iter)\n        s_buf.seek(0)\n\n        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n        if table.schema:\n            table_name = '{}.{}'.format(table.schema, table.name)\n        else:\n            table_name = table.name\n\n        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n            table_name, columns)\n        cur.copy_expert(sql=sql, file=s_buf) \n```", "```py\nIn [650]: pd.read_sql_table(\"data\", engine)\nOut[650]: \n index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True \n```", "```py\nIn [651]: pd.read_sql_table(\"data\", engine, index_col=\"id\")\nOut[651]: \n index       Date Col_1  Col_2  Col_3\nid \n26      0 2010-10-18     X  27.50   True\n42      1 2010-10-19     Y -12.50  False\n63      2 2010-10-20     Z   5.73   True\n\nIn [652]: pd.read_sql_table(\"data\", engine, columns=[\"Col_1\", \"Col_2\"])\nOut[652]: \n Col_1  Col_2\n0     X  27.50\n1     Y -12.50\n2     Z   5.73 \n```", "```py\nIn [653]: pd.read_sql_table(\"data\", engine, parse_dates=[\"Date\"])\nOut[653]: \n index  id       Date Col_1  Col_2  Col_3\n0      0  26 2010-10-18     X  27.50   True\n1      1  42 2010-10-19     Y -12.50  False\n2      2  63 2010-10-20     Z   5.73   True \n```", "```py\npd.read_sql_table(\"data\", engine, parse_dates={\"Date\": \"%Y-%m-%d\"})\npd.read_sql_table(\n    \"data\",\n    engine,\n    parse_dates={\"Date\": {\"format\": \"%Y-%m-%d %H:%M:%S\"}},\n) \n```", "```py\ndf.to_sql(name=\"table\", con=engine, schema=\"other_schema\")\npd.read_sql_table(\"table\", engine, schema=\"other_schema\") \n```", "```py\nIn [654]: pd.read_sql_query(\"SELECT * FROM data\", engine)\nOut[654]: \n index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X  27.50      1\n1      1  42  2010-10-19 00:00:00.000000     Y -12.50      0\n2      2  63  2010-10-20 00:00:00.000000     Z   5.73      1 \n```", "```py\nIn [655]: pd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\nOut[655]: \n id Col_1  Col_2\n0  42     Y  -12.5 \n```", "```py\nIn [656]: df = pd.DataFrame(np.random.randn(20, 3), columns=list(\"abc\"))\n\nIn [657]: df.to_sql(name=\"data_chunks\", con=engine, index=False)\nOut[657]: 20 \n```", "```py\nIn [658]: for chunk in pd.read_sql_query(\"SELECT * FROM data_chunks\", engine, chunksize=5):\n .....:    print(chunk)\n .....: \n a         b         c\n0 -0.395347 -0.822726 -0.363777\n1  1.676124 -0.908102 -1.391346\n2 -1.094269  0.278380  1.205899\n3  1.503443  0.932171 -0.709459\n4 -0.645944 -1.351389  0.132023\n a         b         c\n0  0.210427  0.192202  0.661949\n1  1.690629 -1.046044  0.618697\n2 -0.013863  1.314289  1.951611\n3 -1.485026  0.304662  1.194757\n4 -0.446717  0.528496 -0.657575\n a         b         c\n0 -0.876654  0.336252  0.172668\n1  0.337684 -0.411202 -0.828394\n2 -0.244413  1.094948  0.087183\n3  1.125934 -1.480095  1.205944\n4 -0.451849  0.452214 -2.208192\n a         b         c\n0 -2.061019  0.044184 -0.017118\n1  1.248959 -0.675595 -1.908296\n2 -0.125934  1.491974  0.648726\n3  0.391214  0.438609  1.634248\n4  1.208707 -1.535740  1.620399 \n```", "```py\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\"postgresql://scott:tiger@localhost:5432/mydatabase\")\n\nengine = create_engine(\"mysql+mysqldb://scott:tiger@localhost/foo\")\n\nengine = create_engine(\"oracle://scott:[[email\u00a0protected]](/cdn-cgi/l/email-protection):1521/sidname\")\n\nengine = create_engine(\"mssql+pyodbc://mydsn\")\n\n# sqlite://<nohostname>/<path>\n# where <path> is relative:\nengine = create_engine(\"sqlite:///foo.db\")\n\n# or absolute, starting with a slash:\nengine = create_engine(\"sqlite:////absolute/path/to/foo.db\") \n```", "```py\nIn [659]: import sqlalchemy as sa\n\nIn [660]: pd.read_sql(\n .....:    sa.text(\"SELECT * FROM data where Col_1=:col1\"), engine, params={\"col1\": \"X\"}\n .....: )\n .....: \nOut[660]: \n index  id                        Date Col_1  Col_2  Col_3\n0      0  26  2010-10-18 00:00:00.000000     X   27.5      1 \n```", "```py\nIn [661]: metadata = sa.MetaData()\n\nIn [662]: data_table = sa.Table(\n .....:    \"data\",\n .....:    metadata,\n .....:    sa.Column(\"index\", sa.Integer),\n .....:    sa.Column(\"Date\", sa.DateTime),\n .....:    sa.Column(\"Col_1\", sa.String),\n .....:    sa.Column(\"Col_2\", sa.Float),\n .....:    sa.Column(\"Col_3\", sa.Boolean),\n .....: )\n .....: \n\nIn [663]: pd.read_sql(sa.select(data_table).where(data_table.c.Col_3 is True), engine)\nOut[663]: \nEmpty DataFrame\nColumns: [index, Date, Col_1, Col_2, Col_3]\nIndex: [] \n```", "```py\nIn [664]: import datetime as dt\n\nIn [665]: expr = sa.select(data_table).where(data_table.c.Date > sa.bindparam(\"date\"))\n\nIn [666]: pd.read_sql(expr, engine, params={\"date\": dt.datetime(2010, 10, 18)})\nOut[666]: \n index       Date Col_1  Col_2  Col_3\n0      1 2010-10-19     Y -12.50  False\n1      2 2010-10-20     Z   5.73   True \n```", "```py\nimport sqlite3\n\ncon = sqlite3.connect(\":memory:\") \n```", "```py\ndata.to_sql(\"data\", con)\npd.read_sql_query(\"SELECT * FROM data\", con) \n```", "```py\nIn [667]: df = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [668]: df.to_stata(\"stata.dta\") \n```", "```py\nIn [669]: pd.read_stata(\"stata.dta\")\nOut[669]: \n index         A         B\n0      0 -0.165614  0.490482\n1      1 -0.637829  0.067091\n2      2 -0.242577  1.348038\n3      3  0.647699 -0.644937\n4      4  0.625771  0.918376\n5      5  0.401781 -1.488919\n6      6 -0.981845 -0.046882\n7      7 -0.306796  0.877025\n8      8 -0.336606  0.624747\n9      9 -1.582600  0.806340 \n```", "```py\nIn [670]: with pd.read_stata(\"stata.dta\", chunksize=3) as reader:\n .....:    for df in reader:\n .....:        print(df.shape)\n .....: \n(3, 3)\n(3, 3)\n(3, 3)\n(1, 3) \n```", "```py\nIn [671]: with pd.read_stata(\"stata.dta\", iterator=True) as reader:\n .....:    chunk1 = reader.read(5)\n .....:    chunk2 = reader.read(5)\n .....: \n```", "```py\nIn [667]: df = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nIn [668]: df.to_stata(\"stata.dta\") \n```", "```py\nIn [669]: pd.read_stata(\"stata.dta\")\nOut[669]: \n index         A         B\n0      0 -0.165614  0.490482\n1      1 -0.637829  0.067091\n2      2 -0.242577  1.348038\n3      3  0.647699 -0.644937\n4      4  0.625771  0.918376\n5      5  0.401781 -1.488919\n6      6 -0.981845 -0.046882\n7      7 -0.306796  0.877025\n8      8 -0.336606  0.624747\n9      9 -1.582600  0.806340 \n```", "```py\nIn [670]: with pd.read_stata(\"stata.dta\", chunksize=3) as reader:\n .....:    for df in reader:\n .....:        print(df.shape)\n .....: \n(3, 3)\n(3, 3)\n(3, 3)\n(1, 3) \n```", "```py\nIn [671]: with pd.read_stata(\"stata.dta\", iterator=True) as reader:\n .....:    chunk1 = reader.read(5)\n .....:    chunk2 = reader.read(5)\n .....: \n```", "```py\ndf = pd.read_sas(\"sas_data.sas7bdat\") \n```", "```py\ndef do_something(chunk):\n    pass\n\nwith pd.read_sas(\"sas_xport.xpt\", chunk=100000) as rdr:\n    for chunk in rdr:\n        do_something(chunk) \n```", "```py\ndf = pd.read_spss(\"spss_data.sav\") \n```", "```py\ndf = pd.read_spss(\n    \"spss_data.sav\",\n    usecols=[\"foo\", \"bar\"],\n    convert_categoricals=False,\n) \n```", "```py\nIn [1]: sz = 1000000\nIn [2]: df = pd.DataFrame({'A': np.random.randn(sz), 'B': [1] * sz})\n\nIn [3]: df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 2 columns):\nA    1000000 non-null float64\nB    1000000 non-null int64\ndtypes: float64(1), int64(1)\nmemory usage: 15.3 MB \n```", "```py\nimport numpy as np\n\nimport os\n\nsz = 1000000\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\nsz = 1000000\nnp.random.seed(42)\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\ndef test_sql_write(df):\n    if os.path.exists(\"test.sql\"):\n        os.remove(\"test.sql\")\n    sql_db = sqlite3.connect(\"test.sql\")\n    df.to_sql(name=\"test_table\", con=sql_db)\n    sql_db.close()\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\"test.sql\")\n    pd.read_sql_query(\"select * from test_table\", sql_db)\n    sql_db.close()\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\"test_fixed.hdf\", key=\"test\", mode=\"w\")\n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\"test_fixed.hdf\", \"test\")\n\ndef test_hdf_fixed_write_compress(df):\n    df.to_hdf(\"test_fixed_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\")\n\ndef test_hdf_fixed_read_compress():\n    pd.read_hdf(\"test_fixed_compress.hdf\", \"test\")\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\"test_table.hdf\", key=\"test\", mode=\"w\", format=\"table\")\n\ndef test_hdf_table_read():\n    pd.read_hdf(\"test_table.hdf\", \"test\")\n\ndef test_hdf_table_write_compress(df):\n    df.to_hdf(\n        \"test_table_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\", format=\"table\"\n    )\n\ndef test_hdf_table_read_compress():\n    pd.read_hdf(\"test_table_compress.hdf\", \"test\")\n\ndef test_csv_write(df):\n    df.to_csv(\"test.csv\", mode=\"w\")\n\ndef test_csv_read():\n    pd.read_csv(\"test.csv\", index_col=0)\n\ndef test_feather_write(df):\n    df.to_feather(\"test.feather\")\n\ndef test_feather_read():\n    pd.read_feather(\"test.feather\")\n\ndef test_pickle_write(df):\n    df.to_pickle(\"test.pkl\")\n\ndef test_pickle_read():\n    pd.read_pickle(\"test.pkl\")\n\ndef test_pickle_write_compress(df):\n    df.to_pickle(\"test.pkl.compress\", compression=\"xz\")\n\ndef test_pickle_read_compress():\n    pd.read_pickle(\"test.pkl.compress\", compression=\"xz\")\n\ndef test_parquet_write(df):\n    df.to_parquet(\"test.parquet\")\n\ndef test_parquet_read():\n    pd.read_parquet(\"test.parquet\") \n```", "```py\nIn [4]: %timeit test_sql_write(df)\n3.29 s \u00b1 43.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [5]: %timeit test_hdf_fixed_write(df)\n19.4 ms \u00b1 560 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [6]: %timeit test_hdf_fixed_write_compress(df)\n19.6 ms \u00b1 308 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [7]: %timeit test_hdf_table_write(df)\n449 ms \u00b1 5.61 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [8]: %timeit test_hdf_table_write_compress(df)\n448 ms \u00b1 11.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [9]: %timeit test_csv_write(df)\n3.66 s \u00b1 26.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [10]: %timeit test_feather_write(df)\n9.75 ms \u00b1 117 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [11]: %timeit test_pickle_write(df)\n30.1 ms \u00b1 229 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [12]: %timeit test_pickle_write_compress(df)\n4.29 s \u00b1 15.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [13]: %timeit test_parquet_write(df)\n67.6 ms \u00b1 706 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) \n```", "```py\nIn [14]: %timeit test_sql_read()\n1.77 s \u00b1 17.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [15]: %timeit test_hdf_fixed_read()\n19.4 ms \u00b1 436 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [16]: %timeit test_hdf_fixed_read_compress()\n19.5 ms \u00b1 222 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [17]: %timeit test_hdf_table_read()\n38.6 ms \u00b1 857 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [18]: %timeit test_hdf_table_read_compress()\n38.8 ms \u00b1 1.49 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nIn [19]: %timeit test_csv_read()\n452 ms \u00b1 9.04 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [20]: %timeit test_feather_read()\n12.4 ms \u00b1 99.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [21]: %timeit test_pickle_read()\n18.4 ms \u00b1 191 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nIn [22]: %timeit test_pickle_read_compress()\n915 ms \u00b1 7.48 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nIn [23]: %timeit test_parquet_read()\n24.4 ms \u00b1 146 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) \n```", "```py\n29519500 Oct 10 06:45 test.csv\n16000248 Oct 10 06:45 test.feather\n8281983  Oct 10 06:49 test.parquet\n16000857 Oct 10 06:47 test.pkl\n7552144  Oct 10 06:48 test.pkl.compress\n34816000 Oct 10 06:42 test.sql\n24009288 Oct 10 06:43 test_fixed.hdf\n24009288 Oct 10 06:43 test_fixed_compress.hdf\n24458940 Oct 10 06:44 test_table.hdf\n24458940 Oct 10 06:44 test_table_compress.hdf \n```"]