- en: VisDrone Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/datasets/detect/visdrone/`](https://docs.ultralytics.com/datasets/detect/visdrone/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The [VisDrone Dataset](https://github.com/VisDrone/VisDrone-Dataset) is a large-scale
    benchmark created by the AISKYEYE team at the Lab of Machine Learning and Data
    Mining, Tianjin University, China. It contains carefully annotated ground truth
    data for various computer vision tasks related to drone-based image and video
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: VisDrone is composed of 288 video clips with 261,908 frames and 10,209 static
    images, captured by various drone-mounted cameras. The dataset covers a wide range
    of aspects, including location (14 different cities across China), environment
    (urban and rural), objects (pedestrians, vehicles, bicycles, etc.), and density
    (sparse and crowded scenes). The dataset was collected using various drone platforms
    under different scenarios and weather and lighting conditions. These frames are
    manually annotated with over 2.6 million bounding boxes of targets such as pedestrians,
    cars, bicycles, and tricycles. Attributes like scene visibility, object class,
    and occlusion are also provided for better data utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The VisDrone dataset is organized into five main subsets, each focusing on
    a specific task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task 1**: Object detection in images'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Task 2**: Object detection in videos'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Task 3**: Single-object tracking'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Task 4**: Multi-object tracking'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Task 5**: Crowd counting'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The VisDrone dataset is widely used for training and evaluating deep learning
    models in drone-based computer vision tasks such as object detection, object tracking,
    and crowd counting. The dataset's diverse set of sensor data, object annotations,
    and attributes make it a valuable resource for researchers and practitioners in
    the field of drone-based computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset YAML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A YAML (Yet Another Markup Language) file is used to define the dataset configuration.
    It contains information about the dataset's paths, classes, and other relevant
    information. In the case of the Visdrone dataset, the `VisDrone.yaml` file is
    maintained at [`github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/VisDrone.yaml`](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/VisDrone.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: ultralytics/cfg/datasets/VisDrone.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a YOLOv8n model on the VisDrone dataset for 100 epochs with an image
    size of 640, you can use the following code snippets. For a comprehensive list
    of available arguments, refer to the model Training page.
  prefs: []
  type: TYPE_NORMAL
- en: Train Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Sample Data and Annotations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The VisDrone dataset contains a diverse set of images and videos captured by
    drone-mounted cameras. Here are some examples of data from the dataset, along
    with their corresponding annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset sample image](img/dd8f867b1629372fdd32bcd50c971709.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Task 1**: Object detection in images - This image demonstrates an example
    of object detection in images, where objects are annotated with bounding boxes.
    The dataset provides a wide variety of images taken from different locations,
    environments, and densities to facilitate the development of models for this task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The example showcases the variety and complexity of the data in the VisDrone
    dataset and highlights the importance of high-quality sensor data for drone-based
    computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Citations and Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you use the VisDrone dataset in your research or development work, please
    cite the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We would like to acknowledge the AISKYEYE team at the Lab of Machine Learning
    and Data Mining, Tianjin University, China, for creating and maintaining the VisDrone
    dataset as a valuable resource for the drone-based computer vision research community.
    For more information about the VisDrone dataset and its creators, visit the [VisDrone
    Dataset GitHub repository](https://github.com/VisDrone/VisDrone-Dataset).
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the VisDrone Dataset and what are its key features?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The [VisDrone Dataset](https://github.com/VisDrone/VisDrone-Dataset) is a large-scale
    benchmark created by the AISKYEYE team at Tianjin University, China. It is designed
    for various computer vision tasks related to drone-based image and video analysis.
    Key features include: - **Composition**: 288 video clips with 261,908 frames and
    10,209 static images. - **Annotations**: Over 2.6 million bounding boxes for objects
    like pedestrians, cars, bicycles, and tricycles. - **Diversity**: Collected across
    14 cities, in urban and rural settings, under different weather and lighting conditions.
    - **Tasks**: Split into five main tasks—object detection in images and videos,
    single-object and multi-object tracking, and crowd counting.'
  prefs: []
  type: TYPE_NORMAL
- en: How can I use the VisDrone Dataset to train a YOLOv8 model with Ultralytics?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train a YOLOv8 model on the VisDrone dataset for 100 epochs with an image
    size of 640, you can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For additional configuration options, please refer to the model Training page.
  prefs: []
  type: TYPE_NORMAL
- en: What are the main subsets of the VisDrone dataset and their applications?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The VisDrone dataset is divided into five main subsets, each tailored for a
    specific computer vision task: 1\. **Task 1**: Object detection in images. 2\.
    **Task 2**: Object detection in videos. 3\. **Task 3**: Single-object tracking.
    4\. **Task 4**: Multi-object tracking. 5\. **Task 5**: Crowd counting.'
  prefs: []
  type: TYPE_NORMAL
- en: These subsets are widely used for training and evaluating deep learning models
    in drone-based applications such as surveillance, traffic monitoring, and public
    safety.
  prefs: []
  type: TYPE_NORMAL
- en: Where can I find the configuration file for the VisDrone dataset in Ultralytics?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The configuration file for the VisDrone dataset, `VisDrone.yaml`, can be found
    in the Ultralytics repository at the following link: [VisDrone.yaml](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/VisDrone.yaml).'
  prefs: []
  type: TYPE_NORMAL
- en: How can I cite the VisDrone dataset if I use it in my research?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you use the VisDrone dataset in your research or development work, please
    cite the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: BibTeX
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
