# pandas.read_parquet

> 原文：[`pandas.pydata.org/docs/reference/api/pandas.read_parquet.html`](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)

```py
pandas.read_parquet(path, engine='auto', columns=None, storage_options=None, use_nullable_dtypes=_NoDefault.no_default, dtype_backend=_NoDefault.no_default, filesystem=None, filters=None, **kwargs)
```

从文件路径加载 parquet 对象，返回一个 DataFrame。

参数：

**path**字符串、路径对象或文件对象

字符串、路径对象（实现 `os.PathLike[str]`）、或者实现了二进制 `read()` 函数的文件对象。字符串可以是 URL。有效的 URL 方案包括 http、ftp、s3、gs 和 file。对于文件 URL，期望有一个主机。本地文件可以是：`file://localhost/path/to/table.parquet`。文件 URL 也可以是一个包含多个分区 parquet 文件的目录路径。pyarrow 和 fastparquet 都支持目录路径以及文件 URL。目录路径可以是：`file://localhost/path/to/tables` 或者 `s3://bucket/partition_dir`。

**engine**{‘auto’, ‘pyarrow’, ‘fastparquet’}，默认为 ‘auto’

要使用的 Parquet 库。如果为 ‘auto’，则使用选项 `io.parquet.engine`。默认的 `io.parquet.engine` 行为是尝试 ‘pyarrow’，如果 ‘pyarrow’ 不可用，则回退到 ‘fastparquet’。

当使用 `'pyarrow'` 引擎且没有提供存储选项，且 `pyarrow.fs` 和 `fsspec` 都实现了文件系统时（例如 “s3://”），则首先尝试 `pyarrow.fs` 文件系统。如果希望使用其实现，则使用带有实例化 fsspec 文件系统的 filesystem 关键字。

**columns**列表，默认为 None

如果不是 None，则只从文件中读取这些列。

**storage_options**字典，可选

适用于特定存储连接的额外选项，例如主机、端口、用户名、密码等。对于 HTTP(S) URL，键值对将被转发给 `urllib.request.Request` 作为标头选项。对于其他 URL（例如以 “s3://” 和 “gcs://” 开头的 URL），键值对将被转发给 `fsspec.open`。请参阅 `fsspec` 和 `urllib` 以获取更多详细信息，并参考[此处](https://pandas.pydata.org/docs/user_guide/io.html?highlight=storage_options#reading-writing-remote-files)的存储选项的更多示例。

自版本 1.3.0 新增。

**use_nullable_dtypes**布尔值，默认为 False

如果为 True，则使用在生成的 DataFrame 中将 `pd.NA` 用作缺失值指示符的数据类型。（仅适用于 `pyarrow` 引擎）随着将来添加支持 `pd.NA` 的新数据类型，此选项的输出将更改为使用这些数据类型。注意：这是一个试验性选项，行为（例如额外支持的数据类型）可能会在不经通知的情况下发生变化。

自版本 2.0 起已弃用。

**dtype_backend**{‘numpy_nullable’, ‘pyarrow’}，默认为 ‘numpy_nullable’

应用于生成的 [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame "pandas.DataFrame") 的后端数据类型（仍然是试验性的）。行为如下：

+   `"numpy_nullable"`：返回基于可为空的 dtype 的 [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame "pandas.DataFrame")（默认）。

+   `"pyarrow"`：返回由 pyarrow 支持的可空 `ArrowDtype` DataFrame。

新版本 2.0 中新增。

**filesystem**fsspec 或 pyarrow 文件系统，默认为 None

在读取 parquet 文件时要使用的文件系统对象。仅对`engine="pyarrow"`实现。

新版本 2.1.0 中新增。

**filters**List[Tuple] 或 List[List[Tuple]]，默认为 None

过滤数据。过滤器语法：[[(column, op, val), …],…]，其中 op 为 [==, =, >, >=, <, <=, !=, in, not in]。最内层元组通过 AND 操作转置为一组过滤器。外部列表通过 OR 操作组合这些过滤器集。也可以使用单个元组列表，意味着不进行过滤器集之间的 OR 操作。

使用此参数不会导致最终分区按行进行筛选，除非也指定了`engine="pyarrow"`。对于其他引擎，筛选仅在分区级别执行，即防止加载某些行组和/或文件。

新版本 2.1.0 中新增。

****kwargs**

任何额外的 kwargs 都将传递给引擎。

返回：

DataFrame

另请参阅

`DataFrame.to_parquet`

创建序列化 DataFrame 的 Parquet 对象。

示例

```py
>>> original_df = pd.DataFrame(
...     {"foo": range(5), "bar": range(5, 10)}
...    )
>>> original_df
 foo  bar
0    0    5
1    1    6
2    2    7
3    3    8
4    4    9
>>> df_parquet_bytes = original_df.to_parquet()
>>> from io import BytesIO
>>> restored_df = pd.read_parquet(BytesIO(df_parquet_bytes))
>>> restored_df
 foo  bar
0    0    5
1    1    6
2    2    7
3    3    8
4    4    9
>>> restored_df.equals(original_df)
True
>>> restored_bar = pd.read_parquet(BytesIO(df_parquet_bytes), columns=["bar"])
>>> restored_bar
 bar
0    5
1    6
2    7
3    8
4    9
>>> restored_bar.equals(original_df[['bar']])
True 
```

该函数使用直接传递给引擎的 kwargs。在下面的示例中，我们使用 pyarrow 引擎的 filters 参数来过滤 DataFrame 的行。

由于 pyarrow 是默认引擎，我们可以省略引擎参数。请注意，filters 参数由 pyarrow 引擎实现，这可以从多线程中受益，并且在内存方面也可能更经济。

```py
>>> sel = [("foo", ">", 2)]
>>> restored_part = pd.read_parquet(BytesIO(df_parquet_bytes), filters=sel)
>>> restored_part
 foo  bar
0    3    8
1    4    9 
```
