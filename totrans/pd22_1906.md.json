["```py\nIn [1]: import pyarrow as pa\n\nIn [2]: ser_float = pd.Series([1.0, 2.0, None], dtype=\"float32[pyarrow]\")\n\nIn [3]: ser_float\nOut[3]: \n0     1.0\n1     2.0\n2    <NA>\ndtype: float[pyarrow]\n\nIn [4]: list_of_int_type = pd.ArrowDtype(pa.list_(pa.int64()))\n\nIn [5]: ser_list = pd.Series([[1, 2], [3, None]], dtype=list_of_int_type)\n\nIn [6]: ser_list\nOut[6]: \n0      [1\\. 2.]\n1    [ 3\\. nan]\ndtype: list<item: int64>[pyarrow]\n\nIn [7]: ser_list.take([1, 0])\nOut[7]: \n1    [ 3\\. nan]\n0      [1\\. 2.]\ndtype: list<item: int64>[pyarrow]\n\nIn [8]: ser_float * 5\nOut[8]: \n0     5.0\n1    10.0\n2    <NA>\ndtype: float[pyarrow]\n\nIn [9]: ser_float.mean()\nOut[9]: 1.5\n\nIn [10]: ser_float.dropna()\nOut[10]: \n0    1.0\n1    2.0\ndtype: float[pyarrow] \n```", "```py\nIn [11]: df = pd.DataFrame(\n ....:    {'a': range(6)},\n ....:    index=pd.date_range(\"2021-01-01\", periods=6, freq=\"8H\")\n ....: )\n ....:\n\nIn [12]: df.resample(\"D\", group_keys=True).apply(lambda x: x)\nOut[12]:\n a\n2021-01-01 2021-01-01 00:00:00  0\n 2021-01-01 08:00:00  1\n 2021-01-01 16:00:00  2\n2021-01-02 2021-01-02 00:00:00  3\n 2021-01-02 08:00:00  4\n 2021-01-02 16:00:00  5\n\nIn [13]: df.resample(\"D\", group_keys=False).apply(lambda x: x)\nOut[13]:\n a\n2021-01-01 00:00:00  0\n2021-01-01 08:00:00  1\n2021-01-01 16:00:00  2\n2021-01-02 00:00:00  3\n2021-01-02 08:00:00  4\n2021-01-02 16:00:00  5 \n```", "```py\nIn [1]: # pandas 1.3\nIn [2]: df.resample(\"D\").apply(lambda x: x)\nOut[2]:\n a\n2021-01-01 00:00:00  0\n2021-01-01 08:00:00  1\n2021-01-01 16:00:00  2\n2021-01-02 00:00:00  3\n2021-01-02 08:00:00  4\n2021-01-02 16:00:00  5\n\nIn [3]: df.resample(\"D\").apply(lambda x: x.reset_index())\nOut[3]:\n index  a\n2021-01-01 0 2021-01-01 00:00:00  0\n 1 2021-01-01 08:00:00  1\n 2 2021-01-01 16:00:00  2\n2021-01-02 0 2021-01-02 00:00:00  3\n 1 2021-01-02 08:00:00  4\n 2 2021-01-02 16:00:00  5 \n```", "```py\nIn [11]: import pandas as pd\n\nIn [12]: df = pd.DataFrame({\"col1_a\": [1, 0, 1], \"col1_b\": [0, 1, 0],\n ....:                   \"col2_a\": [0, 1, 0], \"col2_b\": [1, 0, 0],\n ....:                   \"col2_c\": [0, 0, 1]})\n ....: \n\nIn [13]: pd.from_dummies(df, sep=\"_\")\nOut[13]: \n col1 col2\n0    a    b\n1    b    a\n2    a    c \n```", "```py\ndf = pd.DataFrame(data={\"col1\": [1, 2], \"col2\": [3, 4]})\ndf.to_orc(\"./out.orc\") \n```", "```py\ndf = pd.read_csv(\"./movement.tar.gz\")\n# ...\ndf.to_csv(\"./out.tar.gz\") \n```", "```py\ndf = pd.read_csv(some_file_obj, compression={\"method\": \"tar\", \"mode\": \"r:gz\"}) # noqa F821 \n```", "```py\nIn [14]: from io import StringIO\n\nIn [15]: xml_dates = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n ....: <data>\n ....:  <row>\n ....:    <shape>square</shape>\n ....:    <degrees>00360</degrees>\n ....:    <sides>4.0</sides>\n ....:    <date>2020-01-01</date>\n ....:   </row>\n ....:  <row>\n ....:    <shape>circle</shape>\n ....:    <degrees>00360</degrees>\n ....:    <sides/>\n ....:    <date>2021-01-01</date>\n ....:  </row>\n ....:  <row>\n ....:    <shape>triangle</shape>\n ....:    <degrees>00180</degrees>\n ....:    <sides>3.0</sides>\n ....:    <date>2022-01-01</date>\n ....:  </row>\n ....: </data>\"\"\"\n ....: \n\nIn [16]: df = pd.read_xml(\n ....:    StringIO(xml_dates),\n ....:    dtype={'sides': 'Int64'},\n ....:    converters={'degrees': str},\n ....:    parse_dates=['date']\n ....: )\n ....: \n\nIn [17]: df\nOut[17]: \n shape degrees  sides       date\n0    square   00360      4 2020-01-01\n1    circle   00360   <NA> 2021-01-01\n2  triangle   00180      3 2022-01-01\n\nIn [18]: df.dtypes\nOut[18]: \nshape              object\ndegrees            object\nsides               Int64\ndate       datetime64[ns]\ndtype: object \n```", "```py\nIn [1]: df = pd.read_xml(\n...      \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n...      iterparse = {\"page\": [\"title\", \"ns\", \"id\"]})\n...  )\ndf\nOut[2]:\n title   ns        id\n0                                       Gettysburg Address    0     21450\n1                                                Main Page    0     42950\n2                            Declaration by United Nations    0      8435\n3             Constitution of the United States of America    0      8435\n4                     Declaration of Independence (Israel)    0     17858\n...                                                    ...  ...       ...\n3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649\n3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649\n3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649\n3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291\n3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450\n\n[3578765 rows x 3 columns] \n```", "```py\npd.set_option(\"mode.copy_on_write\", True)\npd.options.mode.copy_on_write = True \n```", "```py\nwith pd.option_context(\"mode.copy_on_write\", True):\n    ... \n```", "```py\nIn [19]: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": 1})\n\nIn [20]: view = df[\"foo\"]\n\nIn [21]: view.iloc[0]\nOut[21]: 1\n\nIn [22]: df\nOut[22]: \n foo  bar\n0    1    1\n1    2    1\n2    3    1 \n```", "```py\nIn [23]: with pd.option_context(\"mode.copy_on_write\", True):\n ....:    df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": 1})\n ....:    view = df[\"foo\"]\n ....:    view.iloc[0]\n ....:    df\n ....: \n```", "```py\nIn [24]: df = pd.DataFrame({'a': [1, 1, np.nan], 'b': [2, 3, 4]}) \n```", "```py\nIn [3]: # Value in the last row should be np.nan\n df.groupby('a', dropna=True).transform('sum')\nOut[3]:\n b\n0  5\n1  5\n2  5\n\nIn [3]: # Should have one additional row with the value np.nan\n df.groupby('a', dropna=True).transform(lambda x: x.sum())\nOut[3]:\n b\n0  5\n1  5\n\nIn [3]: # The value in the last row is np.nan interpreted as an integer\n df.groupby('a', dropna=True).transform('ffill')\nOut[3]:\n b\n0                    2\n1                    3\n2 -9223372036854775808\n\nIn [3]: # Should have one additional row with the value np.nan\n df.groupby('a', dropna=True).transform(lambda x: x)\nOut[3]:\n b\n0  2\n1  3 \n```", "```py\nIn [25]: df.groupby('a', dropna=True).transform('sum')\nOut[25]: \n b\n0  5.0\n1  5.0\n2  NaN\n\nIn [26]: df.groupby('a', dropna=True).transform(lambda x: x.sum())\nOut[26]: \n b\n0  5.0\n1  5.0\n2  NaN\n\nIn [27]: df.groupby('a', dropna=True).transform('ffill')\nOut[27]: \n b\n0  2.0\n1  3.0\n2  NaN\n\nIn [28]: df.groupby('a', dropna=True).transform(lambda x: x)\nOut[28]: \n b\n0  2.0\n1  3.0\n2  NaN \n```", "```py\nIn [32]: index = pd.date_range(\n ....:    start='2020-12-28 00:00:00',\n ....:    end='2020-12-28 02:00:00',\n ....:    freq='1H',\n ....: )\n ....:\n\nIn [33]: a = pd.Series(\n ....:    data=range(3),\n ....:    index=index,\n ....: )\n ....:\n\nIn [4]: from io import StringIO\n\nIn [5]: a.to_json(date_format='iso')\nOut[5]: '{\"2020-12-28T00:00:00.000Z\":0,\"2020-12-28T01:00:00.000Z\":1,\"2020-12-28T02:00:00.000Z\":2}'\n\nIn [6]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ=\"series\").index == a.index\nOut[6]: array([False, False, False]) \n```", "```py\nIn [34]: from io import StringIO\n\nIn [35]: a.to_json(date_format='iso')\nOut[35]: '{\"2020-12-28T00:00:00.000Z\":0,\"2020-12-28T01:00:00.000Z\":1,\"2020-12-28T02:00:00.000Z\":2}'\n\n# Roundtripping now works\nIn [36]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ=\"series\").index == a.index\nOut[36]: array([ True,  True,  True]) \n```", "```py\nIn [6]: df = pd.DataFrame([\"a\", \"b\", \"c\"], dtype=\"category\").iloc[0:2]\nIn [7]: df\nOut[7]:\n 0\n0  a\n1  b \n```", "```py\nIn [8]: df.groupby(level=0, observed=True).value_counts()\nOut[8]:\n0  a    1\n1  b    1\ndtype: int64 \n```", "```py\nIn [9]: df.groupby(level=0, observed=True).value_counts()\nOut[9]:\n0  a    1\n1  a    0\n b    1\n0  b    0\n c    0\n1  c    0\ndtype: int64 \n```", "```py\nIn [29]: ser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11]) \n```", "```py\nIn [3]: ser[2:4]\nOut[3]:\n5    3\n7    4\ndtype: int64 \n```", "```py\nIn [4]: ser.loc[2:4]\nOut[4]:\n2    1\n3    2\ndtype: int64 \n```", "```py\nIn [30]: df = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\n\nIn [31]: original_prices = df['price']\n\nIn [32]: new_prices = np.array([98, 99]) \n```", "```py\nIn [3]: df.iloc[:, 0] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: price, float: 64 \n```", "```py\nIn [3]: df.iloc[:, 0] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98.0\nbook2    99.0\nName: price, dtype: float64\nIn [5]: original_prices\nOut[5]:\nbook1    98.0\nbook2    99.0\nName: price, dtype: float64 \n```", "```py\nIn [3]: df[df.columns[0]] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: price, dtype: float64 \n```", "```py\nIn [3]: df_with_duplicated_cols = pd.concat([df, df], axis='columns')\nIn [3]: df_with_duplicated_cols.isetitem(0, new_prices)\nIn [4]: df_with_duplicated_cols.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: 0, dtype: float64 \n```", "```py\nIn [1]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [\"x\", \"y\"]})\n\nIn [2]: # Reading the next line without knowing the contents of df, one would\n # expect the result to contain the products for both columns a and b.\n df[[\"a\", \"b\"]].prod()\nOut[2]:\na    2\ndtype: int64 \n```", "```py\nIn [1]: import pyarrow as pa\n\nIn [2]: ser_float = pd.Series([1.0, 2.0, None], dtype=\"float32[pyarrow]\")\n\nIn [3]: ser_float\nOut[3]: \n0     1.0\n1     2.0\n2    <NA>\ndtype: float[pyarrow]\n\nIn [4]: list_of_int_type = pd.ArrowDtype(pa.list_(pa.int64()))\n\nIn [5]: ser_list = pd.Series([[1, 2], [3, None]], dtype=list_of_int_type)\n\nIn [6]: ser_list\nOut[6]: \n0      [1\\. 2.]\n1    [ 3\\. nan]\ndtype: list<item: int64>[pyarrow]\n\nIn [7]: ser_list.take([1, 0])\nOut[7]: \n1    [ 3\\. nan]\n0      [1\\. 2.]\ndtype: list<item: int64>[pyarrow]\n\nIn [8]: ser_float * 5\nOut[8]: \n0     5.0\n1    10.0\n2    <NA>\ndtype: float[pyarrow]\n\nIn [9]: ser_float.mean()\nOut[9]: 1.5\n\nIn [10]: ser_float.dropna()\nOut[10]: \n0    1.0\n1    2.0\ndtype: float[pyarrow] \n```", "```py\nIn [11]: df = pd.DataFrame(\n ....:    {'a': range(6)},\n ....:    index=pd.date_range(\"2021-01-01\", periods=6, freq=\"8H\")\n ....: )\n ....:\n\nIn [12]: df.resample(\"D\", group_keys=True).apply(lambda x: x)\nOut[12]:\n a\n2021-01-01 2021-01-01 00:00:00  0\n 2021-01-01 08:00:00  1\n 2021-01-01 16:00:00  2\n2021-01-02 2021-01-02 00:00:00  3\n 2021-01-02 08:00:00  4\n 2021-01-02 16:00:00  5\n\nIn [13]: df.resample(\"D\", group_keys=False).apply(lambda x: x)\nOut[13]:\n a\n2021-01-01 00:00:00  0\n2021-01-01 08:00:00  1\n2021-01-01 16:00:00  2\n2021-01-02 00:00:00  3\n2021-01-02 08:00:00  4\n2021-01-02 16:00:00  5 \n```", "```py\nIn [1]: # pandas 1.3\nIn [2]: df.resample(\"D\").apply(lambda x: x)\nOut[2]:\n a\n2021-01-01 00:00:00  0\n2021-01-01 08:00:00  1\n2021-01-01 16:00:00  2\n2021-01-02 00:00:00  3\n2021-01-02 08:00:00  4\n2021-01-02 16:00:00  5\n\nIn [3]: df.resample(\"D\").apply(lambda x: x.reset_index())\nOut[3]:\n index  a\n2021-01-01 0 2021-01-01 00:00:00  0\n 1 2021-01-01 08:00:00  1\n 2 2021-01-01 16:00:00  2\n2021-01-02 0 2021-01-02 00:00:00  3\n 1 2021-01-02 08:00:00  4\n 2 2021-01-02 16:00:00  5 \n```", "```py\nIn [11]: import pandas as pd\n\nIn [12]: df = pd.DataFrame({\"col1_a\": [1, 0, 1], \"col1_b\": [0, 1, 0],\n ....:                   \"col2_a\": [0, 1, 0], \"col2_b\": [1, 0, 0],\n ....:                   \"col2_c\": [0, 0, 1]})\n ....: \n\nIn [13]: pd.from_dummies(df, sep=\"_\")\nOut[13]: \n col1 col2\n0    a    b\n1    b    a\n2    a    c \n```", "```py\ndf = pd.DataFrame(data={\"col1\": [1, 2], \"col2\": [3, 4]})\ndf.to_orc(\"./out.orc\") \n```", "```py\ndf = pd.read_csv(\"./movement.tar.gz\")\n# ...\ndf.to_csv(\"./out.tar.gz\") \n```", "```py\ndf = pd.read_csv(some_file_obj, compression={\"method\": \"tar\", \"mode\": \"r:gz\"}) # noqa F821 \n```", "```py\nIn [14]: from io import StringIO\n\nIn [15]: xml_dates = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n ....: <data>\n ....:  <row>\n ....:    <shape>square</shape>\n ....:    <degrees>00360</degrees>\n ....:    <sides>4.0</sides>\n ....:    <date>2020-01-01</date>\n ....:   </row>\n ....:  <row>\n ....:    <shape>circle</shape>\n ....:    <degrees>00360</degrees>\n ....:    <sides/>\n ....:    <date>2021-01-01</date>\n ....:  </row>\n ....:  <row>\n ....:    <shape>triangle</shape>\n ....:    <degrees>00180</degrees>\n ....:    <sides>3.0</sides>\n ....:    <date>2022-01-01</date>\n ....:  </row>\n ....: </data>\"\"\"\n ....: \n\nIn [16]: df = pd.read_xml(\n ....:    StringIO(xml_dates),\n ....:    dtype={'sides': 'Int64'},\n ....:    converters={'degrees': str},\n ....:    parse_dates=['date']\n ....: )\n ....: \n\nIn [17]: df\nOut[17]: \n shape degrees  sides       date\n0    square   00360      4 2020-01-01\n1    circle   00360   <NA> 2021-01-01\n2  triangle   00180      3 2022-01-01\n\nIn [18]: df.dtypes\nOut[18]: \nshape              object\ndegrees            object\nsides               Int64\ndate       datetime64[ns]\ndtype: object \n```", "```py\nIn [1]: df = pd.read_xml(\n...      \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n...      iterparse = {\"page\": [\"title\", \"ns\", \"id\"]})\n...  )\ndf\nOut[2]:\n title   ns        id\n0                                       Gettysburg Address    0     21450\n1                                                Main Page    0     42950\n2                            Declaration by United Nations    0      8435\n3             Constitution of the United States of America    0      8435\n4                     Declaration of Independence (Israel)    0     17858\n...                                                    ...  ...       ...\n3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649\n3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649\n3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649\n3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291\n3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450\n\n[3578765 rows x 3 columns] \n```", "```py\npd.set_option(\"mode.copy_on_write\", True)\npd.options.mode.copy_on_write = True \n```", "```py\nwith pd.option_context(\"mode.copy_on_write\", True):\n    ... \n```", "```py\nIn [19]: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": 1})\n\nIn [20]: view = df[\"foo\"]\n\nIn [21]: view.iloc[0]\nOut[21]: 1\n\nIn [22]: df\nOut[22]: \n foo  bar\n0    1    1\n1    2    1\n2    3    1 \n```", "```py\nIn [23]: with pd.option_context(\"mode.copy_on_write\", True):\n ....:    df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": 1})\n ....:    view = df[\"foo\"]\n ....:    view.iloc[0]\n ....:    df\n ....: \n```", "```py\nIn [1]: import pyarrow as pa\n\nIn [2]: ser_float = pd.Series([1.0, 2.0, None], dtype=\"float32[pyarrow]\")\n\nIn [3]: ser_float\nOut[3]: \n0     1.0\n1     2.0\n2    <NA>\ndtype: float[pyarrow]\n\nIn [4]: list_of_int_type = pd.ArrowDtype(pa.list_(pa.int64()))\n\nIn [5]: ser_list = pd.Series([[1, 2], [3, None]], dtype=list_of_int_type)\n\nIn [6]: ser_list\nOut[6]: \n0      [1\\. 2.]\n1    [ 3\\. nan]\ndtype: list<item: int64>[pyarrow]\n\nIn [7]: ser_list.take([1, 0])\nOut[7]: \n1    [ 3\\. nan]\n0      [1\\. 2.]\ndtype: list<item: int64>[pyarrow]\n\nIn [8]: ser_float * 5\nOut[8]: \n0     5.0\n1    10.0\n2    <NA>\ndtype: float[pyarrow]\n\nIn [9]: ser_float.mean()\nOut[9]: 1.5\n\nIn [10]: ser_float.dropna()\nOut[10]: \n0    1.0\n1    2.0\ndtype: float[pyarrow] \n```", "```py\nIn [11]: df = pd.DataFrame(\n ....:    {'a': range(6)},\n ....:    index=pd.date_range(\"2021-01-01\", periods=6, freq=\"8H\")\n ....: )\n ....:\n\nIn [12]: df.resample(\"D\", group_keys=True).apply(lambda x: x)\nOut[12]:\n a\n2021-01-01 2021-01-01 00:00:00  0\n 2021-01-01 08:00:00  1\n 2021-01-01 16:00:00  2\n2021-01-02 2021-01-02 00:00:00  3\n 2021-01-02 08:00:00  4\n 2021-01-02 16:00:00  5\n\nIn [13]: df.resample(\"D\", group_keys=False).apply(lambda x: x)\nOut[13]:\n a\n2021-01-01 00:00:00  0\n2021-01-01 08:00:00  1\n2021-01-01 16:00:00  2\n2021-01-02 00:00:00  3\n2021-01-02 08:00:00  4\n2021-01-02 16:00:00  5 \n```", "```py\nIn [1]: # pandas 1.3\nIn [2]: df.resample(\"D\").apply(lambda x: x)\nOut[2]:\n a\n2021-01-01 00:00:00  0\n2021-01-01 08:00:00  1\n2021-01-01 16:00:00  2\n2021-01-02 00:00:00  3\n2021-01-02 08:00:00  4\n2021-01-02 16:00:00  5\n\nIn [3]: df.resample(\"D\").apply(lambda x: x.reset_index())\nOut[3]:\n index  a\n2021-01-01 0 2021-01-01 00:00:00  0\n 1 2021-01-01 08:00:00  1\n 2 2021-01-01 16:00:00  2\n2021-01-02 0 2021-01-02 00:00:00  3\n 1 2021-01-02 08:00:00  4\n 2 2021-01-02 16:00:00  5 \n```", "```py\nIn [11]: import pandas as pd\n\nIn [12]: df = pd.DataFrame({\"col1_a\": [1, 0, 1], \"col1_b\": [0, 1, 0],\n ....:                   \"col2_a\": [0, 1, 0], \"col2_b\": [1, 0, 0],\n ....:                   \"col2_c\": [0, 0, 1]})\n ....: \n\nIn [13]: pd.from_dummies(df, sep=\"_\")\nOut[13]: \n col1 col2\n0    a    b\n1    b    a\n2    a    c \n```", "```py\ndf = pd.DataFrame(data={\"col1\": [1, 2], \"col2\": [3, 4]})\ndf.to_orc(\"./out.orc\") \n```", "```py\ndf = pd.read_csv(\"./movement.tar.gz\")\n# ...\ndf.to_csv(\"./out.tar.gz\") \n```", "```py\ndf = pd.read_csv(some_file_obj, compression={\"method\": \"tar\", \"mode\": \"r:gz\"}) # noqa F821 \n```", "```py\nIn [14]: from io import StringIO\n\nIn [15]: xml_dates = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n ....: <data>\n ....:  <row>\n ....:    <shape>square</shape>\n ....:    <degrees>00360</degrees>\n ....:    <sides>4.0</sides>\n ....:    <date>2020-01-01</date>\n ....:   </row>\n ....:  <row>\n ....:    <shape>circle</shape>\n ....:    <degrees>00360</degrees>\n ....:    <sides/>\n ....:    <date>2021-01-01</date>\n ....:  </row>\n ....:  <row>\n ....:    <shape>triangle</shape>\n ....:    <degrees>00180</degrees>\n ....:    <sides>3.0</sides>\n ....:    <date>2022-01-01</date>\n ....:  </row>\n ....: </data>\"\"\"\n ....: \n\nIn [16]: df = pd.read_xml(\n ....:    StringIO(xml_dates),\n ....:    dtype={'sides': 'Int64'},\n ....:    converters={'degrees': str},\n ....:    parse_dates=['date']\n ....: )\n ....: \n\nIn [17]: df\nOut[17]: \n shape degrees  sides       date\n0    square   00360      4 2020-01-01\n1    circle   00360   <NA> 2021-01-01\n2  triangle   00180      3 2022-01-01\n\nIn [18]: df.dtypes\nOut[18]: \nshape              object\ndegrees            object\nsides               Int64\ndate       datetime64[ns]\ndtype: object \n```", "```py\nIn [1]: df = pd.read_xml(\n...      \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n...      iterparse = {\"page\": [\"title\", \"ns\", \"id\"]})\n...  )\ndf\nOut[2]:\n title   ns        id\n0                                       Gettysburg Address    0     21450\n1                                                Main Page    0     42950\n2                            Declaration by United Nations    0      8435\n3             Constitution of the United States of America    0      8435\n4                     Declaration of Independence (Israel)    0     17858\n...                                                    ...  ...       ...\n3578760               Page:Black cat 1897 07 v2 n10.pdf/17  104    219649\n3578761               Page:Black cat 1897 07 v2 n10.pdf/43  104    219649\n3578762               Page:Black cat 1897 07 v2 n10.pdf/44  104    219649\n3578763      The History of Tom Jones, a Foundling/Book IX    0  12084291\n3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450\n\n[3578765 rows x 3 columns] \n```", "```py\npd.set_option(\"mode.copy_on_write\", True)\npd.options.mode.copy_on_write = True \n```", "```py\nwith pd.option_context(\"mode.copy_on_write\", True):\n    ... \n```", "```py\nIn [19]: df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": 1})\n\nIn [20]: view = df[\"foo\"]\n\nIn [21]: view.iloc[0]\nOut[21]: 1\n\nIn [22]: df\nOut[22]: \n foo  bar\n0    1    1\n1    2    1\n2    3    1 \n```", "```py\nIn [23]: with pd.option_context(\"mode.copy_on_write\", True):\n ....:    df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": 1})\n ....:    view = df[\"foo\"]\n ....:    view.iloc[0]\n ....:    df\n ....: \n```", "```py\nIn [24]: df = pd.DataFrame({'a': [1, 1, np.nan], 'b': [2, 3, 4]}) \n```", "```py\nIn [3]: # Value in the last row should be np.nan\n df.groupby('a', dropna=True).transform('sum')\nOut[3]:\n b\n0  5\n1  5\n2  5\n\nIn [3]: # Should have one additional row with the value np.nan\n df.groupby('a', dropna=True).transform(lambda x: x.sum())\nOut[3]:\n b\n0  5\n1  5\n\nIn [3]: # The value in the last row is np.nan interpreted as an integer\n df.groupby('a', dropna=True).transform('ffill')\nOut[3]:\n b\n0                    2\n1                    3\n2 -9223372036854775808\n\nIn [3]: # Should have one additional row with the value np.nan\n df.groupby('a', dropna=True).transform(lambda x: x)\nOut[3]:\n b\n0  2\n1  3 \n```", "```py\nIn [25]: df.groupby('a', dropna=True).transform('sum')\nOut[25]: \n b\n0  5.0\n1  5.0\n2  NaN\n\nIn [26]: df.groupby('a', dropna=True).transform(lambda x: x.sum())\nOut[26]: \n b\n0  5.0\n1  5.0\n2  NaN\n\nIn [27]: df.groupby('a', dropna=True).transform('ffill')\nOut[27]: \n b\n0  2.0\n1  3.0\n2  NaN\n\nIn [28]: df.groupby('a', dropna=True).transform(lambda x: x)\nOut[28]: \n b\n0  2.0\n1  3.0\n2  NaN \n```", "```py\nIn [32]: index = pd.date_range(\n ....:    start='2020-12-28 00:00:00',\n ....:    end='2020-12-28 02:00:00',\n ....:    freq='1H',\n ....: )\n ....:\n\nIn [33]: a = pd.Series(\n ....:    data=range(3),\n ....:    index=index,\n ....: )\n ....:\n\nIn [4]: from io import StringIO\n\nIn [5]: a.to_json(date_format='iso')\nOut[5]: '{\"2020-12-28T00:00:00.000Z\":0,\"2020-12-28T01:00:00.000Z\":1,\"2020-12-28T02:00:00.000Z\":2}'\n\nIn [6]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ=\"series\").index == a.index\nOut[6]: array([False, False, False]) \n```", "```py\nIn [34]: from io import StringIO\n\nIn [35]: a.to_json(date_format='iso')\nOut[35]: '{\"2020-12-28T00:00:00.000Z\":0,\"2020-12-28T01:00:00.000Z\":1,\"2020-12-28T02:00:00.000Z\":2}'\n\n# Roundtripping now works\nIn [36]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ=\"series\").index == a.index\nOut[36]: array([ True,  True,  True]) \n```", "```py\nIn [6]: df = pd.DataFrame([\"a\", \"b\", \"c\"], dtype=\"category\").iloc[0:2]\nIn [7]: df\nOut[7]:\n 0\n0  a\n1  b \n```", "```py\nIn [8]: df.groupby(level=0, observed=True).value_counts()\nOut[8]:\n0  a    1\n1  b    1\ndtype: int64 \n```", "```py\nIn [9]: df.groupby(level=0, observed=True).value_counts()\nOut[9]:\n0  a    1\n1  a    0\n b    1\n0  b    0\n c    0\n1  c    0\ndtype: int64 \n```", "```py\nIn [24]: df = pd.DataFrame({'a': [1, 1, np.nan], 'b': [2, 3, 4]}) \n```", "```py\nIn [3]: # Value in the last row should be np.nan\n df.groupby('a', dropna=True).transform('sum')\nOut[3]:\n b\n0  5\n1  5\n2  5\n\nIn [3]: # Should have one additional row with the value np.nan\n df.groupby('a', dropna=True).transform(lambda x: x.sum())\nOut[3]:\n b\n0  5\n1  5\n\nIn [3]: # The value in the last row is np.nan interpreted as an integer\n df.groupby('a', dropna=True).transform('ffill')\nOut[3]:\n b\n0                    2\n1                    3\n2 -9223372036854775808\n\nIn [3]: # Should have one additional row with the value np.nan\n df.groupby('a', dropna=True).transform(lambda x: x)\nOut[3]:\n b\n0  2\n1  3 \n```", "```py\nIn [25]: df.groupby('a', dropna=True).transform('sum')\nOut[25]: \n b\n0  5.0\n1  5.0\n2  NaN\n\nIn [26]: df.groupby('a', dropna=True).transform(lambda x: x.sum())\nOut[26]: \n b\n0  5.0\n1  5.0\n2  NaN\n\nIn [27]: df.groupby('a', dropna=True).transform('ffill')\nOut[27]: \n b\n0  2.0\n1  3.0\n2  NaN\n\nIn [28]: df.groupby('a', dropna=True).transform(lambda x: x)\nOut[28]: \n b\n0  2.0\n1  3.0\n2  NaN \n```", "```py\nIn [32]: index = pd.date_range(\n ....:    start='2020-12-28 00:00:00',\n ....:    end='2020-12-28 02:00:00',\n ....:    freq='1H',\n ....: )\n ....:\n\nIn [33]: a = pd.Series(\n ....:    data=range(3),\n ....:    index=index,\n ....: )\n ....:\n\nIn [4]: from io import StringIO\n\nIn [5]: a.to_json(date_format='iso')\nOut[5]: '{\"2020-12-28T00:00:00.000Z\":0,\"2020-12-28T01:00:00.000Z\":1,\"2020-12-28T02:00:00.000Z\":2}'\n\nIn [6]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ=\"series\").index == a.index\nOut[6]: array([False, False, False]) \n```", "```py\nIn [34]: from io import StringIO\n\nIn [35]: a.to_json(date_format='iso')\nOut[35]: '{\"2020-12-28T00:00:00.000Z\":0,\"2020-12-28T01:00:00.000Z\":1,\"2020-12-28T02:00:00.000Z\":2}'\n\n# Roundtripping now works\nIn [36]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ=\"series\").index == a.index\nOut[36]: array([ True,  True,  True]) \n```", "```py\nIn [6]: df = pd.DataFrame([\"a\", \"b\", \"c\"], dtype=\"category\").iloc[0:2]\nIn [7]: df\nOut[7]:\n 0\n0  a\n1  b \n```", "```py\nIn [8]: df.groupby(level=0, observed=True).value_counts()\nOut[8]:\n0  a    1\n1  b    1\ndtype: int64 \n```", "```py\nIn [9]: df.groupby(level=0, observed=True).value_counts()\nOut[9]:\n0  a    1\n1  a    0\n b    1\n0  b    0\n c    0\n1  c    0\ndtype: int64 \n```", "```py\nIn [29]: ser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11]) \n```", "```py\nIn [3]: ser[2:4]\nOut[3]:\n5    3\n7    4\ndtype: int64 \n```", "```py\nIn [4]: ser.loc[2:4]\nOut[4]:\n2    1\n3    2\ndtype: int64 \n```", "```py\nIn [30]: df = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\n\nIn [31]: original_prices = df['price']\n\nIn [32]: new_prices = np.array([98, 99]) \n```", "```py\nIn [3]: df.iloc[:, 0] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: price, float: 64 \n```", "```py\nIn [3]: df.iloc[:, 0] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98.0\nbook2    99.0\nName: price, dtype: float64\nIn [5]: original_prices\nOut[5]:\nbook1    98.0\nbook2    99.0\nName: price, dtype: float64 \n```", "```py\nIn [3]: df[df.columns[0]] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: price, dtype: float64 \n```", "```py\nIn [3]: df_with_duplicated_cols = pd.concat([df, df], axis='columns')\nIn [3]: df_with_duplicated_cols.isetitem(0, new_prices)\nIn [4]: df_with_duplicated_cols.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: 0, dtype: float64 \n```", "```py\nIn [1]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [\"x\", \"y\"]})\n\nIn [2]: # Reading the next line without knowing the contents of df, one would\n # expect the result to contain the products for both columns a and b.\n df[[\"a\", \"b\"]].prod()\nOut[2]:\na    2\ndtype: int64 \n```", "```py\nIn [29]: ser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11]) \n```", "```py\nIn [3]: ser[2:4]\nOut[3]:\n5    3\n7    4\ndtype: int64 \n```", "```py\nIn [4]: ser.loc[2:4]\nOut[4]:\n2    1\n3    2\ndtype: int64 \n```", "```py\nIn [30]: df = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\n\nIn [31]: original_prices = df['price']\n\nIn [32]: new_prices = np.array([98, 99]) \n```", "```py\nIn [3]: df.iloc[:, 0] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: price, float: 64 \n```", "```py\nIn [3]: df.iloc[:, 0] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98.0\nbook2    99.0\nName: price, dtype: float64\nIn [5]: original_prices\nOut[5]:\nbook1    98.0\nbook2    99.0\nName: price, dtype: float64 \n```", "```py\nIn [3]: df[df.columns[0]] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: price, dtype: float64 \n```", "```py\nIn [3]: df_with_duplicated_cols = pd.concat([df, df], axis='columns')\nIn [3]: df_with_duplicated_cols.isetitem(0, new_prices)\nIn [4]: df_with_duplicated_cols.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: 0, dtype: float64 \n```", "```py\nIn [1]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [\"x\", \"y\"]})\n\nIn [2]: # Reading the next line without knowing the contents of df, one would\n # expect the result to contain the products for both columns a and b.\n df[[\"a\", \"b\"]].prod()\nOut[2]:\na    2\ndtype: int64 \n```"]