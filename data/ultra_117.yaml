- en: Machine Learning Best Practices and Tips for Model Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习最佳实践和模型训练技巧
- en: 原文：[`docs.ultralytics.com/guides/model-training-tips/`](https://docs.ultralytics.com/guides/model-training-tips/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`docs.ultralytics.com/guides/model-training-tips/`](https://docs.ultralytics.com/guides/model-training-tips/)
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: One of the most important steps when working on a computer vision project is
    model training. Before reaching this step, you need to define your goals and collect
    and annotate your data. After preprocessing the data to make sure it is clean
    and consistent, you can move on to training your model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行计算机视觉项目的模型训练时，最重要的一步之一是模型训练。在达到这一步之前，您需要明确您的目标，并收集和注释您的数据。在预处理数据以确保其干净一致后，您可以开始训练模型。
- en: So, what is model training? Model training is the process of teaching your model
    to recognize visual patterns and make predictions based on your data. It directly
    impacts the performance and accuracy of your application. In this guide, we'll
    cover best practices, optimization techniques, and troubleshooting tips to help
    you train your computer vision models effectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是模型训练？模型训练是教授您的模型识别视觉模式并基于数据进行预测的过程。它直接影响您的应用程序的性能和准确性。在本指南中，我们将介绍有效训练计算机视觉模型的最佳实践、优化技术和故障排除技巧。
- en: How to Train a Machine Learning Model
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何训练机器学习模型
- en: A computer vision model is trained by adjusting its internal parameters to minimize
    errors. Initially, the model is fed a large set of labeled images. It makes predictions
    about what is in these images, and the predictions are compared to the actual
    labels or contents to calculate errors. These errors show how far off the model's
    predictions are from the true values.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉模型通过调整其内部参数来最小化误差进行训练。最初，模型被提供一组大量带标签的图像。它对这些图像进行预测，预测结果与实际标签或内容进行比较，以计算误差。这些误差显示了模型预测与真实值之间的偏差有多大。
- en: During training, the model iteratively makes predictions, calculates errors,
    and updates its parameters through a process called backpropagation. In this process,
    the model adjusts its internal parameters (weights and biases) to reduce the errors.
    By repeating this cycle many times, the model gradually improves its accuracy.
    Over time, it learns to recognize complex patterns such as shapes, colors, and
    textures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型通过一种称为反向传播的过程，迭代地进行预测、计算误差，并更新其参数。在这个过程中，模型调整其内部参数（权重和偏差），以减少误差。通过多次重复这一周期，模型逐渐提高其准确性。随着时间的推移，它学会识别复杂的模式，如形状、颜色和纹理。
- en: '![What is Backpropagation?](img/420c5cde68b3bbab55e0e3fc50ff336a.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![什么是反向传播？](img/420c5cde68b3bbab55e0e3fc50ff336a.png)'
- en: This learning process makes it possible for the computer vision model to perform
    various tasks, including object detection, instance segmentation, and image classification.
    The ultimate goal is to create a model that can generalize its learning to new,
    unseen images so that it can accurately understand visual data in real-world applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这一学习过程使得计算机视觉模型能够执行各种任务，包括目标检测、实例分割和图像分类。最终目标是创建一个能够将学习泛化到新的、未见过的图像的模型，从而能够准确理解现实世界应用中的视觉数据。
- en: Now that we know what is happening behind the scenes when we train a model,
    let's look at points to consider when training a model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了在训练模型时发生了什么，让我们来看看在训练模型时需要考虑的要点。
- en: Training on Large Datasets
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在大型数据集上进行训练
- en: There are a few different aspects to think about when you are planning on using
    a large dataset to train a model. For example, you can adjust the batch size,
    control the GPU utilization, choose to use multiscale training, etc. Let's walk
    through each of these options in detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在计划使用大型数据集训练模型时，有几个不同的方面需要考虑。例如，可以调整批处理大小，控制GPU利用率，选择使用多尺度训练等。让我们详细介绍每个选项。
- en: Batch Size and GPU Utilization
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批处理大小和GPU利用率
- en: When training models on large datasets, efficiently utilizing your GPU is key.
    Batch size is an important factor. It is the number of data samples that a machine
    learning model processes in a single training iteration. Using the maximum batch
    size supported by your GPU, you can fully take advantage of its capabilities and
    reduce the time model training takes. However, you want to avoid running out of
    GPU memory. If you encounter memory errors, reduce the batch size incrementally
    until the model trains smoothly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据集上训练模型时，有效利用GPU至关重要。批量大小是一个重要因素。它是机器学习模型在单次训练迭代中处理的数据样本数量。利用GPU支持的最大批量大小，您可以充分发挥其性能，并减少模型训练所需的时间。然而，您要避免出现GPU内存不足的情况。如果遇到内存错误，可以逐步减少批量大小，直到模型能够平稳训练。
- en: With respect to YOLOv8, you can set the `batch_size` parameter in the training
    configuration to match your GPU capacity. Also, setting `batch=-1` in your training
    script will automatically determine the batch size that can be efficiently processed
    based on your device's capabilities. By fine-tuning the batch size, you can make
    the most of your GPU resources and improve the overall training process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 关于YOLOv8，您可以在训练配置中设置`batch_size`参数以匹配您的GPU容量。此外，在训练脚本中设置`batch=-1`将自动确定能够高效处理的批量大小，基于您设备的能力。通过微调批量大小，您可以充分利用GPU资源，并改进整体训练过程。
- en: Subset Training
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Subset Training
- en: Subset training is a smart strategy that involves training your model on a smaller
    set of data that represents the larger dataset. It can save time and resources,
    especially during initial model development and testing. If you are running short
    on time or experimenting with different model configurations, subset training
    is a good option.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Subset training是一种聪明的策略，它涉及在代表更大数据集的较小数据集上训练模型。这可以节省时间和资源，尤其是在初始模型开发和测试期间。如果时间紧张或者正在尝试不同的模型配置，子集训练是一个很好的选择。
- en: When it comes to YOLOv8, you can easily implement subset training by using the
    `fraction` parameter. This parameter lets you specify what fraction of your dataset
    to use for training. For example, setting `fraction=0.1` will train your model
    on 10% of the data. You can use this technique for quick iterations and tuning
    your model before committing to training a model using a full dataset. Subset
    training helps you make rapid progress and identify potential issues early on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在YOLOv8中，您可以通过使用`fraction`参数轻松实现子集训练。此参数允许您指定用于训练的数据集的分数。例如，设置`fraction=0.1`将在数据的10%上训练您的模型。您可以在进行完整数据集训练之前，使用这种技术进行快速迭代和调优模型。子集训练帮助您快速取得进展，并及早发现潜在问题。
- en: Multi-scale Training
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Multi-scale Training
- en: Multiscale training is a technique that improves your model's ability to generalize
    by training it on images of varying sizes. Your model can learn to detect objects
    at different scales and distances and become more robust.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Multi-scale training是一种通过训练具有不同尺寸图像的模型来改进其泛化能力的技术。您的模型可以学习检测不同尺度和距离的对象，并变得更加健壮。
- en: For example, when you train YOLOv8, you can enable multiscale training by setting
    the `scale` parameter. This parameter adjusts the size of training images by a
    specified factor, simulating objects at different distances. For example, setting
    `scale=0.5` will reduce the image size by half, while `scale=2.0` will double
    it. Configuring this parameter allows your model to experience a variety of image
    scales and improve its detection capabilities across different object sizes and
    scenarios.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在训练YOLOv8时，可以通过设置`scale`参数来启用多尺度训练。该参数通过指定因子调整训练图像的大小，模拟不同距离的对象。例如，设置`scale=0.5`将减小图像尺寸一半，而`scale=2.0`将使其加倍。配置此参数允许您的模型体验各种图像尺度，并改进其在不同对象大小和场景中的检测能力。
- en: Caching
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缓存
- en: Caching is an important technique to improve the efficiency of training machine
    learning models. By storing preprocessed images in memory, caching reduces the
    time the GPU spends waiting for data to be loaded from the disk. The model can
    continuously receive data without delays caused by disk I/O operations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是提高训练机器学习模型效率的重要技术。通过将预处理的图像存储在内存中，缓存减少了GPU等待从磁盘加载数据的时间。模型可以持续接收数据，而不会受到由磁盘I/O操作引起的延迟影响。
- en: 'Caching can be controlled when training YOLOv8 using the `cache` parameter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存可以通过使用`cache`参数在训练YOLOv8时进行控制：
- en: '*`cache=True`*: Stores dataset images in RAM, providing the fastest access
    speed but at the cost of increased memory usage.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`cache=True`*：将数据集图像存储在RAM中，提供最快的访问速度，但以增加内存使用为代价。'
- en: '*`cache=''disk''`*: Stores the images on disk, slower than RAM but faster than
    loading fresh data each time.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`cache=''disk''`*：将图像存储在磁盘上，比RAM慢但比每次加载新数据更快。'
- en: '*`cache=False`*: Disables caching, relying entirely on disk I/O, which is the
    slowest option.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`cache=False`*：禁用缓存，完全依赖磁盘I/O，这是最慢的选项。'
- en: Mixed Precision Training
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合精度训练
- en: Mixed precision training uses both 16-bit (FP16) and 32-bit (FP32) floating-point
    types. The strengths of both FP16 and FP32 are leveraged by using FP16 for faster
    computation and FP32 to maintain precision where needed. Most of the neural network's
    operations are done in FP16 to benefit from faster computation and lower memory
    usage. However, a master copy of the model's weights is kept in FP32 to ensure
    accuracy during the weight update steps. You can handle larger models or larger
    batch sizes within the same hardware constraints.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度训练同时使用16位（FP16）和32位（FP32）浮点类型。利用FP16进行更快的计算和FP32在需要时保持精度的优势。大多数神经网络操作都是在FP16中进行，以从更快的计算和较低的内存使用中获益。然而，模型权重的主副本保持在FP32中，以确保在权重更新步骤中的准确性。您可以在相同的硬件限制内处理更大的模型或更大的批量大小。
- en: '![Mixed Precision Training Overview](img/6af178f2fe756fe4041f92405d26630e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![混合精度训练概述](img/6af178f2fe756fe4041f92405d26630e.png)'
- en: To implement mixed precision training, you'll need to modify your training scripts
    and ensure your hardware (like GPUs) supports it. Many modern deep learning frameworks,
    such as Tensorflow, offer built-in support for mixed precision.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现混合精度训练，您需要修改您的训练脚本，并确保您的硬件（如GPU）支持它。许多现代深度学习框架，如Tensorflow，提供了混合精度的内置支持。
- en: Mixed precision training is straightforward when working with YOLOv8\. You can
    use the `amp` flag in your training configuration. Setting `amp=True` enables
    Automatic Mixed Precision (AMP) training. Mixed precision training is a simple
    yet effective way to optimize your model training process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用YOLOv8时，混合精度训练非常简单。您可以在训练配置中使用`amp`标志。设置`amp=True`启用自动混合精度（AMP）训练。混合精度训练是优化模型训练过程的一种简单而有效的方式。
- en: Pre-trained Weights
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练权重
- en: Using pretrained weights is a smart way to speed up your model's training process.
    Pretrained weights come from models already trained on large datasets, giving
    your model a head start. Transfer learning adapts pretrained models to new, related
    tasks. Fine-tuning a pre-trained model involves starting with these weights and
    then continuing training on your specific dataset. This method of training results
    in faster training times and often better performance because the model starts
    with a solid understanding of basic features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练权重是加快模型训练过程的聪明方式。预训练权重来自已经在大型数据集上训练过的模型，为您的模型提供了一个良好的起点。迁移学习适应预训练模型到新的相关任务。微调预训练模型涉及使用这些权重开始训练，然后在您特定的数据集上继续训练。这种训练方法能够实现更快的训练时间，通常也能获得更好的性能，因为模型从基本特征开始具有坚实的理解。
- en: The `pretrained` parameter makes transfer learning easy with YOLOv8\. Setting
    `pretrained=True` will use default pre-trained weights, or you can specify a path
    to a custom pre-trained model. Using pre-trained weights and transfer learning
    effectively boosts your model's capabilities and reduces training costs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`pretrained`参数使得在YOLOv8中进行迁移学习变得容易。设置`pretrained=True`将使用默认的预训练权重，或者您可以指定自定义预训练模型的路径。有效地利用预训练权重和迁移学习可以显著提升模型的能力并降低训练成本。'
- en: Other Techniques to Consider When Handling a Large Dataset
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在处理大型数据集时考虑的其他技术
- en: 'There are a couple of other techniques to consider when handling a large dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型数据集时，还有一些其他技术值得考虑：
- en: '**Learning Rate Schedulers**: Implementing learning rate schedulers dynamically
    adjusts the learning rate during training. A well-tuned learning rate can prevent
    the model from overshooting minima and improve stability. When training YOLOv8,
    the `lrf` parameter helps manage learning rate scheduling by setting the final
    learning rate as a fraction of the initial rate.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率调度器**：实现学习率调度器可以在训练过程中动态调整学习率。良好调节的学习率可以防止模型过度逼近极小值，并提高稳定性。在训练YOLOv8时，`lrf`参数通过将最终学习率设置为初始速率的一部分，有助于管理学习率调度。'
- en: '**Distributed Training**: For handling large datasets, distributed training
    can be a game-changer. You can reduce the training time by spreading the training
    workload across multiple GPUs or machines.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式训练**：对于处理大数据集，分布式训练可以改变游戏规则。你可以通过将训练工作负载分布到多个 GPU 或机器上来减少训练时间。'
- en: The Number of Epochs To Train For
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练的迭代次数选择
- en: When training a model, an epoch refers to one complete pass through the entire
    training dataset. During an epoch, the model processes each example in the training
    set once and updates its parameters based on the learning algorithm. Multiple
    epochs are usually needed to allow the model to learn and refine its parameters
    over time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，一个迭代周期指的是完整地通过整个训练数据集一次。在一个迭代周期内，模型处理训练集中的每个示例，并根据学习算法更新其参数。通常需要多个迭代周期来使模型随着时间的推移学习和优化其参数。
- en: A common question that comes up is how to determine the number of epochs to
    train the model for. A good starting point is 300 epochs. If the model overfits
    early, you can reduce the number of epochs. If overfitting does not occur after
    300 epochs, you can extend the training to 600, 1200, or more epochs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的问题是如何确定模型训练的迭代次数。一个好的起点是300个迭代周期。如果模型早期出现过拟合，可以减少迭代次数。如果在300个迭代周期后没有出现过拟合，可以将训练延长至600、1200或更多个迭代周期。
- en: However, the ideal number of epochs can vary based on your dataset's size and
    project goals. Larger datasets might require more epochs for the model to learn
    effectively, while smaller datasets might need fewer epochs to avoid overfitting.
    With respect to YOLOv8, you can set the `epochs` parameter in your training script.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，理想的迭代次数可以根据你的数据集大小和项目目标而变化。较大的数据集可能需要更多的迭代次数才能有效学习模型，而较小的数据集可能需要较少的迭代次数以避免过拟合。关于
    YOLOv8，你可以在训练脚本中设置`epochs`参数。
- en: Early Stopping
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早停
- en: Early stopping is a valuable technique for optimizing model training. By monitoring
    validation performance, you can halt training once the model stops improving.
    You can save computational resources and prevent overfitting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 早停技术是优化模型训练的一种有价值的技术。通过监控验证性能，可以在模型停止改进时停止训练。这样可以节省计算资源并防止过拟合。
- en: The process involves setting a patience parameter that determines how many epochs
    to wait for an improvement in validation metrics before stopping training. If
    the model's performance does not improve within these epochs, training is stopped
    to avoid wasting time and resources.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程涉及设置一个耐心参数，用于确定在停止训练之前等待验证指标改善的迭代周期数。如果模型在这些周期内的表现没有改善，就会停止训练，以避免浪费时间和资源。
- en: '![Early Stopping Overview](img/ecfd06cb65546cf9964874a8a983c402.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![早停概述](img/ecfd06cb65546cf9964874a8a983c402.png)'
- en: For YOLOv8, you can enable early stopping by setting the patience parameter
    in your training configuration. For example, `patience=5` means training will
    stop if there's no improvement in validation metrics for 5 consecutive epochs.
    Using this method ensures the training process remains efficient and achieves
    optimal performance without excessive computation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 YOLOv8，你可以通过在训练配置中设置耐心参数来启用早停技术。例如，`patience=5`表示如果连续5个迭代周期内验证指标没有改善，训练将会停止。使用这种方法可以确保训练过程保持高效，并在不过度计算的情况下实现最佳性能。
- en: Choosing Between Cloud and Local Training
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在云端和本地训练之间进行选择
- en: 'There are two options for training your model: cloud training and local training.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种选项可以用来训练你的模型：云端训练和本地训练。
- en: Cloud training offers scalability and powerful hardware and is ideal for handling
    large datasets and complex models. Platforms like Google Cloud, AWS, and Azure
    provide on-demand access to high-performance GPUs and TPUs, speeding up training
    times and enabling experiments with larger models. However, cloud training can
    be expensive, especially for long periods, and data transfer can add to costs
    and latency.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 云端训练提供了可扩展性和强大的硬件，非常适合处理大数据集和复杂模型。像谷歌云、AWS 和 Azure 这样的平台提供按需访问高性能 GPU 和 TPU，加快了训练时间，并能够进行更大模型的实验。然而，云端训练可能成本高昂，特别是在长时间内，并且数据传输可能会增加成本和延迟。
- en: Local training provides greater control and customization, letting you tailor
    your environment to specific needs and avoid ongoing cloud costs. It can be more
    economical for long-term projects, and since your data stays on-premises, it's
    more secure. However, local hardware may have resource limitations and require
    maintenance, which can lead to longer training times for large models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本地训练提供了更大的控制和定制性，让您可以根据特定需求定制环境，并避免持续的云成本。对于长期项目来说，可能更经济，而且由于数据留存在本地，更安全。然而，本地硬件可能存在资源限制并需要维护，这可能会导致大型模型训练时间较长。
- en: Selecting an Optimizer
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择优化器
- en: An optimizer is an algorithm that adjusts the weights of your neural network
    to minimize the loss function, which measures how well the model is performing.
    In simpler terms, the optimizer helps the model learn by tweaking its parameters
    to reduce errors. Choosing the right optimizer directly affects how quickly and
    accurately the model learns.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是一种调整神经网络权重以最小化损失函数的算法，损失函数衡量模型的性能。简单来说，优化器通过调整参数来减少错误来帮助模型学习。选择正确的优化器直接影响模型学习的速度和准确性。
- en: You can also fine-tune optimizer parameters to improve model performance. Adjusting
    the learning rate sets the size of the steps when updating parameters. For stability,
    you might start with a moderate learning rate and gradually decrease it over time
    to improve long-term learning. Additionally, setting the momentum determines how
    much influence past updates have on current updates. A common value for momentum
    is around 0.9\. It generally provides a good balance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以微调优化器参数以提高模型性能。调整学习率设置参数更新步骤的大小。为了稳定性，您可以从适度的学习率开始，并随着时间逐渐减小以改进长期学习。此外，设置动量确定过去更新对当前更新的影响程度。动量的常见值约为0.9，通常提供良好的平衡。
- en: Common Optimizers
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见优化器
- en: Different optimizers have various strengths and weaknesses. Let's take a glimpse
    at a few common optimizers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的优化器具有各自的优势和劣势。让我们简要看一下几种常见的优化器。
- en: '**SGD (Stochastic Gradient Descent)**:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降法（SGD - Stochastic Gradient Descent）**：'
- en: Updates model parameters using the gradient of the loss function with respect
    to the parameters.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用损失函数梯度更新模型参数。
- en: Simple and efficient but can be slow to converge and might get stuck in local
    minima.
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单高效但可能收敛速度较慢，并可能陷入局部最小值。
- en: '**Adam (Adaptive Moment Estimation)**:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam（自适应矩估计法 - Adaptive Moment Estimation）**：'
- en: Combines the benefits of both SGD with momentum and RMSProp.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合了随机梯度下降法（**SGD**）和动量法以及均方根传播（**RMSProp**）的优点。
- en: Adjusts the learning rate for each parameter based on estimates of the first
    and second moments of the gradients.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据梯度的一阶和二阶矩估计调整每个参数的学习率。
- en: Well-suited for noisy data and sparse gradients.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于嘈杂数据和稀疏梯度。
- en: Efficient and generally requires less tuning, making it a recommended optimizer
    for YOLOv8.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效且通常需要较少的调整，使其成为**YOLOv8**推荐的优化器。
- en: '**RMSProp (Root Mean Square Propagation)**:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根传播（RMSProp - Root Mean Square Propagation）**：'
- en: Adjusts the learning rate for each parameter by dividing the gradient by a running
    average of the magnitudes of recent gradients.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过损失函数梯度除以最近梯度幅值的运行平均值调整每个参数的学习率。
- en: Helps in handling the vanishing gradient problem and is effective for recurrent
    neural networks.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有助于处理梯度消失问题，对递归神经网络有效。
- en: For YOLOv8, the `optimizer` parameter lets you choose from various optimizers,
    including SGD, Adam, AdamW, NAdam, RAdam, and RMSProp, or you can set it to `auto`
    for automatic selection based on model configuration.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**YOLOv8**，`optimizer`参数允许您从多种优化器中选择，包括SGD、Adam、AdamW、NAdam、RAdam和RMSProp，或者根据模型配置设置为`auto`以进行自动选择。
- en: Connecting with the Community
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与社区连接
- en: Being part of a community of computer vision enthusiasts can help you solve
    problems and learn faster. Here are some ways to connect, get help, and share
    ideas.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 成为计算机视觉爱好者社区的一部分可以帮助您解决问题并更快地学习。以下是一些连接、获取帮助和分享想法的方法。
- en: Community Resources
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社区资源
- en: '**GitHub Issues:** Visit the [YOLOv8 GitHub repository](https://github.com/ultralytics/ultralytics/issues)
    and use the Issues tab to ask questions, report bugs, and suggest new features.
    The community and maintainers are very active and ready to help.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub Issues：** 访问[YOLOv8 GitHub 仓库](https://github.com/ultralytics/ultralytics/issues)，并使用
    Issues 标签提问、报告错误以及建议新功能。社区和维护者非常活跃并愿意提供帮助。'
- en: '**Ultralytics Discord Server:** Join the [Ultralytics Discord server](https://ultralytics.com/discord/)
    to chat with other users and developers, get support, and share your experiences.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ultralytics Discord服务器：**加入[Ultralytics Discord服务器](https://ultralytics.com/discord/)，与其他用户和开发者交流，获取支持并分享您的经验。'
- en: Official Documentation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 官方文档
- en: '**Ultralytics YOLOv8 Documentation:** Check out the official YOLOv8 documentation
    for detailed guides and helpful tips on various computer vision projects.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ultralytics YOLOv8文档：**查看官方YOLOv8文档，获取关于各种计算机视觉项目的详细指南和实用技巧。'
- en: Using these resources will help you solve challenges and stay up-to-date with
    the latest trends and practices in the computer vision community.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些资源将帮助您解决挑战，并保持与计算机视觉社区最新趋势和实践的同步。
- en: Key Takeaways
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 要点摘要
- en: Training computer vision models involves following good practices, optimizing
    your strategies, and solving problems as they arise. Techniques like adjusting
    batch sizes, mixed precision training, and starting with pre-trained weights can
    make your models work better and train faster. Methods like subset training and
    early stopping help you save time and resources. Staying connected with the community
    and keeping up with new trends will help you keep improving your model training
    skills.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练计算机视觉模型涉及遵循良好实践、优化策略以及解决问题。调整批次大小、混合精度训练和使用预训练权重等技术可以使模型工作更加高效和快速。子集训练和提前停止等方法有助于节省时间和资源。与社区保持联系并跟踪新趋势将有助于改进您的模型训练技能。
- en: FAQ
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见问题解答
- en: How can I improve GPU utilization when training a large dataset with Ultralytics
    YOLO?
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在使用Ultralytics YOLO训练大型数据集时，如何提高GPU利用率？
- en: To improve GPU utilization, set the `batch_size` parameter in your training
    configuration to the maximum size supported by your GPU. This ensures that you
    make full use of the GPU's capabilities, reducing training time. If you encounter
    memory errors, incrementally reduce the batch size until training runs smoothly.
    For YOLOv8, setting `batch=-1` in your training script will automatically determine
    the optimal batch size for efficient processing. For further information, refer
    to the training configuration.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高GPU利用率，请将训练配置中的`batch_size`参数设置为GPU支持的最大尺寸。这确保充分利用GPU的能力，缩短训练时间。如果遇到内存错误，请逐步减小批次大小，直到训练顺利进行。对于YOLOv8，将`batch=-1`设置在训练脚本中将自动确定高效处理的最佳批次大小。有关详细信息，请参阅训练配置。
- en: What is mixed precision training, and how do I enable it in YOLOv8?
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是混合精度训练，如何在YOLOv8中启用它？
- en: Mixed precision training utilizes both 16-bit (FP16) and 32-bit (FP32) floating-point
    types to balance computational speed and precision. This approach speeds up training
    and reduces memory usage without sacrificing model accuracy. To enable mixed precision
    training in YOLOv8, set the `amp` parameter to `True` in your training configuration.
    This activates Automatic Mixed Precision (AMP) training. For more details on this
    optimization technique, see the training configuration.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度训练利用16位（FP16）和32位（FP32）浮点类型平衡计算速度和精度。这种方法加快了训练速度，减少了内存使用，而不损失模型准确性。要在YOLOv8中启用混合精度训练，请在训练配置中将`amp`参数设置为`True`。这将激活自动混合精度（AMP）训练。有关此优化技术的更多详细信息，请参阅训练配置。
- en: How does multiscale training enhance YOLOv8 model performance?
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多尺度训练如何增强YOLOv8模型性能？
- en: Multiscale training enhances model performance by training on images of varying
    sizes, allowing the model to better generalize across different scales and distances.
    In YOLOv8, you can enable multiscale training by setting the `scale` parameter
    in the training configuration. For example, `scale=0.5` reduces the image size
    by half, while `scale=2.0` doubles it. This technique simulates objects at different
    distances, making the model more robust across various scenarios. For settings
    and more details, check out the training configuration.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度训练通过训练不同尺寸的图像来增强模型性能，使其能够更好地泛化不同的尺度和距离。在YOLOv8中，您可以通过在训练配置中设置`scale`参数来启用多尺度训练。例如，`scale=0.5`将图像尺寸减小一半，而`scale=2.0`则将其放大一倍。这种技术模拟不同距离处的对象，使模型在各种场景中更加健壮。有关设置和更多详细信息，请查阅训练配置。
- en: How can I use pre-trained weights to speed up training in YOLOv8?
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用预训练权重加速YOLOv8的训练？
- en: Using pre-trained weights can significantly reduce training times and improve
    model performance by starting from a model that already understands basic features.
    In YOLOv8, you can set the `pretrained` parameter to `True` or specify a path
    to custom pre-trained weights in your training configuration. This approach, known
    as transfer learning, leverages knowledge from large datasets to adapt to your
    specific task. Learn more about pre-trained weights and their advantages here.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练权重可以显著减少训练时间，并通过从已经理解基本特征的模型开始来提高模型性能。在YOLOv8中，您可以将`pretrained`参数设置为`True`，或在训练配置中指定自定义预训练权重的路径。这种称为迁移学习的方法利用大型数据集的知识来适应您的特定任务。在这里了解更多关于预训练权重及其优势的信息。
- en: What is the recommended number of epochs for training a model, and how do I
    set this in YOLOv8?
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型的推荐周期数是多少，我如何在YOLOv8中设置它？
- en: The number of epochs refers to the complete passes through the training dataset
    during model training. A typical starting point is 300 epochs. If your model overfits
    early, you can reduce the number. Alternatively, if overfitting isn't observed,
    you might extend training to 600, 1200, or more epochs. To set this in YOLOv8,
    use the `epochs` parameter in your training script. For additional advice on determining
    the ideal number of epochs, refer to this section on number of epochs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 周期数指的是模型训练过程中完全通过训练数据集的次数。一个典型的起始点是300个周期。如果您的模型早期出现过拟合，可以减少周期数。或者，如果没有观察到过拟合，可以将训练延长至600、1200或更多个周期。要在YOLOv8中设置这一参数，使用您的训练脚本中的`epochs`参数。有关确定理想周期数的额外建议，请参考关于周期数的这一部分。
