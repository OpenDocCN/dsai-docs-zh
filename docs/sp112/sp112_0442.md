# `scipy.optimize.basinhopping`

> 原文链接：[`docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.basinhopping.html#scipy.optimize.basinhopping`](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.basinhopping.html#scipy.optimize.basinhopping)

```py
scipy.optimize.basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None, seed=None, *, target_accept_rate=0.5, stepwise_factor=0.9)
```

使用盆地跳跃算法寻找函数的全局最小值。

盆地跳跃是一种两阶段方法，结合了全局步进算法和每步的局部最小化。设计用于模仿原子簇能量最小化的自然过程，适用于具有“漏斗形但崎岖”的能量景观的类似问题 [[5]](#r7bc5d3316b4a-5)。

由于步骤采取、步骤接受和最小化方法都是可定制的，因此该函数也可以用于实现其他两阶段方法。

参数：

**func**callable `f(x, *args)`

要优化的函数。`args` 可以作为字典 *minimizer_kwargs* 的可选项传递。

**x0**array_like

初始猜测。

**niter**整数，可选

盆地跳跃迭代次数。将有 `niter + 1` 次局部最小化运行。

**T**浮点数，可选

接受或拒绝标准的“温度”参数。较高的“温度”意味着将接受更大的函数值跳跃。为了获得最佳结果，*T* 应与局部最小值之间的分离（在函数值上）相当。

**stepsize**浮点数，可选

用于随机位移的最大步长。

**minimizer_kwargs**dict，可选

要传递给本地最小化器 `scipy.optimize.minimize` 的额外关键字参数。一些重要的选项可能包括：

> methodstr
> 
> 最小化方法（例如 `"L-BFGS-B"`）
> 
> argstuple
> 
> 传递给目标函数 (*func*) 及其导数（Jacobian、Hessian）的额外参数。

**take_step**callable `take_step(x)`，可选

用此例程替换默认的步进例程。默认的步进例程是坐标的随机位移，但其他步进算法可能对某些系统更好。*take_step* 可以选择具有属性 `take_step.stepsize`。如果存在此属性，则 `basinhopping` 将调整 `take_step.stepsize` 以尝试优化全局最小搜索。

**accept_test**callable，`accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)`，可选

定义一个测试，用于判断是否接受该步骤。这将与基于“温度”*T*的 Metropolis 测试一起使用。可以接受的返回值为 True、False 或`"force accept"`。如果任何测试返回 False，则拒绝该步骤。如果是后者，则这将覆盖任何其他测试以接受该步骤。例如，可以强制性地从`basinhopping`被困住的局部最小值中逃脱。

**callback**可调用对象，`callback(x, f, accept)`，可选

一个回调函数，将为找到的所有最小值调用。*x*和*f*是试探最小值的坐标和函数值，*accept*表示是否接受该最小值。例如，可以使用此功能保存找到的最低的 N 个最小值。此外，*callback*可以用于通过可选地返回 True 来指定用户定义的停止标准，以停止`basinhopping`程序运行。

**interval**整数，可选

用于定期更新*stepsize*的间隔

**disp**布尔值，可选

设置为 True 以打印状态消息

**niter_success**整数，可选

如果全局最小候选在此迭代次数内保持不变，则停止运行。

**seed**{None, 整数, [`numpy.random.Generator`](https://numpy.org/devdocs/reference/random/generator.html#numpy.random.Generator "(在 NumPy v2.0.dev0 中)"), [`numpy.random.RandomState`](https://numpy.org/devdocs/reference/random/legacy.html#numpy.random.RandomState "(在 NumPy v2.0.dev0 中")}，可选

如果*seed*为 None（或*np.random*），则使用[`numpy.random.RandomState`](https://numpy.org/devdocs/reference/random/legacy.html#numpy.random.RandomState "(在 NumPy v2.0.dev0 中)")单例。如果*seed*是整数，则使用一个新的`RandomState`实例，并使用*seed*进行种子化。如果*seed*已经是`Generator`或`RandomState`实例，则直接使用该实例。指定*seed*以进行可重复的最小化。使用此种子生成的随机数仅影响默认的 Metropolis *accept_test*和默认的*take_step*。如果您提供自己的*take_step*和*accept_test*，并且这些函数使用随机数生成，则这些函数负责其随机数生成器的状态。

**target_accept_rate**浮点数，可选

用于调整*stepsize*的目标接受率。如果当前接受率大于目标，则增加*stepsize*。否则，减少*stepsize*。范围为(0, 1)。默认值为 0.5。

自版本 1.8.0 新增。

**stepwise_factor**浮点数，可选

*stepsize*在每次更新时乘以或除以此步进因子。范围为(0, 1)。默认值为 0.9。

自版本 1.8.0 新增。

返回：

**res**OptimizeResult

优化结果表示为`OptimizeResult`对象。重要属性包括：`x` 解数组，`fun` 解处函数值，以及 `message` 描述终止原因。所选最小化器在最低最小值处返回的`OptimizeResult`对象也包含在此对象中，并可通过`lowest_optimization_result`属性访问。参见`OptimizeResult`获取其他属性的描述。

另见

`minimize`

局部最小化函数对每个 basinhopping 步骤调用。*minimizer_kwargs* 被传递给此例程。

注意事项

Basin-hopping 是一种随机算法，旨在找到一个或多个变量的光滑标量函数的全局最小值[[1]](#r7bc5d3316b4a-1) [[2]](#r7bc5d3316b4a-2) [[3]](#r7bc5d3316b4a-3) [[4]](#r7bc5d3316b4a-4)。该算法在目前的形式下由 David Wales 和 Jonathan Doye 描述[[2]](#r7bc5d3316b4a-2) [`www-wales.ch.cam.ac.uk/`](http://www-wales.ch.cam.ac.uk/)。

算法是迭代的，每个周期由以下特征组成

1.  坐标的随机扰动

1.  局部最小化

1.  基于最小化函数值接受或拒绝新坐标

此处使用的接受测试是标准 Monte Carlo 算法的 Metropolis 标准，尽管还有许多其他可能性[[3]](#r7bc5d3316b4a-3)。

已证明该全局最小化方法对物理和化学中的各种问题非常高效。当函数具有由大障碍物分隔的多个极小值时特别有用。请参见[Cambridge Cluster Database](https://www-wales.ch.cam.ac.uk/CCD.html)以获取主要使用 basin-hopping 优化的分子系统数据库。该数据库包括超过 300 个自由度的最小化问题。

有关 basin-hopping 的 Fortran 实现请参见自由软件程序[GMIN](https://www-wales.ch.cam.ac.uk/GMIN)。该实现包括上述过程的许多变体，包括更高级的步骤算法和替代接受标准。

对于随机全局优化，无法确定是否实际上找到了真正的全局最小值。作为一致性检查，可以从多个不同的随机起始点运行算法，以确保每个示例中找到的最低最小值已收敛到全局最小值。因此，默认情况下，`basinhopping` 将仅运行 *niter* 次迭代，并返回找到的最低最小值。用户需要确保这实际上是全局最小值。

选择 *stepsize*：这是 `basinhopping` 中的关键参数，取决于正在解决的问题。步长在每个维度中均匀选择，从 x0-stepsize 到 x0+stepsize 的区域内。理想情况下，它应与被优化函数的局部极小值之间（在参数值上的）典型分离可比较。`basinhopping` 将默认调整 *stepsize* 以找到最优值，但这可能需要多次迭代。如果设置一个合理的初始值给 `stepsize`，则可以更快地获得结果。

选择 *T*：参数 *T* 是 Metropolis 准则中使用的“温度”。如果 `func(xnew) < func(xold)`，则 Basin-hopping 步骤始终被接受。否则，它们将以以下概率被接受：

```py
exp( -(func(xnew) - func(xold)) / T ) 
```

因此，为了获得最佳结果，*T* 应与局部极小值之间（在函数值上的）典型差异可比较。（“墙”高度对局部极小值无关紧要。）

如果 *T* 为 0，则算法变为单调 Basin-Hopping，其中所有增加能量的步骤都被拒绝。

0.12.0 版本中的新内容。

参考文献

[1]

Wales, David J. 2003, 能量景观，剑桥大学出版社，剑桥，英国。

[2] (1,2)

Wales, D J 和 Doye J P K, Lennard-Jones 簇的基态结构的全局优化：通过 Basin-Hopping 和包含多达 110 个原子的结构。《物理化学学报》，1997 年，101，5111。

[3] (1,2)

Li, Z. 和 Scheraga, H. A., 蛋白质折叠中的多极小问题的蒙特卡洛-最小化方法，《美国国家科学院院刊》，1987 年，84，6611。

[4]

Wales, D. J. 和 Scheraga, H. A., 簇、晶体和生物分子的全局优化，《科学》，1999 年，285，1368。

[5]

Olson, B., Hashmi, I., Molloy, K., 和 Shehu1, A., Basin Hopping 作为生物大分子特征化的一般和多功能优化框架，《人工智能进展》，2012 年卷（2012），文章 ID 674832，[DOI:10.1155/2012/674832](https://doi.org/10.1155/2012/674832)

例子

下面的例子是一个一维最小化问题，在抛物线上叠加了许多局部极小值。

```py
>>> import numpy as np
>>> from scipy.optimize import basinhopping
>>> func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x
>>> x0 = [1.] 
```

盆地跳跃内部使用局部最小化算法。我们将使用参数*minimizer_kwargs*告诉盆地跳跃使用哪种算法以及如何设置该最小化器。此参数将传递给`scipy.optimize.minimize`。

```py
>>> minimizer_kwargs = {"method": "BFGS"}
>>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=200)
>>> print("global minimum: x = %.4f, f(x) = %.4f" % (ret.x, ret.fun))
global minimum: x = -0.1951, f(x) = -1.0009 
```

接下来考虑一个二维最小化问题。此外，这次我们将使用梯度信息来显著加速搜索。

```py
>>> def func2d(x):
...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +
...                                                            0.2) * x[0]
...     df = np.zeros(2)
...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2
...     df[1] = 2. * x[1] + 0.2
...     return f, df 
```

我们还将使用不同的局部最小化算法。此外，我们必须告诉最小化器，我们的函数同时返回能量和梯度（雅可比矩阵）。

```py
>>> minimizer_kwargs = {"method":"L-BFGS-B", "jac":True}
>>> x0 = [1.0, 1.0]
>>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=200)
>>> print("global minimum: x = [%.4f, %.4f], f(x) = %.4f" % (ret.x[0],
...                                                           ret.x[1],
...                                                           ret.fun))
global minimum: x = [-0.1951, -0.1000], f(x) = -1.0109 
```

下面是一个使用自定义步进例程的示例。想象一下，你希望第一个坐标采取比其他坐标更大的步骤。可以这样实现：

```py
>>> class MyTakeStep:
...    def __init__(self, stepsize=0.5):
...        self.stepsize = stepsize
...        self.rng = np.random.default_rng()
...    def __call__(self, x):
...        s = self.stepsize
...        x[0] += self.rng.uniform(-2.*s, 2.*s)
...        x[1:] += self.rng.uniform(-s, s, x[1:].shape)
...        return x 
```

由于`MyTakeStep.stepsize`存在，盆地跳跃将调整*stepsize*的大小以优化搜索。我们将使用与之前相同的二维函数。

```py
>>> mytakestep = MyTakeStep()
>>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=200, take_step=mytakestep)
>>> print("global minimum: x = [%.4f, %.4f], f(x) = %.4f" % (ret.x[0],
...                                                           ret.x[1],
...                                                           ret.fun))
global minimum: x = [-0.1951, -0.1000], f(x) = -1.0109 
```

现在，让我们使用一个自定义回调函数的示例，该函数打印出每个找到的最小值的值。

```py
>>> def print_fun(x, f, accepted):
...         print("at minimum %.4f accepted %d" % (f, int(accepted))) 
```

这次我们只运行 10 次盆地跳步。

```py
>>> rng = np.random.default_rng()
>>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=10, callback=print_fun, seed=rng)
at minimum 0.4159 accepted 1
at minimum -0.4317 accepted 1
at minimum -1.0109 accepted 1
at minimum -0.9073 accepted 1
at minimum -0.4317 accepted 0
at minimum -0.1021 accepted 1
at minimum -0.7425 accepted 1
at minimum -0.9073 accepted 1
at minimum -0.4317 accepted 0
at minimum -0.7425 accepted 1
at minimum -0.9073 accepted 1 
```

在第 8 次迭代中已经找到的最小值为-1.0109，实际上是全局最小值。
