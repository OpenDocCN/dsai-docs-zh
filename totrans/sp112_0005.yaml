- en: Optimization (scipy.optimize)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://docs.scipy.org/doc/scipy-1.12.0/tutorial/optimize.html](https://docs.scipy.org/doc/scipy-1.12.0/tutorial/optimize.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Contents
  prefs: []
  type: TYPE_NORMAL
- en: '[Optimization (](#optimization-scipy-optimize)[`scipy.optimize`](../reference/optimize.html#module-scipy.optimize
    "scipy.optimize"))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unconstrained minimization of multivariate scalar functions (](#unconstrained-minimization-of-multivariate-scalar-functions-minimize)[`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize"))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Nelder-Mead Simplex algorithm (`method=''Nelder-Mead''`)](#nelder-mead-simplex-algorithm-method-nelder-mead)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Broyden-Fletcher-Goldfarb-Shanno algorithm (`method=''BFGS''`)](#broyden-fletcher-goldfarb-shanno-algorithm-method-bfgs)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Avoiding Redundant Calculation](#avoiding-redundant-calculation)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Newton-Conjugate-Gradient algorithm (`method=''Newton-CG''`)](#newton-conjugate-gradient-algorithm-method-newton-cg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Full Hessian example:](#full-hessian-example)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hessian product example:](#hessian-product-example)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Trust-Region Newton-Conjugate-Gradient Algorithm (`method=''trust-ncg''`)](#trust-region-newton-conjugate-gradient-algorithm-method-trust-ncg)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Full Hessian example:](#id4)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hessian product example:](#id5)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm
    (`method=''trust-krylov''`)](#trust-region-truncated-generalized-lanczos-conjugate-gradient-algorithm-method-trust-krylov)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Full Hessian example:](#id8)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hessian product example:](#id9)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Trust-Region Nearly Exact Algorithm (`method=''trust-exact''`)](#trust-region-nearly-exact-algorithm-method-trust-exact)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Constrained minimization of multivariate scalar functions (](#constrained-minimization-of-multivariate-scalar-functions-minimize)[`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize"))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Trust-Region Constrained Algorithm (`method=''trust-constr''`)](#trust-region-constrained-algorithm-method-trust-constr)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Defining Bounds Constraints:](#defining-bounds-constraints)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Defining Linear Constraints:](#defining-linear-constraints)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Defining Nonlinear Constraints:](#defining-nonlinear-constraints)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Solving the Optimization Problem:](#solving-the-optimization-problem)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sequential Least SQuares Programming (SLSQP) Algorithm (`method=''SLSQP''`)](#sequential-least-squares-programming-slsqp-algorithm-method-slsqp)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Global optimization](#global-optimization)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Least-squares minimization (](#least-squares-minimization-least-squares)[`least_squares`](../reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares
    "scipy.optimize.least_squares"))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Example of solving a fitting problem](#example-of-solving-a-fitting-problem)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Further examples](#further-examples)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Univariate function minimizers (](#univariate-function-minimizers-minimize-scalar)[`minimize_scalar`](../reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar
    "scipy.optimize.minimize_scalar"))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unconstrained minimization (`method=''brent''`)](#unconstrained-minimization-method-brent)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bounded minimization (`method=''bounded''`)](#bounded-minimization-method-bounded)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Custom minimizers](#custom-minimizers)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Root finding](#root-finding)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scalar functions](#scalar-functions)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fixed-point solving](#fixed-point-solving)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sets of equations](#sets-of-equations)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Root finding for large problems](#root-finding-for-large-problems)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Still too slow? Preconditioning.](#still-too-slow-preconditioning)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear programming (](#linear-programming-linprog)[`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog"))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear programming example](#linear-programming-example)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Assignment problems](#assignment-problems)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear sum assignment problem example](#linear-sum-assignment-problem-example)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mixed integer linear programming](#mixed-integer-linear-programming)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Knapsack problem example](#knapsack-problem-example)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The [`scipy.optimize`](../reference/optimize.html#module-scipy.optimize "scipy.optimize")
    package provides several commonly used optimization algorithms. A detailed listing
    is available: [`scipy.optimize`](../reference/optimize.html#module-scipy.optimize
    "scipy.optimize") (can also be found by `help(scipy.optimize)`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Unconstrained minimization of multivariate scalar functions (](#id21)[`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize"))'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize") function provides a common interface to unconstrained
    and constrained minimization algorithms for multivariate scalar functions in [`scipy.optimize`](../reference/optimize.html#module-scipy.optimize
    "scipy.optimize"). To demonstrate the minimization function, consider the problem
    of minimizing the Rosenbrock function of \(N\) variables:'
  prefs: []
  type: TYPE_NORMAL
- en: \[f\left(\mathbf{x}\right)=\sum_{i=1}^{N-1}100\left(x_{i+1}-x_{i}^{2}\right)^{2}+\left(1-x_{i}\right)^{2}.\]
  prefs: []
  type: TYPE_NORMAL
- en: The minimum value of this function is 0 which is achieved when \(x_{i}=1.\)
  prefs: []
  type: TYPE_NORMAL
- en: Note that the Rosenbrock function and its derivatives are included in [`scipy.optimize`](../reference/optimize.html#module-scipy.optimize
    "scipy.optimize"). The implementations shown in the following sections provide
    examples of how to define an objective function as well as its jacobian and hessian
    functions. Objective functions in [`scipy.optimize`](../reference/optimize.html#module-scipy.optimize
    "scipy.optimize") expect a numpy array as their first parameter which is to be
    optimized and must return a float value. The exact calling signature must be `f(x,
    *args)` where `x` represents a numpy array and `args` a tuple of additional arguments
    supplied to the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: '[Nelder-Mead Simplex algorithm (`method=''Nelder-Mead''`)](#id22)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the example below, the [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize") routine is used with the *Nelder-Mead* simplex algorithm
    (selected through the `method` parameter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The simplex algorithm is probably the simplest way to minimize a fairly well-behaved
    function. It requires only function evaluations and is a good choice for simple
    minimization problems. However, because it does not use any gradient evaluations,
    it may take longer to find the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Another optimization algorithm that needs only function calls to find the minimum
    is *Powell*’s method available by setting `method='powell'` in [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize").
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how to supply additional arguments to an objective function,
    let us minimize the Rosenbrock function with an additional scaling factor *a*
    and an offset *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: \[f\left(\mathbf{x}, a, b\right)=\sum_{i=1}^{N-1}a\left(x_{i+1}-x_{i}^{2}\right)^{2}+\left(1-x_{i}\right)^{2}
    + b.\]
  prefs: []
  type: TYPE_NORMAL
- en: Again using the [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize") routine this can be solved by the following code block
    for the example parameters `a=0.5` and `b=1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As an alternative to using the `args` parameter of [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize"), simply wrap the objective function in a new function
    that accepts only `x`. This approach is also useful when it is necessary to pass
    additional parameters to the objective function as keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Another alternative is to use [`functools.partial`](https://docs.python.org/3/library/functools.html#functools.partial
    "(in Python v3.12)").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Broyden-Fletcher-Goldfarb-Shanno algorithm (`method=''BFGS''`)](#id23)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to converge more quickly to the solution, this routine uses the gradient
    of the objective function. If the gradient is not given by the user, then it is
    estimated using first-differences. The Broyden-Fletcher-Goldfarb-Shanno (BFGS)
    method typically requires fewer function calls than the simplex algorithm even
    when the gradient must be estimated.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this algorithm, the Rosenbrock function is again used. The gradient
    of the Rosenbrock function is the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{eqnarray*} \frac{\partial f}{\partial x_{j}} & = & \sum_{i=1}^{N}200\left(x_{i}-x_{i-1}^{2}\right)\left(\delta_{i,j}-2x_{i-1}\delta_{i-1,j}\right)-2\left(1-x_{i-1}\right)\delta_{i-1,j}.\\
    & = & 200\left(x_{j}-x_{j-1}^{2}\right)-400x_{j}\left(x_{j+1}-x_{j}^{2}\right)-2\left(1-x_{j}\right).\end{eqnarray*}
  prefs: []
  type: TYPE_NORMAL
- en: This expression is valid for the interior derivatives. Special cases are
  prefs: []
  type: TYPE_NORMAL
- en: \begin{eqnarray*} \frac{\partial f}{\partial x_{0}} & = & -400x_{0}\left(x_{1}-x_{0}^{2}\right)-2\left(1-x_{0}\right),\\
    \frac{\partial f}{\partial x_{N-1}} & = & 200\left(x_{N-1}-x_{N-2}^{2}\right).\end{eqnarray*}
  prefs: []
  type: TYPE_NORMAL
- en: 'A Python function which computes this gradient is constructed by the code-segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This gradient information is specified in the [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize") function through the `jac` parameter as illustrated
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Avoiding Redundant Calculation](#id24)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is common for the objective function and its gradient to share parts of the
    calculation. For instance, consider the following problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `expensive` is called 12 times: six times in the objective function and
    six times from the gradient. One way of reducing redundant calculations is to
    create a single function that returns both the objective function and the gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When we call minimize, we specify `jac==True` to indicate that the provided
    function returns both the objective function and its gradient. While convenient,
    not all [`scipy.optimize`](../reference/optimize.html#module-scipy.optimize "scipy.optimize")
    functions support this feature, and moreover, it is only for sharing calculations
    between the function and its gradient, whereas in some problems we will want to
    share calculations with the Hessian (second derivative of the objective function)
    and constraints. A more general approach is to memoize the expensive parts of
    the calculation. In simple situations, this can be accomplished with the [`functools.lru_cache`](https://docs.python.org/3/library/functools.html#functools.lru_cache
    "(in Python v3.12)") wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Newton-Conjugate-Gradient algorithm (`method=''Newton-CG''`)](#id25)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Newton-Conjugate Gradient algorithm is a modified Newton’s method and uses
    a conjugate gradient algorithm to (approximately) invert the local Hessian [[NW]](#nw).
    Newton’s method is based on fitting the function locally to a quadratic form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[f\left(\mathbf{x}\right)\approx f\left(\mathbf{x}_{0}\right)+\nabla f\left(\mathbf{x}_{0}\right)\cdot\left(\mathbf{x}-\mathbf{x}_{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{0}\right)^{T}\mathbf{H}\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right).\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{H}\left(\mathbf{x}_{0}\right)\) is a matrix of second-derivatives
    (the Hessian). If the Hessian is positive definite then the local minimum of this
    function can be found by setting the gradient of the quadratic form to zero, resulting
    in
  prefs: []
  type: TYPE_NORMAL
- en: \[\mathbf{x}_{\textrm{opt}}=\mathbf{x}_{0}-\mathbf{H}^{-1}\nabla f.\]
  prefs: []
  type: TYPE_NORMAL
- en: The inverse of the Hessian is evaluated using the conjugate-gradient method.
    An example of employing this method to minimizing the Rosenbrock function is given
    below. To take full advantage of the Newton-CG method, a function which computes
    the Hessian must be provided. The Hessian matrix itself does not need to be constructed,
    only a vector which is the product of the Hessian with an arbitrary vector needs
    to be available to the minimization routine. As a result, the user can provide
    either a function to compute the Hessian matrix, or a function to compute the
    product of the Hessian with an arbitrary vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[Full Hessian example:](#id26)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Hessian of the Rosenbrock function is
  prefs: []
  type: TYPE_NORMAL
- en: \begin{eqnarray*} H_{ij}=\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}
    & = & 200\left(\delta_{i,j}-2x_{i-1}\delta_{i-1,j}\right)-400x_{i}\left(\delta_{i+1,j}-2x_{i}\delta_{i,j}\right)-400\delta_{i,j}\left(x_{i+1}-x_{i}^{2}\right)+2\delta_{i,j},\\
    & = & \left(202+1200x_{i}^{2}-400x_{i+1}\right)\delta_{i,j}-400x_{i}\delta_{i+1,j}-400x_{i-1}\delta_{i-1,j},\end{eqnarray*}
  prefs: []
  type: TYPE_NORMAL
- en: if \(i,j\in\left[1,N-2\right]\) with \(i,j\in\left[0,N-1\right]\) defining the
    \(N\times N\) matrix. Other non-zero entries of the matrix are
  prefs: []
  type: TYPE_NORMAL
- en: \begin{eqnarray*} \frac{\partial^{2}f}{\partial x_{0}^{2}} & = & 1200x_{0}^{2}-400x_{1}+2,\\
    \frac{\partial^{2}f}{\partial x_{0}\partial x_{1}}=\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{0}} & = & -400x_{0},\\ \frac{\partial^{2}f}{\partial x_{N-1}\partial
    x_{N-2}}=\frac{\partial^{2}f}{\partial x_{N-2}\partial x_{N-1}} & = & -400x_{N-2},\\
    \frac{\partial^{2}f}{\partial x_{N-1}^{2}} & = & 200.\end{eqnarray*}
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Hessian when \(N=5\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}\mathbf{H}=\begin{bmatrix} 1200x_{0}^{2}+2\mkern-2em\\&1200x_{1}^{2}+202\mkern-2em\\&&1200x_{1}^{2}+202\mkern-2em\\&&&1200x_{3}^{2}+202\mkern-1em\\&&&&200\end{bmatrix}-400\begin{bmatrix}
    x_1 & x_0 \\ x_0 & x_2 & x_1 \\ & x_1 & x_3 & x_2\\ & & x_2 & x_4 & x_3 \\ & &
    & x_3 & 0\end{bmatrix}.\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The code which computes this Hessian along with the code to minimize the function
    using Newton-CG method is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[Hessian product example:](#id27)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For larger minimization problems, storing the entire Hessian matrix can consume
    considerable time and memory. The Newton-CG algorithm only needs the product of
    the Hessian times an arbitrary vector. As a result, the user can supply code to
    compute this product rather than the full Hessian by giving a `hess` function
    which take the minimization vector as the first argument and the arbitrary vector
    as the second argument (along with extra arguments passed to the function to be
    minimized). If possible, using Newton-CG with the Hessian product option is probably
    the fastest way to minimize the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the product of the Rosenbrock Hessian with an arbitrary vector
    is not difficult to compute. If \(\mathbf{p}\) is the arbitrary vector, then \(\mathbf{H}\left(\mathbf{x}\right)\mathbf{p}\)
    has elements:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}\mathbf{H}\left(\mathbf{x}\right)\mathbf{p}=\begin{bmatrix} \left(1200x_{0}^{2}-400x_{1}+2\right)p_{0}-400x_{0}p_{1}\\
    \vdots\\ -400x_{i-1}p_{i-1}+\left(202+1200x_{i}^{2}-400x_{i+1}\right)p_{i}-400x_{i}p_{i+1}\\
    \vdots\\ -400x_{N-2}p_{N-2}+200p_{N-1}\end{bmatrix}.\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Code which makes use of this Hessian product to minimize the Rosenbrock function
    using [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize") follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: According to [[NW]](#nw) p. 170 the `Newton-CG` algorithm can be inefficient
    when the Hessian is ill-conditioned because of the poor quality search directions
    provided by the method in those situations. The method `trust-ncg`, according
    to the authors, deals more effectively with this problematic situation and will
    be described next.
  prefs: []
  type: TYPE_NORMAL
- en: '[Trust-Region Newton-Conjugate-Gradient Algorithm (`method=''trust-ncg''`)](#id28)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Newton-CG` method is a line search method: it finds a direction of search
    minimizing a quadratic approximation of the function and then uses a line search
    algorithm to find the (nearly) optimal step size in that direction. An alternative
    approach is to, first, fix the step size limit \(\Delta\) and then find the optimal
    step \(\mathbf{p}\) inside the given trust-radius by solving the following quadratic
    subproblem:'
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{eqnarray*} \min_{\mathbf{p}} f\left(\mathbf{x}_{k}\right)+\nabla f\left(\mathbf{x}_{k}\right)\cdot\mathbf{p}+\frac{1}{2}\mathbf{p}^{T}\mathbf{H}\left(\mathbf{x}_{k}\right)\mathbf{p};&\\
    \text{subject to: } \|\mathbf{p}\|\le \Delta.& \end{eqnarray*}'
  prefs: []
  type: TYPE_NORMAL
- en: The solution is then updated \(\mathbf{x}_{k+1} = \mathbf{x}_{k} + \mathbf{p}\)
    and the trust-radius \(\Delta\) is adjusted according to the degree of agreement
    of the quadratic model with the real function. This family of methods is known
    as trust-region methods. The `trust-ncg` algorithm is a trust-region method that
    uses a conjugate gradient algorithm to solve the trust-region subproblem [[NW]](#nw).
  prefs: []
  type: TYPE_NORMAL
- en: '[Full Hessian example:](#id29)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[Hessian product example:](#id30)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm
    (`method=''trust-krylov''`)](#id31)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to the `trust-ncg` method, the `trust-krylov` method is a method suitable
    for large-scale problems as it uses the hessian only as linear operator by means
    of matrix-vector products. It solves the quadratic subproblem more accurately
    than the `trust-ncg` method.
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{eqnarray*} \min_{\mathbf{p}} f\left(\mathbf{x}_{k}\right)+\nabla f\left(\mathbf{x}_{k}\right)\cdot\mathbf{p}+\frac{1}{2}\mathbf{p}^{T}\mathbf{H}\left(\mathbf{x}_{k}\right)\mathbf{p};&\\
    \text{subject to: } \|\mathbf{p}\|\le \Delta.& \end{eqnarray*}'
  prefs: []
  type: TYPE_NORMAL
- en: This method wraps the [[TRLIB]](#trlib) implementation of the [[GLTR]](#gltr)
    method solving exactly a trust-region subproblem restricted to a truncated Krylov
    subspace. For indefinite problems it is usually better to use this method as it
    reduces the number of nonlinear iterations at the expense of few more matrix-vector
    products per subproblem solve in comparison to the `trust-ncg` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[Full Hessian example:](#id32)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[Hessian product example:](#id33)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[[TRLIB](#id6)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'F. Lenders, C. Kirches, A. Potschka: “trlib: A vector-free implementation of
    the GLTR method for iterative solution of the trust region problem”, [arXiv:1611.04718](https://arxiv.org/abs/1611.04718)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[GLTR](#id7)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'N. Gould, S. Lucidi, M. Roma, P. Toint: “Solving the Trust-Region Subproblem
    using the Lanczos Method”, SIAM J. Optim., 9(2), 504–525, (1999). [DOI:10.1137/S1052623497322735](https://doi.org/10.1137/S1052623497322735)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Trust-Region Nearly Exact Algorithm (`method=''trust-exact''`)](#id34)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All methods `Newton-CG`, `trust-ncg` and `trust-krylov` are suitable for dealing
    with large-scale problems (problems with thousands of variables). That is because
    the conjugate gradient algorithm approximately solve the trust-region subproblem
    (or invert the Hessian) by iterations without the explicit Hessian factorization.
    Since only the product of the Hessian with an arbitrary vector is needed, the
    algorithm is specially suited for dealing with sparse Hessians, allowing low storage
    requirements and significant time savings for those sparse problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For medium-size problems, for which the storage and factorization cost of the
    Hessian are not critical, it is possible to obtain a solution within fewer iteration
    by solving the trust-region subproblems almost exactly. To achieve that, a certain
    nonlinear equations is solved iteratively for each quadratic subproblem [[CGT]](#cgt).
    This solution requires usually 3 or 4 Cholesky factorizations of the Hessian matrix.
    As the result, the method converges in fewer number of iterations and takes fewer
    evaluations of the objective function than the other implemented trust-region
    methods. The Hessian product option is not supported by this algorithm. An example
    using the Rosenbrock function follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[NW] ([1](#id1),[2](#id2),[3](#id3))'
  prefs: []
  type: TYPE_NORMAL
- en: J. Nocedal, S.J. Wright “Numerical optimization.” 2nd edition. Springer Science
    (2006).
  prefs: []
  type: TYPE_NORMAL
- en: '[[CGT](#id10)]'
  prefs: []
  type: TYPE_NORMAL
- en: Conn, A. R., Gould, N. I., & Toint, P. L. “Trust region methods”. Siam. (2000).
    pp. 169-200.
  prefs: []
  type: TYPE_NORMAL
- en: '## [Constrained minimization of multivariate scalar functions (](#id35)[`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize"))'
  prefs: []
  type: TYPE_NORMAL
- en: The [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize") function provides algorithms for constrained minimization,
    namely `'trust-constr'` , `'SLSQP'` and `'COBYLA'`. They require the constraints
    to be defined using slightly different structures. The method `'trust-constr'`
    requires the constraints to be defined as a sequence of objects [`LinearConstraint`](../reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint
    "scipy.optimize.LinearConstraint") and [`NonlinearConstraint`](../reference/generated/scipy.optimize.NonlinearConstraint.html#scipy.optimize.NonlinearConstraint
    "scipy.optimize.NonlinearConstraint"). Methods `'SLSQP'` and `'COBYLA'`, on the
    other hand, require constraints to be defined as a sequence of dictionaries, with
    keys `type`, `fun` and `jac`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example let us consider the constrained minimization of the Rosenbrock
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{eqnarray*} \min_{x_0, x_1} & ~~100\left(x_{1}-x_{0}^{2}\right)^{2}+\left(1-x_{0}\right)^{2}
    &\\ \text{subject to: } & x_0 + 2 x_1 \leq 1 & \\ & x_0^2 + x_1 \leq 1 & \\ &
    x_0^2 - x_1 \leq 1 & \\ & 2 x_0 + x_1 = 1 & \\ & 0 \leq x_0 \leq 1 & \\ & -0.5
    \leq x_1 \leq 2.0\. & \end{eqnarray*}'
  prefs: []
  type: TYPE_NORMAL
- en: This optimization problem has the unique solution \([x_0, x_1] = [0.4149,~ 0.1701]\),
    for which only the first and fourth constraints are active.
  prefs: []
  type: TYPE_NORMAL
- en: '[Trust-Region Constrained Algorithm (`method=''trust-constr''`)](#id36)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The trust-region constrained method deals with constrained minimization problems
    of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{eqnarray*} \min_x & f(x) & \\ \text{subject to: } & ~~~ c^l \leq c(x)
    \leq c^u, &\\ & x^l \leq x \leq x^u. & \end{eqnarray*}'
  prefs: []
  type: TYPE_NORMAL
- en: When \(c^l_j = c^u_j\) the method reads the \(j\)-th constraint as an equality
    constraint and deals with it accordingly. Besides that, one-sided constraint can
    be specified by setting the upper or lower bound to `np.inf` with the appropriate
    sign.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation is based on [[EQSQP]](#eqsqp) for equality-constraint problems
    and on [[TRIP]](#trip) for problems with inequality constraints. Both are trust-region
    type algorithms suitable for large-scale problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[Defining Bounds Constraints:](#id37)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The bound constraints \(0 \leq x_0 \leq 1\) and \(-0.5 \leq x_1 \leq 2.0\) are
    defined using a [`Bounds`](../reference/generated/scipy.optimize.Bounds.html#scipy.optimize.Bounds
    "scipy.optimize.Bounds") object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[Defining Linear Constraints:](#id38)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The constraints \(x_0 + 2 x_1 \leq 1\) and \(2 x_0 + x_1 = 1\) can be written
    in the linear constraint standard format:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation*} \begin{bmatrix}-\infty \\1\end{bmatrix} \leq \begin{bmatrix}
    1& 2 \\ 2& 1\end{bmatrix} \begin{bmatrix} x_0 \\x_1\end{bmatrix} \leq \begin{bmatrix}
    1 \\ 1\end{bmatrix},\end{equation*}
  prefs: []
  type: TYPE_NORMAL
- en: and defined using a [`LinearConstraint`](../reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint
    "scipy.optimize.LinearConstraint") object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[Defining Nonlinear Constraints:](#id39)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The nonlinear constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation*} c(x) = \begin{bmatrix} x_0^2 + x_1 \\ x_0^2 - x_1\end{bmatrix}
    \leq \begin{bmatrix} 1 \\ 1\end{bmatrix}, \end{equation*}
  prefs: []
  type: TYPE_NORMAL
- en: 'with Jacobian matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation*} J(x) = \begin{bmatrix} 2x_0 & 1 \\ 2x_0 & -1\end{bmatrix},\end{equation*}
  prefs: []
  type: TYPE_NORMAL
- en: 'and linear combination of the Hessians:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation*} H(x, v) = \sum_{i=0}^1 v_i \nabla^2 c_i(x) = v_0\begin{bmatrix}
    2 & 0 \\ 0 & 0\end{bmatrix} + v_1\begin{bmatrix} 2 & 0 \\ 0 & 0\end{bmatrix},
    \end{equation*}
  prefs: []
  type: TYPE_NORMAL
- en: is defined using a [`NonlinearConstraint`](../reference/generated/scipy.optimize.NonlinearConstraint.html#scipy.optimize.NonlinearConstraint
    "scipy.optimize.NonlinearConstraint") object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, it is also possible to define the Hessian \(H(x, v)\) as a sparse
    matrix,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: or as a [`LinearOperator`](../reference/generated/scipy.sparse.linalg.LinearOperator.html#scipy.sparse.linalg.LinearOperator
    "scipy.sparse.linalg.LinearOperator") object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: When the evaluation of the Hessian \(H(x, v)\) is difficult to implement or
    computationally infeasible, one may use [`HessianUpdateStrategy`](../reference/generated/scipy.optimize.HessianUpdateStrategy.html#scipy.optimize.HessianUpdateStrategy
    "scipy.optimize.HessianUpdateStrategy"). Currently available strategies are [`BFGS`](../reference/generated/scipy.optimize.BFGS.html#scipy.optimize.BFGS
    "scipy.optimize.BFGS") and [`SR1`](../reference/generated/scipy.optimize.SR1.html#scipy.optimize.SR1
    "scipy.optimize.SR1").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, the Hessian may be approximated using finite differences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The Jacobian of the constraints can be approximated by finite differences as
    well. In this case, however, the Hessian cannot be computed with finite differences
    and needs to be provided by the user or defined using [`HessianUpdateStrategy`](../reference/generated/scipy.optimize.HessianUpdateStrategy.html#scipy.optimize.HessianUpdateStrategy
    "scipy.optimize.HessianUpdateStrategy").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[Solving the Optimization Problem:](#id40)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The optimization problem is solved using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: When needed, the objective function Hessian can be defined using a [`LinearOperator`](../reference/generated/scipy.sparse.linalg.LinearOperator.html#scipy.sparse.linalg.LinearOperator
    "scipy.sparse.linalg.LinearOperator") object,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: or a Hessian-vector product through the parameter `hessp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, the first and second derivatives of the objective function can
    be approximated. For instance, the Hessian can be approximated with [`SR1`](../reference/generated/scipy.optimize.SR1.html#scipy.optimize.SR1
    "scipy.optimize.SR1") quasi-Newton approximation and the gradient with finite
    differences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[[TRIP](#id12)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999. An interior point
    algorithm for large-scale nonlinear programming. SIAM Journal on Optimization
    9.4: 877-900.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[EQSQP](#id11)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998\. On the implementation
    of an algorithm for large-scale equality constrained optimization. SIAM Journal
    on Optimization 8.3: 682-706.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sequential Least SQuares Programming (SLSQP) Algorithm (`method=''SLSQP''`)](#id41)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The SLSQP method deals with constrained minimization problems of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '\begin{eqnarray*} \min_x & f(x) \\ \text{subject to: } & c_j(x) = 0 , &j \in
    \mathcal{E}\\ & c_j(x) \geq 0 , &j \in \mathcal{I}\\ & \text{lb}_i \leq x_i \leq
    \text{ub}_i , &i = 1,...,N. \end{eqnarray*}'
  prefs: []
  type: TYPE_NORMAL
- en: Where \(\mathcal{E}\) or \(\mathcal{I}\) are sets of indices containing equality
    and inequality constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Both linear and nonlinear constraints are defined as dictionaries with keys
    `type`, `fun` and `jac`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And the optimization problem is solved with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Most of the options available for the method `'trust-constr'` are not available
    for `'SLSQP'`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Global optimization](#id42)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Global optimization aims to find the global minimum of a function within given
    bounds, in the presence of potentially many local minima. Typically, global minimizers
    efficiently search the parameter space, while using a local minimizer (e.g., [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize")) under the hood. SciPy contains a number of good global
    optimizers. Here, we’ll use those on the same objective function, namely the (aptly
    named) `eggholder` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This function looks like an egg carton:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '!["A 3-D plot shown from a three-quarter view. The function is very noisy with
    dozens of valleys and peaks. There is no clear min or max discernible from this
    view and it''s not possible to see all the local peaks and valleys from this view."](../Images/f00875f602e0f9af927431623e84f19c.png)'
  prefs: []
  type: TYPE_IMG
- en: We now use the global optimizers to obtain the minimum and the function value
    at the minimum. We’ll store the results in a dictionary so we can compare different
    optimization results later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'All optimizers return an `OptimizeResult`, which in addition to the solution
    contains information on the number of function evaluations, whether the optimization
    was successful, and more. For brevity, we won’t show the full output of the other
    optimizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[`shgo`](../reference/generated/scipy.optimize.shgo.html#scipy.optimize.shgo
    "scipy.optimize.shgo") has a second method, which returns all local minima rather
    than only what it thinks is the global minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll now plot all found minima on a heatmap of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '!["This X-Y plot is a heatmap with the Z value denoted with the lowest points
    as black and the highest values as white. The image resembles a chess board rotated
    45 degrees but heavily smoothed. A red dot is located at many of the minima on
    the grid resulting from the SHGO optimizer. SHGO shows the global minima as a
    red X in the top right. A local minima found with dual annealing is a white circle
    marker in the top left. A different local minima found with basinhopping is a
    yellow marker in the top center. The code is plotting the differential evolution
    result as a cyan circle, but it is not visible on the plot. At a glance it''s
    not clear which of these valleys is the true global minima."](../Images/0809f14215273ae4249636368e116991.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Least-squares minimization (](#id43)[`least_squares`](../reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares
    "scipy.optimize.least_squares"))'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SciPy is capable of solving robustified bound-constrained nonlinear least-squares
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{align} &\min_\mathbf{x} \frac{1}{2} \sum_{i = 1}^m \rho\left(f_i(\mathbf{x})^2\right)
    \\ &\text{subject to }\mathbf{lb} \leq \mathbf{x} \leq \mathbf{ub} \end{align}
  prefs: []
  type: TYPE_NORMAL
- en: Here \(f_i(\mathbf{x})\) are smooth functions from \(\mathbb{R}^n\) to \(\mathbb{R}\),
    we refer to them as residuals. The purpose of a scalar-valued function \(\rho(\cdot)\)
    is to reduce the influence of outlier residuals and contribute to robustness of
    the solution, we refer to it as a loss function. A linear loss function gives
    a standard least-squares problem. Additionally, constraints in a form of lower
    and upper bounds on some of \(x_j\) are allowed.
  prefs: []
  type: TYPE_NORMAL
- en: All methods specific to least-squares minimization utilize a \(m \times n\)
    matrix of partial derivatives called Jacobian and defined as \(J_{ij} = \partial
    f_i / \partial x_j\). It is highly recommended to compute this matrix analytically
    and pass it to [`least_squares`](../reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares
    "scipy.optimize.least_squares"), otherwise, it will be estimated by finite differences,
    which takes a lot of additional time and can be very inaccurate in hard cases.
  prefs: []
  type: TYPE_NORMAL
- en: Function [`least_squares`](../reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares
    "scipy.optimize.least_squares") can be used for fitting a function \(\varphi(t;
    \mathbf{x})\) to empirical data \(\{(t_i, y_i), i = 0, \ldots, m-1\}\). To do
    this, one should simply precompute residuals as \(f_i(\mathbf{x}) = w_i (\varphi(t_i;
    \mathbf{x}) - y_i)\), where \(w_i\) are weights assigned to each observation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example of solving a fitting problem](#id44)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we consider an enzymatic reaction [[1]](#id15). There are 11 residuals
    defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[f_i(x) = \frac{x_0 (u_i^2 + u_i x_1)}{u_i^2 + u_i x_2 + x_3} - y_i, \quad
    i = 0, \ldots, 10,\]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(y_i\) are measurement values and \(u_i\) are values of the independent
    variable. The unknown vector of parameters is \(\mathbf{x} = (x_0, x_1, x_2, x_3)^T\).
    As was said previously, it is recommended to compute Jacobian matrix in a closed
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: \begin{align} &J_{i0} = \frac{\partial f_i}{\partial x_0} = \frac{u_i^2 + u_i
    x_1}{u_i^2 + u_i x_2 + x_3} \\ &J_{i1} = \frac{\partial f_i}{\partial x_1} = \frac{u_i
    x_0}{u_i^2 + u_i x_2 + x_3} \\ &J_{i2} = \frac{\partial f_i}{\partial x_2} = -\frac{x_0
    (u_i^2 + u_i x_1) u_i}{(u_i^2 + u_i x_2 + x_3)^2} \\ &J_{i3} = \frac{\partial
    f_i}{\partial x_3} = -\frac{x_0 (u_i^2 + u_i x_1)}{(u_i^2 + u_i x_2 + x_3)^2}
    \end{align}
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use the “hard” starting point defined in [[2]](#id16). To find
    a physically meaningful solution, avoid potential division by zero and assure
    convergence to the global minimum we impose constraints \(0 \leq x_j \leq 100,
    j = 0, 1, 2, 3\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code below implements least-squares estimation of \(\mathbf{x}\) and finally
    plots the original data and the fitted model function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '!["This code plots an X-Y time-series. The series starts in the lower left
    at (0, 0) and rapidly trends up to the maximum of 0.2 then flattens out. The fitted
    model is shown as a smooth orange trace and is well fit to the data."](../Images/d7351b0461faa8d2d74c1fec2f2ac2fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Further examples](#id45)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Three interactive examples below illustrate usage of [`least_squares`](../reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares
    "scipy.optimize.least_squares") in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: '[Large-scale bundle adjustment in scipy](https://scipy-cookbook.readthedocs.io/items/bundle_adjustment.html)
    demonstrates large-scale capabilities of [`least_squares`](../reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares
    "scipy.optimize.least_squares") and how to efficiently compute finite difference
    approximation of sparse Jacobian.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Robust nonlinear regression in scipy](https://scipy-cookbook.readthedocs.io/items/robust_regression.html)
    shows how to handle outliers with a robust loss function in a nonlinear regression.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Solving a discrete boundary-value problem in scipy](https://scipy-cookbook.readthedocs.io/items/discrete_bvp.html)
    examines how to solve a large system of equations and use bounds to achieve desired
    properties of the solution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the details about mathematical algorithms behind the implementation refer
    to documentation of [`least_squares`](../reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares
    "scipy.optimize.least_squares").
  prefs: []
  type: TYPE_NORMAL
- en: '[Univariate function minimizers (](#id46)[`minimize_scalar`](../reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar
    "scipy.optimize.minimize_scalar"))'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often only the minimum of an univariate function (i.e., a function that takes
    a scalar as input) is needed. In these circumstances, other optimization techniques
    have been developed that can work faster. These are accessible from the [`minimize_scalar`](../reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar
    "scipy.optimize.minimize_scalar") function, which proposes several algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '[Unconstrained minimization (`method=''brent''`)](#id47)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are, actually, two methods that can be used to minimize an univariate
    function: [`brent`](../reference/generated/scipy.optimize.brent.html#scipy.optimize.brent
    "scipy.optimize.brent") and [`golden`](../reference/generated/scipy.optimize.golden.html#scipy.optimize.golden
    "scipy.optimize.golden"), but [`golden`](../reference/generated/scipy.optimize.golden.html#scipy.optimize.golden
    "scipy.optimize.golden") is included only for academic purposes and should rarely
    be used. These can be respectively selected through the *method* parameter in
    [`minimize_scalar`](../reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar
    "scipy.optimize.minimize_scalar"). The [`brent`](../reference/generated/scipy.optimize.brent.html#scipy.optimize.brent
    "scipy.optimize.brent") method uses Brent’s algorithm for locating a minimum.
    Optimally, a bracket (the [`bracket`](../reference/generated/scipy.optimize.bracket.html#scipy.optimize.bracket
    "scipy.optimize.bracket") parameter) should be given which contains the minimum
    desired. A bracket is a triple \(\left( a, b, c \right)\) such that \(f \left(
    a \right) > f \left( b \right) < f \left( c \right)\) and \(a < b < c\) . If this
    is not given, then alternatively two starting points can be chosen and a bracket
    will be found from these points using a simple marching algorithm. If these two
    starting points are not provided, *0* and *1* will be used (this may not be the
    right choice for your function and result in an unexpected minimum being returned).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[Bounded minimization (`method=''bounded''`)](#id48)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very often, there are constraints that can be placed on the solution space before
    minimization occurs. The *bounded* method in [`minimize_scalar`](../reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar
    "scipy.optimize.minimize_scalar") is an example of a constrained minimization
    procedure that provides a rudimentary interval constraint for scalar functions.
    The interval constraint allows the minimization to occur only between two fixed
    endpoints, specified using the mandatory *bounds* parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to find the minimum of \(J_{1}\left( x \right)\) near \(x=5\)
    , [`minimize_scalar`](../reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar
    "scipy.optimize.minimize_scalar") can be called using the interval \(\left[ 4,
    7 \right]\) as a constraint. The result is \(x_{\textrm{min}}=5.3314\) :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[Custom minimizers](#id49)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, it may be useful to use a custom method as a (multivariate or univariate)
    minimizer, for example, when using some library wrappers of [`minimize`](../reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize
    "scipy.optimize.minimize") (e.g., [`basinhopping`](../reference/generated/scipy.optimize.basinhopping.html#scipy.optimize.basinhopping
    "scipy.optimize.basinhopping")).
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve that by, instead of passing a method name, passing a callable
    (either a function or an object implementing a *__call__* method) as the *method*
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider an (admittedly rather virtual) need to use a trivial custom
    multivariate minimization method that will just search the neighborhood in each
    dimension independently with a fixed step size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This will work just as well in case of univariate optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[Root finding](#id50)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Scalar functions](#id51)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If one has a single-variable equation, there are multiple different root finding
    algorithms that can be tried. Most of these algorithms require the endpoints of
    an interval in which a root is expected (because the function changes signs).
    In general, [`brentq`](../reference/generated/scipy.optimize.brentq.html#scipy.optimize.brentq
    "scipy.optimize.brentq") is the best choice, but the other methods may be useful
    in certain circumstances or for academic purposes. When a bracket is not available,
    but one or more derivatives are available, then [`newton`](../reference/generated/scipy.optimize.newton.html#scipy.optimize.newton
    "scipy.optimize.newton") (or `halley`, `secant`) may be applicable. This is especially
    the case if the function is defined on a subset of the complex plane, and the
    bracketing methods cannot be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[Fixed-point solving](#id52)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A problem closely related to finding the zeros of a function is the problem
    of finding a fixed point of a function. A fixed point of a function is the point
    at which evaluation of the function returns the point: \(g\left(x\right)=x.\)
    Clearly, the fixed point of \(g\) is the root of \(f\left(x\right)=g\left(x\right)-x.\)
    Equivalently, the root of \(f\) is the fixed point of \(g\left(x\right)=f\left(x\right)+x.\)
    The routine [`fixed_point`](../reference/generated/scipy.optimize.fixed_point.html#scipy.optimize.fixed_point
    "scipy.optimize.fixed_point") provides a simple iterative method using Aitkens
    sequence acceleration to estimate the fixed point of \(g\) given a starting point.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Sets of equations](#id53)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finding a root of a set of non-linear equations can be achieved using the [`root`](../reference/generated/scipy.optimize.root.html#scipy.optimize.root
    "scipy.optimize.root") function. Several methods are available, amongst which
    `hybr` (the default) and `lm`, which, respectively, use the hybrid method of Powell
    and the Levenberg-Marquardt method from MINPACK.
  prefs: []
  type: TYPE_NORMAL
- en: The following example considers the single-variable transcendental equation
  prefs: []
  type: TYPE_NORMAL
- en: \[x+2\cos\left(x\right)=0,\]
  prefs: []
  type: TYPE_NORMAL
- en: 'a root of which can be found as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Consider now a set of non-linear equations
  prefs: []
  type: TYPE_NORMAL
- en: \begin{eqnarray*} x_{0}\cos\left(x_{1}\right) & = & 4,\\ x_{0}x_{1}-x_{1} &
    = & 5. \end{eqnarray*}
  prefs: []
  type: TYPE_NORMAL
- en: We define the objective function so that it also returns the Jacobian and indicate
    this by setting the `jac` parameter to `True`. Also, the Levenberg-Marquardt solver
    is used here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[Root finding for large problems](#id54)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Methods `hybr` and `lm` in [`root`](../reference/generated/scipy.optimize.root.html#scipy.optimize.root
    "scipy.optimize.root") cannot deal with a very large number of variables (*N*),
    as they need to calculate and invert a dense *N x N* Jacobian matrix on every
    Newton step. This becomes rather inefficient when *N* grows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider, for instance, the following problem: we need to solve the following
    integrodifferential equation on the square \([0,1]\times[0,1]\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[(\partial_x^2 + \partial_y^2) P + 5 \left(\int_0^1\int_0^1\cosh(P)\,dx\,dy\right)^2
    = 0\]
  prefs: []
  type: TYPE_NORMAL
- en: with the boundary condition \(P(x,1) = 1\) on the upper edge and \(P=0\) elsewhere
    on the boundary of the square. This can be done by approximating the continuous
    function *P* by its values on a grid, \(P_{n,m}\approx{}P(n h, m h)\), with a
    small grid spacing *h*. The derivatives and integrals can then be approximated;
    for instance \(\partial_x^2 P(x,y)\approx{}(P(x+h,y) - 2 P(x,y) + P(x-h,y))/h^2\).
    The problem is then equivalent to finding the root of some function `residual(P)`,
    where `P` is a vector of length \(N_x N_y\).
  prefs: []
  type: TYPE_NORMAL
- en: Now, because \(N_x N_y\) can be large, methods `hybr` or `lm` in [`root`](../reference/generated/scipy.optimize.root.html#scipy.optimize.root
    "scipy.optimize.root") will take a long time to solve this problem. The solution
    can, however, be found using one of the large-scale solvers, for example `krylov`,
    `broyden2`, or `anderson`. These use what is known as the inexact Newton method,
    which instead of computing the Jacobian matrix exactly, forms an approximation
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem we have can now be solved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '!["This code generates a 2-D heatmap with Z values from 0 to 1\. The graph
    resembles a smooth, dark blue-green, U shape, with an open yellow top. The right,
    bottom, and left edges have a value near zero and the top has a value close to
    1\. The center of the solution space has a value close to 0.8."](../Images/400641c4415abf89b06b3df5e6b75895.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Still too slow? Preconditioning.](#id55)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When looking for the zero of the functions \(f_i({\bf x}) = 0\), *i = 1, 2,
    …, N*, the `krylov` solver spends most of the time inverting the Jacobian matrix,
  prefs: []
  type: TYPE_NORMAL
- en: \[J_{ij} = \frac{\partial f_i}{\partial x_j} .\]
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have an approximation for the inverse matrix \(M\approx{}J^{-1}\), you
    can use it for *preconditioning* the linear-inversion problem. The idea is that
    instead of solving \(J{\bf s}={\bf y}\) one solves \(MJ{\bf s}=M{\bf y}\): since
    matrix \(MJ\) is “closer” to the identity matrix than \(J\) is, the equation should
    be easier for the Krylov method to deal with.'
  prefs: []
  type: TYPE_NORMAL
- en: The matrix *M* can be passed to [`root`](../reference/generated/scipy.optimize.root.html#scipy.optimize.root
    "scipy.optimize.root") with method `krylov` as an option `options['jac_options']['inner_M']`.
    It can be a (sparse) matrix or a [`scipy.sparse.linalg.LinearOperator`](../reference/generated/scipy.sparse.linalg.LinearOperator.html#scipy.sparse.linalg.LinearOperator
    "scipy.sparse.linalg.LinearOperator") instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the problem in the previous section, we note that the function to solve
    consists of two parts: the first one is the application of the Laplace operator,
    \([\partial_x^2 + \partial_y^2] P\), and the second is the integral. We can actually
    easily compute the Jacobian corresponding to the Laplace operator part: we know
    that in 1-D'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}\partial_x^2 \approx \frac{1}{h_x^2} \begin{pmatrix} -2 & 1 &
    0 & 0 \cdots \\ 1 & -2 & 1 & 0 \cdots \\ 0 & 1 & -2 & 1 \cdots \\ \ldots \end{pmatrix}
    = h_x^{-2} L\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so that the whole 2-D operator is represented by
  prefs: []
  type: TYPE_NORMAL
- en: \[J_1 = \partial_x^2 + \partial_y^2 \simeq h_x^{-2} L \otimes I + h_y^{-2} I
    \otimes L\]
  prefs: []
  type: TYPE_NORMAL
- en: The matrix \(J_2\) of the Jacobian corresponding to the integral is more difficult
    to calculate, and since *all* of it entries are nonzero, it will be difficult
    to invert. \(J_1\) on the other hand is a relatively simple matrix, and can be
    inverted by [`scipy.sparse.linalg.splu`](../reference/generated/scipy.sparse.linalg.splu.html#scipy.sparse.linalg.splu
    "scipy.sparse.linalg.splu") (or the inverse can be approximated by [`scipy.sparse.linalg.spilu`](../reference/generated/scipy.sparse.linalg.spilu.html#scipy.sparse.linalg.spilu
    "scipy.sparse.linalg.spilu")). So we are content to take \(M\approx{}J_1^{-1}\)
    and hope for the best.
  prefs: []
  type: TYPE_NORMAL
- en: In the example below, we use the preconditioner \(M=J_1^{-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Resulting run, first without preconditioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'and then with preconditioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Using a preconditioner reduced the number of evaluations of the `residual` function
    by a factor of *4*. For problems where the residual is expensive to compute, good
    preconditioning can be crucial — it can even decide whether the problem is solvable
    in practice or not.
  prefs: []
  type: TYPE_NORMAL
- en: Preconditioning is an art, science, and industry. Here, we were lucky in making
    a simple choice that worked reasonably well, but there is a lot more depth to
    this topic than is shown here.
  prefs: []
  type: TYPE_NORMAL
- en: '[Linear programming (](#id56)[`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog"))'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The function [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog") can minimize a linear objective function subject to
    linear equality and inequality constraints. This kind of problem is well known
    as linear programming. Linear programming solves problems of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}\min_x \ & c^T x \\ \mbox{such that} \ & A_{ub} x \leq b_{ub},\\
    & A_{eq} x = b_{eq},\\ & l \leq x \leq u ,\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(x\) is a vector of decision variables; \(c\), \(b_{ub}\), \(b_{eq}\),
    \(l\), and \(u\) are vectors; and \(A_{ub}\) and \(A_{eq}\) are matrices.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will try to solve a typical linear programming problem
    using [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog").
  prefs: []
  type: TYPE_NORMAL
- en: '[Linear programming example](#id57)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider the following simple linear programming problem:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}\max_{x_1, x_2, x_3, x_4} \ & 29x_1 + 45x_2 \\ \mbox{such that}
    \ & x_1 -x_2 -3x_3 \leq 5\\ & 2x_1 -3x_2 -7x_3 + 3x_4 \geq 10\\ & 2x_1 + 8x_2
    + x_3 = 60\\ & 4x_1 + 4x_2 + x_4 = 60\\ & 0 \leq x_0\\ & 0 \leq x_1 \leq 5\\ &
    x_2 \leq 0.5\\ & -3 \leq x_3\\\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We need some mathematical manipulations to convert the target problem to the
    form accepted by [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog").
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let’s consider the objective function. We want to maximize the
    objective function, but [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog") can only accept a minimization problem. This is easily
    remedied by converting the maximize \(29x_1 + 45x_2\) to minimizing \(-29x_1 -45x_2\).
    Also, \(x_3, x_4\) are not shown in the objective function. That means the weights
    corresponding with \(x_3, x_4\) are zero. So, the objective function can be converted
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\min_{x_1, x_2, x_3, x_4} \ -29x_1 -45x_2 + 0x_3 + 0x_4\]
  prefs: []
  type: TYPE_NORMAL
- en: If we define the vector of decision variables \(x = [x_1, x_2, x_3, x_4]^T\),
    the objective weights vector \(c\) of [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog") in this problem should be
  prefs: []
  type: TYPE_NORMAL
- en: \[c = [-29, -45, 0, 0]^T\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s consider the two inequality constraints. The first one is a “less
    than” inequality, so it is already in the form accepted by [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog"). The second one is a “greater than” inequality, so we
    need to multiply both sides by \(-1\) to convert it to a “less than” inequality.
    Explicitly showing zero coefficients, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}x_1 -x_2 -3x_3 + 0x_4 &\leq 5\\ -2x_1 + 3x_2 + 7x_3 - 3x_4 &\leq
    -10\\\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'These equations can be converted to matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}A_{ub} x \leq b_{ub}\\\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation*} A_{ub} = \begin{bmatrix} 1 & -1 & -3 & 0 \\ -2 & 3 & 7 & -3
    \end{bmatrix} \end{equation*}\begin{equation*} b_{ub} = \begin{bmatrix} 5 \\ -10
    \end{bmatrix} \end{equation*}
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s consider the two equality constraints. Showing zero weights explicitly,
    these are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}2x_1 + 8x_2 + 1x_3 + 0x_4 &= 60\\ 4x_1 + 4x_2 + 0x_3 + 1x_4 &=
    60\\\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'These equations can be converted to matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split}A_{eq} x = b_{eq}\\\end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \begin{equation*} A_{eq} = \begin{bmatrix} 2 & 8 & 1 & 0 \\ 4 & 4 & 0 & 1 \end{bmatrix}
    \end{equation*}\begin{equation*} b_{eq} = \begin{bmatrix} 60 \\ 60 \end{bmatrix}
    \end{equation*}
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let’s consider the separate inequality constraints on individual decision
    variables, which are known as “box constraints” or “simple bounds”. These constraints
    can be applied using the bounds argument of [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog"). As noted in the [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog") documentation, the default value of bounds is `(0, None)`,
    meaning that the lower bound on each decision variable is 0, and the upper bound
    on each decision variable is infinity: all the decision variables are non-negative.
    Our bounds are different, so we will need to specify the lower and upper bound
    on each decision variable as a tuple and group these tuples into a list.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can solve the transformed problem using [`linprog`](../reference/generated/scipy.optimize.linprog.html#scipy.optimize.linprog
    "scipy.optimize.linprog").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The result states that our problem is infeasible, meaning that there is no
    solution vector that satisfies all the constraints. That doesn’t necessarily mean
    we did anything wrong; some problems truly are infeasible. Suppose, however, that
    we were to decide that our bound constraint on \(x_1\) was too tight and that
    it could be loosened to \(0 \leq x_1 \leq 6\). After adjusting our code `x1_bounds
    = (0, 6)` to reflect the change and executing it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The result shows the optimization was successful. We can check the objective
    value (`result.fun`) is same as \(c^Tx\):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also check that all constraints are satisfied within reasonable tolerances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[Assignment problems](#id58)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linear sum assignment problem example](#id59)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider the problem of selecting students for a swimming medley relay team.
    We have a table showing times for each swimming style of five students:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Student | backstroke | breaststroke | butterfly | freestyle |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A | 43.5 | 47.1 | 48.4 | 38.2 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 45.5 | 42.1 | 49.6 | 36.8 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 43.4 | 39.1 | 42.1 | 43.2 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 46.5 | 44.1 | 44.5 | 41.2 |'
  prefs: []
  type: TYPE_TB
- en: '| E | 46.3 | 47.8 | 50.4 | 37.2 |'
  prefs: []
  type: TYPE_TB
- en: We need to choose a student for each of the four swimming styles such that the
    total relay time is minimized. This is a typical linear sum assignment problem.
    We can use [`linear_sum_assignment`](../reference/generated/scipy.optimize.linear_sum_assignment.html#scipy.optimize.linear_sum_assignment
    "scipy.optimize.linear_sum_assignment") to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: The linear sum assignment problem is one of the most famous combinatorial optimization
    problems. Given a “cost matrix” \(C\), the problem is to choose
  prefs: []
  type: TYPE_NORMAL
- en: exactly one element from each row
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: without choosing more than one element from any column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: such that the sum of the chosen elements is minimized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, we need to assign each row to one column such that the sum of
    the corresponding entries is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, let \(X\) be a boolean matrix where \(X[i,j] = 1\) iff row \(i\) is
    assigned to column \(j\). Then the optimal assignment has cost
  prefs: []
  type: TYPE_NORMAL
- en: \[\min \sum_i \sum_j C_{i,j} X_{i,j}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to define the cost matrix. In this example, we want to assign
    each swimming style to a student. [`linear_sum_assignment`](../reference/generated/scipy.optimize.linear_sum_assignment.html#scipy.optimize.linear_sum_assignment
    "scipy.optimize.linear_sum_assignment") is able to assign each row of a cost matrix
    to a column. Therefore, to form the cost matrix, the table above needs to be transposed
    so that the rows correspond with swimming styles and the columns correspond with
    students:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We can solve the assignment problem with [`linear_sum_assignment`](../reference/generated/scipy.optimize.linear_sum_assignment.html#scipy.optimize.linear_sum_assignment
    "scipy.optimize.linear_sum_assignment"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The `row_ind` and `col_ind` are optimal assigned matrix indexes of the cost
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The optimal assignment is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The optimal total medley time is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this result is not the same as the sum of the minimum times for each
    swimming style:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: because student “C” is the best swimmer in both “breaststroke” and “butterfly”
    style. We cannot assign student “C” to both styles, so we assigned student C to
    the “breaststroke” style and D to the “butterfly” style to minimize the total
    time.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs: []
  type: TYPE_NORMAL
- en: 'Some further reading and related software, such as Newton-Krylov [[KK]](#kk),
    PETSc [[PP]](#pp), and PyAMG [[AMG]](#amg):'
  prefs: []
  type: TYPE_NORMAL
- en: '[[KK](#id17)]'
  prefs: []
  type: TYPE_NORMAL
- en: D.A. Knoll and D.E. Keyes, “Jacobian-free Newton-Krylov methods”, J. Comp. Phys.
    193, 357 (2004). [DOI:10.1016/j.jcp.2003.08.010](https://doi.org/10.1016/j.jcp.2003.08.010)
  prefs: []
  type: TYPE_NORMAL
- en: '[[PP](#id18)]'
  prefs: []
  type: TYPE_NORMAL
- en: PETSc [https://www.mcs.anl.gov/petsc/](https://www.mcs.anl.gov/petsc/) and its
    Python bindings [https://bitbucket.org/petsc/petsc4py/](https://bitbucket.org/petsc/petsc4py/)
  prefs: []
  type: TYPE_NORMAL
- en: '[[AMG](#id19)]'
  prefs: []
  type: TYPE_NORMAL
- en: PyAMG (algebraic multigrid preconditioners/solvers) [https://github.com/pyamg/pyamg/issues](https://github.com/pyamg/pyamg/issues)
  prefs: []
  type: TYPE_NORMAL
- en: '## [Mixed integer linear programming](#id60)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Knapsack problem example](#id61)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The knapsack problem is a well known combinatorial optimization problem. Given
    a set of items, each with a size and a value, the problem is to choose the items
    that maximize the total value under the condition that the total size is below
    a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, let
  prefs: []
  type: TYPE_NORMAL
- en: \(x_i\) be a boolean variable that indicates whether item \(i\) is included
    in the knapsack,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(n\) be the total number of items,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(v_i\) be the value of item \(i\),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(s_i\) be the size of item \(i\), and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(C\) be the capacity of the knapsack.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then the problem is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\max \sum_i^n v_{i} x_{i}\]\[\text{subject to} \sum_i^n s_{i} x_{i} \leq C,
    x_{i} \in {0, 1}\]
  prefs: []
  type: TYPE_NORMAL
- en: Although the objective function and inequality constraints are linear in the
    *decision variables* \(x_i\), this differs from a typical linear programming problem
    in that the decision variables can only assume integer values. Specifically, our
    decision variables can only be \(0\) or \(1\), so this is known as a *binary integer
    linear program* (BILP). Such a problem falls within the larger class of *mixed
    integer linear programs* (MILPs), which we we can solve with [`milp`](../reference/generated/scipy.optimize.milp.html#scipy.optimize.milp
    "scipy.optimize.milp").
  prefs: []
  type: TYPE_NORMAL
- en: In our example, there are 8 items to choose from, and the size and value of
    each is specified as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to constrain our eight decision variables to be binary. We do so by
    adding a [`Bounds`](../reference/generated/scipy.optimize.Bounds.html#scipy.optimize.Bounds
    "scipy.optimize.Bounds"): constraint to ensure that they lie between \(0\) and
    \(1\), and we apply “integrality” constraints to ensure that they are *either*
    \(0\) *or* \(1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The knapsack capacity constraint is specified using [`LinearConstraint`](../reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint
    "scipy.optimize.LinearConstraint").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: If we are following the usual rules of linear algebra, the input `A` should
    be a two-dimensional matrix, and the lower and upper bounds `lb` and `ub` should
    be one-dimensional vectors, but [`LinearConstraint`](../reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint
    "scipy.optimize.LinearConstraint") is forgiving as long as the inputs can be broadcast
    to consistent shapes.
  prefs: []
  type: TYPE_NORMAL
- en: Using the variables defined above, we can solve the knapsack problem using [`milp`](../reference/generated/scipy.optimize.milp.html#scipy.optimize.milp
    "scipy.optimize.milp"). Note that [`milp`](../reference/generated/scipy.optimize.milp.html#scipy.optimize.milp
    "scipy.optimize.milp") minimizes the objective function, but we want to maximize
    the total value, so we set *c* to be negative of the values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: This means that we should select the items 1, 2, 4, 5, 6 to optimize the total
    value under the size constraint. Note that this is different from we would have
    obtained had we solved the *linear programming relaxation* (without integrality
    constraints) and attempted to round the decision variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: If we were to round this solution up to `array([1., 1., 1., 1., 1., 1., 0.,
    0.])`, our knapsack would be over the capacity constraint, whereas if we were
    to round down to `array([1., 1., 1., 1., 0., 1., 0., 0.])`, we would have a sub-optimal
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more MILP tutorials, see the Jupyter notebooks on SciPy Cookbooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Compressed Sensing l1 program](https://nbviewer.org/github/scipy/scipy-cookbook/blob/main/ipython/LinearAndMixedIntegerLinearProgramming/compressed_sensing_milp_tutorial_1.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Compressed Sensing l0 program](https://nbviewer.org/github/scipy/scipy-cookbook/blob/main/ipython/LinearAndMixedIntegerLinearProgramming/compressed_sensing_milp_tutorial_2.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
