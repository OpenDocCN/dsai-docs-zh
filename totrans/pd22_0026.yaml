- en: IO tools (text, CSV, HDF5, …)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IO工具（文本，CSV，HDF5，…）
- en: 原文：[https://pandas.pydata.org/docs/user_guide/io.html](https://pandas.pydata.org/docs/user_guide/io.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://pandas.pydata.org/docs/user_guide/io.html](https://pandas.pydata.org/docs/user_guide/io.html)
- en: The pandas I/O API is a set of top level `reader` functions accessed like [`pandas.read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") that generally return a pandas object. The corresponding `writer`
    functions are object methods that are accessed like [`DataFrame.to_csv()`](../reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv
    "pandas.DataFrame.to_csv"). Below is a table containing available `readers` and
    `writers`.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: pandas I/O API是一组顶级`reader`函数，如[`pandas.read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")通常返回一个pandas对象。相应的`writer`函数是对象方法，如[`DataFrame.to_csv()`](../reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv
    "pandas.DataFrame.to_csv")。下面是包含可用`reader`和`writer`的表格。
- en: '| Format Type | Data Description | Reader | Writer |'
  id: totrans-3
  prefs: []
  type: TYPE_TB
  zh: '| 格式类型 | 数据描述 | 读取器 | 写入器 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-4
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| text | [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) | [read_csv](#io-read-csv-table)
    | [to_csv](#io-store-in-csv) |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) | [read_csv](#io-read-csv-table)
    | [to_csv](#io-store-in-csv) |'
- en: '| text | Fixed-Width Text File | [read_fwf](#io-fwf-reader) |  |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 定宽文本文件 | [read_fwf](#io-fwf-reader) |  |'
- en: '| text | [JSON](https://www.json.org/) | [read_json](#io-json-reader) | [to_json](#io-json-writer)
    |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | [JSON](https://www.json.org/) | [read_json](#io-json-reader) | [to_json](#io-json-writer)
    |'
- en: '| text | [HTML](https://en.wikipedia.org/wiki/HTML) | [read_html](#io-read-html)
    | [to_html](#io-html) |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | [HTML](https://en.wikipedia.org/wiki/HTML) | [read_html](#io-read-html)
    | [to_html](#io-html) |'
- en: '| text | [LaTeX](https://en.wikipedia.org/wiki/LaTeX) |  | [Styler.to_latex](#io-latex)
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | [LaTeX](https://en.wikipedia.org/wiki/LaTeX) |  | [Styler.to_latex](#io-latex)
    |'
- en: '| text | [XML](https://www.w3.org/standards/xml/core) | [read_xml](#io-read-xml)
    | [to_xml](#io-xml) |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | [XML](https://www.w3.org/standards/xml/core) | [read_xml](#io-read-xml)
    | [to_xml](#io-xml) |'
- en: '| text | Local clipboard | [read_clipboard](#io-clipboard) | [to_clipboard](#io-clipboard)
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 本地剪贴板 | [read_clipboard](#io-clipboard) | [to_clipboard](#io-clipboard)
    |'
- en: '| binary | [MS Excel](https://en.wikipedia.org/wiki/Microsoft_Excel) | [read_excel](#io-excel-reader)
    | [to_excel](#io-excel-writer) |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [MS Excel](https://en.wikipedia.org/wiki/Microsoft_Excel) | [read_excel](#io-excel-reader)
    | [to_excel](#io-excel-writer) |'
- en: '| binary | [OpenDocument](http://opendocumentformat.org) | [read_excel](#io-ods)
    |  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [OpenDocument](http://opendocumentformat.org) | [read_excel](#io-ods)
    |  |'
- en: '| binary | [HDF5 Format](https://support.hdfgroup.org/HDF5/whatishdf5.html)
    | [read_hdf](#io-hdf5) | [to_hdf](#io-hdf5) |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [HDF5 格式](https://support.hdfgroup.org/HDF5/whatishdf5.html) | [read_hdf](#io-hdf5)
    | [to_hdf](#io-hdf5) |'
- en: '| binary | [Feather Format](https://github.com/wesm/feather) | [read_feather](#io-feather)
    | [to_feather](#io-feather) |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [Feather 格式](https://github.com/wesm/feather) | [read_feather](#io-feather)
    | [to_feather](#io-feather) |'
- en: '| binary | [Parquet Format](https://parquet.apache.org/) | [read_parquet](#io-parquet)
    | [to_parquet](#io-parquet) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [Parquet 格式](https://parquet.apache.org/) | [read_parquet](#io-parquet)
    | [to_parquet](#io-parquet) |'
- en: '| binary | [ORC Format](https://orc.apache.org/) | [read_orc](#io-orc) | [to_orc](#io-orc)
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [ORC 格式](https://orc.apache.org/) | [read_orc](#io-orc) | [to_orc](#io-orc)
    |'
- en: '| binary | [Stata](https://en.wikipedia.org/wiki/Stata) | [read_stata](#io-stata-reader)
    | [to_stata](#io-stata-writer) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [Stata](https://en.wikipedia.org/wiki/Stata) | [read_stata](#io-stata-reader)
    | [to_stata](#io-stata-writer) |'
- en: '| binary | [SAS](https://en.wikipedia.org/wiki/SAS_(software)) | [read_sas](#io-sas-reader)
    |  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [SAS](https://en.wikipedia.org/wiki/SAS_(software)) | [read_sas](#io-sas-reader)
    |  |'
- en: '| binary | [SPSS](https://en.wikipedia.org/wiki/SPSS) | [read_spss](#io-spss-reader)
    |  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [SPSS](https://en.wikipedia.org/wiki/SPSS) | [read_spss](#io-spss-reader)
    |  |'
- en: '| binary | [Python Pickle Format](https://docs.python.org/3/library/pickle.html)
    | [read_pickle](#io-pickle) | [to_pickle](#io-pickle) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 二进制 | [Python Pickle 格式](https://docs.python.org/3/library/pickle.html) |
    [read_pickle](#io-pickle) | [to_pickle](#io-pickle) |'
- en: '| SQL | [SQL](https://en.wikipedia.org/wiki/SQL) | [read_sql](#io-sql) | [to_sql](#io-sql)
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| SQL | [SQL](https://en.wikipedia.org/wiki/SQL) | [read_sql](#io-sql) | [to_sql](#io-sql)
    |'
- en: '| SQL | [Google BigQuery](https://en.wikipedia.org/wiki/BigQuery) | [read_gbq](#io-bigquery)
    | [to_gbq](#io-bigquery) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| SQL | [Google BigQuery](https://en.wikipedia.org/wiki/BigQuery) | [read_gbq](#io-bigquery)
    | [to_gbq](#io-bigquery) |'
- en: '[Here](#io-perf) is an informal performance comparison for some of these IO
    methods.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里](#io-perf)是一些IO方法的非正式性能比较。'
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For examples that use the `StringIO` class, make sure you import it with `from
    io import StringIO` for Python 3.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用`StringIO`类的示例，请确保在Python 3中导入它时使用`from io import StringIO`。
- en: '## CSV & text files'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '## CSV & 文本文件'
- en: The workhorse function for reading text files (a.k.a. flat files) is [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"). See the [cookbook](cookbook.html#cookbook-csv) for some advanced
    strategies.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 用于读取文本文件（也称为平面文件）的主要函数是 [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")。查看[食谱](cookbook.html#cookbook-csv)以获取一些高级策略。
- en: Parsing options
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析选项
- en: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    accepts the following common arguments:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    接受以下常见参数：'
- en: Basic
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本
- en: filepath_or_buffervarious
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: filepath_or_buffervarious
- en: Either a path to a file (a [`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)"), or `py:py._path.local.LocalPath`), URL (including http,
    ftp, and S3 locations), or any object with a `read()` method (such as an open
    file or [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO "(in
    Python v3.12)")).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要么是文件的路径（[`str`](https://docs.python.org/3/library/stdtypes.html#str "(在 Python
    v3.12 中)")，[`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(在 Python v3.12 中)")，或 `py:py._path.local.LocalPath`），URL（包括 http、ftp 和 S3 地址），或具有
    `read()` 方法的任何对象（例如打开的文件或 [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO
    "(在 Python v3.12 中)")）。
- en: sepstr, defaults to `','` for [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), `\t` for [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: sepstr，默认为 [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") 的 `','`，[`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table") 的 `\t`
- en: 'Delimiter to use. If sep is `None`, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will be used
    and automatically detect the separator by Python’s builtin sniffer tool, [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)"). In addition, separators longer than 1 character and different
    from `''\s+''` will be interpreted as regular expressions and will also force
    the use of the Python parsing engine. Note that regex delimiters are prone to
    ignoring quoted data. Regex example: `''\\r\\t''`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的分隔符。如果 sep 为 `None`，则 C 引擎无法自动检测分隔符，但 Python 解析引擎可以，这意味着将使用后者，并通过 Python
    的内置嗅探工具 [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(在 Python v3.12 中)") 自动检测分隔符。此外，长度大于 1 且不同于 `'\s+'` 的分隔符将被解释为正则表达式，并且还将强制使用 Python
    解析引擎。请注意，正则表达式分隔符容易忽略带引号的数据。正则表达式示例：`'\\r\\t'`。
- en: delimiterstr, default `None`
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: delimiterstr，默认为 `None`
- en: Alternative argument name for sep.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: sep 的替代参数名称。
- en: delim_whitespaceboolean, default False
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: delim_whitespaceboolean，默认为 False
- en: Specifies whether or not whitespace (e.g. `' '` or `'\t'`) will be used as the
    delimiter. Equivalent to setting `sep='\s+'`. If this option is set to `True`,
    nothing should be passed in for the `delimiter` parameter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 指定是否使用空格（例如 `' '` 或 `'\t'`）作为分隔符。等同于设置 `sep='\s+'`。如果此选项设置为 `True`，则不应为 `delimiter`
    参数传递任何内容。
- en: Column and index locations and names
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 列和索引位置及名称
- en: headerint or list of ints, default `'infer'`
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: headerint 或整数列表，默认为 `'infer'`
- en: 'Row number(s) to use as the column names, and the start of the data. Default
    behavior is to infer the column names: if no names are passed the behavior is
    identical to `header=0` and column names are inferred from the first line of the
    file, if column names are passed explicitly then the behavior is identical to
    `header=None`. Explicitly pass `header=0` to be able to replace existing names.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 用作列名和数据起始位置的行号。默认行为是推断列名：如果没有传递名称，则行为与 `header=0` 相同，并且列名从文件的第一行推断出来，如果显式传递列名，则行为与
    `header=None` 相同。显式传递 `header=0` 以能够替换现有名称。
- en: The header can be a list of ints that specify row locations for a MultiIndex
    on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this parameter ignores
    commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes
    the first line of data rather than the first line of the file.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 头部可以是指定列的 MultiIndex 的行位置的整数列表，例如 `[0,1,3]`。未指定的中间行将被跳过（例如在此示例中跳过了 2）。请注意，如果
    `skip_blank_lines=True`，此参数将忽略注释行和空行，因此 `header=0` 表示数据的第一行而不是文件的第一行。
- en: namesarray-like, default `None`
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: namesarray-like，默认为 `None`
- en: List of column names to use. If file contains no header row, then you should
    explicitly pass `header=None`. Duplicates in this list are not allowed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的列名列表。如果文件不包含标题行，则应明确传递`header=None`。此列表中不允许重复项。
- en: index_colint, str, sequence of int / str, or False, optional, default `None`
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: index_colint，str，int/str序列或False，可选，默认为`None`
- en: Column(s) to use as the row labels of the `DataFrame`, either given as string
    name or column index. If a sequence of int / str is given, a MultiIndex is used.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 用作`DataFrame`行标签的列，可以作为字符串名称或列索引给出。如果给出int/str序列，则使用MultiIndex。
- en: Note
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`index_col=False` can be used to force pandas to *not* use the first column
    as the index, e.g. when you have a malformed file with delimiters at the end of
    each line.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`index_col=False`来强制pandas*不*使用第一列作为索引，例如当您有一个每行末尾都有分隔符的格式错误文件时。
- en: The default value of `None` instructs pandas to guess. If the number of fields
    in the column header row is equal to the number of fields in the body of the data
    file, then a default index is used. If it is larger, then the first columns are
    used as index so that the remaining number of fields in the body are equal to
    the number of fields in the header.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`None`的默认值指示pandas进行猜测。如果列标题行中的字段数等于数据文件主体中的字段数，则使用默认索引。如果大于此数，则使用前几列作为索引，以使数据主体中的剩余字段数等于标题中的字段数。'
- en: The first row after the header is used to determine the number of columns, which
    will go into the index. If the subsequent rows contain less columns than the first
    row, they are filled with `NaN`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在标题之后的第一行用于确定要放入索引的列数。如果后续行的列数少于第一行，则用`NaN`填充。
- en: This can be avoided through `usecols`. This ensures that the columns are taken
    as is and the trailing data are ignored.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`usecols`来避免这种情况。这确保了列按原样获取，而尾随数据被忽略。
- en: usecolslist-like or callable, default `None`
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: usecols类似列表或可调用对象，默认为`None`
- en: Return a subset of the columns. If list-like, all elements must either be positional
    (i.e. integer indices into the document columns) or strings that correspond to
    column names provided either by the user in `names` or inferred from the document
    header row(s). If `names` are given, the document header row(s) are not taken
    into account. For example, a valid list-like `usecols` parameter would be `[0,
    1, 2]` or `['foo', 'bar', 'baz']`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 返回列的子集。如果类似列表，则所有元素必须是位置的（即整数索引到文档列）或与用户在`names`中提供的列名对应的字符串。如果给出了`names`，则不考虑文档标题行。例如，一个有效的类似列表`usecols`参数可以是`[0,
    1, 2]`或`['foo', 'bar', 'baz']`。
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate
    a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo',
    'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 元素顺序被忽略，因此`usecols=[0, 1]`与`[1, 0]`相同。要从具有保留元素顺序的`data`实例化数据帧，请使用`pd.read_csv(data,
    usecols=['foo', 'bar'])[['foo', 'bar']]`以`['foo', 'bar']`顺序或`pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]`以`['bar', 'foo']`顺序。
- en: 'If callable, the callable function will be evaluated against the column names,
    returning names where the callable function evaluates to True:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可调用，则将对列名评估可调用函数，返回可调用函数评估为True的名称：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using this parameter results in much faster parsing time and lower memory usage
    when using the c engine. The Python engine loads the data first before deciding
    which columns to drop.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此参数可在使用c引擎时获得更快的解析时间和更低的内存使用率。Python引擎在决定要删除哪些列之前首先加载数据。
- en: General parsing configuration
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通用解析配置
- en: dtypeType name or dict of column -> type, default `None`
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: dtype类型名称或列->类型的字典，默认为`None`
- en: 'Data type for data or columns. E.g. `{''a'': np.float64, ''b'': np.int32, ''c'':
    ''Int64''}` Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype. If converters are specified, they will be
    applied INSTEAD of dtype conversion.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '数据或列的数据类型。例如`{''a'': np.float64, ''b'': np.int32, ''c'': ''Int64''}` 使用`str`或`object`与适当的`na_values`设置一起使用以保留并不解释数据类型。如果指定了转换器，则将应用转换器，而不是数据类型转换。'
- en: 'New in version 1.5.0: Support for defaultdict was added. Specify a defaultdict
    as input where the default determines the dtype of the columns which are not explicitly
    listed.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 1.5.0版本中的新功能：添加了对defaultdict的支持。指定一个defaultdict作为输入，其中默认值确定未明确列出的列的数据类型。
- en: dtype_backend{“numpy_nullable”, “pyarrow”}, defaults to NumPy backed DataFrames
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: dtype_backend{“numpy_nullable”，“pyarrow”}，默认为NumPy支持的数据帧
- en: Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
    nullable dtypes are used for all dtypes that have a nullable implementation when
    “numpy_nullable” is set, pyarrow is used for all dtypes if “pyarrow” is set.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的 dtype_backend，例如 DataFrame 是否应具有 NumPy 数组，当设置“numpy_nullable”时，所有具有可为空实现的
    dtype 都使用可为空 dtype，如果设置“pyarrow”，则所有 dtype 都使用 pyarrow。
- en: The dtype_backends are still experimential.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: dtype_backends 仍处于实验阶段。
- en: New in version 2.0.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 2.0 版本中新增。
- en: engine{`'c'`, `'python'`, `'pyarrow'`}
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: engine{`'c'`, `'python'`, `'pyarrow'`}
- en: Parser engine to use. The C and pyarrow engines are faster, while the python
    engine is currently more feature-complete. Multithreading is currently only supported
    by the pyarrow engine.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的解析引擎。C 和 pyarrow 引擎速度更快，而 python 引擎目前功能更完整。目前只有 pyarrow 引擎支持多线程。
- en: 'New in version 1.4.0: The “pyarrow” engine was added as an *experimental* engine,
    and some features are unsupported, or may not work correctly, with this engine.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4.0 版本中新增：添加了“pyarrow”引擎作为*实验性*引擎，并且某些功能不受支持，或者可能无法正常工作。
- en: convertersdict, default `None`
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器字典，默认为`None`
- en: Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 用于转换某些列中值的函数字典。键可以是整数或列标签。
- en: true_valueslist, default `None`
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: true_values列表，默认为`None`
- en: Values to consider as `True`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要视为`True`的值。
- en: false_valueslist, default `None`
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: false_values列表，默认为`None`
- en: Values to consider as `False`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要视为`False`的值。
- en: skipinitialspaceboolean, default `False`
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: skipinitialspace布尔值，默认为`False`
- en: Skip spaces after delimiter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在分隔符后跳过空格。
- en: skiprowslist-like or integer, default `None`
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: skiprows类似列表或整数，默认为`None`
- en: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
    of the file.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要跳过的行号（从 0 开始计数）或要在文件开头跳过的行数（整数）。
- en: 'If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可调用，则将针对行索引评估可调用函数，如果应跳过该行则返回 True，否则返回 False：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: skipfooterint, default `0`
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: skipfooterint，默认为`0`
- en: Number of lines at bottom of file to skip (unsupported with engine=’c’).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要跳过文件底部的行数（与 engine=’c’ 不兼容）。
- en: nrowsint, default `None`
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: nrows整数，默认为`None`
- en: Number of rows of file to read. Useful for reading pieces of large files.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取的文件行数。用于读取大文件的片段。
- en: low_memoryboolean, default `True`
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: low_memory布尔值，默认为`True`
- en: Internally process the file in chunks, resulting in lower memory use while parsing,
    but possibly mixed type inference. To ensure no mixed types either set `False`,
    or specify the type with the `dtype` parameter. Note that the entire file is read
    into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter
    to return the data in chunks. (Only valid with C parser)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在块中内部处理文件，导致解析时使用更少的内存，但可能混合类型推断。为确保没有混合类型，要么设置为`False`，要么使用`dtype`参数指定类型。请注意，无论如何整个文件都会读入单个`DataFrame`，使用`chunksize`或`iterator`参数以返回分块数据。
    （仅适用于 C 解析器）
- en: memory_mapboolean, default False
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: memory_map布尔值，默认为 False
- en: If a filepath is provided for `filepath_or_buffer`, map the file object directly
    onto memory and access the data directly from there. Using this option can improve
    performance because there is no longer any I/O overhead.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`filepath_or_buffer`提供了文件路径，则直接将文件对象映射到内存，并直接从那里访问数据。使用此选项可以提高性能，因为不再有任何
    I/O 开销。
- en: NA and missing data handling
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NA 和缺失数据处理
- en: na_valuesscalar, str, list-like, or dict, default `None`
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: na_values标量、字符串、类似列表或字典，默认为`None`
- en: Additional strings to recognize as NA/NaN. If dict passed, specific per-column
    NA values. See [na values const](#io-navaluesconst) below for a list of the values
    interpreted as NaN by default.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 附加字符串识别为 NA/NaN。如果传递了字典，则为每列指定特定的 NA 值。请参见下面的[na values const](#io-navaluesconst)以获取默认情况下解释为
    NaN 的值列表。
- en: keep_default_naboolean, default `True`
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: keep_default_na布尔值，默认为`True`
- en: 'Whether or not to include the default NaN values when parsing the data. Depending
    on whether `na_values` is passed in, the behavior is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 是否在解析数据时包括默认的 NaN 值。根据是否传递了`na_values`，行为如下：
- en: If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is
    appended to the default NaN values used for parsing.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`keep_default_na`为`True`，并且指定了`na_values`，则`na_values`将附加到用于解析的默认 NaN 值。
- en: If `keep_default_na` is `True`, and `na_values` are not specified, only the
    default NaN values are used for parsing.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`keep_default_na`为`True`，并且未指定`na_values`，则仅使用默认 NaN 值进行解析。
- en: If `keep_default_na` is `False`, and `na_values` are specified, only the NaN
    values specified `na_values` are used for parsing.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`keep_default_na`为`False`，且指定了`na_values`，则只使用指定的NaN值`na_values`进行解析。
- en: If `keep_default_na` is `False`, and `na_values` are not specified, no strings
    will be parsed as NaN.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`keep_default_na`为`False`，且未指定`na_values`，则不会将任何字符串解析为NaN。
- en: Note that if `na_filter` is passed in as `False`, the `keep_default_na` and
    `na_values` parameters will be ignored.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果传递`na_filter`为`False`，则`keep_default_na`和`na_values`参数将被忽略。
- en: na_filterboolean, default `True`
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: na_filter布尔值，默认为`True`
- en: Detect missing value markers (empty strings and the value of na_values). In
    data without any NAs, passing `na_filter=False` can improve the performance of
    reading a large file.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 检测缺失值标记（空字符串和na_values的值）。在没有任何NA的数据中，传递`na_filter=False`可以提高读取大文件的性能。
- en: verboseboolean, default `False`
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: verbose布尔值，默认为`False`
- en: Indicate number of NA values placed in non-numeric columns.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 指示放置在非数字列中的NA值的数量。
- en: skip_blank_linesboolean, default `True`
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: skip_blank_lines布尔值，默认为`True`
- en: If `True`, skip over blank lines rather than interpreting as NaN values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`True`，则跳过空行而不解释为NaN值。
- en: '#### Datetime handling'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 日期时间处理'
- en: parse_datesboolean or list of ints or names or list of lists or dict, default
    `False`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: parse_dates布尔值或整数列表或名称列表或列表列表或字典，默认为`False`。
- en: If `True` -> try parsing the index.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为`True` -> 尝试解析索引。
- en: If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`[1, 2, 3]` -> 尝试将列1、2、3分别解析为单独的日期列。
- en: If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`[[1, 3]]` -> 合并列1和3并解析为单个日期列。
- en: 'If `{''foo'': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果`{''foo'': [1, 3]}` -> 解析列1、3为日期，并将结果命名为‘foo’。'
- en: Note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A fast-path exists for iso8601-formatted dates.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 存在用于iso8601格式日期的快速路径。
- en: infer_datetime_formatboolean, default `False`
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: infer_datetime_format布尔值，默认为`False`
- en: If `True` and parse_dates is enabled for a column, attempt to infer the datetime
    format to speed up the processing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`True`并且启用了parse_dates用于某一列，则尝试推断日期时间格式以加快处理速度。
- en: 'Deprecated since version 2.0.0: A strict version of this argument is now the
    default, passing it has no effect.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 自2.0.0版本起弃用：此参数的严格版本现在是默认值，传递它不会产生任何效果。
- en: keep_date_colboolean, default `False`
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: keep_date_col布尔值，默认为`False`
- en: If `True` and parse_dates specifies combining multiple columns then keep the
    original columns.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`True`并且parse_dates指定了组合多个列，则保留原始列。
- en: date_parserfunction, default `None`
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: date_parser函数，默认为`None`
- en: 'Function to use for converting a sequence of string columns to an array of
    datetime instances. The default uses `dateutil.parser.parser` to do the conversion.
    pandas will try to call date_parser in three different ways, advancing to the
    next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates)
    as arguments; 2) concatenate (row-wise) the string values from the columns defined
    by parse_dates into a single array and pass that; and 3) call date_parser once
    for each row using one or more strings (corresponding to the columns defined by
    parse_dates) as arguments.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将一系列字符串列转换为日期时间实例数组的函数。默认使用`dateutil.parser.parser`进行转换。pandas将尝试以三种不同的方式调用date_parser，如果发生异常，则继续下一个：1)
    将一个或多个数组（由parse_dates定义）作为参数传递；2) 将由parse_dates定义的列中的字符串值（按行）连接成单个数组并传递；3) 对每一行使用一个或多个字符串（对应于由parse_dates定义的列）调用date_parser。
- en: 'Deprecated since version 2.0.0: Use `date_format` instead, or read in as `object`
    and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 自2.0.0版本起弃用：改用`date_format`，或者读取为`object`，然后根据需要应用[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: date_formatstr or dict of column -> format, default `None`
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: date_format字符串或列->格式字典，默认为`None`
- en: If used in conjunction with `parse_dates`, will parse dates according to this
    format. For anything more complex, please read in as `object` and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果与`parse_dates`一起使用，将根据此格式解析日期。对于更复杂的情况，请按照`object`读取，然后根据需要应用[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: New in version 2.0.0.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 2.0.0版本中的新功能。
- en: dayfirstboolean, default `False`
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: dayfirst布尔值，默认为`False`
- en: DD/MM format dates, international and European format.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: DD/MM格式日期，国际和欧洲格式。
- en: cache_datesboolean, default True
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: cache_dates布尔值，默认为True
- en: If True, use a cache of unique, converted dates to apply the datetime conversion.
    May produce significant speed-up when parsing duplicate date strings, especially
    ones with timezone offsets.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为True，则使用唯一的转换日期缓存来应用日期时间转换。在解析重复日期字符串时可能会产生显著的加速，特别是带有时区偏移的日期字符串。
- en: Iteration
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 迭代
- en: iteratorboolean, default `False`
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器布尔值，默认为`False`
- en: Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 返回用于迭代或使用`get_chunk()`获取块的`TextFileReader`对象。
- en: chunksizeint, default `None`
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小整数，默认为`None`
- en: Return `TextFileReader` object for iteration. See [iterating and chunking](#io-chunking)
    below.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 返回用于迭代的`TextFileReader`对象。参见下面的[迭代和分块](#io-chunking)。
- en: Quoting, compression, and file format
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 引用、压缩和文件格式
- en: compression{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`,
    `dict`}, default `'infer'`
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`, `dict`}，默认为`'infer'`
- en: 'For on-the-fly decompression of on-disk data. If ‘infer’, then use gzip, bz2,
    zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in ‘.gz’, ‘.bz2’,
    ‘.zip’, ‘.xz’, ‘.zst’, respectively, and no decompression otherwise. If using
    ‘zip’, the ZIP file must contain only one data file to be read in. Set to `None`
    for no decompression. Can also be a dict with key `''method''` set to one of {`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd''`} and other key-value pairs are forwarded to
    `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`.
    As an example, the following could be passed for faster compression and to create
    a reproducible gzip archive: `compression={''method'': ''gzip'', ''compresslevel'':
    1, ''mtime'': 1}`.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '用于在磁盘数据的即时解压缩。如果为‘infer’，则如果`filepath_or_buffer`是以‘.gz’、‘.bz2’、‘.zip’、‘.xz’、‘.zst’结尾的路径，则使用gzip、bz2、zip、xz或zstandard，否则不进行解压缩。如果使用‘zip’，ZIP文件必须只包含一个要读取的数据文件。设置为`None`表示不进行解压缩。也可以是一个字典，其中键‘method’设置为其中之一{`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd`}，其他键值对转发到`zipfile.ZipFile`、`gzip.GzipFile`、`bz2.BZ2File`或`zstandard.ZstdDecompressor`。例如，可以传递以下内容以获得更快的压缩和创建可重现的gzip存档：`compression={''method'':
    ''gzip'', ''compresslevel'': 1, ''mtime'': 1}`。'
- en: 'Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’
    to `gzip.open`.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本1.2.0更改：以前的版本将‘gzip’的字典条目转发到`gzip.open`。
- en: thousandsstr, default `None`
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 千位分隔符字符串，默认为`None`
- en: Thousands separator.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 千位分隔符。
- en: decimalstr, default `'.'`
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 十进制字符串，默认为`'.'`
- en: Character to recognize as decimal point. E.g. use `','` for European data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 用于识别为小数点的字符。例如，对于欧洲数据使用`','`。
- en: float_precisionstring, default None
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 浮点精度字符串，默认为None
- en: Specifies which converter the C engine should use for floating-point values.
    The options are `None` for the ordinary converter, `high` for the high-precision
    converter, and `round_trip` for the round-trip converter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 指定C引擎应使用哪个转换器处理浮点值。选项为`None`表示普通转换器，`high`表示高精度转换器，`round_trip`表示往返转换器。
- en: lineterminatorstr (length 1), default `None`
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 行终止符字符串（长度为1），默认为`None`
- en: Character to break file into lines. Only valid with C parser.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将文件分成行的字符。仅与C解析器有效。
- en: quotecharstr (length 1)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 引用字符字符串（长度为1）
- en: The character used to denote the start and end of a quoted item. Quoted items
    can include the delimiter and it will be ignored.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 用于表示引用项的起始和结束的字符。引用项可以包括分隔符，它将被忽略。
- en: quotingint or `csv.QUOTE_*` instance, default `0`
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 引用int或`csv.QUOTE_*`实例，默认为`0`
- en: Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL`
    (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 控制字段引用行为的`csv.QUOTE_*`常量。使用`QUOTE_MINIMAL`（0）、`QUOTE_ALL`（1）、`QUOTE_NONNUMERIC`（2）或`QUOTE_NONE`（3）中的一个。
- en: doublequoteboolean, default `True`
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 双引号布尔值，默认为`True`
- en: When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether
    or not to interpret two consecutive `quotechar` elements **inside** a field as
    a single `quotechar` element.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当指定`quotechar`并且`quoting`不是`QUOTE_NONE`时，指示是否将字段内两个连续的`quotechar`元素解释为单个`quotechar`元素。
- en: escapecharstr (length 1), default `None`
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 转义字符字符串（长度为1），默认为`None`
- en: One-character string used to escape delimiter when quoting is `QUOTE_NONE`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在引用方式为`QUOTE_NONE`时用于转义分隔符的单字符字符串。
- en: commentstr, default `None`
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注释字符串，默认为`None`
- en: Indicates remainder of line should not be parsed. If found at the beginning
    of a line, the line will be ignored altogether. This parameter must be a single
    character. Like empty lines (as long as `skip_blank_lines=True`), fully commented
    lines are ignored by the parameter `header` but not by `skiprows`. For example,
    if `comment='#'`, parsing ‘#empty\na,b,c\n1,2,3’ with `header=0` will result in
    ‘a,b,c’ being treated as the header.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 指示不应解析行的其余部分。如果在行的开头找到，整行将被完全忽略。此参数必须是单个字符。与空行一样（只要`skip_blank_lines=True`），完全注释的行由参数`header`忽略，但不由`skiprows`忽略。例如，如果`comment='#'`，使用`header=0`解析‘#empty\na,b,c\n1,2,3’将导致���a,b,c’被视为标题。
- en: encodingstr, default `None`
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: encodingstr，默认为`None`
- en: Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python
    standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 读取/写入UTF时要使用的编码（例如，`'utf-8'`）。[Python标准编码列表](https://docs.python.org/3/library/codecs.html#standard-encodings)。
- en: dialectstr or [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") instance, default `None`
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: dialectstr或[`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(在Python v3.12中)")实例，默认为`None`
- en: 'If provided, this parameter will override values (default or not) for the following
    parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`,
    and `quoting`. If it is necessary to override values, a ParserWarning will be
    issued. See [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") documentation for more details.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供，此参数将覆盖以下参数的值（默认或非默认）：`delimiter`、`doublequote`、`escapechar`、`skipinitialspace`、`quotechar`和`quoting`。如果需要覆盖值，将发出ParserWarning。有关更多详细信息，请参阅[`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(在Python v3.12中)")文档。
- en: Error handling
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误处理
- en: on_bad_lines(‘error’, ‘warn’, ‘skip’), default ‘error’
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: on_bad_lines（‘error’、‘warn’、‘skip’），默认为‘error’
- en: 'Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 指定在遇到坏行（字段过多的行）时要执行的操作。允许的值为：
- en: ‘error’, raise an ParserError when a bad line is encountered.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘error’，遇到坏行时引发ParserError。
- en: ‘warn’, print a warning when a bad line is encountered and skip that line.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘warn’，遇到坏行时打印警告并跳过该行。
- en: ‘skip’, skip bad lines without raising or warning when they are encountered.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘skip’，遇到坏行时跳过而不引发或警告。
- en: New in version 1.3.0.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3.0版中的新功能。
- en: '### Specifying column data types'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '### 指定列数据类型'
- en: 'You can indicate the data type for the whole `DataFrame` or individual columns:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指示整个`DataFrame`或单独的列的数据类型：
- en: '[PRE2]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Fortunately, pandas offers more than one way to ensure that your column(s) contain
    only one `dtype`. If you’re unfamiliar with these concepts, you can see [here](basics.html#basics-dtypes)
    to learn more about dtypes, and [here](basics.html#basics-object-conversion) to
    learn more about `object` conversion in pandas.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，pandas提供了多种方法来确保您的列只包含一个`dtype`。如果您对这些概念不熟悉，可以查看[这里](basics.html#basics-dtypes)了解有关dtypes的更多信息，以及[这里](basics.html#basics-object-conversion)了解有关pandas中`object`转换的更多信息。
- en: 'For instance, you can use the `converters` argument of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以使用[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")的`converters`参数：
- en: '[PRE3]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Or you can use the [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") function to coerce the dtypes after reading in the data,
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您可以在读取数据后使用[`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric")函数强制转换dtypes，
- en: '[PRE4]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: which will convert all valid parsing to floats, leaving the invalid parsing
    as `NaN`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将将所有有效解析转换为浮点数，将无效解析保留为`NaN`。
- en: Ultimately, how you deal with reading in columns containing mixed dtypes depends
    on your specific needs. In the case above, if you wanted to `NaN` out the data
    anomalies, then [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") is probably your best option. However, if you wanted for
    all the data to be coerced, no matter the type, then using the `converters` argument
    of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    would certainly be worth trying.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，如何处理包含混合dtypes的列取决于您的具体需求。在上面的情况下，如果您想要将数据异常值设置为`NaN`，那么[`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric")可能是您最好的选择。然而，如果您希望所有数据被强制转换，无论类型如何，那么使用[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")的`converters`参数肯定值得一试。
- en: Note
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In some cases, reading in abnormal data with columns containing mixed dtypes
    will result in an inconsistent dataset. If you rely on pandas to infer the dtypes
    of your columns, the parsing engine will go and infer the dtypes for different
    chunks of the data, rather than the whole dataset at once. Consequently, you can
    end up with column(s) with mixed dtypes. For example,
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，读取包含混合dtype列的异常数据将导致数据集不一致。如果依赖pandas推断列的dtype，解析引擎将会推断数据的不同块的dtype，而不是一次推断整个数据集。因此，可能会出现具有混合dtype的列。例如，
- en: '[PRE5]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: will result with `mixed_df` containing an `int` dtype for certain chunks of
    the column, and `str` for others due to the mixed dtypes from the data that was
    read in. It is important to note that the overall column will be marked with a
    `dtype` of `object`, which is used for columns with mixed dtypes.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '将导致`mixed_df`包含某些列块的`int` dtype，以及由于读取的数据中混合dtype而导致其他列块的`str`。重要的是要注意，整体列将被标记为`object`的`dtype`，用于具有混合dtype的列。 '
- en: Setting `dtype_backend="numpy_nullable"` will result in nullable dtypes for
    every column.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`dtype_backend="numpy_nullable"`将导致每列具有可空dtype。
- en: '[PRE6]  ### Specifying categorical dtype'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE6]  ### 指定分类dtype'
- en: '`Categorical` columns can be parsed directly by specifying `dtype=''category''`
    or `dtype=CategoricalDtype(categories, ordered)`.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`Categorical`列可以直接通过指定`dtype=''category''`或`dtype=CategoricalDtype(categories,
    ordered)`来解析。'
- en: '[PRE7]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Individual columns can be parsed as a `Categorical` using a dict specification:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用字典规范将单独的列解析为`Categorical`：
- en: '[PRE8]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Specifying `dtype='category'` will result in an unordered `Categorical` whose
    `categories` are the unique values observed in the data. For more control on the
    categories and order, create a `CategoricalDtype` ahead of time, and pass that
    for that column’s `dtype`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`dtype='category'`将导致一个无序的`Categorical`，其`categories`是数据中观察到的唯一值。要对类别和顺序进行更多控制，预先创建一个`CategoricalDtype`，并将其传递给该列的`dtype`。
- en: '[PRE9]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When using `dtype=CategoricalDtype`, “unexpected” values outside of `dtype.categories`
    are treated as missing values.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dtype=CategoricalDtype`时，`dtype.categories`之外的“意外”值被视为缺失值。
- en: '[PRE10]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This matches the behavior of `Categorical.set_categories()`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这与`Categorical.set_categories()`的行为相匹配。
- en: Note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: With `dtype='category'`, the resulting categories will always be parsed as strings
    (object dtype). If the categories are numeric they can be converted using the
    [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric "pandas.to_numeric")
    function, or as appropriate, another converter such as [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime").
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dtype='category'`，生成的类别将始终被解析为字符串（对象dtype）。如果类别是数字的，可以使用[`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric")函数进行转换，或者根据需要使用另一个转换器，如[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: When `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all numeric,
    all datetimes, etc.), the conversion is done automatically.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当`dtype`是具有同质`categories`（全部是数字，全部是日期时间等）的`CategoricalDtype`时，转换会自动完成。
- en: '[PRE11]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Naming and using columns
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名和使用列
- en: '#### Handling column names'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 处理列名'
- en: 'A file may or may not have a header row. pandas assumes the first row should
    be used as the column names:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 文件可能有或没有标题行。pandas假定第一行应该用作列名：
- en: '[PRE12]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By specifying the `names` argument in conjunction with `header` you can indicate
    other names to use and whether or not to throw away the header row (if any):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`header`中与`names`参数结合使用，可以指示要使用的其他名称以及是否丢弃标题行（如果有）：
- en: '[PRE13]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If the header is in a row other than the first, pass the row number to `header`.
    This will skip the preceding rows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标题在第一行之外的行中，将行号传递给`header`。这将跳过前面的行：
- en: '[PRE14]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Default behavior is to infer the column names: if no names are passed the behavior
    is identical to `header=0` and column names are inferred from the first non-blank
    line of the file, if column names are passed explicitly then the behavior is identical
    to `header=None`.  ### Duplicate names parsing'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '默认行为是推断列名：如果没有传递列名，则行为与`header=0`相同，并且列名是从文件的第一行非空行推断出来的，如果显式传递了列名，则行为与`header=None`相同。  ###
    重复名称解析'
- en: 'If the file or header contains duplicate names, pandas will by default distinguish
    between them so as to prevent overwriting data:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件或标题包含重复的名称，pandas默认会区分它们，以防止数据被覆盖：
- en: '[PRE15]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: There is no more duplicate data because duplicate columns ‘X’, …, ‘X’ become
    ‘X’, ‘X.1’, …, ‘X.N’.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 不再有重复数据，因为重复列‘X’，…，‘X’变为‘X’，‘X.1’，…，‘X.N’。
- en: '#### Filtering columns (`usecols`)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 过滤列（`usecols`）'
- en: 'The `usecols` argument allows you to select any subset of the columns in a
    file, either using the column names, position numbers or a callable:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`usecols`参数允许您选择文件中任意列的子集，可以使用列名、位置编号或可调用对象：'
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `usecols` argument can also be used to specify which columns not to use
    in the final result:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`usecols`参数也可以用于指定最终结果中不使用的列：'
- en: '[PRE17]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this case, the callable is specifying that we exclude the “a” and “c” columns
    from the output.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可调用对象指定我们从输出中排除“a”和“c”列。
- en: Comments and empty lines
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释和空行
- en: '#### Ignoring line comments and empty lines'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 忽略行注释和空行'
- en: If the `comment` parameter is specified, then completely commented lines will
    be ignored. By default, completely blank lines will be ignored as well.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定了`comment`参数，则完全注释的行将被忽略。默认情况下，完全空白行也将被忽略。
- en: '[PRE18]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`skip_blank_lines=False`，那么`read_csv`将不会忽略空行：
- en: '[PRE19]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Warning
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: 'The presence of ignored lines might create ambiguities involving line numbers;
    the parameter `header` uses row numbers (ignoring commented/empty lines), while
    `skiprows` uses line numbers (including commented/empty lines):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略行的存在可能会导致涉及行号的歧义；参数`header`使用行号（忽略注释/空行），而`skiprows`使用行号（包括注释/空行）：
- en: '[PRE20]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If both `header` and `skiprows` are specified, `header` will be relative to
    the end of `skiprows`. For example:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果同时指定了`header`和`skiprows`，`header`将相对于`skiprows`的末尾。例如：
- en: '[PRE21]  #### Comments'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE21]  #### 注释'
- en: 'Sometimes comments or meta data may be included in a file:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 有时文件中可能包含注释或元数据：
- en: '[PRE22]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By default, the parser includes the comments in the output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，解析器会将注释包含在输出中：
- en: '[PRE23]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can suppress the comments using the `comment` keyword:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`comment`关键字来抑制注释：
- en: '[PRE24]  ### Dealing with Unicode data'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE24]  ### 处理Unicode数据'
- en: 'The `encoding` argument should be used for encoded unicode data, which will
    result in byte strings being decoded to unicode in the result:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 应该使用`encoding`参数来处理编码的Unicode数据，这将导致字节字符串在结果中被解码为Unicode：
- en: '[PRE25]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Some formats which encode all characters as multiple bytes, like UTF-16, won’t
    parse correctly at all without specifying the encoding. [Full list of Python standard
    encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).  ###
    Index columns and trailing delimiters'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '一些将所有字符编码为多字节的格式，如UTF-16，如果不指定编码，则根本无法正确解析。[Python标准编码的完整列表](https://docs.python.org/3/library/codecs.html#standard-encodings)。  ###
    索引列和尾随分隔符'
- en: 'If a file has one more column of data than the number of column names, the
    first column will be used as the `DataFrame`’s row names:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件的数据列比列名多一个，第一列将被用作`DataFrame`的行名：
- en: '[PRE26]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Ordinarily, you can achieve this behavior using the `index_col` option.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您可以使用`index_col`选项来实现这种行为。
- en: 'There are some exception cases when a file has been prepared with delimiters
    at the end of each data line, confusing the parser. To explicitly disable the
    index column inference and discard the last column, pass `index_col=False`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些异常情况下，文件在每个数据行末尾都有分隔符，这会使解析器混淆。要显式禁用索引列推断并丢弃最后一列，请传入`index_col=False`：
- en: '[PRE28]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If a subset of data is being parsed using the `usecols` option, the `index_col`
    specification is based on that subset, not the original data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果正在使用`usecols`选项解析数据的子集，则`index_col`规范是基于该子集而不是原始数据的。
- en: '[PRE29]  ### Date Handling'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE29]  ### 日期处理'
- en: Specifying date columns
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指定日期列
- en: To better facilitate working with datetime data, [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") uses the keyword arguments `parse_dates` and `date_format`
    to allow users to specify a variety of columns and date/time formats to turn the
    input text data into `datetime` objects.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地处理日期时间数据，[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")使用关键字参数`parse_dates`和`date_format`，允许用户指定各种列和日期/时间格式，将输入文本数据转换为`datetime`对象。
- en: 'The simplest case is to just pass in `parse_dates=True`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的情况是只传入`parse_dates=True`：
- en: '[PRE30]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: It is often the case that we may want to store date and time data separately,
    or store various date fields separately. the `parse_dates` keyword can be used
    to specify a combination of columns to parse the dates and/or times from.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们可能希望将日期和时间数据分开存储，或将各种日期字段分开存储。`parse_dates`关键字可用于指定要从中解析日期和/或时间的列的组合。
- en: 'You can specify a list of column lists to `parse_dates`, the resulting date
    columns will be prepended to the output (so as to not affect the existing column
    order) and the new column names will be the concatenation of the component column
    names:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将列列表的列表指定为 `parse_dates`，生成的日期列将被添加到输出中（以不影响现有列顺序），新列名将是组件列名的连接：
- en: '[PRE31]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'By default the parser removes the component date columns, but you can choose
    to retain them via the `keep_date_col` keyword:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，解析器会删除组件日期列，但您可以通过 `keep_date_col` 关键字选择保留它们：
- en: '[PRE32]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that if you wish to combine multiple columns into a single date column,
    a nested list must be used. In other words, `parse_dates=[1, 2]` indicates that
    the second and third columns should each be parsed as separate date columns while
    `parse_dates=[[1, 2]]` means the two columns should be parsed into a single column.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您希望将多个列合并为单个日期列，则必须使用嵌套列表。换句话说，`parse_dates=[1, 2]` 表示应将第二和第三列分别解析为单独的日期列，而
    `parse_dates=[[1, 2]]` 表示应将这两列解析为单个列。
- en: 'You can also use a dict to specify custom name columns:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用字典指定自定义列名：
- en: '[PRE33]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It is important to remember that if multiple text columns are to be parsed
    into a single date column, then a new column is prepended to the data. The `index_col`
    specification is based off of this new set of columns rather than the original
    data columns:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，如果要将多个文本列解析为单个日期列，则会在数据前添加一个新列。`index_col` 规范是基于这组新列而不是原始数据列的：
- en: '[PRE34]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意
- en: If a column or index contains an unparsable date, the entire column or index
    will be returned unaltered as an object data type. For non-standard datetime parsing,
    use [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") after `pd.read_csv`.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果列或索引包含无法解析的日期，则整个列或索引将以对象数据类型不变返回。对于非标准日期时间解析，请在 `pd.read_csv` 后使用 [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: Note
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意
- en: read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g
    “2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your
    data to store datetimes in this format, load times will be significantly faster,
    ~20x has been observed.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: read_csv 在解析 iso8601 格式的日期时间字符串（例如“2000-01-01T00:01:02+00:00”及类似变体）时具有快速路径。如果您可以安排数据以这种格式存储日期时间，加载时间将显著加快，观察到的速度提升约为20倍。
- en: 'Deprecated since version 2.2.0: Combining date columns inside read_csv is deprecated.
    Use `pd.to_datetime` on the relevant result columns instead.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 自版本 2.2.0 起已弃用：在 read_csv 中合并日期列已弃用。请改为在相关结果列上使用 `pd.to_datetime`。
- en: Date parsing functions
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 日期解析函数
- en: 'Finally, the parser allows you to specify a custom `date_format`. Performance-wise,
    you should try these methods of parsing dates in order:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，解析器允许您指定自定义的 `date_format`。就性能而言，您应该按照以下顺序尝试这些日期解析方法：
- en: 'If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or
    `date_format={column_name: "%d/%m/%Y"}`.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '如果您知道格式，请使用 `date_format`，例如：`date_format="%d/%m/%Y"` 或 `date_format={column_name:
    "%d/%m/%Y"}`。'
- en: If you different formats for different columns, or want to pass any extra options
    (such as `utc`) to `to_datetime`, then you should read in your data as `object`
    dtype, and then use `to_datetime`.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不同列有不同格式，或者想要向 `to_datetime` 传递任何额外选项（如 `utc`），则应以 `object` 类型读取数据，然后使用 `to_datetime`。
- en: '#### Parsing a CSV with mixed timezones'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 解析具有混合时区的 CSV'
- en: pandas cannot natively represent a column or index with mixed timezones. If
    your CSV file contains columns with a mixture of timezones, the default result
    will be an object-dtype column with strings, even with `parse_dates`. To parse
    the mixed-timezone values as a datetime column, read in as `object` dtype and
    then call [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") with `utc=True`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 无法原生表示具有混合时区的列或索引。如果您的 CSV 文件包含具有混合时区的列，则默认结果将是一个对象类型的列，其中包含字符串，即使使用
    `parse_dates` 也是如此。要将混合时区值解析为日期时间列，请以 `object` 类型读取，然后调用 [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") 并设置 `utc=True`。
- en: '[PRE35]  #### Inferring datetime format'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE35]  #### 推断日期时间格式'
- en: 'Here are some examples of datetime strings that can be guessed (all representing
    December 30th, 2011 at 00:00:00):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以猜测的日期时间字符串示例（均表示2011年12月30日00:00:00）：
- en: “20111230”
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “20111230”
- en: “2011/12/30”
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “2011/12/30”
- en: “20111230 00:00:00”
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “20111230 00:00:00”
- en: “12/30/2011 00:00:00”
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “12/30/2011 00:00:00”
- en: “30/Dec/2011 00:00:00”
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “30/Dec/2011 00:00:00”
- en: “30/December/2011 00:00:00”
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “30/December/2011 00:00:00”
- en: Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`,
    it will guess “01/12/2011” to be December 1st. With `dayfirst=False` (default)
    it will guess “01/12/2011” to be January 12th.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，格式推断对 `dayfirst` 敏感。当 `dayfirst=True` 时，它会猜测“01/12/2011”是12月1日。当 `dayfirst=False`（默认）时，它会猜测“01/12/2011”是1月12日。
- en: If you try to parse a column of date strings, pandas will attempt to guess the
    format from the first non-NaN element, and will then parse the rest of the column
    with that format. If pandas fails to guess the format (for example if your first
    string is `'01 December US/Pacific 2000'`), then a warning will be raised and
    each row will be parsed individually by `dateutil.parser.parse`. The safest way
    to parse dates is to explicitly set `format=`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试解析日期字符串列，pandas 将尝试从第一个非 NaN 元素猜测格式，然后使用该格式解析列的其余部分。如果 pandas 无法猜测格式（例如，如果你的第一个字符串是
    `'01 December US/Pacific 2000'`），那么将会发出警告，并且每一行将通过 `dateutil.parser.parse` 单独解析。解析日期的最安全方式是明确设置
    `format=`。
- en: '[PRE36]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the case that you have mixed datetime formats within the same column, you
    can pass `format='mixed'`
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在同一列中有混合的日期时间格式，你可以传递 `format='mixed'`
- en: '[PRE37]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'or, if your datetime formats are all ISO8601 (possibly not identically-formatted):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你的日期时间格式都是 ISO8601（可能不完全相同格式）：
- en: '[PRE38]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: International date formats
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 国际日期格式
- en: 'While US date formats tend to be MM/DD/YYYY, many international formats use
    DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管美国日期格式倾向于 MM/DD/YYYY，许多国际格式使用 DD/MM/YYYY。为方便起见，提供了一个 `dayfirst` 关键字：
- en: '[PRE39]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Writing CSVs to binary file objects
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将 CSV 写入二进制文件对象
- en: New in version 1.2.0.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 1.2.0 中的新功能。
- en: '`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary
    mode. In most cases, it is not necessary to specify `mode` as Pandas will auto-detect
    whether the file object is opened in text or binary mode.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`df.to_csv(..., mode="wb")` 允许将 CSV 写入以二进制模式打开的文件对象。在大多数情况下，不需要指定 `mode`，因为
    Pandas 将自动检测文件对象是以文本模式还是二进制模式打开的。'
- en: '[PRE40]  ### Specifying method for floating-point conversion'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE40]  ### 指定浮点数转换方法'
- en: 'The parameter `float_precision` can be specified in order to use a specific
    floating-point converter during parsing with the C engine. The options are the
    ordinary converter, the high-precision converter, and the round-trip converter
    (which is guaranteed to round-trip values after writing to a file). For example:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 可以指定参数 `float_precision`，以在使用 C 引擎解析时使用特定的浮点数转换器。选项包括普通转换器、高���度转换器和往返转换器（在写入文件后保证往返值）。例如：
- en: '[PRE41]  ### Thousand separators'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE41]  ### 千位分隔符'
- en: 'For large numbers that have been written with a thousands separator, you can
    set the `thousands` keyword to a string of length 1 so that integers will be parsed
    correctly:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用千位分隔符编写的大数字，你可以将 `thousands` 关键字设置为长度为 1 的字符串，以便正确解析整数：
- en: 'By default, numbers with a thousands separator will be parsed as strings:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，带有千位分隔符的数字将被解析为字符串：
- en: '[PRE42]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `thousands` keyword allows integers to be parsed correctly:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`thousands` 关键字允许整数被正确解析：'
- en: '[PRE43]  ### NA values'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE43]  ### 缺失值'
- en: To control which values are parsed as missing values (which are signified by
    `NaN`), specify a string in `na_values`. If you specify a list of strings, then
    all values in it are considered to be missing values. If you specify a number
    (a `float`, like `5.0` or an `integer` like `5`), the corresponding equivalent
    values will also imply a missing value (in this case effectively `[5.0, 5]` are
    recognized as `NaN`).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制哪些值被解析为缺失值（用 `NaN` 表示），请在 `na_values` 中指定一个字符串。如果你指定一个字符串列表，那么其中的所有值都被视为缺失值。如果你指定一个数字（一个
    `float`，比如 `5.0` 或一个 `integer`，比如 `5`），则相应的等效值也将被视为缺失值（在这种情况下，实际上 `[5.0, 5]` 被识别为
    `NaN`）。
- en: To completely override the default values that are recognized as missing, specify
    `keep_default_na=False`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全覆盖默认识别为缺失的值，请指定 `keep_default_na=False`。
- en: The default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',
    '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN',
    '-NaN', 'nan', '-nan', 'None', '']`.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 默认识别的 `NaN` 值为 `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A',
    'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan',
    'None', '']`。
- en: 'Let us consider some examples:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些例子：
- en: '[PRE44]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In the example above `5` and `5.0` will be recognized as `NaN`, in addition
    to the defaults. A string will first be interpreted as a numerical `5`, then as
    a `NaN`.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，`5` 和 `5.0` 将被识别为 `NaN`，除了默认值。一个字符串首先被解释为数值 `5`，然后作为 `NaN`。
- en: '[PRE45]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Above, only an empty field will be recognized as `NaN`.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，只有空字段将被识别为`NaN`。
- en: '[PRE46]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Above, both `NA` and `0` as strings are `NaN`.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，`NA` 和 `0` 作为字符串都被识别为 `NaN`。
- en: '[PRE47]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The default values, in addition to the string `"Nope"` are recognized as `NaN`.  ###
    Infinity'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值，除了字符串`"Nope"`，也被识别为`NaN`。### 无穷大
- en: '`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf`
    as `-np.inf` (negative infinity). These will ignore the case of the value, meaning
    `Inf`, will also be parsed as `np.inf`.  ### Boolean values'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`inf` 类似的值将被解析为`np.inf`（正无穷大），而 `-inf` 将被解析为`-np.inf`（负无穷大）。这些将忽略值的大小写，意思是`Inf`也将被解析为`np.inf`。###
    布尔值'
- en: 'The common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as
    boolean. Occasionally you might want to recognize other values as being boolean.
    To do this, use the `true_values` and `false_values` options as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的值`True`、`False`、`TRUE`和`FALSE`都被识别为布尔值。偶尔你可能希望识别其他值为布尔值。要做到这一点，使用`true_values`和`false_values`选项如下：
- en: '[PRE48]  ### Handling “bad” lines'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE48]### 处理“坏”行'
- en: 'Some files may have malformed lines with too few fields or too many. Lines
    with too few fields will have NA values filled in the trailing fields. Lines with
    too many fields will raise an error by default:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文件可能有格式错误的行，字段太少或太多。字段太少的行将在尾部字段中填充NA值。字段太多的行将默认引发错误：
- en: '[PRE49]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You can elect to skip bad lines:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择跳过坏行：
- en: '[PRE50]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: New in version 1.4.0.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在版本1.4.0中新增。
- en: 'Or pass a callable function to handle the bad line if `engine="python"`. The
    bad line will be a list of strings that was split by the `sep`:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 或者通过传递一个可调用函数来处理`engine="python"`时的错误行。错误行将是由`sep`分割的字符串列表：
- en: '[PRE51]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The callable function will handle only a line with too many fields. Bad lines
    caused by other errors will be silently skipped.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 可调用函数只会处理字段过多的行。其他错误导致的坏行将被默默跳过。
- en: '[PRE52]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The line was not processed in this case, as a “bad line” here is caused by an
    escape character.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，该行未被处理，因为这里的“坏行”是由转义字符引起的。
- en: 'You can also use the `usecols` parameter to eliminate extraneous column data
    that appear in some lines but not others:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`usecols`参数来消除一些行中出现但其他行中没有的多余列数据：
- en: '[PRE53]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In case you want to keep all data including the lines with too many fields,
    you can specify a sufficient number of `names`. This ensures that lines with not
    enough fields are filled with `NaN`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想保留所有数据，包括字段过多的行，你可以指定足够数量的`names`。这样可以确保字段不足的行填充为`NaN`。
- en: '[PRE54]  ### Dialect'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE54]### 方言'
- en: The `dialect` keyword gives greater flexibility in specifying the file format.
    By default it uses the Excel dialect but you can specify either the dialect name
    or a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect "(in
    Python v3.12)") instance.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '`dialect`关键字提供了更大的灵活性，用于指定文件格式。默认情况下，它使用Excel方言，但你可以指定方言名称或[`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(在Python v3.12)")实例。'
- en: 'Suppose you had data with unenclosed quotes:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一些未封闭引号的数据：
- en: '[PRE55]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: By default, `read_csv` uses the Excel dialect and treats the double quote as
    the quote character, which causes it to fail when it finds a newline before it
    finds the closing double quote.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`read_csv`使用Excel方言，并将双引号视为引号字符，这会导致在找到关闭双引号之前找到换行符时失败。
- en: 'We can get around this using `dialect`:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`dialect`来避免这种情况：
- en: '[PRE56]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'All of the dialect options can be specified separately by keyword arguments:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的方言选项都可以通过关键字参数单独指定：
- en: '[PRE57]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Another common dialect option is `skipinitialspace`, to skip any whitespace
    after a delimiter:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的方言选项是`skipinitialspace`，用于跳过分隔符后的任何空白：
- en: '[PRE58]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The parsers make every attempt to “do the right thing” and not be fragile.
    Type inference is a pretty big deal. If a column can be coerced to integer dtype
    without altering the contents, the parser will do so. Any non-numeric columns
    will come through as object dtype as with the rest of pandas objects.  ### Quoting
    and Escape Characters'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器会尽力“做正确的事情”，并且不易受损。类型推断是一件很重要的事情。如果一个列可以被强制转换为整数类型而不改变内容，解析器将这样做。任何非数字列将与其他pandas对象一样以对象dtype传递。###
    引用和转义字符
- en: 'Quotes (and other escape characters) in embedded fields can be handled in any
    number of ways. One way is to use backslashes; to properly parse this data, you
    should pass the `escapechar` option:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套字段中的引号（和其他转义字符）可以以多种方式处理。一种方法是使用反斜杠；为了正确解析这些数据，你应该传递`escapechar`选项：
- en: '[PRE59]  ### Files with fixed width columns'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE59]### 固定宽度列的文件'
- en: 'While [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") reads delimited data, the [`read_fwf()`](../reference/api/pandas.read_fwf.html#pandas.read_fwf
    "pandas.read_fwf") function works with data files that have known and fixed column
    widths. The function parameters to `read_fwf` are largely the same as `read_csv`
    with two extra parameters, and a different usage of the `delimiter` parameter:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 当 [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    读取分隔数据时，[`read_fwf()`](../reference/api/pandas.read_fwf.html#pandas.read_fwf "pandas.read_fwf")
    函数与具有已知和固定列宽的数据文件一起工作。`read_fwf` 的函数参数与 `read_csv` 大致相同，但有两个额外参数，并且 `delimiter`
    参数的用法不同：
- en: '`colspecs`: A list of pairs (tuples) giving the extents of the fixed-width
    fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’
    can be used to instruct the parser to try detecting the column specifications
    from the first 100 rows of the data. Default behavior, if not specified, is to
    infer.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colspecs`：一个给出每行固定宽度字段范围的对（元组）列表，作为半开区间（即，[from, to[）。字符串值 ‘infer’ 可以用于指示解析器尝试从数据的前
    100 行检测列规格。如果未指定，默认行为是推断。'
- en: '`widths`: A list of field widths which can be used instead of ‘colspecs’ if
    the intervals are contiguous.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`widths`：一个字段宽度列表，可以代替 ‘colspecs’ 使用，如果间隔是连续的。'
- en: '`delimiter`: Characters to consider as filler characters in the fixed-width
    file. Can be used to specify the filler character of the fields if it is not spaces
    (e.g., ‘~’).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delimiter`：固定宽度文件中要考虑为填充字符的字符。如果字段的填充字符不是空格（例如，‘~’），可以使用它来指定填充字符。'
- en: 'Consider a typical fixed-width data file:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个典型的固定宽度数据文件：
- en: '[PRE60]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In order to parse this file into a `DataFrame`, we simply need to supply the
    column specifications to the `read_fwf` function along with the file name:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将此文件解析为 `DataFrame`，我们只需向 `read_fwf` 函数提供列规范和文件名即可：
- en: '[PRE61]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Note how the parser automatically picks column names X.<column number> when
    `header=None` argument is specified. Alternatively, you can supply just the column
    widths for contiguous columns:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当指定了 `header=None` 参数时，解析器会自动选择列名 X.<column number>。或者，您可以只为连续的列提供列宽度：
- en: '[PRE62]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The parser will take care of extra white spaces around the columns so it’s ok
    to have extra separation between the columns in the file.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器将处理围绕列周围的额外空白，因此在文件中列之间有额外分隔是可以的。
- en: By default, `read_fwf` will try to infer the file’s `colspecs` by using the
    first 100 rows of the file. It can do it only in cases when the columns are aligned
    and correctly separated by the provided `delimiter` (default delimiter is whitespace).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`read_fwf` 将尝试通过使用文件的前 100 行推断文件的 `colspecs`。它只能在列对齐并且由提供的 `delimiter`（默认分隔符是空白符）正确分隔的情况下执行此操作。
- en: '[PRE63]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '`read_fwf` supports the `dtype` parameter for specifying the types of parsed
    columns to be different from the inferred type.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_fwf` 支持 `dtype` 参数，用于指定解析列的类型与推断类型不同。'
- en: '[PRE64]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Indexes
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 索引
- en: Files with an “implicit” index column
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 具有“隐式”索引列的文件
- en: 'Consider a file with one less entry in the header than the number of data column:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑标题的条目比数据列的数量少一个的文件：
- en: '[PRE65]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In this special case, `read_csv` assumes that the first column is to be used
    as the index of the `DataFrame`:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特殊情况下，`read_csv` 假定第一列将用作 `DataFrame` 的索引：
- en: '[PRE66]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Note that the dates weren’t automatically parsed. In that case you would need
    to do as before:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，日期没有自动解析。在这种情况下，您需要像以前一样操作：
- en: '[PRE67]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Reading an index with a `MultiIndex`
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 `MultiIndex` 读取索引
- en: 'Suppose you have data indexed by two columns:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据由两列索引：
- en: '[PRE68]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The `index_col` argument to `read_csv` can take a list of column numbers to
    turn multiple columns into a `MultiIndex` for the index of the returned object:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv` 的 `index_col` 参数可以接受一个列编号的列表，将多列转换为返回对象的索引的 `MultiIndex`：'
- en: '[PRE69]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '#### Reading columns with a `MultiIndex`'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 使用 `MultiIndex` 读取列'
- en: By specifying list of row locations for the `header` argument, you can read
    in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the
    intervening rows.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为 `header` 参数指定行位置列表，您可以读取列的 `MultiIndex`。指定非连续行将跳过中间行。
- en: '[PRE70]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '`read_csv` is also able to interpret a more common format of multi-columns
    indices.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv` 也能够解释更常见的多列索引格式。'
- en: '[PRE71]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Note
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If an `index_col` is not specified (e.g. you don’t have an index, or wrote
    it with `df.to_csv(..., index=False)`, then any `names` on the columns index will
    be *lost*.  ### Automatically “sniffing” the delimiter'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未指定`index_col`（例如您没有索引，或者使用`df.to_csv(..., index=False)`写入它），则列索引上的任何`names`都将*丢失*。###
    自动“嗅探”分隔符
- en: '`read_csv` is capable of inferring delimited (not necessarily comma-separated)
    files, as pandas uses the [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)") class of the csv module. For this, you have to specify `sep=None`.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv`能够推断出分隔的（不一定是逗号分隔的）文件，因为pandas使用了csv模块的[`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(在Python v3.12中)")类。为此，您必须指定`sep=None`。'
- en: '[PRE72]  ### Reading multiple files to create a single DataFrame'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE72]### 读取多个文件以创建单个DataFrame'
- en: 'It’s best to use [`concat()`](../reference/api/pandas.concat.html#pandas.concat
    "pandas.concat") to combine multiple files. See the [cookbook](cookbook.html#cookbook-csv-multiple-files)
    for an example.  ### Iterating through files chunk by chunk'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 最好使用[`concat()`](../reference/api/pandas.concat.html#pandas.concat "pandas.concat")来合并多个文件。请参阅[cookbook](cookbook.html#cookbook-csv-multiple-files)以获取示例。###
    通过文件逐块迭代
- en: 'Suppose you wish to iterate through a (potentially very large) file lazily
    rather than reading the entire file into memory, such as the following:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您希望懒惰地迭代（而不是将整个文件读入内存），比如以下内容：
- en: '[PRE73]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'By specifying a `chunksize` to `read_csv`, the return value will be an iterable
    object of type `TextFileReader`:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`read_csv`中指定`chunksize`，返回值将是一种`TextFileReader`的可迭代对象：
- en: '[PRE74]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Changed in version 1.2: `read_csv/json/sas` return a context-manager when iterating
    through a file.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本1.2更改：`read_csv/json/sas`通过文件进行迭代时返回上下文管理器。
- en: 'Specifying `iterator=True` will also return the `TextFileReader` object:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`iterator=True`还将返回`TextFileReader`对象：
- en: '[PRE75]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Specifying the parser engine
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指定解析器引擎
- en: Pandas currently supports three engines, the C engine, the python engine, and
    an experimental pyarrow engine (requires the `pyarrow` package). In general, the
    pyarrow engine is fastest on larger workloads and is equivalent in speed to the
    C engine on most other workloads. The python engine tends to be slower than the
    pyarrow and C engines on most workloads. However, the pyarrow engine is much less
    robust than the C engine, which lacks a few features compared to the Python engine.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas目前支持三种引擎，即C引擎、Python引擎和实验性的pyarrow引擎（需要`pyarrow`包）。一般来说，对于较大的工作负载，pyarrow引擎速度最快，在大多数其他工作负载上与C引擎速度相当。Python引擎在大多数工作负载上往往比pyarrow和C引擎慢。但是，pyarrow引擎比C引擎要脆弱得多，与Python引擎相比，缺少一些功能。
- en: Where possible, pandas uses the C parser (specified as `engine='c'`), but it
    may fall back to Python if C-unsupported options are specified.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，pandas使用C解析器（指定为`engine='c'`），但如果指定了C不支持的选项，可能会退回到Python。
- en: 'Currently, options unsupported by the C and pyarrow engines include:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，C和pyarrow引擎不支持的选项包括：
- en: '`sep` other than a single character (e.g. regex separators)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep`除了单个字符（例如正则表达式分隔符）之外的其他字符'
- en: '`skipfooter`'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skipfooter`'
- en: '`sep=None` with `delim_whitespace=False`'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep=None`与`delim_whitespace=False`'
- en: Specifying any of the above options will produce a `ParserWarning` unless the
    python engine is selected explicitly using `engine='python'`.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 除非显式使用`engine='python'`选择Python引擎，否则指定上述任何选项都会产生`ParserWarning`。
- en: 'Options that are unsupported by the pyarrow engine which are not covered by
    the list above include:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: pyarrow引擎不支持的选项，未在上述列表中列出的包括：
- en: '`float_precision`'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float_precision`'
- en: '`chunksize`'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunksize`'
- en: '`comment`'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`comment`'
- en: '`nrows`'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nrows`'
- en: '`thousands`'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thousands`'
- en: '`memory_map`'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory_map`'
- en: '`dialect`'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dialect`'
- en: '`on_bad_lines`'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_bad_lines`'
- en: '`delim_whitespace`'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delim_whitespace`'
- en: '`quoting`'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quoting`'
- en: '`lineterminator`'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lineterminator`'
- en: '`converters`'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`converters`'
- en: '`decimal`'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decimal`'
- en: '`iterator`'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterator`'
- en: '`dayfirst`'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dayfirst`'
- en: '`infer_datetime_format`'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`infer_datetime_format`'
- en: '`verbose`'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`'
- en: '`skipinitialspace`'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skipinitialspace`'
- en: '`low_memory`'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_memory`'
- en: Specifying these options with `engine='pyarrow'` will raise a `ValueError`.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`engine='pyarrow'`指定这些选项将引发`ValueError`。
- en: '### Reading/writing remote files'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '### 读取/写入远程文件'
- en: 'You can pass in a URL to read or write remote files to many of pandas’ IO functions
    - the following example shows reading a CSV file:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以传递URL以读取或写入许多pandas的IO函数的远程文件 - 以下示例显示了如何读取CSV文件：
- en: '[PRE76]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: New in version 1.3.0.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 版本1.3.0中的新功能。
- en: 'A custom header can be sent alongside HTTP(s) requests by passing a dictionary
    of header key value mappings to the `storage_options` keyword argument as shown
    below:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将键值映射的字典传递给`storage_options`关键字参数来发送自定义标头以及HTTP(s)请求：
- en: '[PRE77]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'All URLs which are not local files or HTTP(s) are handled by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/),
    if installed, and its various filesystem implementations (including Amazon S3,
    Google Cloud, SSH, FTP, webHDFS…). Some of these implementations will require
    additional packages to be installed, for example S3 URLs require the [s3fs](https://pypi.org/project/s3fs/)
    library:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 所有不是本地文件或 HTTP(s) 的 URL 都由[fsspec](https://filesystem-spec.readthedocs.io/en/latest/)处理（如果安装了），以及它的各种文件系统实现（包括
    Amazon S3、Google Cloud、SSH、FTP、webHDFS 等）。其中一些实现将需要安装其他包，例如 S3 URL 需要[s3fs](https://pypi.org/project/s3fs/)库：
- en: '[PRE78]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: When dealing with remote storage systems, you might need extra configuration
    with environment variables or config files in special locations. For example,
    to access data in your S3 bucket, you will need to define credentials in one of
    the several ways listed in the [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/#credentials).
    The same is true for several of the storage backends, and you should follow the
    links at [fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations)
    for implementations built into `fsspec` and [fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)
    for those not included in the main `fsspec` distribution.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及远程存储系统时，你可能需要通过环境变量或特殊位置的配置文件进行额外配置。例如，要访问 S3 存储桶中的数据，你需要在[S3Fs文档](https://s3fs.readthedocs.io/en/latest/#credentials)中列出的几种方式之一中定义凭据。对于几个存储后端也是如此，你应该按照[fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations)中内置到`fsspec`中的实现和[fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)中未包含在主`fsspec`分发中的实现的链接进行操作。
- en: 'You can also pass parameters directly to the backend driver. Since `fsspec`
    does not utilize the `AWS_S3_HOST` environment variable, we can directly define
    a dictionary containing the endpoint_url and pass the object into the storage
    option parameter:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以直接将参数传递给后端驱动程序。由于 `fsspec` 不使用 `AWS_S3_HOST` 环境变量，因此我们可以直接定义一个包含 endpoint_url
    的字典，并将对象传递给 storage option 参数：
- en: '[PRE79]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: More sample configurations and documentation can be found at [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 更多示例配置和文档可以在[S3Fs文档](https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage)中找到。
- en: If you do *not* have S3 credentials, you can still access public data by specifying
    an anonymous connection, such as
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你*没有* S3 凭据，仍然可以通过指定匿名连接来访问公共数据，例如
- en: New in version 1.2.0.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 1.2.0 中新增。
- en: '[PRE80]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '`fsspec` also allows complex URLs, for accessing data in compressed archives,
    local caching of files, and more. To locally cache the above example, you would
    modify the call to'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsspec` 还允许复杂的 URL，用于访问压缩存档中的数据、文件的本地缓存等等。要在本地缓存上述示例，你需要修改调用为'
- en: '[PRE81]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: where we specify that the “anon” parameter is meant for the “s3” part of the
    implementation, not to the caching implementation. Note that this caches to a
    temporary directory for the duration of the session only, but you can also specify
    a permanent store.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里指定，“anon”参数是针对实现的“s3”部分，而不是缓存实现。请注意，这只会缓存到一个临时目录，只在会话期间有效，但你也可以指定一个永久存储。
- en: Writing out data
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写出数据
- en: '#### Writing to CSV format'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 写入 CSV 格式'
- en: The `Series` and `DataFrame` objects have an instance method `to_csv` which
    allows storing the contents of the object as a comma-separated-values file. The
    function takes a number of arguments. Only the first is required.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '`Series` 和 `DataFrame` 对象有一个实例方法 `to_csv`，它允许将对象的内容存储为逗号分隔值文件。该函数接受多个参数。只有第一个是必需的。'
- en: '`path_or_buf`: A string path to the file to write or a file object. If a file
    object it must be opened with `newline=''''`'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path_or_buf`：要写入的文件的字符串路径或文件对象。如果是文件对象，必须使用`newline=''''`打开它'
- en: '`sep` : Field delimiter for the output file (default “,”)'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep`：输出文件的字段分隔符（默认为“,”）'
- en: '`na_rep`: A string representation of a missing value (default ‘’)'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`na_rep`：缺失值的字符串表示（默认为‘’）'
- en: '`float_format`: Format string for floating point numbers'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float_format`：浮点数的格式字符串'
- en: '`columns`: Columns to write (default None)'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns`：要写入的列（默认为 None）'
- en: '`header`: Whether to write out the column names (default True)'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header`：是否写出列名（默认为 True）'
- en: '`index`: whether to write row (index) names (default True)'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`：是否写入行（索引）名称（默认为 True）'
- en: '`index_label`: Column label(s) for index column(s) if desired. If None (default),
    and `header` and `index` are True, then the index names are used. (A sequence
    should be given if the `DataFrame` uses MultiIndex).'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_label`: 如果需要，用于索引列的列标签。如果为 None（默认），并且 `header` 和 `index` 为 True，则使用索引名称。（如果
    `DataFrame` 使用 MultiIndex，则应给出一个序列）。'
- en: '`mode` : Python write mode, default ‘w’'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode` : Python 写入模式，默认为 ‘w’'
- en: '`encoding`: a string representing the encoding to use if the contents are non-ASCII,
    for Python versions prior to 3'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding`: 表示要使用的编码的字符串，如果内容为非 ASCII，则用于 Python 版本 3 之前'
- en: '`lineterminator`: Character sequence denoting line end (default `os.linesep`)'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lineterminator`: 表示行结束的字符序列（默认为 `os.linesep`）'
- en: '`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).
    Note that if you have set a `float_format` then floats are converted to strings
    and csv.QUOTE_NONNUMERIC will treat them as non-numeric'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quoting`: 设置引用规则，如 csv 模块（默认为 csv.QUOTE_MINIMAL）。请注意，如果设置了 `float_format`，则浮点数将被转换为字符串，csv.QUOTE_NONNUMERIC
    将将其视为非数值'
- en: '`quotechar`: Character used to quote fields (default ‘”’)'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quotechar`: 用于引用字段的字符（默认为 ‘”’）'
- en: '`doublequote`: Control quoting of `quotechar` in fields (default True)'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doublequote`: 控制字段中 `quotechar` 的引用（默认为 True）'
- en: '`escapechar`: Character used to escape `sep` and `quotechar` when appropriate
    (default None)'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`escapechar`: 用于适当时转义 `sep` 和 `quotechar` 的字符（默认为 None）'
- en: '`chunksize`: Number of rows to write at a time'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunksize`: 每次写入的行数'
- en: '`date_format`: Format string for datetime objects'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date_format`: 日期时间对象的格式字符串'
- en: Writing a formatted string
  id: totrans-452
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写入格式化字符串
- en: 'The `DataFrame` object has an instance method `to_string` which allows control
    over the string representation of the object. All arguments are optional:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame` 对象有一个实例方法 `to_string`，允许控制对象的字符串表示。所有参数都是可选的：'
- en: '`buf` default None, for example a StringIO object'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`buf` 默认为 None，例如一个 StringIO 对象'
- en: '`columns` default None, which columns to write'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns` 默认为 None，要写入的列'
- en: '`col_space` default None, minimum width of each column.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`col_space` 默认为 None，每列的最小宽度。'
- en: '`na_rep` default `NaN`, representation of NA value'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`na_rep` 默认为 `NaN`，NA 值的表示'
- en: '`formatters` default None, a dictionary (by column) of functions each of which
    takes a single argument and returns a formatted string'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`formatters` 默认为 None，一个字典（按列）的函数，每个函数接受一个参数并返回一个格式化的字符串'
- en: '`float_format` default None, a function which takes a single (float) argument
    and returns a formatted string; to be applied to floats in the `DataFrame`.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float_format` 默认为 None，一个接受单个（浮点数）参数并返回格式化字符串的函数；应用于 `DataFrame` 中的浮点数。'
- en: '`sparsify` default True, set to False for a `DataFrame` with a hierarchical
    index to print every MultiIndex key at each row.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparsify` 默认为 True，设置为 False 以在具有分层索引的 `DataFrame` 中打印每个行的每个 MultiIndex 键。'
- en: '`index_names` default True, will print the names of the indices'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_names` 默认为 True，将打印索引的名称'
- en: '`index` default True, will print the index (ie, row labels)'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index` 默认为 True，将打印索引（即，行标签）'
- en: '`header` default True, will print the column labels'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header` 默认为 True，将打印列标签'
- en: '`justify` default `left`, will print column headers left- or right-justified'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`justify` 默认为 `left`，将列标题左对齐或右对齐'
- en: 'The `Series` object also has a `to_string` method, but with only the `buf`,
    `na_rep`, `float_format` arguments. There is also a `length` argument which, if
    set to `True`, will additionally output the length of the Series.  ## JSON'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '`Series` 对象也有一个 `to_string` 方法，但只有 `buf`、`na_rep`、`float_format` 参数。还有一个 `length`
    参数，如果设置为 `True`，还会输出 Series 的长度。  ## JSON'
- en: Read and write `JSON` format files and strings.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和写入 `JSON` 格式文件和字符串。
- en: '### Writing JSON'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '### 写入 JSON'
- en: 'A `Series` or `DataFrame` can be converted to a valid JSON string. Use `to_json`
    with optional parameters:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将 `Series` 或 `DataFrame` 转��为有效的 JSON 字符串。使用 `to_json` 和可选参数：
- en: '`path_or_buf` : the pathname or buffer to write the output. This can be `None`
    in which case a JSON string is returned.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path_or_buf` : 要写入输出的路径名或缓冲区。如果为 `None`，则返回一个 JSON 字符串。'
- en: '`orient` :'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orient` :'
- en: '`Series`:'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Series`:'
- en: default is `index`
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认为 `index`
- en: allowed values are {`split`, `records`, `index`}
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许的值为 {`split`, `records`, `index`}
- en: '`DataFrame`:'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`DataFrame`:'
- en: default is `columns`
  id: totrans-475
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认为 `columns`
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许的值为 {`split`, `records`, `index`, `columns`, `values`, `table`}
- en: The format of the JSON string
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JSON 字符串的格式
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `split` | 类似字典 {index -> [index], columns -> [columns], data -> [values]}
    |'
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `records` | 类似列表 [{column -> value}, … , {column -> value}] |'
- en: '| `index` | dict like {index -> {column -> value}} |'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `index` | 类似字典 {index -> {column -> value}} |'
- en: '| `columns` | dict like {column -> {index -> value}} |'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `columns` | 类似于{column -> {index -> value}}的字典 |'
- en: '| `values` | just the values array |'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `values` | 仅值数组 |'
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `table` | 符合JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)的表格
    |'
- en: '`date_format` : string, type of date conversion, ‘epoch’ for timestamp, ‘iso’
    for ISO8601.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date_format`：日期转换类型，‘epoch’表示时间戳，‘iso’表示ISO8601。'
- en: '`double_precision` : The number of decimal places to use when encoding floating
    point values, default 10.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`double_precision`：编码浮点值时要使用的小数位数，默认为10。'
- en: '`force_ascii` : force encoded string to be ASCII, default True.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_ascii`：强制编码字符串为ASCII，默认为True。'
- en: '`date_unit` : The time unit to encode to, governs timestamp and ISO8601 precision.
    One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds
    respectively. Default ‘ms’.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date_unit`：要编码的时间单位，控制时间戳和ISO8601精度。其中之一为''s''、''ms''、''us''或''ns''，分别表示秒、毫秒、微秒和纳秒。默认为''ms''。'
- en: '`default_handler` : The handler to call if an object cannot otherwise be converted
    to a suitable format for JSON. Takes a single argument, which is the object to
    convert, and returns a serializable object.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default_handler`：如果对象无法以其他方式转换为适合JSON格式的格式，则调用的处理程序。接受一个参数，即要转换的对象，并返回一个可序列化的对象。'
- en: '`lines` : If `records` orient, then will write each record per line as json.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lines`：如果是`records`方向，则将每个记录写成一行json。'
- en: '`mode` : string, writer mode when writing to path. ‘w’ for write, ‘a’ for append.
    Default ‘w’'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`：写入路径时的字符串，写入模式。‘w’表示写入，‘a’表示追加。默认为‘w’'
- en: Note `NaN`’s, `NaT`’s and `None` will be converted to `null` and `datetime`
    objects will be converted based on the `date_format` and `date_unit` parameters.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`NaN`、`NaT`和`None`将被转换为`null`，而`datetime`对象将根据`date_format`和`date_unit`参数进行转换。
- en: '[PRE82]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Orient options
  id: totrans-493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方向选项
- en: 'There are a number of different options for the format of the resulting JSON
    file / string. Consider the following `DataFrame` and `Series`:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 对生成的JSON文件/字符串的格式有许多不同的选项。考虑以下`DataFrame`和`Series`：
- en: '[PRE83]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '**Column oriented** (the default for `DataFrame`) serializes the data as nested
    JSON objects with column labels acting as the primary index:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '**列导向**（`DataFrame`的默认值）将数据序列化为嵌套的JSON对象，其中列标签充当主要索引：'
- en: '[PRE84]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '**Index oriented** (the default for `Series`) similar to column oriented but
    the index labels are now primary:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '**索引导向**（`Series`的默认值）类似于列导向，但现在索引标签是主要的：'
- en: '[PRE85]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '**Record oriented** serializes the data to a JSON array of column -> value
    records, index labels are not included. This is useful for passing `DataFrame`
    data to plotting libraries, for example the JavaScript library `d3.js`:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '**记录导向**将数据序列化为列->值记录的JSON数组，不包括索引标签。这对于将`DataFrame`数据传递给绘图库非常有用，例如JavaScript库`d3.js`：'
- en: '[PRE86]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '**Value oriented** is a bare-bones option which serializes to nested JSON arrays
    of values only, column and index labels are not included:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '**值导向**是一个简单的选项，它将值仅序列化为嵌套的JSON值数组，不包括列和索引标签：'
- en: '[PRE87]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '**Split oriented** serializes to a JSON object containing separate entries
    for values, index and columns. Name is also included for `Series`:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '**拆分导向**序列化为包含值、索引和列的单独条目的JSON对象。对于`Series`，还包括名称：'
- en: '[PRE88]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/),
    allowing for the preservation of metadata including but not limited to dtypes
    and index names.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '**表导向**序列化为JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)，允许保留元数据，包括但不限于dtypes和索引名称。'
- en: Note
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Any orient option that encodes to a JSON object will not preserve the ordering
    of index and column labels during round-trip serialization. If you wish to preserve
    label ordering use the `split` option as it uses ordered containers.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 任何编码为JSON对象的方向选项在往返序列化期间不会保留索引和列标签的顺序。如果希望保留标签顺序，请使用`split`选项，因为它使用有序容器。
- en: Date handling
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 日期处理
- en: 'Writing in ISO date format:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 以ISO日期格式编写：
- en: '[PRE89]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Writing in ISO date format, with microseconds:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 以微秒为单位的ISO日期格式编写：
- en: '[PRE90]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Epoch timestamps, in seconds:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch时间戳，以秒为单位：
- en: '[PRE91]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Writing to a file, with a date index and a date column:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 写入文件，带有日期索引和日期列：
- en: '[PRE92]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Fallback behavior
  id: totrans-518
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 回退行为
- en: 'If the JSON serializer cannot handle the container contents directly it will
    fall back in the following manner:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 如果JSON序列化程序无法直接处理容器内容，则会以以下方式回退：
- en: if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,
    if provided, will be called for each value, otherwise an exception is raised.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果dtype不受支持（例如`np.complex_`），则会调用`default_handler`（如果提供）处理每个值，否则会引发异常。
- en: 'if an object is unsupported it will attempt the following:'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个对象不受支持，它将尝试以下操作：
- en: check if the object has defined a `toDict` method and call it. A `toDict` method
    should return a `dict` which will then be JSON serialized.
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查对象是否定义了`toDict`方法并调用它。`toDict`方法应返回一个将被 JSON 序列化的`dict`。
- en: ''
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-524
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: invoke the `default_handler` if one was provided.
  id: totrans-525
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果提供了一个，则调用`default_handler`。
- en: ''
  id: totrans-526
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-527
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: convert the object to a `dict` by traversing its contents. However this will
    often fail with an `OverflowError` or give unexpected results.
  id: totrans-528
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过遍历其内容将对象转换为`dict`。但是这通常会失败并出现`OverflowError`或给出意外结果。
- en: 'In general the best approach for unsupported objects or dtypes is to provide
    a `default_handler`. For example:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不受支持的对象或数据类型，通常最好的方法是提供一个`default_handler`。例如：
- en: '[PRE93]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'can be dealt with by specifying a simple `default_handler`:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过指定简单的`default_handler`来处理：
- en: '[PRE94]  ### Reading JSON'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE94]  ### 读取 JSON'
- en: Reading a JSON string to pandas object can take a number of parameters. The
    parser will try to parse a `DataFrame` if `typ` is not supplied or is `None`.
    To explicitly force `Series` parsing, pass `typ=series`
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 将 JSON 字符串读取到 pandas 对象可以使用多个参数。如果未提供或提供的是`None`，则解析器将尝试解析`DataFrame`。要明确强制解析`Series`，请传递`typ=series`。
- en: '`filepath_or_buffer` : a **VALID** JSON string or file handle / StringIO. The
    string could be a URL. Valid URL schemes include http, ftp, S3, and file. For
    file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filepath_or_buffer`：**有效**的 JSON 字符串或文件句柄 / StringIO。该字符串可以是 URL。有效的 URL 方案包括
    http、ftp、S3 和 file。对于文件 URL，预期有一个主机。例如，本地文件可以是 file ://localhost/path/to/table.json'
- en: '`typ` : type of object to recover (series or frame), default ‘frame’'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`typ`：要恢复的对象类型（series 或 frame），默认为‘frame’。'
- en: '`orient` :'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orient`：'
- en: 'Series :'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Series：
- en: default is `index`
  id: totrans-538
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认为`index`
- en: allowed values are {`split`, `records`, `index`}
  id: totrans-539
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许的值为{`split`，`records`，`index`}。
- en: DataFrame
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DataFrame
- en: default is `columns`
  id: totrans-541
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认为`columns`
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  id: totrans-542
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许的值为{`split`，`records`，`index`，`columns`，`values`，`table`}。
- en: The format of the JSON string
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JSON 字符串的格式
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `split` | 类似字典 {index -> [index]，columns -> [columns]，data -> [values]} |'
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `records` | 类似列表 [{column -> value}，…，{column -> value}] |'
- en: '| `index` | dict like {index -> {column -> value}} |'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `index` | 类似字典 {index -> {column -> value}} |'
- en: '| `columns` | dict like {column -> {index -> value}} |'
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `columns` | 类似字典 {column -> {index -> value}} |'
- en: '| `values` | just the values array |'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `values` | 仅值数组 |'
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| `table` | 符合 JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    的规范 |'
- en: '`dtype` : if True, infer dtypes, if a dict of column to dtype, then use those,
    if `False`, then don’t infer dtypes at all, default is True, apply only to the
    data.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`：如果为 True，则推断数据类型；如果是列到数据类型的字典，则使用它们；如果为`False`，则根本不推断数据类型，默认为 True，仅适用于数据。'
- en: '`convert_axes` : boolean, try to convert the axes to the proper dtypes, default
    is `True`'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`convert_axes`：布尔值，尝试将轴转换为正确的数据类型，默认为`True`。'
- en: '`convert_dates` : a list of columns to parse for dates; If `True`, then try
    to parse date-like columns, default is `True`.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`convert_dates`：要解析日期的列的列表；如果为`True`，则尝试解析类似日期的列，默认为`True`。'
- en: '`keep_default_dates` : boolean, default `True`. If parsing dates, then parse
    the default date-like columns.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_default_dates`：布尔值，默认为`True`。如果解析日期，则解析默认的类似日期的列。'
- en: '`precise_float` : boolean, default `False`. Set to enable usage of higher precision
    (strtod) function when decoding string to double values. Default (`False`) is
    to use fast but less precise builtin functionality.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`precise_float`：布尔值，默认为`False`。设置为启用更高精度（strtod）函数在将字符串解码为双精度值时的使用。默认（`False`）为使用快速但不太精确的内置功能。'
- en: '`date_unit` : string, the timestamp unit to detect if converting dates. Default
    None. By default the timestamp precision will be detected, if this is not desired
    then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force timestamp precision to seconds,
    milliseconds, microseconds or nanoseconds respectively.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date_unit`：字符串，用于检测日期转换的时间戳单位。默认为 None。默认情况下，将检测时间戳精度，如果不希望这样，则传递‘s’，‘ms’，‘us’或‘ns’中的一个来强制时间戳精度为秒，毫秒，微秒或纳秒。'
- en: '`lines` : reads file as one json object per line.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lines`：每行读取一个 json 对象。'
- en: '`encoding` : The encoding to use to decode py3 bytes.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding`：用于解码 py3 字节的编码。'
- en: '`chunksize` : when used in combination with `lines=True`, return a `pandas.api.typing.JsonReader`
    which reads in `chunksize` lines per iteration.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunksize`：与`lines=True`组合使用时，每次迭代读取`chunksize`行的`pandas.api.typing.JsonReader`。'
- en: '`engine`: Either `"ujson"`, the built-in JSON parser, or `"pyarrow"` which
    dispatches to pyarrow’s `pyarrow.json.read_json`. The `"pyarrow"` is only available
    when `lines=True`'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`engine`: 要么是 `"ujson"`，内置的 JSON 解析器，要么是 `"pyarrow"`，它会分派到 pyarrow 的 `pyarrow.json.read_json`。当
    `lines=True` 时，仅可用 `"pyarrow"`。'
- en: The parser will raise one of `ValueError/TypeError/AssertionError` if the JSON
    is not parseable.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 JSON 不可解析，解析器将引发 `ValueError/TypeError/AssertionError` 中的一个。
- en: If a non-default `orient` was used when encoding to JSON be sure to pass the
    same option here so that decoding produces sensible results, see [Orient Options](#orient-options)
    for an overview.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在编码为 JSON 时使用了非默认的 `orient`，请确保在此处传递相同的选项，以便解码产生合理的结果，请参阅 [Orient Options](#orient-options)
    获取概述。
- en: Data conversion
  id: totrans-562
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据转换
- en: The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will
    try to parse the axes, and all of the data into appropriate types, including dates.
    If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes`
    should only be set to `False` if you need to preserve string-like numbers (e.g.
    ‘1’, ‘2’) in an axes.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下 `convert_axes=True`、`dtype=True` 和 `convert_dates=True` 将尝试解析轴和所有数据为适当的类型，包括日期。如果需要覆盖特定的
    dtypes，请将字典传递给 `dtype`。只有在需要保留类似字符串的数字（例如 '1'、'2'）时，才应将 `convert_axes` 设置为 `False`。
- en: Note
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Large integer values may be converted to dates if `convert_dates=True` and
    the data and / or column labels appear ‘date-like’. The exact threshold depends
    on the `date_unit` specified. ‘date-like’ means that the column label meets one
    of the following criteria:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `convert_dates=True` 并且数据和/或列标签看起来像是日期，则大整数值可能会转换为日期。确切的阈值取决于指定的 `date_unit`。‘看起来像日期’
    意味着列标签符合以下标准之一：
- en: it ends with `'_at'`
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以 `'_at'` 结尾
- en: it ends with `'_time'`
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以 `'_time'` 结尾
- en: it begins with `'timestamp'`
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它以 `'timestamp'` 开始
- en: it is `'modified'`
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是 `'modified'`
- en: it is `'date'`
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是 `'date'`
- en: Warning
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: 'When reading JSON data, automatic coercing into dtypes has some quirks:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取 JSON 数据时，自动强制转换为 dtypes 会有一些怪异之处：
- en: an index can be reconstructed in a different order from serialization, that
    is, the returned order is not guaranteed to be the same as before serialization
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引可以以不同的顺序从序列化中重建，即，返回的顺序不能保证与序列化之前相同。
- en: a column that was `float` data will be converted to `integer` if it can be done
    safely, e.g. a column of `1.`
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果列是 `float` 数据，则如果安全的话会转换为 `integer`，例如列为 `1.`。
- en: bool columns will be converted to `integer` on reconstruction
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布尔列将在重建时转换为 `integer`
- en: Thus there are times where you may want to specify specific dtypes via the `dtype`
    keyword argument.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有时您可能希望通过 `dtype` 关键字参数指定特定的 dtypes。
- en: 'Reading from a JSON string:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 从 JSON 字符串中读取：
- en: '[PRE95]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Reading from a file:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 从文件中读取：
- en: '[PRE96]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Don’t convert any data (but still convert axes and dates):'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 不要转换任何数据（但仍然转换轴和日期）：
- en: '[PRE97]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Specify dtypes for conversion:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 指定转换的 dtypes：
- en: '[PRE98]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Preserve string indices:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 保留字符串索引：
- en: '[PRE99]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Dates written in nanoseconds need to be read back in nanoseconds:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 以纳秒写入的日期需要以纳秒读取：
- en: '[PRE100]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置 `dtype_backend` 参数，您可以控制用于结果 DataFrame 的默认 dtypes。
- en: '[PRE101]  ### Normalization'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE101]  ### 规范化'
- en: pandas provides a utility function to take a dict or list of dicts and *normalize*
    this semi-structured data into a flat table.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 提供了一个实用函数，可以接受字典或字典列表，并将这种半结构化数据 *规范化* 为一个平面表。
- en: '[PRE102]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: The max_level parameter provides more control over which level to end normalization.
    With max_level=1 the following snippet normalizes until 1st nesting level of the
    provided dict.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: max_level 参数提供了更多控制规范化结束的级别。使用 max_level=1 将规范化到所提供字典的第一个嵌套级别。
- en: '[PRE104]  ### Line delimited json'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE104]  ### 行分隔的 json'
- en: pandas is able to read and write line-delimited json files that are common in
    data processing pipelines using Hadoop or Spark.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 能够读取和写入行分隔的 JSON 文件，这在使用 Hadoop 或 Spark 进行数据处理的流水线中很常见。
- en: For line-delimited json files, pandas can also return an iterator which reads
    in `chunksize` lines at a time. This can be useful for large files or to read
    from a stream.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以行分隔的 JSON 文件，pandas 还可以返回一个迭代器，每次读取 `chunksize` 行。这对于大文件或从流中读取非常有用。
- en: '[PRE105]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Line-limited json can also be read using the pyarrow reader by specifying `engine="pyarrow"`.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过指定 `engine="pyarrow"` 来使用 pyarrow 读取行分隔的 json。
- en: '[PRE106]'
  id: totrans-600
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'New in version 2.0.0.  ### Table schema'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '新版本 2.0.0 中的新增功能。  ### 表模式'
- en: '[Table Schema](https://specs.frictionlessdata.io/table-schema/) is a spec for
    describing tabular datasets as a JSON object. The JSON includes information on
    the field names, types, and other attributes. You can use the orient `table` to
    build a JSON string with two fields, `schema` and `data`.'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table Schema](https://specs.frictionlessdata.io/table-schema/)是一种描述表格数据集的JSON对象的规范。JSON包括有关字段名称、类型和其他属性的信息。您可以使用`table`方向构建一个具有两个字段`schema`和`data`的JSON字符串。'
- en: '[PRE107]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: The `schema` field contains the `fields` key, which itself contains a list of
    column name to type pairs, including the `Index` or `MultiIndex` (see below for
    a list of types). The `schema` field also contains a `primaryKey` field if the
    (Multi)index is unique.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '`schema`字段包含`fields`键，它本身包含列名到类型对的列表，包括`Index`或`MultiIndex`（请参阅下面的类型列表）。如果（多）索引是唯一的，则`schema`字段还包含一个`primaryKey`字段。'
- en: The second field, `data`, contains the serialized data with the `records` orient.
    The index is included, and any datetimes are ISO 8601 formatted, as required by
    the Table Schema spec.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个字段`data`包含使用`records`方向序列化的数据。索引包括在内，任何日期时间都是ISO 8601格式，根据Table Schema规范的要求。
- en: 'The full list of types supported are described in the Table Schema spec. This
    table shows the mapping from pandas types:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的类型的完整列表在Table Schema规范中有描述。此表显示了从pandas类型的映射：
- en: '| pandas type | Table Schema type |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| pandas类型 | Table Schema类型 |'
- en: '| --- | --- |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| int64 | integer |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| int64 | integer |'
- en: '| float64 | number |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| float64 | number |'
- en: '| bool | boolean |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| bool | boolean |'
- en: '| datetime64[ns] | datetime |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| datetime64[ns] | datetime |'
- en: '| timedelta64[ns] | duration |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| timedelta64[ns] | duration |'
- en: '| categorical | any |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| categorical | any |'
- en: '| object | str |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| object | str |'
- en: 'A few notes on the generated table schema:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 有关生成的表模式的一些注意事项：
- en: The `schema` object contains a `pandas_version` field. This contains the version
    of pandas’ dialect of the schema, and will be incremented with each revision.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schema`对象包含一个`pandas_version`字段。这包含pandas模式的版本，并将随每个修订版递增。'
- en: All dates are converted to UTC when serializing. Even timezone naive values,
    which are treated as UTC with an offset of 0.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在序列化时，所有日期都转换为UTC。即使是时区无关的值，也被视为具有偏移量为0的UTC时间。
- en: '[PRE108]'
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: datetimes with a timezone (before serializing), include an additional field
    `tz` with the time zone name (e.g. `'US/Central'`).
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有时区的日期时间（在序列化之前），包含一个额外的字段`tz`，其中包含时区名称（例如`'US/Central'`）。
- en: '[PRE109]'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Periods are converted to timestamps before serialization, and so have the same
    behavior of being converted to UTC. In addition, periods will contain and additional
    field `freq` with the period’s frequency, e.g. `'A-DEC'`.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在序列化之前，将周期转换为时间戳，因此具有被转换为UTC的相同行为。此外，周期将包含一个额外的字段`freq`，其中包含周期的频率，例如`'A-DEC'`。
- en: '[PRE110]'
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Categoricals use the `any` type and an `enum` constraint listing the set of
    possible values. Additionally, an `ordered` field is included:'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类使用`any`类型和列出可能值集合的`enum`约束。此外，还包括一个`ordered`字段：
- en: '[PRE111]'
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'A `primaryKey` field, containing an array of labels, is included *if the index
    is unique*:'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果索引是唯一的，则包含一个包含标签数组的`primaryKey`字段：
- en: '[PRE112]'
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'The `primaryKey` behavior is the same with MultiIndexes, but in this case the
    `primaryKey` is an array:'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`primaryKey`的行为与MultiIndexes相同，但在这种情况下，`primaryKey`是一个数组：'
- en: '[PRE113]'
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'The default naming roughly follows these rules:'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认命名大致遵循以下规则：
- en: For series, the `object.name` is used. If that’s none, then the name is `values`
  id: totrans-631
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于系列，使用`object.name`。如果没有，则名称为`values`。
- en: ''
  id: totrans-632
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-633
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `DataFrames`, the stringified version of the column name is used
  id: totrans-634
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`DataFrames`，使用列名的字符串版本。
- en: ''
  id: totrans-635
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-636
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `Index` (not `MultiIndex`), `index.name` is used, with a fallback to `index`
    if that is None.
  id: totrans-637
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`Index`（而不是`MultiIndex`），使用`index.name`，如果为None，则使用`index`。
- en: ''
  id: totrans-638
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-639
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `MultiIndex`, `mi.names` is used. If any level has no name, then `level_<i>`
    is used.
  id: totrans-640
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`MultiIndex`，使用`mi.names`。如果任何级别没有名称，则使用`level_<i>`。
- en: '`read_json` also accepts `orient=''table''` as an argument. This allows for
    the preservation of metadata such as dtypes and index names in a round-trippable
    manner.'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_json`还接受`orient=''table''`作为参数。这样可以以往返的方式保留元数据，如数据类型和索引名称。'
- en: '[PRE114]'
  id: totrans-642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Please note that the literal string ‘index’ as the name of an [`Index`](../reference/api/pandas.Index.html#pandas.Index
    "pandas.Index") is not round-trippable, nor are any names beginning with `'level_'`
    within a [`MultiIndex`](../reference/api/pandas.MultiIndex.html#pandas.MultiIndex
    "pandas.MultiIndex"). These are used by default in [`DataFrame.to_json()`](../reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json
    "pandas.DataFrame.to_json") to indicate missing values and the subsequent read
    cannot distinguish the intent.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，作为 [`Index`](../reference/api/pandas.Index.html#pandas.Index "pandas.Index")
    的名称的文字字符串 'index' 不具有往返性，[`MultiIndex`](../reference/api/pandas.MultiIndex.html#pandas.MultiIndex
    "pandas.MultiIndex") 中以 'level_' 开头的任何名称也是如此。这些在 [`DataFrame.to_json()`](../reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json
    "pandas.DataFrame.to_json") 中默认用于指示缺失值，随后的读取无法区分意图。
- en: '[PRE115]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: When using `orient='table'` along with user-defined `ExtensionArray`, the generated
    schema will contain an additional `extDtype` key in the respective `fields` element.
    This extra key is not standard but does enable JSON roundtrips for extension types
    (e.g. `read_json(df.to_json(orient="table"), orient="table")`).
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `orient='table'` 以及用户定义的 `ExtensionArray` 时，生成的模式将在相应的 `fields` 元素中包含一个额外的
    `extDtype` 键。这个额外的键不是标准的，但确实可以为扩展类型（例如 `read_json(df.to_json(orient="table"),
    orient="table")`）启用 JSON 往返。
- en: The `extDtype` key carries the name of the extension, if you have properly registered
    the `ExtensionDtype`, pandas will use said name to perform a lookup into the registry
    and re-convert the serialized data into your custom dtype.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已正确注册了 `ExtensionDtype`，那么`extDtype`键将携带扩展名的名称，pandas 将使用该名称进行查找并将序列化的数据重新转换为您的自定义
    dtype。
- en: HTML
  id: totrans-647
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HTML
- en: '### Reading HTML content'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '### 读取 HTML 内容'
- en: Warning
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: We **highly encourage** you to read the [HTML Table Parsing gotchas](#io-html-gotchas)
    below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 我们**强烈建议**您阅读下面关于 BeautifulSoup4/html5lib/lxml 解析器的 [HTML 表格解析陷阱](#io-html-gotchas)。
- en: The top-level `read_html()` function can accept an HTML string/file/URL and
    will parse HTML tables into list of pandas `DataFrames`. Let’s look at a few examples.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 顶级的 `read_html()` 函数可以接受一个 HTML 字符串/文件/URL，并将 HTML 表格解析为 pandas `DataFrame`
    的列表。让我们看一些例子。
- en: Note
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`read_html` returns a `list` of `DataFrame` objects, even if there is only
    a single table contained in the HTML content.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_html` 返回一个 `DataFrame` 对象的 `list`，即使在 HTML 内容中只包含一个表格。'
- en: 'Read a URL with no options:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 无选项读取 URL：
- en: '[PRE116]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Note
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The data from the above URL changes every Monday so the resulting data above
    may be slightly different.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 URL 的数据每个星期一都会更改，因此上面生成的数据可能会略有不同。
- en: 'Read a URL while passing headers alongside the HTTP request:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HTTP 请求中传递标题时读取 URL：
- en: '[PRE117]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Note
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We see above that the headers we passed are reflected in the HTTP request.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到上面我们传递的标题反映在 HTTP 请求中。
- en: 'Read in the content of the file from the above URL and pass it to `read_html`
    as a string:'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述 URL 中读取文件内容，并将其作为字符串传递给 `read_html`：
- en: '[PRE118]'
  id: totrans-663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'You can even pass in an instance of `StringIO` if you so desire:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，甚至可以传递一个 `StringIO` 的实例：
- en: '[PRE119]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Note
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The following examples are not run by the IPython evaluator due to the fact
    that having so many network-accessing functions slows down the documentation build.
    If you spot an error or an example that doesn’t run, please do not hesitate to
    report it over on [pandas GitHub issues page](https://github.com/pandas-dev/pandas/issues).
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 由于具有如此多的网络访问功能会减慢文档构建速度，因此 IPython 评估器未运行以下示例。如果您发现错误或无法运行的示例，请毫不犹豫地在 [pandas
    GitHub 问题页面](https://github.com/pandas-dev/pandas/issues) 上报告。
- en: 'Read a URL and match a table that contains specific text:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 读取一个包含特定文本的表格的 URL：
- en: '[PRE120]'
  id: totrans-669
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Specify a header row (by default `<th>` or `<td>` elements located within a
    `<thead>` are used to form the column index, if multiple rows are contained within
    `<thead>` then a MultiIndex is created); if specified, the header row is taken
    from the data minus the parsed header elements (`<th>` elements).
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 指定一个标题行（默认情况下，`<thead>` 中的 `<th>` 或 `<td>` 元素用于形成列索引，如果 `<thead>` 中包含多行，则会创建一个
    MultiIndex）；如果指定了，则标题行取自数据减去已解析的标题元素（`<th>` 元素）。
- en: '[PRE121]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Specify an index column:'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 指定一个索引列：
- en: '[PRE122]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Specify a number of rows to skip:'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 指定要跳过的行数：
- en: '[PRE123]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Specify a number of rows to skip using a list (`range` works as well):'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表指定要跳过的行数（`range` 也适用）：
- en: '[PRE124]'
  id: totrans-677
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Specify an HTML attribute:'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 指定 HTML 属性：
- en: '[PRE125]'
  id: totrans-679
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Specify values that should be converted to NaN:'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 指定应转换为 NaN 的值：
- en: '[PRE126]'
  id: totrans-681
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Specify whether to keep the default set of NaN values:'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 指定是否保留默认的 NaN 值集合：
- en: '[PRE127]'
  id: totrans-683
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: Specify converters for columns. This is useful for numerical text data that
    has leading zeros. By default columns that are numerical are cast to numeric types
    and the leading zeros are lost. To avoid this, we can convert these columns to
    strings.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 为列指定转换器。这对于具有前导零的数值文本数据非常有用。默认情况下，数值列会转换为数值类型，前导零会丢失。为了避免这种情况，我们可以将这些列转换为字符串。
- en: '[PRE128]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Use some combination of the above:'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述某种组合：
- en: '[PRE129]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Read in pandas `to_html` output (with some loss of floating point precision):'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 pandas `to_html` 输出（会损失浮点数精度）：
- en: '[PRE130]'
  id: totrans-689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'The `lxml` backend will raise an error on a failed parse if that is the only
    parser you provide. If you only have a single parser you can provide just a string,
    but it is considered good practice to pass a list with one string if, for example,
    the function expects a sequence of strings. You may use:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `lxml` 后端在提供唯一解析器的情况下解析失败，则会引发错误。如果您只有一个解析器，可以只提供一个字符串，但是，如果函数期望一个字符串序列，那么传递一个包含一个字符串的列表被认为是一种良好的做法。您可以使用：
- en: '[PRE131]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Or you could pass `flavor=''lxml''` without a list:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您可以不带列表传递 `flavor='lxml'`：
- en: '[PRE132]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: However, if you have bs4 and html5lib installed and pass `None` or `['lxml',
    'bs4']` then the parse will most likely succeed. Note that *as soon as a parse
    succeeds, the function will return*.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你已经安装了 bs4 和 html5lib，并且传递了 `None` 或 `['lxml', 'bs4']`，那么解析很可能会成功。请注意，*一旦解析成功，函数将立即返回*。
- en: '[PRE133]'
  id: totrans-695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: Links can be extracted from cells along with the text using `extract_links="all"`.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `extract_links="all"` 从单元格中提取链接和文本。
- en: '[PRE134]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'New in version 1.5.0.  ### Writing to HTML files'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '版本 1.5.0 中的新功能。  ### 写入 HTML 文件'
- en: '`DataFrame` objects have an instance method `to_html` which renders the contents
    of the `DataFrame` as an HTML table. The function arguments are as in the method
    `to_string` described above.'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame` 对象具有一个实例方法 `to_html`，它将 `DataFrame` 的内容呈现为 HTML 表格。函数参数与上面描述的 `to_string`
    方法相同。'
- en: Note
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Not all of the possible options for `DataFrame.to_html` are shown here for brevity’s
    sake. See `DataFrame.to_html()` for the full set of options.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，这里没有显示 `DataFrame.to_html` 的所有可能选项。有关完整选项集，请参阅 `DataFrame.to_html()`。
- en: Note
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In an HTML-rendering supported environment like a Jupyter Notebook, `display(HTML(...))``
    will render the raw HTML into the environment.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持 HTML 渲染的环境（如 Jupyter Notebook）中，`display(HTML(...))`` 将把原始 HTML 渲染到环境中。
- en: '[PRE135]'
  id: totrans-704
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'The `columns` argument will limit the columns shown:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '`columns` 参数将限制显示的列：'
- en: '[PRE136]'
  id: totrans-706
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '`float_format` takes a Python callable to control the precision of floating
    point values:'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: '`float_format` 接受一个 Python 可调用对象来控制浮点数值的精度：'
- en: '[PRE137]'
  id: totrans-708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '`bold_rows` will make the row labels bold by default, but you can turn that
    off:'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '`bold_rows` 默认会使行标签加粗，但你可以关闭这个选项：'
- en: '[PRE138]'
  id: totrans-710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: The `classes` argument provides the ability to give the resulting HTML table
    CSS classes. Note that these classes are *appended* to the existing `'dataframe'`
    class.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '`classes` 参数提供了为生成的 HTML 表格添加 CSS 类的功能。请注意，这些类会 *追加* 到现有的 `''dataframe''` 类中。'
- en: '[PRE139]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: The `render_links` argument provides the ability to add hyperlinks to cells
    that contain URLs.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '`render_links` 参数提供了向包含 URL 的单元格添加超链接的功能。'
- en: '[PRE140]'
  id: totrans-714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: Finally, the `escape` argument allows you to control whether the “<”, “>” and
    “&” characters escaped in the resulting HTML (by default it is `True`). So to
    get the HTML without escaped characters pass `escape=False`
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`escape` 参数允许您控制结果 HTML 中是否转义 “<”，“>” 和 “&” 字符（默认情况下为 `True`）。因此，要获得不带转义字符的
    HTML，请传递 `escape=False`。
- en: '[PRE141]'
  id: totrans-716
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'Escaped:'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 已转义：
- en: '[PRE142]'
  id: totrans-718
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: 'Not escaped:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 未转义：
- en: '[PRE143]'
  id: totrans-720
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: Note
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Some browsers may not show a difference in the rendering of the previous two
    HTML tables.  ### HTML Table Parsing Gotchas'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '一些浏览器可能无法显示前两个 HTML 表格的渲染差异。  ### HTML 表格解析的陷阱'
- en: There are some versioning issues surrounding the libraries that are used to
    parse HTML tables in the top-level pandas io function `read_html`.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析顶级 pandas io 函数 `read_html` 中用于解析 HTML 表格的库的版本存在一些问题。
- en: '**Issues with** [**lxml**](https://lxml.de)'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '**与** [**lxml**](https://lxml.de) **有关的问题**'
- en: Benefits
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点
- en: '[**lxml**](https://lxml.de) is very fast.'
  id: totrans-726
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**lxml**](https://lxml.de) 非常快速。'
- en: ''
  id: totrans-727
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-728
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) requires Cython to install correctly.'
  id: totrans-729
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**lxml**](https://lxml.de) 需要正确安装 Cython 才能安装。'
- en: Drawbacks
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺点
- en: '[**lxml**](https://lxml.de) does *not* make any guarantees about the results
    of its parse *unless* it is given [**strictly valid markup**](https://validator.w3.org/docs/help.html#validation_basics).'
  id: totrans-731
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**lxml**](https://lxml.de) 在没有提供 [**严格有效的标记**](https://validator.w3.org/docs/help.html#validation_basics)
    的情况下，*不* 对其解析结果做出任何保证。'
- en: ''
  id: totrans-732
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-733
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: In light of the above, we have chosen to allow you, the user, to use the [**lxml**](https://lxml.de)
    backend, but **this backend will use** [**html5lib**](https://github.com/html5lib/html5lib-python)
    if [**lxml**](https://lxml.de) fails to parse
  id: totrans-734
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 综上所述，我们选择允许您，用户，使用[**lxml**](https://lxml.de)后端，但是**如果**[**lxml**](https://lxml.de)无法解析，则将使用[**html5lib**](https://github.com/html5lib/html5lib-python)。
- en: ''
  id: totrans-735
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-736
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: It is therefore *highly recommended* that you install both [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    and [**html5lib**](https://github.com/html5lib/html5lib-python), so that you will
    still get a valid result (provided everything else is valid) even if [**lxml**](https://lxml.de)
    fails.
  id: totrans-737
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，强烈建议您安装[**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)和[**html5lib**](https://github.com/html5lib/html5lib-python)，这样即使[**lxml**](https://lxml.de)失败，您仍将获得有效结果（假设其他一切有效）。
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**lxml**](https://lxml.de) **as a backend**'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup) **使用**[**lxml**](https://lxml.de)
    **作为后端的问题**
- en: The above issues hold here as well since [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    is essentially just a wrapper around a parser backend.
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于[**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)本质上只是一个围绕解析器后端的包装器，因此上述问题在这里同样存在。
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**html5lib**](https://github.com/html5lib/html5lib-python) **as a backend**'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup) **使用**[**html5lib**](https://github.com/html5lib/html5lib-python)
    **作为后端的问题**
- en: Benefits
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is far more lenient
    than [**lxml**](https://lxml.de) and consequently deals with *real-life markup*
    in a much saner way rather than just, e.g., dropping an element without notifying
    you.'
  id: totrans-742
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**html5lib**](https://github.com/html5lib/html5lib-python)比[**lxml**](https://lxml.de)宽容得多，因此以更理智的方式处理*现实中的标记*，而不仅仅是，例如，删除一个元素而不通知您。'
- en: ''
  id: totrans-743
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-744
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) *generates valid
    HTML5 markup from invalid markup automatically*. This is extremely important for
    parsing HTML tables, since it guarantees a valid document. However, that does
    NOT mean that it is “correct”, since the process of fixing markup does not have
    a single definition.'
  id: totrans-745
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**html5lib**](https://github.com/html5lib/html5lib-python) *会自动从无效标记生成有效的
    HTML5 标记*。这对于解析 HTML 表格非常重要，因为它保证了一个有效的文档。但是，这并不意味着它是“正确的”，因为修复标记的过程没有一个单一的定义。'
- en: ''
  id: totrans-746
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-747
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is pure Python
    and requires no additional build steps beyond its own installation.'
  id: totrans-748
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**html5lib**](https://github.com/html5lib/html5lib-python)是纯 Python 的，除了自己的安装之外，不需要额外的构建步骤。'
- en: Drawbacks
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺点
- en: 'The biggest drawback to using [**html5lib**](https://github.com/html5lib/html5lib-python)
    is that it is slow as molasses. However consider the fact that many tables on
    the web are not big enough for the parsing algorithm runtime to matter. It is
    more likely that the bottleneck will be in the process of reading the raw text
    from the URL over the web, i.e., IO (input-output). For very large tables, this
    might not be true.  ## LaTeX'
  id: totrans-750
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用[**html5lib**](https://github.com/html5lib/html5lib-python)的最大缺点是速度极慢。但是请考虑到许多网页上的表格都不足以使解析算法运行时间成为问题。更可能的是瓶颈将出现在通过网络从
    URL 读取原始文本的过程中，即 IO（输入输出）。对于非常大的表格，这可能不成立。## LaTeX
- en: New in version 1.3.0.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 在版本 1.3.0 中新增。
- en: Currently there are no methods to read from LaTeX, only output methods.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 目前没有从 LaTeX 读取的方法，只有输出方法。
- en: Writing to LaTeX files
  id: totrans-753
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写到 LaTeX 文件
- en: Note
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: DataFrame *and* Styler objects currently have a `to_latex` method. We recommend
    using the [Styler.to_latex()](../reference/api/pandas.io.formats.style.Styler.to_latex.html)
    method over [DataFrame.to_latex()](../reference/api/pandas.DataFrame.to_latex.html)
    due to the former’s greater flexibility with conditional styling, and the latter’s
    possible future deprecation.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 和 Styler 对象目前具有`to_latex`方法。我们建议使用[Styler.to_latex()](../reference/api/pandas.io.formats.style.Styler.to_latex.html)方法，因为它在条件样式方面更灵活，而后者可能会在将来被弃用。
- en: Review the documentation for [Styler.to_latex](../reference/api/pandas.io.formats.style.Styler.to_latex.html),
    which gives examples of conditional styling and explains the operation of its
    keyword arguments.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 请查阅[Styler.to_latex](../reference/api/pandas.io.formats.style.Styler.to_latex.html)的文档，其中提供了条件样式的示例并解释了其关键字参数的操作。
- en: For simple application the following pattern is sufficient.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的应用程序，以下模式已足够。
- en: '[PRE144]'
  id: totrans-758
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: To format values before output, chain the [Styler.format](../reference/api/pandas.io.formats.style.Styler.format.html)
    method.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 要在输出之前格式化值，请链式调用 [Styler.format](../reference/api/pandas.io.formats.style.Styler.format.html)
    方法。
- en: '[PRE145]'
  id: totrans-760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: XML
  id: totrans-761
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XML
- en: '### Reading XML'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '### 读取 XML'
- en: New in version 1.3.0.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 1.3.0 中的新功能。
- en: The top-level `read_xml()` function can accept an XML string/file/URL and will
    parse nodes and attributes into a pandas `DataFrame`.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 顶级的 `read_xml()` 函数可以接受 XML 字符串/文件/URL，并将节点和属性解析到 pandas 的 `DataFrame` 中。
- en: Note
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since there is no standard XML structure where design types can vary in many
    ways, `read_xml` works best with flatter, shallow versions. If an XML document
    is deeply nested, use the `stylesheet` feature to transform XML into a flatter
    version.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有标准的 XML 结构，设计类型可以以多种方式变化，`read_xml` 最适用于较平坦、较浅的版本。如果 XML 文档嵌套层级较深，则使用 `stylesheet`
    功能将 XML 转换为较平坦的版本。
- en: Let’s look at a few examples.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看几个例子。
- en: 'Read an XML string:'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 XML 字符串：
- en: '[PRE146]'
  id: totrans-769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Read a URL with no options:'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 读取无选项的 URL：
- en: '[PRE147]'
  id: totrans-771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'Read in the content of the “books.xml” file and pass it to `read_xml` as a
    string:'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 “books.xml” 文件的内容并将其作为字符串传递给 `read_xml`：
- en: '[PRE148]'
  id: totrans-773
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'Read in the content of the “books.xml” as instance of `StringIO` or `BytesIO`
    and pass it to `read_xml`:'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 将 “books.xml” 的内容读取为 `StringIO` 或 `BytesIO` 实例，并将其传递给 `read_xml`：
- en: '[PRE149]'
  id: totrans-775
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-776
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Even read XML from AWS S3 buckets such as NIH NCBI PMC Article Datasets providing
    Biomedical and Life Science Jorurnals:'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至可以从 AWS S3 存储桶读取 XML，例如 NIH NCBI PMC Article Datasets，提供生物医学和生命科学期刊：
- en: '[PRE151]'
  id: totrans-778
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'With [lxml](https://lxml.de) as default `parser`, you access the full-featured
    XML library that extends Python’s ElementTree API. One powerful tool is ability
    to query nodes selectively or conditionally with more expressive XPath:'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [lxml](https://lxml.de) 作为默认的 `parser`，您可以访问扩展了 Python 的 ElementTree API
    的功能齐全的 XML 库。其中一个强大的工具是能够使用更具表达力的 XPath 有选择地或有条件地查询节点：
- en: '[PRE152]'
  id: totrans-780
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'Specify only elements or only attributes to parse:'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 仅指定要解析的元素或属性：
- en: '[PRE153]'
  id: totrans-782
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: XML documents can have namespaces with prefixes and default namespaces without
    prefixes both of which are denoted with a special attribute `xmlns`. In order
    to parse by node under a namespace context, `xpath` must reference a prefix.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: XML 文档可以具有带有前缀的命名空间和不带前缀的默认命名空间，两者都用特殊属性 `xmlns` 表示。为了在命名空间上下文中按节点解析，`xpath`
    必须引用一个前缀。
- en: For example, below XML contains a namespace with prefix, `doc`, and URI at `https://example.com`.
    In order to parse `doc:row` nodes, `namespaces` must be used.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面的 XML 包含一个带有前缀 `doc` 和 URI 为 `https://example.com` 的命名空间。为了解析 `doc:row`
    节点，必须使用 `namespaces`。
- en: '[PRE155]'
  id: totrans-786
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: Similarly, an XML document can have a default namespace without prefix. Failing
    to assign a temporary prefix will return no nodes and raise a `ValueError`. But
    assigning *any* temporary name to correct URI allows parsing by nodes.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，XML 文档可以具有没有前缀的默认命名空间。未分配临时前缀将返回零个节点并引发 `ValueError`。但是，分配 *任何* 临时名称以更正
    URI 允许按节点解析。
- en: '[PRE156]'
  id: totrans-788
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: However, if XPath does not reference node names such as default, `/*`, then
    `namespaces` is not required.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果 XPath 不引用默认的节点名称，例如 `/*`，则不需要 `namespaces`。
- en: Note
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since `xpath` identifies the parent of content to be parsed, only immediate
    desendants which include child nodes or current attributes are parsed. Therefore,
    `read_xml` will not parse the text of grandchildren or other descendants and will
    not parse attributes of any descendant. To retrieve lower level content, adjust
    xpath to lower level. For example,
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `xpath` 标识要解析的内容的父级，因此仅解析包含子节点或当前属性的直接后代。因此，`read_xml` 将不会解析孙子节点或其他后代的文本，并且不会解析任何后代的属性。要检索更低级别的内容，请将
    xpath 调整为更低级别。例如，
- en: '[PRE157]'
  id: totrans-792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: shows the attribute `sides` on `shape` element was not parsed as expected since
    this attribute resides on the child of `row` element and not `row` element itself.
    In other words, `sides` attribute is a grandchild level descendant of `row` element.
    However, the `xpath` targets `row` element which covers only its children and
    attributes.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 显示在 `shape` 元素上的属性 `sides` 未按预期解析，因为此属性位于 `row` 元素的子节点而不是 `row` 元素本身。换句话说，`sides`
    属性是 `row` 元素的孙级后代。但是，`xpath` 目标是 `row` 元素，仅涵盖其子节点和属性。
- en: With [lxml](https://lxml.de) as parser, you can flatten nested XML documents
    with an XSLT script which also can be string/file/URL types. As background, [XSLT](https://www.w3.org/TR/xslt/)
    is a special-purpose language written in a special XML file that can transform
    original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using
    an XSLT processor.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [lxml](https://lxml.de) 作为解析器，您可以使用 XSLT 脚本展平嵌套的 XML 文档，该脚本也可以是字符串/文件/URL
    类型。作为背景，[XSLT](https://www.w3.org/TR/xslt/) 是一种特殊用途的语言，写在一个特殊的 XML 文件中，可以使用 XSLT
    处理器将原始 XML 文档转换为其他 XML、HTML，甚至文本（CSV、JSON 等）。
- en: 'For example, consider this somewhat nested structure of Chicago “L” Rides where
    station and rides elements encapsulate data in their own sections. With below
    XSLT, `lxml` can transform original nested document into a flatter output (as
    shown below for demonstration) for easier parse into `DataFrame`:'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑芝加哥“L”列车的稍微嵌套的结构，其中 `station` 和 `rides` 元素将数据封装在各自的部分中。使用下面的 XSLT，`lxml`
    可以将原始的嵌套文档转换为更扁平的输出（如下所示，仅用于演示），以便更容易解析为 `DataFrame`：
- en: '[PRE158]'
  id: totrans-796
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: For very large XML files that can range in hundreds of megabytes to gigabytes,
    [`pandas.read_xml()`](../reference/api/pandas.read_xml.html#pandas.read_xml "pandas.read_xml")
    supports parsing such sizeable files using [lxml’s iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk)
    and [etree’s iterparse](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse)
    which are memory-efficient methods to iterate through an XML tree and extract
    specific elements and attributes. without holding entire tree in memory.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常大的 XML 文件，其大小可能在几百兆字节到几十个字节之间，[`pandas.read_xml()`](../reference/api/pandas.read_xml.html#pandas.read_xml
    "pandas.read_xml") 支持使用 [lxml 的 iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk)
    和 [etree 的 iterparse](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse)
    解析这些庞大文件，并且这些方法是内存高效的方法，可以遍历 XML 树并提取特定的元素和属性，而无需将整个树保留在内存中。
- en: New in version 1.5.0.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 新功能，版本 1.5.0。
- en: To use this feature, you must pass a physical XML file path into `read_xml`
    and use the `iterparse` argument. Files should not be compressed or point to online
    sources but stored on local disk. Also, `iterparse` should be a dictionary where
    the key is the repeating nodes in document (which become the rows) and the value
    is a list of any element or attribute that is a descendant (i.e., child, grandchild)
    of repeating node. Since XPath is not used in this method, descendants do not
    need to share same relationship with one another. Below shows example of reading
    in Wikipedia’s very large (12 GB+) latest article data dump.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此功能，必须将物理 XML 文件路径传递给 `read_xml` 并使用 `iterparse` 参数。文件不应该被压缩或指向在线源，而应存储在本地磁盘上。此外，`iterparse`
    应该是一个字典，其中键是文档中的重复节点（它们成为行），值是任何重复节点的后代（即，子节点、孙子节点）的元素或属性的列表。由于此方法不使用 XPath，因此后代不需要彼此共享相同的关系。下面显示了读取维基百科非常大（12
    GB+）的最新文章数据转储的示例。
- en: '[PRE159]  ### Writing XML'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE159]  ### 编写 XML'
- en: New in version 1.3.0.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 新功能，版本 1.3.0。
- en: '`DataFrame` objects have an instance method `to_xml` which renders the contents
    of the `DataFrame` as an XML document.'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame` 对象具有一个名为 `to_xml` 的实例方法，它将 `DataFrame` 的内容呈现为 XML 文档。'
- en: Note
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This method does not support special properties of XML including DTD, CData,
    XSD schemas, processing instructions, comments, and others. Only namespaces at
    the root level is supported. However, `stylesheet` allows design changes after
    initial output.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法不支持 XML 的特殊属性，包括 DTD、CData、XSD 模式、处理指令、注释等。只支持根级别的命名空间。但是，`stylesheet` 允许在初始输出之后进行设计更改。
- en: Let’s look at a few examples.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看几个示例。
- en: 'Write an XML without options:'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 编写没有选项的 XML：
- en: '[PRE160]'
  id: totrans-807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'Write an XML with new root and row name:'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 编写具有新根和行名称的 XML：
- en: '[PRE161]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'Write an attribute-centric XML:'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 编写基于属性的 XML：
- en: '[PRE162]'
  id: totrans-811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'Write a mix of elements and attributes:'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 编写混合元素和属性：
- en: '[PRE163]'
  id: totrans-813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'Any `DataFrames` with hierarchical columns will be flattened for XML element
    names with levels delimited by underscores:'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 任何具有分层列的 `DataFrame` 将被展平为以下划线分隔的 XML 元素名称：
- en: '[PRE164]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'Write an XML with default namespace:'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认命名空间编写 XML：
- en: '[PRE165]'
  id: totrans-817
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'Write an XML with namespace prefix:'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命名空间前缀编写 XML：
- en: '[PRE166]'
  id: totrans-819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'Write an XML without declaration or pretty print:'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 编写没有声明或美化打印的 XML：
- en: '[PRE167]'
  id: totrans-821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: 'Write an XML and transform with stylesheet:'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 编写 XML 并使用样式表进行转换：
- en: '[PRE168]'
  id: totrans-823
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: XML Final Notes
  id: totrans-824
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XML 最终注意事项
- en: All XML documents adhere to [W3C specifications](https://www.w3.org/TR/xml/).
    Both `etree` and `lxml` parsers will fail to parse any markup document that is
    not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document
    unless it follows XHTML specs. However, other popular markup types including KML,
    XAML, RSS, MusicML, MathML are compliant [XML schemas](https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas).
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有XML文档遵循[W3C规范](https://www.w3.org/TR/xml/)。`etree`和`lxml`解析器将无法解析任何不符合规范或遵循XML语法规则的标记文档。请注意，除非遵循XHTML规范，否则HTML不是XML文档。然而，其他流行的标记类型，包括KML、XAML、RSS、MusicML、MathML都符合[XML模式](https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas)。
- en: For above reason, if your application builds XML prior to pandas operations,
    use appropriate DOM libraries like `etree` and `lxml` to build the necessary document
    and not by string concatenation or regex adjustments. Always remember XML is a
    *special* text file with markup rules.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于上述原因，如果您的应用在pandas操作之前构建XML，请使用适当的DOM库（如`etree`和`lxml`）构建必要的文档，而不是通过字符串连接或正则表达式调��。请始终记住，XML是一个带有标记规则的*特殊*文本文件。
- en: With very large XML files (several hundred MBs to GBs), XPath and XSLT can become
    memory-intensive operations. Be sure to have enough available RAM for reading
    and writing to large XML files (roughly about 5 times the size of text).
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非常大的XML文件（几百MB到GB），XPath和XSLT可能会成为占用大量内存的操作。确保有足够的可用RAM来读取和写入大型XML文件（大约是文本大小的5倍）。
- en: Because XSLT is a programming language, use it with caution since such scripts
    can pose a security risk in your environment and can run large or infinite recursive
    operations. Always test scripts on small fragments before full run.
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为XSLT是一种编程语言，请谨慎使用，因为这样的脚本可能在您的环境中构成安全风险，并且可能运行大型或无限递归操作。始终在小片段上测试脚本，然后再进行完整运行。
- en: The [etree](https://docs.python.org/3/library/xml.etree.elementtree.html) parser
    supports all functionality of both `read_xml` and `to_xml` except for complex
    XPath and any XSLT. Though limited in features, `etree` is still a reliable and
    capable parser and tree builder. Its performance may trail `lxml` to a certain
    degree for larger files but relatively unnoticeable on small to medium size files.
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[etree](https://docs.python.org/3/library/xml.etree.elementtree.html)解析器支持`read_xml`和`to_xml`的所有功能，除了复杂的XPath和任何XSLT。尽管功能有限，`etree`仍然是一个可靠且功能强大的解析器和树构建器。对于较大的文件，其性能可能略逊于`lxml`，但在小到中等大小的文件上相对不易察觉。'
- en: '## Excel files'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: '## Excel文件'
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read Excel 2007+ (`.xlsx`) files using the `openpyxl`
    Python module. Excel 2003 (`.xls`) files can be read using `xlrd`. Binary Excel
    (`.xlsb`) files can be read using `pyxlsb`. All formats can be read using [calamine](#io-calamine)
    engine. The [`to_excel()`](../reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel
    "pandas.DataFrame.to_excel") instance method is used for saving a `DataFrame`
    to Excel. Generally the semantics are similar to working with [csv](#io-read-csv-table)
    data. See the [cookbook](cookbook.html#cookbook-excel) for some advanced strategies.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel")方法可以使用`openpyxl` Python模块读取Excel 2007+（.xlsx）文件。可以使用`xlrd`读取Excel
    2003（.xls）文件。可以使用`pyxlsb`读取二进制Excel（.xlsb）文件。所有格式都可以使用[calamine](#io-calamine)引擎读取。[`to_excel()`](../reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel
    "pandas.DataFrame.to_excel")实例方法用于将`DataFrame`保存到Excel。通常语义与处理[csv](#io-read-csv-table)数据类似。有关一些高级策略，请参阅[cookbook](cookbook.html#cookbook-excel)。'
- en: Note
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'When `engine=None`, the following logic will be used to determine the engine:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 当`engine=None`时，将使用以下逻辑确定引擎：
- en: If `path_or_buffer` is an OpenDocument format (.odf, .ods, .odt), then [odf](https://pypi.org/project/odfpy/)
    will be used.
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`path_or_buffer`是OpenDocument格式（.odf，.ods，.odt），那么将使用[odf](https://pypi.org/project/odfpy/)。
- en: Otherwise if `path_or_buffer` is an xls format, `xlrd` will be used.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，如果`path_or_buffer`是xls格式，则将使用`xlrd`。
- en: Otherwise if `path_or_buffer` is in xlsb format, `pyxlsb` will be used.
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，如果`path_or_buffer`是xlsb格式，则将使用`pyxlsb`。
- en: Otherwise `openpyxl` will be used.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则将使用`openpyxl`。
- en: '### Reading Excel files'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '### 读取Excel文件'
- en: In the most basic use-case, `read_excel` takes a path to an Excel file, and
    the `sheet_name` indicating which sheet to parse.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本的用例中，`read_excel`接受Excel文件的路径，以及指示要解析哪个工作表的`sheet_name`。
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`engine_kwargs`参数时，pandas将这些参数传递给引擎。因此，重要的是要知道pandas内部使用的函数。
- en: For the engine openpyxl, pandas is using `openpyxl.load_workbook()` to read
    in (`.xlsx`) and (`.xlsm`) files.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于引擎openpyxl，pandas使用`openpyxl.load_workbook()`来读取（.xlsx）和（.xlsm）文件。
- en: For the engine xlrd, pandas is using `xlrd.open_workbook()` to read in (`.xls`)
    files.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于引擎xlrd，pandas使用`xlrd.open_workbook()`来读取（.xls）文件。
- en: For the engine pyxlsb, pandas is using `pyxlsb.open_workbook()` to read in (`.xlsb`)
    files.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于引擎pyxlsb，pandas使用`pyxlsb.open_workbook()`来读取（.xlsb）文件。
- en: For the engine odf, pandas is using `odf.opendocument.load()` to read in (`.ods`)
    files.
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于引擎odf，pandas使用`odf.opendocument.load()`来读取（.ods）文件。
- en: For the engine calamine, pandas is using `python_calamine.load_workbook()` to
    read in (`.xlsx`), (`.xlsm`), (`.xls`), (`.xlsb`), (`.ods`) files.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于引擎calamine，pandas使用`python_calamine.load_workbook()`来读取（.xlsx）、（.xlsm）、（.xls）、（.xlsb）、（.ods）文件。
- en: '[PRE169]'
  id: totrans-846
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '#### `ExcelFile` class'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `ExcelFile`类'
- en: To facilitate working with multiple sheets from the same file, the `ExcelFile`
    class can be used to wrap the file and can be passed into `read_excel` There will
    be a performance benefit for reading multiple sheets as the file is read into
    memory only once.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便处理同一文件中的多个工作表，可以使用`ExcelFile`类来包装文件，并可以将其传递给`read_excel`。读取多个工作表时将获得性能优势，因为文件只会读入内存一次。
- en: '[PRE170]'
  id: totrans-849
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: The `ExcelFile` class can also be used as a context manager.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExcelFile`类也可以用作上下文管理器。'
- en: '[PRE171]'
  id: totrans-851
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: The `sheet_names` property will generate a list of the sheet names in the file.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: '`sheet_names`属性将生成文件中工作表名称的列表。'
- en: 'The primary use-case for an `ExcelFile` is parsing multiple sheets with different
    parameters:'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExcelFile`的主要用例是使用不同参数解析多个工作表：'
- en: '[PRE172]'
  id: totrans-854
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: Note that if the same parsing parameters are used for all sheets, a list of
    sheet names can simply be passed to `read_excel` with no loss in performance.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果所有工作表都使用相同的解析参数，则可以简单地将工作表名称列表传递给`read_excel`，而不会降低性能。
- en: '[PRE173]'
  id: totrans-856
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.
    This allows the user to control how the excel file is read. For example, sheets
    can be loaded on demand by calling `xlrd.open_workbook()` with `on_demand=True`.'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExcelFile`也可以使用`xlrd.book.Book`对象作为参数调用。这允许用户控制如何读取Excel文件。例如，可以通过调用`xlrd.open_workbook()`并使用`on_demand=True`来按需加载工作表。'
- en: '[PRE174]  #### Specifying sheets'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE174]  #### 指定工作表'
- en: Note
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是`sheet_name`，不要与`ExcelFile.sheet_names`混淆。
- en: Note
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: An ExcelFile’s attribute `sheet_names` provides access to a list of sheets.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: ExcelFile的属性`sheet_names`提供对工作表列表的访问。
- en: The arguments `sheet_name` allows specifying the sheet or sheets to read.
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数`sheet_name`允许指定要读取的工作表。
- en: The default value for `sheet_name` is 0, indicating to read the first sheet
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数`sheet_name`的默认值为0，表示读取第一个工作表
- en: Pass a string to refer to the name of a particular sheet in the workbook.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递一个字符串来引用工作簿中特定工作表的名称。
- en: Pass an integer to refer to the index of a sheet. Indices follow Python convention,
    beginning at 0.
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递一个整数来引用工作表的索引。索引遵循Python约定，从0开始。
- en: Pass a list of either strings or integers, to return a dictionary of specified
    sheets.
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递一个字符串或整数列表，返回指定工作表的字典。
- en: Pass a `None` to return a dictionary of all available sheets.
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递`None`返回所有可用工作表的字典。
- en: '[PRE175]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: 'Using the sheet index:'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工作表索引：
- en: '[PRE176]'
  id: totrans-871
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'Using all default values:'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有默认值：
- en: '[PRE177]'
  id: totrans-873
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'Using None to get all sheets:'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 使用None获取所有工作表：
- en: '[PRE178]'
  id: totrans-875
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: 'Using a list to get multiple sheets:'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表获取多个工作表：
- en: '[PRE179]'
  id: totrans-877
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '`read_excel` can read more than one sheet, by setting `sheet_name` to either
    a list of sheet names, a list of sheet positions, or `None` to read all sheets.
    Sheets can be specified by sheet index or sheet name, using an integer or string,
    respectively.  #### Reading a `MultiIndex`'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_excel`可以通过将`sheet_name`设置为工作表名称列表、工作表位置列表或`None`来读取多个工作表。可以通过工作表索引或工作表名称指定工作表，分别使用整数或字符串。  ####
    读取`MultiIndex`'
- en: '`read_excel` can read a `MultiIndex` index, by passing a list of columns to
    `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If
    either the `index` or `columns` have serialized level names those will be read
    in as well by specifying the rows/columns that make up the levels.'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_excel`可以通过将列列表传递给`index_col`和将行列表传递给`header`来读取`MultiIndex`索引。如果`index`或`columns`具有序列化级别名称，也可以通过指定构成级别的行/列来读取这些级别。'
- en: 'For example, to read in a `MultiIndex` index without names:'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要读取没有名称的`MultiIndex`索引：
- en: '[PRE180]'
  id: totrans-881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: If the index has level names, they will parsed as well, using the same parameters.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 如果索引具有级别名称，则将使用相同的参数进行解析。
- en: '[PRE181]'
  id: totrans-883
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'If the source file has both `MultiIndex` index and columns, lists specifying
    each should be passed to `index_col` and `header`:'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 如果源文件既有 `MultiIndex` 索引又有列，则应将分别指定的列表传递给 `index_col` 和 `header`：
- en: '[PRE182]'
  id: totrans-885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: Missing values in columns specified in `index_col` will be forward filled to
    allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward
    filling the missing values use `set_index` after reading the data instead of `index_col`.
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定了 `index_col` 中的列中的缺失值将被向前填充，以允许使用 `to_excel` 的 `merged_cells=True` 进行往返。为了避免向前填充缺失值，请在读取数据后使用
    `set_index` 而不是 `index_col`。
- en: Parsing specific columns
  id: totrans-887
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解析特定列
- en: It is often the case that users will insert columns to do temporary computations
    in Excel and you may not want to read in those columns. `read_excel` takes a `usecols`
    keyword to allow you to specify a subset of columns to parse.
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Excel 中，用户经常会插入列进行临时计算，而您可能不想读取这些列。`read_excel` 接受一个 `usecols` 关键字，允许您指定要解析的列的子集。
- en: 'You can specify a comma-delimited set of Excel columns and ranges as a string:'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将逗号分隔的一组 Excel 列和范围指定为字符串：
- en: '[PRE183]'
  id: totrans-890
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: If `usecols` is a list of integers, then it is assumed to be the file column
    indices to be parsed.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `usecols` 是一个整数列表，则假定它是要解析的文件列索引。
- en: '[PRE184]'
  id: totrans-892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 元素顺序被忽略，因此 `usecols=[0, 1]` 与 `[1, 0]` 相同。
- en: 'If `usecols` is a list of strings, it is assumed that each string corresponds
    to a column name provided either by the user in `names` or inferred from the document
    header row(s). Those strings define which columns will be parsed:'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `usecols` 是一个字符串列表，则假定每个字符串对应于用户在 `names` 中提供的列名或从文档标题行中推断出的列名。这些字符串定义了将要解析的列：
- en: '[PRE185]'
  id: totrans-895
  prefs: []
  type: TYPE_PRE
  zh: '[PRE185]'
- en: Element order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',
    'baz']`.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 元素顺��被忽略，因此 `usecols=['baz', 'joe']` 与 `['joe', 'baz']` 相同。
- en: If `usecols` is callable, the callable function will be evaluated against the
    column names, returning names where the callable function evaluates to `True`.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `usecols` 是可调用的，则将对列名评估可调用函数，返回可调用函数评估为 `True` 的列名。
- en: '[PRE186]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE186]'
- en: Parsing dates
  id: totrans-899
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解析日期
- en: 'Datetime-like values are normally automatically converted to the appropriate
    dtype when reading the excel file. But if you have a column of strings that *look*
    like dates (but are not actually formatted as dates in excel), you can use the
    `parse_dates` keyword to parse those strings to datetimes:'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 当读取 Excel 文件时，类似日期时间的值通常会自动转换为适当的 dtype。但是，如果您有一列看起来像日期的字符串（但实际上在 Excel 中没有格式化为日期），您可以使用
    `parse_dates` 关键字将这些字符串解析为日期时间：
- en: '[PRE187]'
  id: totrans-901
  prefs: []
  type: TYPE_PRE
  zh: '[PRE187]'
- en: Cell converters
  id: totrans-902
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单元格转换器
- en: 'It is possible to transform the contents of Excel cells via the `converters`
    option. For instance, to convert a column to boolean:'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 `converters` 选项转换 Excel 单元格的内容。例如，要将列转换为布尔值：
- en: '[PRE188]'
  id: totrans-904
  prefs: []
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'This options handles missing values and treats exceptions in the converters
    as missing data. Transformations are applied cell by cell rather than to the column
    as a whole, so the array dtype is not guaranteed. For instance, a column of integers
    with missing values cannot be transformed to an array with integer dtype, because
    NaN is strictly a float. You can manually mask missing data to recover integer
    dtype:'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项处理缺失值，并将转换器中的异常视为缺失数据。转换是逐个单元格应用的，而不是整个列，因此不能保证数组 dtype。例如，具有缺失值的整数列无法转换为具有整数
    dtype 的数组，因为 NaN 严格是浮点数。您可以手动屏蔽缺失数据以恢复整数 dtype：
- en: '[PRE189]'
  id: totrans-906
  prefs: []
  type: TYPE_PRE
  zh: '[PRE189]'
- en: Dtype specifications
  id: totrans-907
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dtype 规范
- en: As an alternative to converters, the type for an entire column can be specified
    using the `dtype` keyword, which takes a dictionary mapping column names to types.
    To interpret data with no type inference, use the type `str` or `object`.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 作为转换器的替代方案，可以使用 `dtype` 关键字指定整个列的类型，它接受一个将列名映射到类型的字典。要解释没有类型推断的数据，请使用类型 `str`
    或 `object`。
- en: '[PRE190]  ### Writing Excel files'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE190]  ### 写入 Excel 文件'
- en: Writing Excel files to disk
  id: totrans-910
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将 Excel 文件写入磁盘
- en: 'To write a `DataFrame` object to a sheet of an Excel file, you can use the
    `to_excel` instance method. The arguments are largely the same as `to_csv` described
    above, the first argument being the name of the excel file, and the optional second
    argument the name of the sheet to which the `DataFrame` should be written. For
    example:'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 `DataFrame` 对象写入 Excel 文件的一个工作表中，可以使用 `to_excel` 实例方法。参数与上面描述的 `to_csv` 大致相同，第一个参数是
    Excel 文件的名称，可选的第二个参数是应将 `DataFrame` 写入的工作表的名称。例如：
- en: '[PRE191]'
  id: totrans-912
  prefs: []
  type: TYPE_PRE
  zh: '[PRE191]'
- en: Files with a `.xlsx` extension will be written using `xlsxwriter` (if available)
    or `openpyxl`.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 具有 `.xlsx` 扩展名的文件将使用 `xlsxwriter`（如果可用）或 `openpyxl` 进行写入。
- en: 'The `DataFrame` will be written in a way that tries to mimic the REPL output.
    The `index_label` will be placed in the second row instead of the first. You can
    place it in the first row by setting the `merge_cells` option in `to_excel()`
    to `False`:'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`将以尽量模仿REPL输出的方式写入。`index_label`将放在第二行而不是第一行。您可以通过将`to_excel()`中的`merge_cells`选项设置为`False`将其放在第一行。'
- en: '[PRE192]'
  id: totrans-915
  prefs: []
  type: TYPE_PRE
  zh: '[PRE192]'
- en: In order to write separate `DataFrames` to separate sheets in a single Excel
    file, one can pass an `ExcelWriter`.
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将单独的`DataFrame`写入单个Excel文件的不同工作表中，可以传递一个`ExcelWriter`。
- en: '[PRE193]'
  id: totrans-917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE193]'
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`engine_kwargs`参数时，pandas将这些参数传递给引擎。因此，重要的是要知道pandas内部使用的是哪个函数。
- en: For the engine openpyxl, pandas is using `openpyxl.Workbook()` to create a new
    sheet and `openpyxl.load_workbook()` to append data to an existing sheet. The
    openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于引擎openpyxl，pandas使用`openpyxl.Workbook()`创建一个新工作表，使用`openpyxl.load_workbook()`将数据追加到现有工作表。openpyxl引擎写入（`.xlsx`）和（`.xlsm`）文件。
- en: For the engine xlsxwriter, pandas is using `xlsxwriter.Workbook()` to write
    to (`.xlsx`) files.
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于引擎xlsxwriter，pandas使用`xlsxwriter.Workbook()`写入（`.xlsx`）文件。
- en: For the engine odf, pandas is using `odf.opendocument.OpenDocumentSpreadsheet()`
    to write to (`.ods`) files.
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于引擎odf，pandas使用`odf.opendocument.OpenDocumentSpreadsheet()`写入（`.ods`）文件。
- en: Writing Excel files to memory
  id: totrans-922
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将Excel文件写入内存
- en: pandas supports writing Excel files to buffer-like objects such as `StringIO`
    or `BytesIO` using `ExcelWriter`.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: pandas支持将Excel文件写入类似缓冲区的对象，如`StringIO`或`BytesIO`，使用`ExcelWriter`。
- en: '[PRE194]'
  id: totrans-924
  prefs: []
  type: TYPE_PRE
  zh: '[PRE194]'
- en: Note
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`engine` is optional but recommended. Setting the engine determines the version
    of workbook produced. Setting `engine=''xlrd''` will produce an Excel 2003-format
    workbook (xls). Using either `''openpyxl''` or `''xlsxwriter''` will produce an
    Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook
    is produced.  ### Excel writer engines'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: '`engine`是可选的但建议设置。设置引擎确定生成的工作簿版本。设置`engine=''xlrd''`将生成一个Excel 2003格式的工作簿（xls）。使用`''openpyxl''`或`''xlsxwriter''`将生成一个Excel
    2007格式的工作簿（xlsx）。如果省略，将生成一个Excel 2007格式的工作簿。### Excel写入器引擎'
- en: 'pandas chooses an Excel writer via two methods:'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: pandas通过两种方法选择Excel写入器：
- en: the `engine` keyword argument
  id: totrans-928
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`engine`关键字参数'
- en: the filename extension (via the default specified in config options)
  id: totrans-929
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件扩展名（通过配置选项中指定的默认值）
- en: By default, pandas uses the [XlsxWriter](https://xlsxwriter.readthedocs.io)
    for `.xlsx`, [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsm`. If you
    have multiple engines installed, you can set the default engine through [setting
    the config options](options.html#options) `io.excel.xlsx.writer` and `io.excel.xls.writer`.
    pandas will fall back on [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsx`
    files if [Xlsxwriter](https://xlsxwriter.readthedocs.io) is not available.
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，pandas使用[XlsxWriter](https://xlsxwriter.readthedocs.io)用于`.xlsx`，[openpyxl](https://openpyxl.readthedocs.io/)用于`.xlsm`。如果安装了多个引擎，可以通过[设置配置选项](options.html#options)`io.excel.xlsx.writer`和`io.excel.xls.writer`来设置默认引擎。如果[Xlsxwriter](https://xlsxwriter.readthedocs.io)不可用，pandas将回退到[openpyxl](https://openpyxl.readthedocs.io/)用于`.xlsx`文件。
- en: 'To specify which writer you want to use, you can pass an engine keyword argument
    to `to_excel` and to `ExcelWriter`. The built-in engines are:'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 要指定要使用的写入器，可以将引擎关键字参数传递给`to_excel`和`ExcelWriter`。内置引擎有：
- en: '`openpyxl`: version 2.4 or higher is required'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openpyxl`：需要2.4或更高版本'
- en: '`xlsxwriter`'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xlsxwriter`'
- en: '[PRE195]  ### Style and formatting'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE195]### 样式和格式'
- en: The look and feel of Excel worksheets created from pandas can be modified using
    the following parameters on the `DataFrame`’s `to_excel` method.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`DataFrame`的`to_excel`方法上的以下参数修改从pandas创建的Excel工作表的外观和感觉。
- en: '`float_format` : Format string for floating point numbers (default `None`).'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float_format`：浮点数的格式字符串（默认为`None`）。'
- en: '`freeze_panes` : A tuple of two integers representing the bottommost row and
    rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will
    freeze the first row and first column (default `None`).'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`freeze_panes`：表示要冻结的最底行和最右列的两个整数的元组。这些参数都是基于一的，因此(1, 1)将冻结第一行和第一列（默认为`None`）。'
- en: 'Using the [Xlsxwriter](https://xlsxwriter.readthedocs.io) engine provides many
    options for controlling the format of an Excel worksheet created with the `to_excel`
    method. Excellent examples can be found in the [Xlsxwriter](https://xlsxwriter.readthedocs.io)
    documentation here: [https://xlsxwriter.readthedocs.io/working_with_pandas.html](https://xlsxwriter.readthedocs.io/working_with_pandas.html)  ##
    OpenDocument Spreadsheets'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 [Xlsxwriter](https://xlsxwriter.readthedocs.io) 引擎提供了许多控制使用 `to_excel` 方法创建的
    Excel 工作表格式的选项。在 [Xlsxwriter](https://xlsxwriter.readthedocs.io) 文档中可以找到出色的示例：[https://xlsxwriter.readthedocs.io/working_with_pandas.html](https://xlsxwriter.readthedocs.io/working_with_pandas.html)  ##
    OpenDocument 电子表格'
- en: The io methods for [Excel files](#excel-files) also support reading and writing
    OpenDocument spreadsheets using the [odfpy](https://pypi.org/project/odfpy/) module.
    The semantics and features for reading and writing OpenDocument spreadsheets match
    what can be done for [Excel files](#excel-files) using `engine='odf'`. The optional
    dependency ‘odfpy’ needs to be installed.
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: '[Excel 文件](#excel-files) 的 io 方法还支持使用 [odfpy](https://pypi.org/project/odfpy/)
    模块读取和写入 OpenDocument 电子表格。读取和写入 OpenDocument 电子表格���语义和功能与使用 `engine=''odf''`
    可以为 [Excel 文件](#excel-files) 做的事情相匹配。需要安装可选依赖‘odfpy’。'
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read OpenDocument spreadsheets
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") 方法可以读取 OpenDocument 电子表格'
- en: '[PRE196]'
  id: totrans-941
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: Similarly, the `to_excel()` method can write OpenDocument spreadsheets
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`to_excel()` 方法可以写入 OpenDocument 电子表格
- en: '[PRE197]  ## Binary Excel (.xlsb) files'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE197]  ## 二进制 Excel（.xlsb）文件'
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can also read binary Excel files using the `pyxlsb`
    module. The semantics and features for reading binary Excel files mostly match
    what can be done for [Excel files](#excel-files) using `engine='pyxlsb'`. `pyxlsb`
    does not recognize datetime types in files and will return floats instead (you
    can use [calamine](#io-calamine) if you need recognize datetime types).
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") 方法还可以使用 `pyxlsb` 模块读取二进制 Excel 文件。读取二进制 Excel 文件的语义和功能大部分与使用
    `engine=''pyxlsb''` 可以为 [Excel 文件](#excel-files) 做的事情相匹配。`pyxlsb` 不识别文件中的日期时间类型，而会返回浮点数（如果需要识别日期时间类型，可以使用
    [calamine](#io-calamine)）。'
- en: '[PRE198]'
  id: totrans-945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE198]'
- en: Note
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Currently pandas only supports *reading* binary Excel files. Writing is not
    implemented.  ## Calamine (Excel and ODS files)'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: '目前 pandas 只支持*读取*二进制 Excel 文件。写入尚未实现。  ## Calamine（Excel 和 ODS 文件）'
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read Excel file (`.xlsx`, `.xlsm`, `.xls`, `.xlsb`)
    and OpenDocument spreadsheets (`.ods`) using the `python-calamine` module. This
    module is a binding for Rust library [calamine](https://crates.io/crates/calamine)
    and is faster than other engines in most cases. The optional dependency ‘python-calamine’
    needs to be installed.
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") 方法可以使用 `python-calamine` 模块读取 Excel 文件（`.xlsx`, `.xlsm`,
    `.xls`, `.xlsb`）和 OpenDocument 电子表格（`.ods`）。该模块是 Rust 库 [calamine](https://crates.io/crates/calamine)
    的绑定，大多数情况下比其他引擎更快。需要安装可选依赖‘python-calamine’。'
- en: '[PRE199]  ## Clipboard'
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE199]  ## 剪贴板'
- en: 'A handy way to grab data is to use the `read_clipboard()` method, which takes
    the contents of the clipboard buffer and passes them to the `read_csv` method.
    For instance, you can copy the following text to the clipboard (CTRL-C on many
    operating systems):'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 抓取数据的一个方便方法是使用 `read_clipboard()` 方法，它获取剪贴板缓冲区的内容并将其传递给 `read_csv` 方法。例如，您可以将以下文本复制到剪贴板（在许多操作系统上为
    CTRL-C）：
- en: '[PRE200]'
  id: totrans-951
  prefs: []
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'And then import the data directly to a `DataFrame` by calling:'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过调用以下方式直接将数据导入到 `DataFrame` 中：
- en: '[PRE201]'
  id: totrans-953
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: The `to_clipboard` method can be used to write the contents of a `DataFrame`
    to the clipboard. Following which you can paste the clipboard contents into other
    applications (CTRL-V on many operating systems). Here we illustrate writing a
    `DataFrame` into clipboard and reading it back.
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_clipboard` 方法可用于将 `DataFrame` 的内容写入剪贴板。然后您可以将剪贴板内容粘贴到其他应用程序中（在许多操作系统上为
    CTRL-V）。这里我们演示将 `DataFrame` 写入剪贴板并读取回来。'
- en: '[PRE202]'
  id: totrans-955
  prefs: []
  type: TYPE_PRE
  zh: '[PRE202]'
- en: We can see that we got the same content back, which we had earlier written to
    the clipboard.
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们得到了之前写入剪贴板的相同内容。
- en: Note
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux
    to use these methods.  ## Pickling'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Linux 上，您可能需要安装 xclip 或 xsel（与 PyQt5、PyQt4 或 qtpy 一起）才能使用这些方法。  ## Pickling'
- en: All pandas objects are equipped with `to_pickle` methods which use Python’s
    `cPickle` module to save data structures to disk using the pickle format.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 所有pandas对象都配备有`to_pickle`方法，使用Python的`cPickle`模块将数据结构保存到磁盘使用pickle格式。
- en: '[PRE203]'
  id: totrans-960
  prefs: []
  type: TYPE_PRE
  zh: '[PRE203]'
- en: 'The `read_pickle` function in the `pandas` namespace can be used to load any
    pickled pandas object (or any other pickled object) from file:'
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`命名空间中的`read_pickle`函数可用于从文件加载任何pickled pandas对象（或任何其他pickled对象）：'
- en: '[PRE204]'
  id: totrans-962
  prefs: []
  type: TYPE_PRE
  zh: '[PRE204]'
- en: Warning
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Loading pickled data received from untrusted sources can be unsafe.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 从不受信任的来源接收pickled数据可能不安全。
- en: 'See: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 参见：[https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)
- en: Warning
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle") is only guaranteed backwards compatible back to a few minor
    release.'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle") 仅向后兼容到几个次要版本。'
- en: '### Compressed pickle files'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '### 压缩的pickle文件'
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle"), [`DataFrame.to_pickle()`](../reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle
    "pandas.DataFrame.to_pickle") and [`Series.to_pickle()`](../reference/api/pandas.Series.to_pickle.html#pandas.Series.to_pickle
    "pandas.Series.to_pickle") can read and write compressed pickle files. The compression
    types of `gzip`, `bz2`, `xz`, `zstd` are supported for reading and writing. The
    `zip` file format only supports reading and must contain only one data file to
    be read.'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle")、[`DataFrame.to_pickle()`](../reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle
    "pandas.DataFrame.to_pickle")和[`Series.to_pickle()`](../reference/api/pandas.Series.to_pickle.html#pandas.Series.to_pickle
    "pandas.Series.to_pickle")可以读取和写入压缩的pickle文件。支持`gzip`、`bz2`、`xz`、`zstd`的压缩类型用于读取和写入。`zip`文件格式仅支持读取，且必须只包含一个要读取的数据文件。'
- en: The compression type can be an explicit parameter or be inferred from the file
    extension. If ‘infer’, then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename
    ends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩类型可以是一个显式参数，也可以从文件扩展名中推断出来。如果是‘infer’，则在文件名以`'.gz'`、`'.bz2'`、`'.zip'`、`'.xz'`或`'.zst'`结尾时使用`gzip`、`bz2`、`zip`、`xz`或`zstd`。
- en: The compression parameter can also be a `dict` in order to pass options to the
    compression protocol. It must have a `'method'` key set to the name of the compression
    protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`, `'xz'`, `'zstd'`}.
    All other key-value pairs are passed to the underlying compression library.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩参数也可以是一个`dict`，以便传递选项给压缩协议。必须有一个设置为压缩协议名称的`'method'`键，必须是{`'zip'`、`'gzip'`、`'bz2'`、`'xz'`、`'zstd'`}之一。所有其他键值对都传递给底层压缩库。
- en: '[PRE205]'
  id: totrans-972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'Using an explicit compression type:'
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 使用显式压缩类型：
- en: '[PRE206]'
  id: totrans-974
  prefs: []
  type: TYPE_PRE
  zh: '[PRE206]'
- en: 'Inferring compression type from the extension:'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 从扩展名推断压缩类型：
- en: '[PRE207]'
  id: totrans-976
  prefs: []
  type: TYPE_PRE
  zh: '[PRE207]'
- en: 'The default is to ‘infer’:'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 默认为‘infer’：
- en: '[PRE208]'
  id: totrans-978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE208]'
- en: 'Passing options to the compression protocol in order to speed up compression:'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 传递选项给压缩协议以加快压缩速度：
- en: '[PRE209]  ## msgpack'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE209]  ## msgpack'
- en: pandas support for `msgpack` has been removed in version 1.0.0\. It is recommended
    to use [pickle](#io-pickle) instead.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: pandas在1.0.0版本中移除了对`msgpack`的支持。建议改用[pickle](#io-pickle)。
- en: 'Alternatively, you can also the Arrow IPC serialization format for on-the-wire
    transmission of pandas objects. For documentation on pyarrow, see [here](https://arrow.apache.org/docs/python/ipc.html).  ##
    HDF5 (PyTables)'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: '或者，您也可以使用Arrow IPC序列化格式来传输pandas对象。有关pyarrow的文档，请参见[这里](https://arrow.apache.org/docs/python/ipc.html)。  ##
    HDF5（PyTables）'
- en: '`HDFStore` is a dict-like object which reads and writes pandas using the high
    performance HDF5 format using the excellent [PyTables](https://www.pytables.org/)
    library. See the [cookbook](cookbook.html#cookbook-hdf) for some advanced strategies'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: '`HDFStore`是一个类似字典的对象，使用高性能HDF5格式读写pandas，使用优秀的[PyTables](https://www.pytables.org/)库。查看[cookbook](cookbook.html#cookbook-hdf)了解一些高级策略'
- en: Warning
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: pandas uses PyTables for reading and writing HDF5 files, which allows serializing
    object-dtype data with pickle. Loading pickled data received from untrusted sources
    can be unsafe.
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: pandas使用PyTables来读写HDF5文件，允许使用pickle序列化对象数据。从不受信任的来源接收pickled数据可能不安全。
- en: 'See: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)
    for more.'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息请参见：[https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)。
- en: '[PRE210]'
  id: totrans-987
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: 'Objects can be written to the file just like adding key-value pairs to a dict:'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 对象可以像向字典添加键值对一样写入文件：
- en: '[PRE211]'
  id: totrans-989
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: 'In a current or later Python session, you can retrieve stored objects:'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前或以后的Python会话中，您可以检索存储的对象：
- en: '[PRE212]'
  id: totrans-991
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: 'Deletion of the object specified by the key:'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 删除由键指定的对象：
- en: '[PRE213]'
  id: totrans-993
  prefs: []
  type: TYPE_PRE
  zh: '[PRE213]'
- en: 'Closing a Store and using a context manager:'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭存储并使用上下文管理器：
- en: '[PRE214]'
  id: totrans-995
  prefs: []
  type: TYPE_PRE
  zh: '[PRE214]'
- en: Read/write API
  id: totrans-996
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读/写API
- en: '`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf`
    for writing, similar to how `read_csv` and `to_csv` work.'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: '`HDFStore`支持使用`read_hdf`进行读取和`to_hdf`进行写入的顶级API，类似于`read_csv`和`to_csv`的工作方式。'
- en: '[PRE215]'
  id: totrans-998
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: HDFStore will by default not drop rows that are all missing. This behavior can
    be changed by setting `dropna=True`.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: HDFStore默认情况下不会删除所有缺失的行。可以通过设置`dropna=True`来更改此行为。
- en: '[PRE216]'
  id: totrans-1000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '### Fixed format'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: '### 固定格式'
- en: The examples above show storing using `put`, which write the HDF5 to `PyTables`
    in a fixed array format, called the `fixed` format. These types of stores are
    **not** appendable once written (though you can simply remove them and rewrite).
    Nor are they **queryable**; they must be retrieved in their entirety. They also
    do not support dataframes with non-unique column names. The `fixed` format stores
    offer very fast writing and slightly faster reading than `table` stores. This
    format is specified by default when using `put` or `to_hdf` or by `format='fixed'`
    or `format='f'`.
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例显示了使用`put`进行存储，它将HDF5写入`PyTables`中的固定数组格式，称为`fixed`格式。这些类型的存储一旦写入就**不可追加**（尽管您可以简单地删除它们并重新写入）。它们也**不可查询**；必须完全检索它们。它们也不支持具有非唯一列名的数据框。使用`put`或`to_hdf`时，默认情况下指定`fixed`格式，或通过`format='fixed'`或`format='f'`指定。
- en: Warning
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: 'A `fixed` format will raise a `TypeError` if you try to retrieve using a `where`:'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试使用`where`检索`fixed`格式，将引发`TypeError`：
- en: '[PRE217]  ### Table format'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE217]  ### 表格格式'
- en: '`HDFStore` supports another `PyTables` format on disk, the `table` format.
    Conceptually a `table` is shaped very much like a DataFrame, with rows and columns.
    A `table` may be appended to in the same or other sessions. In addition, delete
    and query type operations are supported. This format is specified by `format=''table''`
    or `format=''t''` to `append` or `put` or `to_hdf`.'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: '`HDFStore`支持另一种磁盘上的`PyTables`格式，即`table`格式。在概念上，`table`的形状非常类似于DataFrame，具有行和列。`table`可以在相同或其他会话中追加。此外，支持删除和查询类型操作。通过`format=''table''`或`format=''t''`指定此格式以进行`append`或`put`或`to_hdf`。'
- en: This format can be set as an option as well `pd.set_option('io.hdf.default_format','table')`
    to enable `put/append/to_hdf` to by default store in the `table` format.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以将此格式设置为选项`pd.set_option('io.hdf.default_format','table')`，以使`put/append/to_hdf`默认存储为`table`格式。
- en: '[PRE218]'
  id: totrans-1008
  prefs: []
  type: TYPE_PRE
  zh: '[PRE218]'
- en: Note
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can also create a `table` by passing `format=''table''` or `format=''t''`
    to a `put` operation.  ### Hierarchical keys'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: '您还可以通过将`format=''table''`或`format=''t''`传递给`put`操作来创建一个`table`。  ### 分层键'
- en: Keys to a store can be specified as a string. These can be in a hierarchical
    path-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of
    sub-stores (or `Groups` in PyTables parlance). Keys can be specified without the
    leading ‘/’ and are **always** absolute (e.g. ‘foo’ refers to ‘/foo’). Removal
    operations can remove everything in the sub-store and **below**, so be *careful*.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: 存储的键可以指定为字符串。这些可以采用分层路径名称格式（例如`foo/bar/bah`），这将生成子存储（或`PyTables`术语中的`Groups`）的层次结构。键可以指定为没有前导‘/’的，并且**始终**是绝对的（例如，‘foo’指的是‘/foo’）。删除操作可以删除子存储中的所有内容以及**以下内容**，因此要*小心*。
- en: '[PRE219]'
  id: totrans-1012
  prefs: []
  type: TYPE_PRE
  zh: '[PRE219]'
- en: You can walk through the group hierarchy using the `walk` method which will
    yield a tuple for each group key along with the relative keys of its contents.
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`walk`方法遍历组层次结构，该方法将为每个组键生成一个元组，以及其内容的相对键。
- en: '[PRE220]'
  id: totrans-1014
  prefs: []
  type: TYPE_PRE
  zh: '[PRE220]'
- en: Warning
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Hierarchical keys cannot be retrieved as dotted (attribute) access as described
    above for items stored under the root node.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 无法像上面描述的在根节点下存储的项目那样，以点（属性）访问的方式检索分层键。
- en: '[PRE221]'
  id: totrans-1017
  prefs: []
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-1018
  prefs: []
  type: TYPE_PRE
  zh: '[PRE222]'
- en: 'Instead, use explicit string based keys:'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，使用显式的基于字符串的键：
- en: '[PRE223]  ### Storing types'
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE223]  ### 存储类型'
- en: Storing mixed types in a table
  id: totrans-1021
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在表中存储混合类型
- en: Storing mixed-dtype data is supported. Strings are stored as a fixed-width using
    the maximum size of the appended column. Subsequent attempts at appending longer
    strings will raise a `ValueError`.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 支持存储混合dtype数据。字符串以使用附加列的最大大小存储为固定宽度。尝试追加更长字符串将引发`ValueError`。
- en: 'Passing `min_itemsize={`values`: size}` as a parameter to append will set a
    larger minimum for the string columns. Storing `floats, strings, ints, bools,
    datetime64` are currently supported. For string columns, passing `nan_rep = ''nan''`
    to append will change the default nan representation on disk (which converts to/from
    `np.nan`), this defaults to `nan`.'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: '将 `min_itemsize={''values'': size}` 作为附加参数传递给 append 将为字符串列设置更大的最小值。目前支持存储
    `floats, strings, ints, bools, datetime64`。对于字符串列，将 `nan_rep = ''nan''` 传递给 append
    将更改磁盘上的默认 nan 表示（将转换为/从 `np.nan`），默认为 `nan`。'
- en: '[PRE224]'
  id: totrans-1024
  prefs: []
  type: TYPE_PRE
  zh: '[PRE224]'
- en: Storing MultiIndex DataFrames
  id: totrans-1025
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 存储多级索引的 `DataFrames`
- en: Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting
    from homogeneous index `DataFrames`.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 将多级索引的 `DataFrames` 存储为表与存储/选择同质索引的 `DataFrames` 非常相似。
- en: '[PRE225]'
  id: totrans-1027
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: Note
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `index` keyword is reserved and cannot be use as a level name.  ### Querying'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: '`index` 关键字是保留的，不能用作级别名称。### 查询'
- en: Querying a table
  id: totrans-1030
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查询表
- en: '`select` and `delete` operations have an optional criterion that can be specified
    to select/delete only a subset of the data. This allows one to have a very large
    on-disk table and retrieve only a portion of the data.'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: '`select` 和 `delete` 操作有一个可选的条件，可以指定选择/删除数据的子集。这允许在磁盘上有一个非常大的表，并且只检索数据的一部分。'
- en: A query is specified using the `Term` class under the hood, as a boolean expression.
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Term` 类在底层指定查询，作为布尔表达式。
- en: '`index` and `columns` are supported indexers of `DataFrames`.'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index` 和 `columns` 是 `DataFrames` 的支持索引器。'
- en: if `data_columns` are specified, these can be used as additional indexers.
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果指定了 `data_columns`，则可以将其用作额外的索引器。
- en: level name in a MultiIndex, with default name `level_0`, `level_1`, … if not
    provided.
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多级索引中的级别名称，默认名称为 `level_0`、`level_1`，如果未提供。
- en: 'Valid comparison operators are:'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的比较运算符有：
- en: '`=, ==, !=, >, >=, <, <=`'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: '`=, ==, !=, >, >=, <, <=`'
- en: 'Valid boolean expressions are combined with:'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的布尔表达式与以下组合：
- en: '`|` : or'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`|`：或'
- en: '`&` : and'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`&`：和'
- en: '`(` and `)` : for grouping'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(` 和 `)`：用于分组'
- en: These rules are similar to how boolean expressions are used in pandas for indexing.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则类似于在 pandas 中用于索引的布尔表达式的使用方式。
- en: Note
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`=` will be automatically expanded to the comparison operator `==`'
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`=` 将自动扩展为比较运算符 `==`'
- en: '`~` is the not operator, but can only be used in very limited circumstances'
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`~` 是非运算符，但只能在非��有限的情况下使用'
- en: If a list/tuple of expressions is passed they will be combined via `&`
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果传递了表达式的列表/元组，它们将通过 `&` 组合。
- en: 'The following are valid expressions:'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是有效的表达式：
- en: '`''index >= date''`'
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''index >= date''`'
- en: '`"columns = [''A'', ''D'']"`'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"columns = [''A'', ''D'']"`'
- en: '`"columns in [''A'', ''D'']"`'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"columns in [''A'', ''D'']"`'
- en: '`''columns = A''`'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''columns = A''`'
- en: '`''columns == A''`'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''columns == A''`'
- en: '`"~(columns = [''A'', ''B''])"`'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"~(columns = [''A'', ''B''])"`'
- en: '`''index > df.index[3] & string = "bar"''`'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''index > df.index[3] & string = "bar"''`'
- en: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
- en: '`"ts >= Timestamp(''2012-02-01'')"`'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"ts >= Timestamp(''2012-02-01'')"`'
- en: '`"major_axis>=20130101"`'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"major_axis>=20130101"`'
- en: 'The `indexers` are on the left-hand side of the sub-expression:'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: '`indexers` 在子表达式的左侧：'
- en: '`columns`, `major_axis`, `ts`'
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: '`columns`、`major_axis`、`ts`'
- en: 'The right-hand side of the sub-expression (after a comparison operator) can
    be:'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 子表达式的右侧（比较运算符后）可以是：
- en: functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将被评估的函数，例如`Timestamp('2012-02-01')`
- en: strings, e.g. `"bar"`
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串，例如`"bar"`
- en: date-like, e.g. `20130101`, or `"20130101"`
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似日期的格式，例如`20130101`，或`"20130101"`
- en: lists, e.g. `"['A', 'B']"`
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表，例如`"['A', 'B']"`
- en: variables that are defined in the local names space, e.g. `date`
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地命名空间中定义的变量，例如`date`
- en: Note
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Passing a string to a query by interpolating it into the query expression is
    not recommended. Simply assign the string of interest to a variable and use that
    variable in an expression. For example, do this
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议通过将字符串插入查询表达式来查询字符串。只需将感兴趣的字符串赋值给一个变量，并在表达式中使用该变量。例如，这样做
- en: '[PRE226]'
  id: totrans-1068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE226]'
- en: instead of this
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是这样
- en: '[PRE227]'
  id: totrans-1070
  prefs: []
  type: TYPE_PRE
  zh: '[PRE227]'
- en: The latter will **not** work and will raise a `SyntaxError`.Note that there’s
    a single quote followed by a double quote in the `string` variable.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 后者将**不起作用**，并将引发 `SyntaxError`。请注意 `string` 变量中有一个单引号后跟一个双引号。
- en: If you *must* interpolate, use the `'%r'` format specifier
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: 如果必须插值，请使用 `'%r'` 格式说明符
- en: '[PRE228]'
  id: totrans-1073
  prefs: []
  type: TYPE_PRE
  zh: '[PRE228]'
- en: which will quote `string`.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 将引用 `string`。
- en: 'Here are some examples:'
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例：
- en: '[PRE229]'
  id: totrans-1076
  prefs: []
  type: TYPE_PRE
  zh: '[PRE229]'
- en: Use boolean expressions, with in-line function evaluation.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内联列引用。
- en: '[PRE230]'
  id: totrans-1078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: Use inline column reference.
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内联列引用。
- en: '[PRE231]'
  id: totrans-1080
  prefs: []
  type: TYPE_PRE
  zh: '[PRE231]'
- en: 'The `columns` keyword can be supplied to select a list of columns to be returned,
    this is equivalent to passing a `''columns=list_of_columns_to_filter''`:'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: '`columns` 关键字可以用来选择要返回的列的列表，这相当于传递 `''columns=list_of_columns_to_filter''`：'
- en: '[PRE232]'
  id: totrans-1082
  prefs: []
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '`start` and `stop` parameters can be specified to limit the total search space.
    These are in terms of the total number of rows in a table.'
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: 可以指定 `start` 和 `stop` 参数以限制总搜索空间。这些是以表中总行数为单位的。
- en: Note
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`select` will raise a `ValueError` if the query expression has an unknown variable
    reference. Usually this means that you are trying to select on a column that is
    **not** a data_column.'
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 如果查询表达式具有未知变量引用，则 `select` 将引发 `ValueError`。通常，这意味着您正在尝试选择一个**不是**数据列的列。
- en: '`select` will raise a `SyntaxError` if the query expression is not valid.'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: 如果查询表达式无效，则 `select` 将引发 `SyntaxError`。
- en: '#### Query timedelta64[ns]'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 查询 timedelta64[ns]'
- en: 'You can store and query using the `timedelta64[ns]` type. Terms can be specified
    in the format: `<float>(<unit>)`, where float may be signed (and fractional),
    and unit can be `D,s,ms,us,ns` for the timedelta. Here’s an example:'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `timedelta64[ns]` 类型进行存储和查询。时间间隔可以以 `<float>(<unit>)` 的格式指定，其中浮点数可以是有符号的（也可以是分数），单位可以是
    `D，s，ms，us，ns` 用于时间间隔。以下是一个示例：
- en: '[PRE233]  #### Query MultiIndex'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE233]  #### 查询 MultiIndex'
- en: Selecting from a `MultiIndex` can be achieved by using the name of the level.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用级别的名称可以实现从 `MultiIndex` 中选择。
- en: '[PRE234]'
  id: totrans-1091
  prefs: []
  type: TYPE_PRE
  zh: '[PRE234]'
- en: If the `MultiIndex` levels names are `None`, the levels are automatically made
    available via the `level_n` keyword with `n` the level of the `MultiIndex` you
    want to select from.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `MultiIndex` 级别名称为 `None`，则可以通过 `level_n` 关键字自动使用 `level_n` 选择 `MultiIndex`
    的级别。
- en: '[PRE235]'
  id: totrans-1093
  prefs: []
  type: TYPE_PRE
  zh: '[PRE235]'
- en: Indexing
  id: totrans-1094
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 索引
- en: You can create/modify an index for a table with `create_table_index` after data
    is already in the table (after and `append/put` operation). Creating a table index
    is **highly** encouraged. This will speed your queries a great deal when you use
    a `select` with the indexed dimension as the `where`.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在数据已经在表中的情况下（在 `append/put` 操作之后）使用 `create_table_index` 为表创建/修改索引。强烈建议创建表索引。当您使用具有索引维度作为
    `where` 的 `select` 时，这将大大加快查询速度。
- en: Note
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Indexes are automagically created on the indexables and any data columns you
    specify. This behavior can be turned off by passing `index=False` to `append`.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 索引会自动创建在可索引和您指定的任何数据列上。通过向 `append` 传递 `index=False` 可以关闭此行为。
- en: '[PRE236]'
  id: totrans-1098
  prefs: []
  type: TYPE_PRE
  zh: '[PRE236]'
- en: Oftentimes when appending large amounts of data to a store, it is useful to
    turn off index creation for each append, then recreate at the end.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 在向存储附加大量数据时，通常很有用关闭每次附加的索引创建，然后在最后重新创建。
- en: '[PRE237]'
  id: totrans-1100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE237]'
- en: Then create the index when finished appending.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在附加完成后创建索引。
- en: '[PRE238]'
  id: totrans-1102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE238]'
- en: See [here](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)
    for how to create a completely-sorted-index (CSI) on an existing store.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[这里](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)如何在现有存储上创建完全排序索引（CSI）。
- en: '#### Query via data columns'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 通过数据列查询'
- en: You can designate (and index) certain columns that you want to be able to perform
    queries (other than the `indexable` columns, which you can always query). For
    instance say you want to perform this common operation, on-disk, and return just
    the frame that matches this query. You can specify `data_columns = True` to force
    all columns to be `data_columns`.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指定（并索引）您希望能够执行查询的特定列（除了可始终查询的 `indexable` 列之外）。例如，假设您想要执行此常见操作，在磁盘上，并仅返回与此查询匹配的框架。您可以指定
    `data_columns = True` 来强制所有列都成为 `data_columns`。
- en: '[PRE239]'
  id: totrans-1106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: There is some performance degradation by making lots of columns into `data columns`,
    so it is up to the user to designate these. In addition, you cannot change data
    columns (nor indexables) after the first append/put operation (Of course you can
    simply read in the data and create a new table!).
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 将许多列转换为 `data columns` 会导致一些性能下降，因此用户需要指定这些列。此外，在第一次附加/放置操作之后，您不能更改数据列（也不能更改索引列）（当然，您可以简单地读取数据并创建新表！）。
- en: Iterator
  id: totrans-1108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 迭代器
- en: You can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and
    `select_as_multiple` to return an iterator on the results. The default is 50,000
    rows returned in a chunk.
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将 `iterator=True` 或 `chunksize=number_in_a_chunk` 传递给 `select` 和 `select_as_multiple`
    以返回结果的迭代器。默认情况下，每次返回 50,000 行。
- en: '[PRE240]'
  id: totrans-1110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: Note
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can also use the iterator with `read_hdf` which will open, then automatically
    close the store when finished iterating.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用带有 `read_hdf` 的迭代器，该迭代器在完成迭代时会自动打开然后关闭存储。
- en: '[PRE241]'
  id: totrans-1113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: Note, that the chunksize keyword applies to the **source** rows. So if you are
    doing a query, then the chunksize will subdivide the total rows in the table and
    the query applied, returning an iterator on potentially unequal sized chunks.
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，chunksize关键字适用于**源**行。因此，如果你正在进行一个查询，那么chunksize将把表中的总行数细分，并应用查询，返回一个可能大小不等的块的迭代器。
- en: Here is a recipe for generating a query and using it to create equal sized return
    chunks.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个生成查询并使用它创建相等大小返回块的方法。
- en: '[PRE242]'
  id: totrans-1116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: Advanced queries
  id: totrans-1117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高级查询
- en: Select a single column
  id: totrans-1118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择单列
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索单个可索引或数据列，请使用方法`select_column`。这将使你能够快速获取索引。这些返回一个结果的`Series`，由行号索引。目前这些方法不接受`where`选择器。
- en: '[PRE243]'
  id: totrans-1120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE243]'
- en: '##### Selecting coordinates'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: '##### 选择坐标'
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你想要获取查询的坐标（也就是索引位置）。这将返回结果位置的`Index`。这些坐标也可以传递给后续的`where`操作。
- en: '[PRE244]  ##### Selecting using a where mask'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE244]  ##### 使用where掩码进行选择'
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你的查询可能涉及创建一个要选择的行列表。通常这个`mask`会是一个索引操作的结果`index`。这个示例选择了一个datetimeindex中为5的月份。
- en: '[PRE245]'
  id: totrans-1125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE245]'
- en: Storer object
  id: totrans-1126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 存储器对象
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要检查存储的对象，请通过`get_storer`检索。你可以在程序中使用这个方法来获取对象中的行数。
- en: '[PRE246]'
  id: totrans-1128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: Multiple table queries
  id: totrans-1129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多表查询
- en: The methods `append_to_multiple` and `select_as_multiple` can perform appending/selecting
    from multiple tables at once. The idea is to have one table (call it the selector
    table) that you index most/all of the columns, and perform your queries. The other
    table(s) are data tables with an index matching the selector table’s index. You
    can then perform a very fast query on the selector table, yet get lots of data
    back. This method is similar to having a very wide table, but enables more efficient
    queries.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 方法`append_to_multiple`和`select_as_multiple`可以同时从多个表中执行追加/选择操作。其思想是有一个表（称之为选择器表），你在这个表中索引大部分/全部列，并执行你的查询。其他表是数据表，其索引与选择器表的索引匹配。然后你可以在选择器表上执行非常快速的查询，同时获取大量数据。这种方法类似于拥有一个非常宽的表，但能够实现更高效的查询。
- en: The `append_to_multiple` method splits a given single DataFrame into multiple
    tables according to `d`, a dictionary that maps the table names to a list of ‘columns’
    you want in that table. If `None` is used in place of a list, that table will
    have the remaining unspecified columns of the given DataFrame. The argument `selector`
    defines which table is the selector table (which you can make queries from). The
    argument `dropna` will drop rows from the input `DataFrame` to ensure tables are
    synchronized. This means that if a row for one of the tables being written to
    is entirely `np.nan`, that row will be dropped from all tables.
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: '`append_to_multiple`方法根据`d`，一个将表名映射到你想要在该表中的‘列’列表的字典，将给定的单个DataFrame拆分成多个表。如果在列表的位置使用`None`，那么该表将具有给定DataFrame的其余未指定的列。参数`selector`定义了哪个表是选择器表（你可以从中进行查询）。参数`dropna`将从输入的`DataFrame`中删除行，以确保表同步。这意味着如果要写入的表中的一行完全由`np.nan`组成，那么该行将从所有表中删除。'
- en: If `dropna` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
    Remember that entirely `np.Nan` rows are not written to the HDFStore, so if you
    choose to call `dropna=False`, some tables may have more rows than others, and
    therefore `select_as_multiple` may not work or it may return unexpected results.
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`dropna`为False，**用户需要负责同步表格**。请记住，完全由`np.Nan`行组成的行不会被写入HDFStore，因此如果选择调用`dropna=False`，某些表可能比其他表有更多的行，因此`select_as_multiple`可能无法工作，或者可能返回意外结果。
- en: '[PRE247]'
  id: totrans-1133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: Delete from a table
  id: totrans-1134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从表中删除
- en: You can delete from a table selectively by specifying a `where`. In deleting
    rows, it is important to understand the `PyTables` deletes rows by erasing the
    rows, then **moving** the following data. Thus deleting can potentially be a very
    expensive operation depending on the orientation of your data. To get optimal
    performance, it’s worthwhile to have the dimension you are deleting be the first
    of the `indexables`.
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过指定`where`有选择性地从表中删除。在删除行时，重要的是要了解`PyTables`通过擦除行然后**移动**后续数据来删除行。因此，删除操作可能是一个非常昂贵的操作，具体取决于数据的方向。为了获得最佳性能，最好让您要删除的维度成为`indexables`的第一个维度。
- en: 'Data is ordered (on the disk) in terms of the `indexables`. Here’s a simple
    use case. You store panel-type data, with dates in the `major_axis` and ids in
    the `minor_axis`. The data is then interleaved like this:'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: 数据按照`indexables`的顺序（在磁盘上）进行排序。这里有一个简单的用例。你存储面板类型的数据，日期在`major_axis`中，id在`minor_axis`中。然后数据被交错存储如下：
- en: date_1
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: date_1
- en: id_1
  id: totrans-1138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: id_1
- en: id_2
  id: totrans-1139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: id_2
- en: .
  id: totrans-1140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: .
- en: id_n
  id: totrans-1141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: id_n
- en: date_2
  id: totrans-1142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: date_2
- en: id_1
  id: totrans-1143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: id_1
- en: .
  id: totrans-1144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: .
- en: id_n
  id: totrans-1145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: id_n
- en: It should be clear that a delete operation on the `major_axis` will be fairly
    quick, as one chunk is removed, then the following data moved. On the other hand
    a delete operation on the `minor_axis` will be very expensive. In this case it
    would almost certainly be faster to rewrite the table using a `where` that selects
    all but the missing data.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 应该清楚，对`major_axis`进行删除操作会相当快，因为一个块被移除，然后后续数据被移动。另一方面，对`minor_axis`进行删除操作将非常昂贵。在这种情况下，重新编写使用`where`选择除缺失数据外的所有数据的表几乎肯定会更快。
- en: Warning
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files automatically.
    Thus, repeatedly deleting (or removing nodes) and adding again, **WILL TEND TO
    INCREASE THE FILE SIZE**.
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，HDF5 **不会自动回收** h5文件中的空间。因此，反复删除（或移除节点）然后再添加，**会增加文件大小**。
- en: To *repack and clean* the file, use [ptrepack](#io-hdf5-ptrepack).
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 若要*重新打包和清理*文件，请使用[ptrepack](#io-hdf5-ptrepack)。
- en: '### Notes & caveats'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: '### 注意事项 & 警告'
- en: Compression
  id: totrans-1151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 压缩
- en: '`PyTables` allows the stored data to be compressed. This applies to all kinds
    of stores, not just tables. Two parameters are used to control compression: `complevel`
    and `complib`.'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTables`允许对存储的数据进行压缩。这适用于���有类型的存储，不仅仅是表格。用于控制压缩的两个参数是`complevel`和`complib`。'
- en: '`complevel` specifies if and how hard data is to be compressed. `complevel=0`
    and `complevel=None` disables compression and `0<complevel<10` enables compression.'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`complevel`指定数据压缩的难度。`complevel=0`和`complevel=None`禁用压缩，`0<complevel<10`启用压缩。'
- en: '`complib` specifies which compression library to use. If nothing is specified
    the default library `zlib` is used. A compression library usually optimizes for
    either good compression rates or speed and the results will depend on the type
    of data. Which type of compression to choose depends on your specific needs and
    data. The list of supported compression libraries:'
  id: totrans-1154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`complib`指定要使用的压缩库。如果未指定任何内容，则使用默认库`zlib`。压缩库通常会针对良好的压缩率或速度进行优化，结果将取决于数据类型。选择哪种类型的压缩取决于您的具体需求和数据。支持的压缩库列表：'
- en: '[zlib](https://zlib.net/): The default compression library. A classic in terms
    of compression, achieves good compression rates but is somewhat slow.'
  id: totrans-1155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[zlib](https://zlib.net/)：默认的压缩库。在压缩方面经典，能够获得很高的压缩率，但速度有些慢。'
- en: '[lzo](https://www.oberhumer.com/opensource/lzo/): Fast compression and decompression.'
  id: totrans-1156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[lzo](https://www.oberhumer.com/opensource/lzo/)：快速的压缩和解压。'
- en: '[bzip2](https://sourceware.org/bzip2/): Good compression rates.'
  id: totrans-1157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[bzip2](https://sourceware.org/bzip2/)：压缩率很高。'
- en: '[blosc](https://www.blosc.org/): Fast compression and decompression.'
  id: totrans-1158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[blosc](https://www.blosc.org/)：快速的压缩和解压。'
- en: 'Support for alternative blosc compressors:'
  id: totrans-1159
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持替代的blosc压缩器：
- en: '[blosc:blosclz](https://www.blosc.org/) This is the default compressor for
    `blosc`'
  id: totrans-1160
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[blosc:blosclz](https://www.blosc.org/) 这是`blosc`的默认压缩器'
- en: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html): A compact, very
    popular and fast compressor.'
  id: totrans-1161
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html)：紧凑、非常流行且快速的压缩器。'
- en: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html): A tweaked version
    of LZ4, produces better compression ratios at the expense of speed.'
  id: totrans-1162
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html)：LZ4的改进版本，在牺牲速度的情况下产生更好的压缩比。'
- en: '[blosc:snappy](https://google.github.io/snappy/): A popular compressor used
    in many places.'
  id: totrans-1163
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[blosc:snappy](https://google.github.io/snappy/)：在许多地方使用的流行压缩器。'
- en: '[blosc:zlib](https://zlib.net/): A classic; somewhat slower than the previous
    ones, but achieving better compression ratios.'
  id: totrans-1164
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[blosc:zlib](https://zlib.net/)：经典；比前几个稍慢，但实现更好的压缩比。'
- en: '[blosc:zstd](https://facebook.github.io/zstd/): An extremely well balanced
    codec; it provides the best compression ratios among the others above, and at
    reasonably fast speed.'
  id: totrans-1165
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[blosc:zstd](https://facebook.github.io/zstd/)：一个极其平衡的编解码器；它在以上其他编解码器中提供最佳的���缩比，并且速度相当快。'
- en: If `complib` is defined as something other than the listed libraries a `ValueError`
    exception is issued.
  id: totrans-1166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`complib`被定义为除列出的库之外的内容，则会引发`ValueError`异常。
- en: Note
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the library specified with the `complib` option is missing on your platform,
    compression defaults to `zlib` without further ado.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在您的平台上缺少`complib`选项指定的库，则压缩默认为`zlib`，无需进一步操作。
- en: 'Enable compression for all objects within the file:'
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 为文件中的所有对象启用压缩：
- en: '[PRE248]'
  id: totrans-1170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE248]'
- en: 'Or on-the-fly compression (this only applies to tables) in stores where compression
    is not enabled:'
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 或在未启用压缩的存储中进行即时压缩（仅适用于表）：
- en: '[PRE249]'
  id: totrans-1172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE249]'
- en: '#### ptrepack'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: '#### ptrepack'
- en: '`PyTables` offers better write performance when tables are compressed after
    they are written, as opposed to turning on compression at the very beginning.
    You can use the supplied `PyTables` utility `ptrepack`. In addition, `ptrepack`
    can change compression levels after the fact.'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 当表在写入后进行压缩时，`PyTables`提供更好的写入性能，而不是在一开始就打开压缩。您可以使用提供的`PyTables`实用程序`ptrepack`。此外，`ptrepack`可以在事后更改压缩级别。
- en: '[PRE250]'
  id: totrans-1175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE250]'
- en: 'Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow you to
    reuse previously deleted space. Alternatively, one can simply remove the file
    and write again, or use the `copy` method.  #### Caveats'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，`ptrepack in.h5 out.h5`将*重新打包*文件，以便您可以重用先前删除的空间。或者，可以简单地删除文件并重新写入，或者使用`copy`方法。  ####
    注意事项'
- en: Warning
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '`HDFStore` is **not-threadsafe for writing**. The underlying `PyTables` only
    supports concurrent reads (via threading or processes). If you need reading and
    writing *at the same time*, you need to serialize these operations in a single
    thread in a single process. You will corrupt your data otherwise. See the ([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397)) for more information.'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: '`HDFStore`对于写入**不是线程安全**的。底层的`PyTables`仅支持并发读取（通过线程或进程）。如果您需要同时进行读取和写入，您需要在单个线程中的单个进程中串行化这些操作。否则，您的数据将被破坏。有关更多信息，请参见([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397))。'
- en: If you use locks to manage write access between multiple processes, you may
    want to use [`fsync()`](https://docs.python.org/3/library/os.html#os.fsync "(in
    Python v3.12)") before releasing write locks. For convenience you can use `store.flush(fsync=True)`
    to do this for you.
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用锁来管理多个进程之间的写入访问权限，可能需要在释放写入锁之前使用[`fsync()`](https://docs.python.org/3/library/os.html#os.fsync
    "(在Python v3.12中)")。为了方便起见，您可以使用`store.flush(fsync=True)`来为您执行此操作。
- en: Once a `table` is created columns (DataFrame) are fixed; only exactly the same
    columns can be appended
  id: totrans-1180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦创建了`table`，列（DataFrame）就是固定的；只能追加完全相同的列
- en: Be aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not necessarily
    equal across timezone versions. So if data is localized to a specific timezone
    in the HDFStore using one version of a timezone library and that data is updated
    with another version, the data will be converted to UTC since these timezones
    are not considered equal. Either use the same version of timezone library or use
    `tz_convert` with the updated timezone definition.
  id: totrans-1181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意时区（例如，`pytz.timezone('US/Eastern')`）在不同时区版本之间不一定相等。因此，如果使用一个版本的时区库将数据本地化到HDFStore中的特定时区，并且使用另一个版本更新数据，则数据将被转换为UTC，因为这些时区不被视为相等。要么使用相同版本的时区库，要么使用带有更新时区定义的`tz_convert`。
- en: Warning
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '`PyTables` will show a `NaturalNameWarning` if a column name cannot be used
    as an attribute selector. *Natural* identifiers contain only letters, numbers,
    and underscores, and may not begin with a number. Other identifiers cannot be
    used in a `where` clause and are generally a bad idea.  ### DataTypes'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: '如果列名不能用作属性选择器，则`PyTables`将显示`NaturalNameWarning`。*自然*标识符仅包含字母、数字和下划线，并且不能以数字开头。其他标识符不能在`where`子句中使用，通常是一个坏主意。  ###
    数据类型'
- en: '`HDFStore` will map an object dtype to the `PyTables` underlying dtype. This
    means the following types are known to work:'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: '`HDFStore`将对象dtype映射到`PyTables`底层dtype。这意味着以下类型已知可用：'
- en: '| Type | Represents missing values |'
  id: totrans-1185
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 表示缺失值 |'
- en: '| --- | --- |'
  id: totrans-1186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| floating : `float64, float32, float16` | `np.nan` |'
  id: totrans-1187
  prefs: []
  type: TYPE_TB
  zh: '| floating : `float64, float32, float16` | `np.nan` |'
- en: '| integer : `int64, int32, int8, uint64,uint32, uint8` |  |'
  id: totrans-1188
  prefs: []
  type: TYPE_TB
  zh: '| integer : `int64, int32, int8, uint64,uint32, uint8` |  |'
- en: '| boolean |  |'
  id: totrans-1189
  prefs: []
  type: TYPE_TB
  zh: '| 布尔值 |  |'
- en: '| `datetime64[ns]` | `NaT` |'
  id: totrans-1190
  prefs: []
  type: TYPE_TB
  zh: '| `datetime64[ns]` | `NaT` |'
- en: '| `timedelta64[ns]` | `NaT` |'
  id: totrans-1191
  prefs: []
  type: TYPE_TB
  zh: '| `timedelta64[ns]` | `NaT` |'
- en: '| categorical : see the section below |  |'
  id: totrans-1192
  prefs: []
  type: TYPE_TB
  zh: '| 分类：请参见下面的部分 |  |'
- en: '| object : `strings` | `np.nan` |'
  id: totrans-1193
  prefs: []
  type: TYPE_TB
  zh: '| object：`strings` | `np.nan` |'
- en: '`unicode` columns are not supported, and **WILL FAIL**.'
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 不支持`unicode`列，**将失败**。
- en: '#### Categorical data'
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 分类数据'
- en: You can write data that contains `category` dtypes to a `HDFStore`. Queries
    work the same as if it was an object array. However, the `category` dtyped data
    is stored in a more efficient manner.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将包含`category` dtypes的数据写入`HDFStore`。查询的工作方式与对象数组相同。但是，`category` dtyped数据以更有效的方式存储。
- en: '[PRE251]'
  id: totrans-1197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE251]'
- en: String columns
  id: totrans-1198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 字符串列
- en: '**min_itemsize**'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: '**min_itemsize**'
- en: The underlying implementation of `HDFStore` uses a fixed column width (itemsize)
    for string columns. A string column itemsize is calculated as the maximum of the
    length of data (for that column) that is passed to the `HDFStore`, **in the first
    append**. Subsequent appends, may introduce a string for a column **larger** than
    the column can hold, an Exception will be raised (otherwise you could have a silent
    truncation of these columns, leading to loss of information). In the future we
    may relax this and allow a user-specified truncation to occur.
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: '`HDFStore`的底层实现对字符串列使用固定的列宽（itemsize）。字符串列的itemsize是在第一次追加时传递给`HDFStore`的数据的长度的最大值。后续的追加可能会引入一个比列能容纳的更大的字符串，将引发异常（否则可能会对这些列进行静默截断，导致信息丢失）。在未来，我们可能会放宽这一限制，允许用户指定截断。'
- en: Pass `min_itemsize` on the first table creation to a-priori specify the minimum
    length of a particular string column. `min_itemsize` can be an integer, or a dict
    mapping a column name to an integer. You can pass `values` as a key to allow all
    *indexables* or *data_columns* to have this min_itemsize.
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次创建表时传递`min_itemsize`，以先验指定特定字符串列的最小长度。`min_itemsize`可以是一个整数，或将列名映射到整数的字典。您可以将`values`作为一个键传递，以允许所有*可索引*或*data_columns*具有此最小长度。
- en: Passing a `min_itemsize` dict will cause all passed columns to be created as
    *data_columns* automatically.
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: 传递`min_itemsize`字典将导致所有传递的列自动创建为*data_columns*。
- en: Note
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are not passing any `data_columns`, then the `min_itemsize` will be the
    maximum of the length of any string passed
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有传递任何`data_columns`，那么`min_itemsize`将是传递的任何字符串的长度的最大值
- en: '[PRE252]'
  id: totrans-1205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '**nan_rep**'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: '**nan_rep**'
- en: String columns will serialize a `np.nan` (a missing value) with the `nan_rep`
    string representation. This defaults to the string value `nan`. You could inadvertently
    turn an actual `nan` value into a missing value.
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串列将使用`nan_rep`字符串表示来序列化`np.nan`（缺失值）。默认为字符串值`nan`。您可能会无意中将实际的`nan`值转换为缺失值。
- en: '[PRE253]'
  id: totrans-1208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE253]'
- en: Performance
  id: totrans-1209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能
- en: '`tables` format come with a writing performance penalty as compared to `fixed`
    stores. The benefit is the ability to append/delete and query (potentially very
    large amounts of data). Write times are generally longer as compared with regular
    stores. Query times can be quite fast, especially on an indexed axis.'
  id: totrans-1210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与`fixed`存储相比，`tables`格式会带来写入性能的损失。好处在于能够追加/删除和查询（可能是非常大量的数据）。与常规存储相比，写入时间通常更长。查询时间可能非常快，特别是在索引轴上。
- en: You can pass `chunksize=<int>` to `append`, specifying the write chunksize (default
    is 50000). This will significantly lower your memory usage on writing.
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过在`append`中传递`chunksize=<int>`来指定写入的块大小（默认为50000）。这将显著降低写入时的内存使用。
- en: You can pass `expectedrows=<int>` to the first `append`, to set the TOTAL number
    of rows that `PyTables` will expect. This will optimize read/write performance.
  id: totrans-1212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过在第一次`append`中传递`expectedrows=<int>`来设置`PyTables`预期的总行数。这将优化读/写性能。
- en: Duplicate rows can be written to tables, but are filtered out in selection (with
    the last items being selected; thus a table is unique on major, minor pairs)
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将重复行写入表中，但在选择时会被过滤掉（选择最后的项目；因此表在主要、次要对上是唯一的）
- en: 'A `PerformanceWarning` will be raised if you are attempting to store types
    that will be pickled by PyTables (rather than stored as endemic types). See [Here](https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190)
    for more information and some solutions.  ## Feather'
  id: totrans-1214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果您尝试存储将由PyTables进行pickle处理的类型（而不是作为固有类型存储），将会引发`PerformanceWarning`。有关更多信息和一些解决方案，请参见[这里](https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190)。  ##
    Feather'
- en: Feather provides binary columnar serialization for data frames. It is designed
    to make reading and writing data frames efficient, and to make sharing data across
    data analysis languages easy.
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: Feather 为数据框提供了二进制列序列化。它旨在使数据框的读写高效，并使数据在数据分析语言之间的共享变得容易。
- en: Feather is designed to faithfully serialize and de-serialize DataFrames, supporting
    all of the pandas dtypes, including extension dtypes such as categorical and datetime
    with tz.
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: Feather 旨在忠实地序列化和反序列化 DataFrames，支持所有 pandas 的数据类型，包括分类和带有时区的日期时间等扩展数据类型。
- en: 'Several caveats:'
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 几个注意事项：
- en: The format will NOT write an `Index`, or `MultiIndex` for the `DataFrame` and
    will raise an error if a non-default one is provided. You can `.reset_index()`
    to store the index or `.reset_index(drop=True)` to ignore it.
  id: totrans-1218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该格式不会为 `DataFrame` 写入 `Index` 或 `MultiIndex`，如果提供了非默认的索引，则会引发错误。您可以使用 `.reset_index()`
    存储索引，或使用 `.reset_index(drop=True)` 忽略它。
- en: Duplicate column names and non-string columns names are not supported
  id: totrans-1219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持重复的列名和非字符串的列名
- en: Actual Python objects in object dtype columns are not supported. These will
    raise a helpful error message on an attempt at serialization.
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持对象数据类型列中的实际 Python 对象。在尝试序列化时，这些将引发一个有用的错误消息。
- en: See the [Full Documentation](https://github.com/wesm/feather).
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[完整文档](https://github.com/wesm/feather)。
- en: '[PRE254]'
  id: totrans-1222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE254]'
- en: Write to a feather file.
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: 写入一个 feather 文件。
- en: '[PRE255]'
  id: totrans-1224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE255]'
- en: Read from a feather file.
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个 feather 文件中读取。
- en: '[PRE256]  ## Parquet'
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE256]  ## Parquet'
- en: '[Apache Parquet](https://parquet.apache.org/) provides a partitioned binary
    columnar serialization for data frames. It is designed to make reading and writing
    data frames efficient, and to make sharing data across data analysis languages
    easy. Parquet can use a variety of compression techniques to shrink the file size
    as much as possible while still maintaining good read performance.'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: '[Apache Parquet](https://parquet.apache.org/) 为数据框提供了分区的二进制列序列化。它旨在使数据框的读写高效，并使数据在数据分析语言之间的共享变得容易。Parquet
    可以使用各种压缩技术来尽可能地缩小文件大小，同时保持良好的读取性能。'
- en: Parquet is designed to faithfully serialize and de-serialize `DataFrame` s,
    supporting all of the pandas dtypes, including extension dtypes such as datetime
    with tz.
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 旨在忠实地序列化和反序列化 `DataFrame`，支持所有 pandas 的数据类型，包括带有时区的日期时间等扩展数据类型。
- en: Several caveats.
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: 几个注意事项。
- en: Duplicate column names and non-string columns names are not supported.
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持重复的列名和非字符串的列名。
- en: The `pyarrow` engine always writes the index to the output, but `fastparquet`
    only writes non-default indexes. This extra column can cause problems for non-pandas
    consumers that are not expecting it. You can force including or omitting indexes
    with the `index` argument, regardless of the underlying engine.
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyarrow` 引擎始终将索引写入输出，但 `fastparquet` 仅写入非默认索引。这个额外的列可能会给那些不希望看到它的非 pandas
    消费者带来问题。您可以使用 `index` 参数强制包含或省略索引，而不管底层引擎如何。'
- en: Index level names, if specified, must be strings.
  id: totrans-1232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果指定了索引级别名称，则必须是字符串。
- en: In the `pyarrow` engine, categorical dtypes for non-string types can be serialized
    to parquet, but will de-serialize as their primitive dtype.
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `pyarrow` 引擎中，非字符串类型的分类数据类型可以序列化为 parquet，但会反序列化为其原始数据类型。
- en: The `pyarrow` engine preserves the `ordered` flag of categorical dtypes with
    string types. `fastparquet` does not preserve the `ordered` flag.
  id: totrans-1234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyarrow` 引擎保留了具有字符串类型的分类数据类型的 `ordered` 标志。`fastparquet` 不保留 `ordered` 标志。'
- en: Non supported types include `Interval` and actual Python object types. These
    will raise a helpful error message on an attempt at serialization. `Period` type
    is supported with pyarrow >= 0.16.0.
  id: totrans-1235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持的类型包括 `Interval` 和实际的 Python 对象类型。在尝试序列化时，这些将引发一个有用的错误消息。`Period` 类型在 pyarrow
    >= 0.16.0 中受支持。
- en: The `pyarrow` engine preserves extension data types such as the nullable integer
    and string data type (requiring pyarrow >= 0.16.0, and requiring the extension
    type to implement the needed protocols, see the [extension types documentation](../development/extending.html#extending-extension-arrow)).
  id: totrans-1236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyarrow` 引擎保留扩展数据类型，如可空整数和字符串数据类型（需要 pyarrow >= 0.16.0，并要求扩展类型实现所需的协议，请参阅[扩展类型文档](../development/extending.html#extending-extension-arrow)）。'
- en: You can specify an `engine` to direct the serialization. This can be one of
    `pyarrow`, or `fastparquet`, or `auto`. If the engine is NOT specified, then the
    `pd.options.io.parquet.engine` option is checked; if this is also `auto`, then
    `pyarrow` is tried, and falling back to `fastparquet`.
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指定一个`engine`来指导序列化。这可以是`pyarrow`、`fastparquet`或`auto`中的一个。如果未指定引擎，则会检查`pd.options.io.parquet.engine`选项；如果这也是`auto`，则尝试`pyarrow`，并回退到`fastparquet`。
- en: See the documentation for [pyarrow](https://arrow.apache.org/docs/python/) and
    [fastparquet](https://fastparquet.readthedocs.io/en/latest/).
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 参阅[pyarrow](https://arrow.apache.org/docs/python/)和[fastparquet](https://fastparquet.readthedocs.io/en/latest/)的文档。
- en: Note
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: These engines are very similar and should read/write nearly identical parquet
    format files. `pyarrow>=8.0.0` supports timedelta data, `fastparquet>=0.1.4` supports
    timezone aware datetimes. These libraries differ by having different underlying
    dependencies (`fastparquet` by using `numba`, while `pyarrow` uses a c-library).
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: 这些引擎非常相似，几乎可以读/写完全相同的Parquet格式文件。`pyarrow>=8.0.0`支持时间间隔数据，`fastparquet>=0.1.4`支持时区感知日期时间。这些库之间的区别在于具有不同的底层依赖关系（`fastparquet`使用`numba`，而`pyarrow`使用C库）。
- en: '[PRE257]'
  id: totrans-1241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE257]'
- en: Write to a parquet file.
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: 写入Parquet文件。
- en: '[PRE258]'
  id: totrans-1243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE258]'
- en: Read from a parquet file.
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: 从Parquet文件中读取。
- en: '[PRE259]'
  id: totrans-1245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE259]'
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`dtype_backend`参数，您可以控制生成的DataFrame使用的默认数据类型。
- en: '[PRE260]'
  id: totrans-1247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE260]'
- en: Note
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that this is not supported for `fastparquet`.
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这对于`fastparquet`不受支持。
- en: Read only certain columns of a parquet file.
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
  zh: 仅读取Parquet文件的特定列。
- en: '[PRE261]'
  id: totrans-1251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE261]'
- en: Handling indexes
  id: totrans-1252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理索引
- en: 'Serializing a `DataFrame` to parquet may include the implicit index as one
    or more columns in the output file. Thus, this code:'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: 将DataFrame序列化为parquet文件可能会将隐式索引作为一个或多个列包含在输出文件中。因此，这段代码：
- en: '[PRE262]'
  id: totrans-1254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE262]'
- en: 'creates a parquet file with *three* columns if you use `pyarrow` for serialization:
    `a`, `b`, and `__index_level_0__`. If you’re using `fastparquet`, the index [may
    or may not](https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write)
    be written to the file.'
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用`pyarrow`进行序列化，将创建一个包含*三*列的Parquet文件：`a`、`b`和`__index_level_0__`。如果您使用`fastparquet`，索引[可能会或可能不会](https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write)写入文件。
- en: This unexpected extra column causes some databases like Amazon Redshift to reject
    the file, because that column doesn’t exist in the target table.
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: 这意外的额外列会导致一些数据库（如Amazon Redshift）拒绝该文件，因为该列在目标表中不存在。
- en: 'If you want to omit a dataframe’s indexes when writing, pass `index=False`
    to [`to_parquet()`](../reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet
    "pandas.DataFrame.to_parquet"):'
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在写入时省略数据框的索引，请在[`to_parquet()`](../reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet
    "pandas.DataFrame.to_parquet")中传递`index=False`：
- en: '[PRE263]'
  id: totrans-1258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE263]'
- en: This creates a parquet file with just the two expected columns, `a` and `b`.
    If your `DataFrame` has a custom index, you won’t get it back when you load this
    file into a `DataFrame`.
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将这个文件加载到`DataFrame`中时，这将创建一个只包含两个预期列`a`和`b`的Parquet文件。如果你的`DataFrame`有自定义索引，当你加载这个文件时将不会得到它。
- en: Passing `index=True` will *always* write the index, even if that’s not the underlying
    engine’s default behavior.
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: 传递`index=True`将*始终*写入索引，即使这不是底层引擎的默认行为。
- en: Partitioning Parquet files
  id: totrans-1261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对Parquet文件进行分区
- en: Parquet supports partitioning of data based on the values of one or more columns.
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet支持根据一个或多个列的值对数据进行分区。
- en: '[PRE264]'
  id: totrans-1263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: 'The `path` specifies the parent directory to which data will be saved. The
    `partition_cols` are the column names by which the dataset will be partitioned.
    Columns are partitioned in the order they are given. The partition splits are
    determined by the unique values in the partition columns. The above example creates
    a partitioned dataset that may look like:'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: '`path`指定将数据保存到的父目录。`partition_cols`是数据集将根据其进行分区的列名。列按给定顺序进行分区。分区拆分由分区列中的唯一值确定。上面的示例创建了一个可能如下所示的分区数据集：'
- en: '[PRE265]  ## ORC'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE265]  ## ORC'
- en: Similar to the [parquet](#io-parquet) format, the [ORC Format](https://orc.apache.org/)
    is a binary columnar serialization for data frames. It is designed to make reading
    data frames efficient. pandas provides both the reader and the writer for the
    ORC format, [`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc
    "pandas.read_orc") and [`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc"). This requires the [pyarrow](https://arrow.apache.org/docs/python/)
    library.
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: 与[parquet](#io-parquet)格式类似，[ORC格式](https://orc.apache.org/)是用于数据框的二进制列序列化。它旨在使数据框的读取效率更高。pandas为ORC格式提供了读取器和写入器，[`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc
    "pandas.read_orc")和[`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc")。这需要[pyarrow](https://arrow.apache.org/docs/python/)库。
- en: Warning
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: It is *highly recommended* to install pyarrow using conda due to some issues
    occurred by pyarrow.
  id: totrans-1268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议使用conda安装pyarrow，因为pyarrow存在一些问题。
- en: '[`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc") requires pyarrow>=7.0.0.'
  id: totrans-1269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc")需要pyarrow>=7.0.0。'
- en: '[`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc "pandas.read_orc")
    and [`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc") are not supported on Windows yet, you can find valid
    environments on [install optional dependencies](../getting_started/install.html#install-warn-orc).'
  id: totrans-1270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc "pandas.read_orc")和[`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc")目前尚不支持Windows，您可以在[安装可选依赖项](../getting_started/install.html#install-warn-orc)中找到有效的环境。'
- en: For supported dtypes please refer to [supported ORC features in Arrow](https://arrow.apache.org/docs/cpp/orc.html#data-types).
  id: totrans-1271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关支持的数据类型，请参考[Arrow中支持的ORC功能](https://arrow.apache.org/docs/cpp/orc.html#data-types)。
- en: Currently timezones in datetime columns are not preserved when a dataframe is
    converted into ORC files.
  id: totrans-1272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前，将数据框转换为ORC文件时，日期时间列中的时区信息不会被保留。
- en: '[PRE266]'
  id: totrans-1273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE266]'
- en: Write to an orc file.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: 写入orc文件。
- en: '[PRE267]'
  id: totrans-1275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE267]'
- en: Read from an orc file.
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: 从orc文件中读取。
- en: '[PRE268]'
  id: totrans-1277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE268]'
- en: Read only certain columns of an orc file.
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: 仅读取orc文件的某些列。
- en: '[PRE269]  ## SQL queries'
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE269]  ## SQL查询'
- en: The `pandas.io.sql` module provides a collection of query wrappers to both facilitate
    data retrieval and to reduce dependency on DB-specific API.
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas.io.sql`模块提供了一组查询包装器，旨在促进数据检索并减少对特定于数据库的API的依赖。'
- en: Where available, users may first want to opt for [Apache Arrow ADBC](https://arrow.apache.org/adbc/current/index.html)
    drivers. These drivers should provide the best performance, null handling, and
    type detection.
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: 如有可能，用户可能首选选择[Apache Arrow ADBC](https://arrow.apache.org/adbc/current/index.html)驱动程序。这些驱动程序应提供最佳性能、空值处理和类型检测。
- en: 'New in version 2.2.0: Added native support for ADBC drivers'
  id: totrans-1282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 版本2.2.0中的新功能：增加了对ADBC驱动程序的本机支持
- en: For a full list of ADBC drivers and their development status, see the [ADBC
    Driver Implementation Status](https://arrow.apache.org/adbc/current/driver/status.html)
    documentation.
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: 有关ADBC驱动程序及其开发状态的完整列表，请参阅[ADBC驱动程序实现状态](https://arrow.apache.org/adbc/current/driver/status.html)文档。
- en: Where an ADBC driver is not available or may be missing functionality, users
    should opt for installing SQLAlchemy alongside their database driver library.
    Examples of such drivers are [psycopg2](https://www.psycopg.org/) for PostgreSQL
    or [pymysql](https://github.com/PyMySQL/PyMySQL) for MySQL. For [SQLite](https://docs.python.org/3/library/sqlite3.html)
    this is included in Python’s standard library by default. You can find an overview
    of supported drivers for each SQL dialect in the [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/dialects/index.html).
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有ADBC驱动程序或缺少功能，则用户应选择安装SQLAlchemy以及其数据库驱动程序库。这些驱动程序的示例是[psycopg2](https://www.psycopg.org/)用于PostgreSQL或[pymysql](https://github.com/PyMySQL/PyMySQL)用于MySQL。对于[SQLite](https://docs.python.org/3/library/sqlite3.html)，这在Python的标准库中默认包含。您可以在[SQLAlchemy文档](https://docs.sqlalchemy.org/en/latest/dialects/index.html)中找到每个SQL方言支持的驱动程序的概述。
- en: If SQLAlchemy is not installed, you can use a [`sqlite3.Connection`](https://docs.python.org/3/library/sqlite3.html#sqlite3.Connection
    "(in Python v3.12)") in place of a SQLAlchemy engine, connection, or URI string.
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未安装SQLAlchemy，可以使用[`sqlite3.Connection`](https://docs.python.org/3/library/sqlite3.html#sqlite3.Connection
    "(在Python v3.12中)")代替SQLAlchemy引擎、连接或URI字符串。
- en: See also some [cookbook examples](cookbook.html#cookbook-sql) for some advanced
    strategies.
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以查看一些[烹饪书示例](cookbook.html#cookbook-sql)以获取一些高级策略。
- en: 'The key functions are:'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: 关键函数包括：
- en: '| [`read_sql_table`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table")(table_name, con[, schema, ...]) | Read SQL database table
    into a DataFrame. |'
  id: totrans-1288
  prefs: []
  type: TYPE_TB
  zh: '| [`read_sql_table`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table")(table_name, con[, schema, ...]) | 将 SQL 数据库表读取到数据框中。
    |'
- en: '| [`read_sql_query`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query")(sql, con[, index_col, ...]) | Read SQL query into a DataFrame.
    |'
  id: totrans-1289
  prefs: []
  type: TYPE_TB
  zh: '| [`read_sql_query`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query")(sql, con[, index_col, ...]) | 将 SQL 查询读取到数据框中。 |'
- en: '| [`read_sql`](../reference/api/pandas.read_sql.html#pandas.read_sql "pandas.read_sql")(sql, con[, index_col, ...])
    | Read SQL query or database table into a DataFrame. |'
  id: totrans-1290
  prefs: []
  type: TYPE_TB
  zh: '| [`read_sql`](../reference/api/pandas.read_sql.html#pandas.read_sql "pandas.read_sql")(sql, con[, index_col, ...])
    | 将 SQL 查询或数据库表读取到数据框中。 |'
- en: '| [`DataFrame.to_sql`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql")(name, con, *[, schema, ...]) | Write records stored
    in a DataFrame to a SQL database. |'
  id: totrans-1291
  prefs: []
  type: TYPE_TB
  zh: '| [`DataFrame.to_sql`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql")(name, con, *[, schema, ...]) | 将存储在数据框中的记录写入 SQL 数据库。
    |'
- en: Note
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The function [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") is a convenience wrapper around [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") (and for backward compatibility) and will delegate to
    specific function depending on the provided input (database table name or sql
    query). Table names do not need to be quoted if they have special characters.
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: 函数[`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql "pandas.read_sql")是对[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table")和[`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query")（以及向后兼容性）的便捷包装，并根据提供的输入（数据库表名或 SQL 查询）委托给特定函数。如果表名包含特殊字符，则不需要对表名加引号。
- en: In the following example, we use the [SQlite](https://www.sqlite.org/index.html)
    SQL database engine. You can use a temporary SQLite database where data are stored
    in “memory”.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用 [SQlite](https://www.sqlite.org/index.html) SQL 数据库引擎。您可以使用一个临时
    SQLite 数据库，其中数据存储在“内存”中。
- en: To connect using an ADBC driver you will want to install the `adbc_driver_sqlite`
    using your package manager. Once installed, you can use the DBAPI interface provided
    by the ADBC driver to connect to your database.
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 ADBC 驱动程序进行连接，您需要使用包管理器安装`adbc_driver_sqlite`。安装后，您可以使用 ADBC 驱动程序提供的 DBAPI
    接口连接到数据库。
- en: '[PRE270]'
  id: totrans-1296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE270]'
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to. For more information on `create_engine()` and
    the URI formatting, see the examples below and the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 SQLAlchemy 进行连接，您可以使用`create_engine()`函数从数据库 URI 创建一个引擎对象。您只需要为每个要连接的数据库创建一次引擎。有关`create_engine()`和
    URI 格式化的更多信息，请参见下面的示例和 SQLAlchemy [文档](https://docs.sqlalchemy.org/en/latest/core/engines.html)
- en: '[PRE271]'
  id: totrans-1298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE271]'
- en: If you want to manage your own connections you can pass one of those instead.
    The example below opens a connection to the database using a Python context manager
    that automatically closes the connection after the block has completed. See the
    [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/core/connections.html#basic-usage)
    for an explanation of how the database connection is handled.
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想管理自己的连接，可以传递其中一个。下面的示例使用 Python 上下文管理器打开与数据库的连接，在块完成后自动关闭连接。请参阅[SQLAlchemy
    文档](https://docs.sqlalchemy.org/en/latest/core/connections.html#basic-usage)了解数据库连接是如何处理的。
- en: '[PRE272]'
  id: totrans-1300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE272]'
- en: Warning
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: When you open a connection to a database you are also responsible for closing
    it. Side effects of leaving a connection open may include locking the database
    or other breaking behaviour.
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打开与数据库的连接时，你也有责任关闭它。保持连接打开的副作用可能包括锁定数据库或其他破坏性行为。
- en: Writing DataFrames
  id: totrans-1303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写入数据框
- en: Assuming the following data is in a `DataFrame` `data`, we can insert it into
    the database using [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql").
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 假设以下数据存储在一个`DataFrame` `data`中，我们可以使用[`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql)将其插入到数据库中。
- en: '| id | Date | Col_1 | Col_2 | Col_3 |'
  id: totrans-1305
  prefs: []
  type: TYPE_TB
  zh: '| id | 日期 | 列_1 | 列_2 | 列_3 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1306
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 26 | 2012-10-18 | X | 25.7 | True |'
  id: totrans-1307
  prefs: []
  type: TYPE_TB
  zh: '| 26 | 2012-10-18 | X | 25.7 | True |'
- en: '| 42 | 2012-10-19 | Y | -12.4 | False |'
  id: totrans-1308
  prefs: []
  type: TYPE_TB
  zh: '| 42 | 2012-10-19 | Y | -12.4 | False |'
- en: '| 63 | 2012-10-20 | Z | 5.73 | True |'
  id: totrans-1309
  prefs: []
  type: TYPE_TB
  zh: '| 63 | 2012-10-20 | Z | 5.73 | True |'
- en: '[PRE273]'
  id: totrans-1310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE273]'
- en: 'With some databases, writing large DataFrames can result in errors due to packet
    size limitations being exceeded. This can be avoided by setting the `chunksize`
    parameter when calling `to_sql`. For example, the following writes `data` to the
    database in batches of 1000 rows at a time:'
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些数据库中，写入大型 DataFrame 可能会因超出数据包大小限制而导致错误。可以通过在调用 `to_sql` 时设置 `chunksize` 参数来避免这种情况。例如，以下代码将以每次
    1000 行的批量方式将 `data` 写入数据库：
- en: '[PRE274]'
  id: totrans-1312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE274]'
- en: SQL data types
  id: totrans-1313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SQL 数据类型
- en: Ensuring consistent data type management across SQL databases is challenging.
    Not every SQL database offers the same types, and even when they do the implementation
    of a given type can vary in ways that have subtle effects on how types can be
    preserved.
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 确保跨 SQL 数据库的一致数据类型管理具有挑战性。并非每个 SQL 数据库都提供相同的类型，即使提供了，给定类型的实现方式也可能有微妙的差异，对类型的保留方式可能产生细微影响。
- en: 'For the best odds at preserving database types users are advised to use ADBC
    drivers when available. The Arrow type system offers a wider array of types that
    more closely match database types than the historical pandas/NumPy type system.
    To illustrate, note this (non-exhaustive) listing of types available in different
    databases and pandas backends:'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大限度地保留数据库类型，建议用户在可用时使用 ADBC 驱动程序。Arrow 类型系统提供了更广泛的类型数组，与历史上的 pandas/NumPy
    类型系统更接近匹配数据库类型。举例来说，注意一下不同数据库和 pandas 后端中可用的类型（非穷尽列表）：
- en: '| numpy/pandas | arrow | postgres | sqlite |'
  id: totrans-1316
  prefs: []
  type: TYPE_TB
  zh: '| numpy/pandas | arrow | postgres | sqlite |'
- en: '| --- | --- | --- | --- |'
  id: totrans-1317
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
  id: totrans-1318
  prefs: []
  type: TYPE_TB
  zh: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
- en: '| int32/Int32 | int32 | INTEGER | INTEGER |'
  id: totrans-1319
  prefs: []
  type: TYPE_TB
  zh: '| int32/Int32 | int32 | INTEGER | INTEGER |'
- en: '| int64/Int64 | int64 | BIGINT | INTEGER |'
  id: totrans-1320
  prefs: []
  type: TYPE_TB
  zh: '| int64/Int64 | int64 | BIGINT | INTEGER |'
- en: '| float32 | float32 | REAL | REAL |'
  id: totrans-1321
  prefs: []
  type: TYPE_TB
  zh: '| float32 | float32 | REAL | REAL |'
- en: '| float64 | float64 | DOUBLE PRECISION | REAL |'
  id: totrans-1322
  prefs: []
  type: TYPE_TB
  zh: '| float64 | float64 | DOUBLE PRECISION | REAL |'
- en: '| object | string | TEXT | TEXT |'
  id: totrans-1323
  prefs: []
  type: TYPE_TB
  zh: '| object | string | TEXT | TEXT |'
- en: '| bool | `bool_` | BOOLEAN |  |'
  id: totrans-1324
  prefs: []
  type: TYPE_TB
  zh: '| bool | `bool_` | BOOLEAN |  |'
- en: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
  id: totrans-1325
  prefs: []
  type: TYPE_TB
  zh: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
- en: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
  id: totrans-1326
  prefs: []
  type: TYPE_TB
  zh: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
- en: '|  | date32 | DATE |  |'
  id: totrans-1327
  prefs: []
  type: TYPE_TB
  zh: '|  | date32 | DATE |  |'
- en: '|  | month_day_nano_interval | INTERVAL |  |'
  id: totrans-1328
  prefs: []
  type: TYPE_TB
  zh: '|  | month_day_nano_interval | INTERVAL |  |'
- en: '|  | binary | BINARY | BLOB |'
  id: totrans-1329
  prefs: []
  type: TYPE_TB
  zh: '|  | binary | BINARY | BLOB |'
- en: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
  id: totrans-1330
  prefs: []
  type: TYPE_TB
  zh: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
- en: '|  | list | ARRAY [[1]](#f1) |  |'
  id: totrans-1331
  prefs: []
  type: TYPE_TB
  zh: '|  | list | ARRAY [[1]](#f1) |  |'
- en: '|  | struct |'
  id: totrans-1332
  prefs: []
  type: TYPE_TB
  zh: '|  | struct |'
- en: COMPOSITE TYPE
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: 复合类型
- en: '[[1]](#f1)'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#f1)'
- en: '|  |'
  id: totrans-1335
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Footnotes
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 脚注
- en: If you are interested in preserving database types as best as possible throughout
    the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"`
    argument of [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql")
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望在 DataFrame 的整个生命周期中尽可能保留数据库类型，建议用户利用 [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") 的 `dtype_backend="pyarrow"` 参数。
- en: '[PRE275]'
  id: totrans-1338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE275]'
- en: This will prevent your data from being converted to the traditional pandas/NumPy
    type system, which often converts SQL types in ways that make them impossible
    to round-trip.
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: 这将防止您的数据被转换为传统的 pandas/NumPy 类型系统，后者经常以使 SQL 类型无法往返的方式进行转换。
- en: In case an ADBC driver is not available, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") will try to map your data to an appropriate SQL data
    type based on the dtype of the data. When you have columns of dtype `object`,
    pandas will try to infer the data type.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有可用的 ADBC 驱动程序，[`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") 将尝试根据数据的 dtype 将数据映射到适当的 SQL 数据类型。当您有 dtype 为 `object`
    的列时，pandas 将尝试推断数据类型。
- en: 'You can always override the default type by specifying the desired SQL type
    of any of the columns by using the `dtype` argument. This argument needs a dictionary
    mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback
    mode). For example, specifying to use the sqlalchemy `String` type instead of
    the default `Text` type for string columns:'
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用 `dtype` 参数指定任何列的所需 SQL 类型来始终覆盖默认类型。该参数需要一个将列名映射到 SQLAlchemy 类型（或字符串以用于
    sqlite3 回退模式）的字典。例如，指定为字符串列使用 sqlalchemy 的 `String` 类型而不是默认的 `Text` 类型：
- en: '[PRE276]'
  id: totrans-1342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE276]'
- en: Note
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Due to the limited support for timedelta’s in the different database flavors,
    columns with type `timedelta64` will be written as integer values as nanoseconds
    to the database and a warning will be raised. The only exception to this is when
    using the ADBC PostgreSQL driver in which case a timedelta will be written to
    the database as an `INTERVAL`
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同数据库版本对 timedelta 的支持有限，类型为`timedelta64`的列将被写入为纳秒整数值到数据库中，并会引发警告。唯一的例外是在使用
    ADBC PostgreSQL 驱动程序时，此时 timedelta 将被写入数据库作为`INTERVAL`。
- en: Note
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Columns of `category` dtype will be converted to the dense representation as
    you would get with `np.asarray(categorical)` (e.g. for string categories this
    gives an array of strings). Because of this, reading the database table back in
    does **not** generate a categorical.
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: '`category` dtype 的列将被转换为密集表示，就像使用`np.asarray(categorical)`一样（例如，对于字符串类别，这将生成一个字符串数组）。因此，将数据库表重新读取时**不会**生成分类数据。'
- en: '### Datetime data types'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: '### 日期时间数据类型'
- en: Using ADBC or SQLAlchemy, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") is capable of writing datetime data that is timezone
    naive or timezone aware. However, the resulting data stored in the database ultimately
    depends on the supported data type for datetime data of the database system being
    used.
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ADBC 或 SQLAlchemy，[`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql)
    能够写入时区无关或时区感知的日期时间数据。然而，最终存储在数据库中的数据取决于所使用的数据库系统支持的日期时间数据类型。
- en: The following table lists supported data types for datetime data for some common
    databases. Other database dialects may have different data types for datetime
    data.
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: 下表列出了一些常见数据库支持的日期时间数据类型。其他数据库方言可能有不同的日期时间数据类型。
- en: '| Database | SQL Datetime Types | Timezone Support |'
  id: totrans-1350
  prefs: []
  type: TYPE_TB
  zh: '| 数据库 | SQL 日期时间类型 | 时区支持 |'
- en: '| --- | --- | --- |'
  id: totrans-1351
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SQLite | `TEXT` | No |'
  id: totrans-1352
  prefs: []
  type: TYPE_TB
  zh: '| SQLite | `TEXT` | 否 |'
- en: '| MySQL | `TIMESTAMP` or `DATETIME` | No |'
  id: totrans-1353
  prefs: []
  type: TYPE_TB
  zh: '| MySQL | `TIMESTAMP` 或 `DATETIME` | 否 |'
- en: '| PostgreSQL | `TIMESTAMP` or `TIMESTAMP WITH TIME ZONE` | Yes |'
  id: totrans-1354
  prefs: []
  type: TYPE_TB
  zh: '| PostgreSQL | `TIMESTAMP` 或 `TIMESTAMP WITH TIME ZONE` | 是 |'
- en: When writing timezone aware data to databases that do not support timezones,
    the data will be written as timezone naive timestamps that are in local time with
    respect to the timezone.
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
  zh: 当将带有时区信息的数据写入不支持时区的数据库时，数据将被写入为相对于时区的本地时间的时区无关时间戳。
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") is also capable of reading datetime data that is timezone
    aware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will convert
    the data to UTC.'
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table)
    也能够读取时区感知或时区无关的日期时间数据。当读取`TIMESTAMP WITH TIME ZONE`类型时，pandas 将数据转换为 UTC 时间。'
- en: '#### Insertion method'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 插入方法'
- en: 'The parameter `method` controls the SQL insertion clause used. Possible values
    are:'
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`method`控制所使用的 SQL 插入子句。可能的值包括：
- en: '`None`: Uses standard SQL `INSERT` clause (one per row).'
  id: totrans-1359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`None`：使用标准 SQL `INSERT`子句（每行一个）。'
- en: '`''multi''`: Pass multiple values in a single `INSERT` clause. It uses a *special*
    SQL syntax not supported by all backends. This usually provides better performance
    for analytic databases like *Presto* and *Redshift*, but has worse performance
    for traditional SQL backend if the table contains many columns. For more information
    check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).'
  id: totrans-1360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''multi''`：在单个`INSERT`子句中传递多个值。它使用一种*特殊*的 SQL 语法，不是所有后端都支持。这通常对于像*Presto*和*Redshift*这样的分析数据库提供更好的性能，但如果表包含许多列，则传统
    SQL 后端的性能会更差。有关更多信息，请查看 SQLAlchemy 的[文档](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args)。'
- en: 'callable with signature `(pd_table, conn, keys, data_iter)`: This can be used
    to implement a more performant insertion method based on specific backend dialect
    features.'
  id: totrans-1361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有签名`(pd_table, conn, keys, data_iter)`的可调用函数：这可用于基于特定后端方言功能实现更高性能的插入方法。
- en: 'Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):'
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PostgreSQL 的示例可调用[复制子句](https://www.postgresql.org/docs/current/sql-copy.html)：
- en: '[PRE277]'
  id: totrans-1363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE277]'
- en: Reading tables
  id: totrans-1364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取表
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") will read a database table given the table name and optionally
    a subset of columns to read.'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table)
    将读取给定表名的数据库表，可选择性地读取一部分列。'
- en: Note
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In order to use [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table"), you **must** have the ADBC driver or SQLAlchemy optional
    dependency installed.
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table")，你**必须**安装ADBC驱动程序或SQLAlchemy可选依赖项。
- en: '[PRE278]'
  id: totrans-1368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE278]'
- en: Note
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: ADBC drivers will map database types directly back to arrow types. For other
    drivers note that pandas infers column dtypes from query outputs, and not by looking
    up data types in the physical database schema. For example, assume `userid` is
    an integer column in a table. Then, intuitively, `select userid ...` will return
    integer-valued series, while `select cast(userid as text) ...` will return object-valued
    (str) series. Accordingly, if the query output is empty, then all resulting columns
    will be returned as object-valued (since they are most general). If you foresee
    that your query will sometimes generate an empty result, you may want to explicitly
    typecast afterwards to ensure dtype integrity.
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: ADBC驱动程序将数据库类型直接映射回arrow类型。对于其他驱动程序，请注意pandas从查询输出中推断列dtype，而不是通过查找物理数据库模式中的数据类型。例如，假设`userid`是表中的整数列。那么，直观地，`select
    userid ...`将返回整数值系列，而`select cast(userid as text) ...`将返回对象值（str）系列。因此，如果查询输出为空，则所有生成的列将作为对象值返回（因为它们是最一般的）。如果你预见到你的查询有时会生成��结果，你可能希望在之后明确进行类型转换以确保dtype的完整性。
- en: You can also specify the name of the column as the `DataFrame` index, and specify
    a subset of columns to be read.
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以指定列的名称作为`DataFrame`索引，并指定要读取的列的子集。
- en: '[PRE279]'
  id: totrans-1372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE279]'
- en: 'And you can explicitly force columns to be parsed as dates:'
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以明确强制将列解析为日期：
- en: '[PRE280]'
  id: totrans-1374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE280]'
- en: 'If needed you can explicitly specify a format string, or a dict of arguments
    to pass to [`pandas.to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime"):'
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，你可以明确指定格式字符串，或传递给[`pandas.to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")的参数字典：
- en: '[PRE281]'
  id: totrans-1376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE281]'
- en: You can check if a table exists using `has_table()`
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`has_table()`可以检查表是否存在
- en: Schema support
  id: totrans-1378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模式支持
- en: 'Reading from and writing to different schema’s is supported through the `schema`
    keyword in the [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") functions. Note however that this depends on the database
    flavor (sqlite does not have schema’s). For example:'
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table")和[`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql")函数中的`schema`关键字支持从不同模式读取和写入。但请注意，这取决于数据库类型（sqlite没有模式）。例如：
- en: '[PRE282]'
  id: totrans-1380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE282]'
- en: Querying
  id: totrans-1381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询
- en: You can query using raw SQL in the [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function. In this case you must use the SQL variant appropriate
    for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression
    language constructs, which are database-agnostic.
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query")函数中使用原始SQL进行查询。在这种情况下，你必须使用适合你的数据库的SQL变体。当使用SQLAlchemy时，你还可以传递数据库无关的SQLAlchemy表达式语言构造。
- en: '[PRE283]'
  id: totrans-1383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE283]'
- en: Of course, you can specify a more “complex” query.
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以指定一个更“复杂”的查询。
- en: '[PRE284]'
  id: totrans-1385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE284]'
- en: 'The [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function supports a `chunksize` argument. Specifying
    this will return an iterator through chunks of the query result:'
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query")函数支持`chunksize`参数。指定这个参数将返回查询结果的迭代器：'
- en: '[PRE285]'
  id: totrans-1387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE285]'
- en: '[PRE286]'
  id: totrans-1388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE286]'
- en: Engine connection examples
  id: totrans-1389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引擎连接示例
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to.
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用SQLAlchemy连接，你可以使用`create_engine()`函数从数据库URI创建一个引擎对象。你只需要为每个要连接的数据库创建一次引擎。
- en: '[PRE287]'
  id: totrans-1391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE287]'
- en: For more information see the examples the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参阅SQLAlchemy文档中的示例[文档](https://docs.sqlalchemy.org/en/latest/core/engines.html)
- en: Advanced SQLAlchemy queries
  id: totrans-1393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级SQLAlchemy查询
- en: You can use SQLAlchemy constructs to describe your query.
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用SQLAlchemy构造描述你的查询。
- en: Use `sqlalchemy.text()` to specify query parameters in a backend-neutral way
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sqlalchemy.text()`以与后端无关的方式指定查询参数
- en: '[PRE288]'
  id: totrans-1396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE288]'
- en: If you have an SQLAlchemy description of your database you can express where
    conditions using SQLAlchemy expressions
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有数据库的 SQLAlchemy 描述，可以使用 SQLAlchemy 表达式表示 where 条件
- en: '[PRE289]'
  id: totrans-1398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE289]'
- en: You can combine SQLAlchemy expressions with parameters passed to [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") using `sqlalchemy.bindparam()`
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将 SQLAlchemy 表达式与传递给 [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") 的参数结合使用 `sqlalchemy.bindparam()`
- en: '[PRE290]'
  id: totrans-1400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE290]'
- en: Sqlite fallback
  id: totrans-1401
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sqlite 回退
- en: The use of sqlite is supported without using SQLAlchemy. This mode requires
    a Python database adapter which respect the [Python DB-API](https://www.python.org/dev/peps/pep-0249/).
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: 支持使用 sqlite 而不使用 SQLAlchemy。此模式需要一个遵守 [Python DB-API](https://www.python.org/dev/peps/pep-0249/)
    的 Python 数据库适配器。
- en: 'You can create connections like so:'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以这样创建连接：
- en: '[PRE291]'
  id: totrans-1404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE291]'
- en: 'And then issue the following queries:'
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: 然后发出以下查询：
- en: '[PRE292]  ## Google BigQuery'
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE292]  ## Google BigQuery'
- en: The `pandas-gbq` package provides functionality to read/write from Google BigQuery.
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas-gbq` 包提供了与 Google BigQuery 读写的功能。'
- en: pandas integrates with this external package. if `pandas-gbq` is installed,
    you can use the pandas methods `pd.read_gbq` and `DataFrame.to_gbq`, which will
    call the respective functions from `pandas-gbq`.
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 与这个外部包集成。如果安装了 `pandas-gbq`，则可以使用 pandas 方法 `pd.read_gbq` 和 `DataFrame.to_gbq`，这将调用
    `pandas-gbq` 中的相应函数。
- en: 'Full documentation can be found [here](https://pandas-gbq.readthedocs.io/en/latest/).  ##
    Stata format'
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: '完整文档可以在[这里](https://pandas-gbq.readthedocs.io/en/latest/)找到。  ## Stata 格式'
- en: '### Writing to stata format'
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: '### 写入到 Stata 格式'
- en: The method `DataFrame.to_stata()` will write a DataFrame into a .dta file. The
    format version of this file is always 115 (Stata 12).
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 `DataFrame.to_stata()` 将 DataFrame 写入 .dta 文件。此文件的格式版本始终为 115（Stata 12）。
- en: '[PRE293]'
  id: totrans-1412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE293]'
- en: '*Stata* data files have limited data type support; only strings with 244 or
    fewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be stored
    in `.dta` files. Additionally, *Stata* reserves certain values to represent missing
    data. Exporting a non-missing value that is outside of the permitted range in
    Stata for a particular data type will retype the variable to the next larger size.
    For example, `int8` values are restricted to lie between -127 and 100 in Stata,
    and so variables with values above 100 will trigger a conversion to `int16`. `nan`
    values in floating points data types are stored as the basic missing data type
    (`.` in *Stata*).'
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: '*Stata* 数据文件具有有限的数据类型支持；只能在 `.dta` 文件中存储包含 244 个或更少字符的字符串，`int8`, `int16`,
    `int32`, `float32` 和 `float64`。此外，*Stata* 保留某些值来表示缺失数据。导出特定数据类型的非缺失值超出 Stata 允许范围的值将重新定义变量为下一个更大的大小。例如，在
    Stata 中，`int8` 值限制在 -127 和 100 之间，因此值大于 100 的变量将触发转换为 `int16`。浮点数据类型中的 `nan` 值存储为基本缺失数据类型（*Stata*
    中的 `.`）。'
- en: Note
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is not possible to export missing data values for integer data types.
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 无法导出整数数据类型的缺失数据值。
- en: The *Stata* writer gracefully handles other data types including `int64`, `bool`,
    `uint8`, `uint16`, `uint32` by casting to the smallest supported type that can
    represent the data. For example, data with a type of `uint8` will be cast to `int8`
    if all values are less than 100 (the upper bound for non-missing `int8` data in
    *Stata*), or, if values are outside of this range, the variable is cast to `int16`.
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: '*Stata* 写入器优雅地处理其他数据类型，包括 `int64`, `bool`, `uint8`, `uint16`, `uint32`，通过将其转换为可以表示数据的最小支持类型。例如，类型为
    `uint8` 的数据将被转换为 `int8`，如果所有值都小于 100（*Stata* 中非缺失 `int8` 数据的上限），或者，如果值超出此范围，则变量将被转换为
    `int16`。'
- en: Warning
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Conversion from `int64` to `float64` may result in a loss of precision if `int64`
    values are larger than 2**53.
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `int64` 转换为 `float64` 可能会导致精度损失，如果 `int64` 值大于 2**53。
- en: Warning
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '`StataWriter` and `DataFrame.to_stata()` only support fixed width strings containing
    up to 244 characters, a limitation imposed by the version 115 dta file format.
    Attempting to write *Stata* dta files with strings longer than 244 characters
    raises a `ValueError`.  ### Reading from Stata format'
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: '`StataWriter` 和 `DataFrame.to_stata()` 仅支持包含最多 244 个字符的固定宽度字符串，这是版本 115 dta
    文件格式所施加的限制。尝试写入长度超过 244 个字符的字符串的 *Stata* dta 文件会引发 `ValueError`。  ### 从 Stata
    格式读取'
- en: The top-level function `read_stata` will read a dta file and return either a
    `DataFrame` or a `pandas.api.typing.StataReader` that can be used to read the
    file incrementally.
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层函数 `read_stata` 将读取一个 dta 文件，并返回一个 `DataFrame` 或一个 `pandas.api.typing.StataReader`，可用于逐步读取文件。
- en: '[PRE294]'
  id: totrans-1422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE294]'
- en: Specifying a `chunksize` yields a `pandas.api.typing.StataReader` instance that
    can be used to read `chunksize` lines from the file at a time. The `StataReader`
    object can be used as an iterator.
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`chunksize`会产生一个`pandas.api.typing.StataReader`实例，可以用来一次从文件中读取`chunksize`行。`StataReader`对象可以用作迭代器。
- en: '[PRE295]'
  id: totrans-1424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE295]'
- en: For more fine-grained control, use `iterator=True` and specify `chunksize` with
    each call to `read()`.
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得更精细的控制，请使用`iterator=True`并在每次调用`read()`时指定`chunksize`。
- en: '[PRE296]'
  id: totrans-1426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE296]'
- en: Currently the `index` is retrieved as a column.
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: 目前`index`被检索为一列。
- en: The parameter `convert_categoricals` indicates whether value labels should be
    read and used to create a `Categorical` variable from them. Value labels can also
    be retrieved by the function `value_labels`, which requires `read()` to be called
    before use.
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`convert_categoricals`指示是否应读取值标签并使用它们创建`Categorical`变量。值标签也可以通过函数`value_labels`检索，但在使用之前需要调用`read()`。
- en: The parameter `convert_missing` indicates whether missing value representations
    in Stata should be preserved. If `False` (the default), missing values are represented
    as `np.nan`. If `True`, missing values are represented using `StataMissingValue`
    objects, and columns containing missing values will have `object` data type.
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`convert_missing`指示是否应保留Stata中的缺失值表示。如果为`False`（默认值），缺失值将表示为`np.nan`。如果为`True`，缺失值将使用`StataMissingValue`对象表示，并且包含缺失值的列将具有`object`数据类型。
- en: Note
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '[`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") and `StataReader` support .dta formats 113-115 (Stata 10-12),
    117 (Stata 13), and 118 (Stata 14).'
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata")和`StataReader`支持.dta格式113-115（Stata 10-12）、117（Stata 13）和118（Stata
    14）。'
- en: Note
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Setting `preserve_dtypes=False` will upcast to the standard pandas data types:
    `int64` for all integer types and `float64` for floating point data. By default,
    the Stata data types are preserved when importing.'
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: ��置`preserve_dtypes=False`将升级为标准的pandas数据类型：所有整数类型为`int64`，浮点数据为`float64`。默认情况下，导入时保留Stata数据类型。
- en: Note
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All `StataReader` objects, whether created by [`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") (when using `iterator=True` or `chunksize`) or instantiated
    by hand, must be used as context managers (e.g. the `with` statement). While the
    `close()` method is available, its use is unsupported. It is not part of the public
    API and will be removed in with future without warning.
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: 所有`StataReader`对象，无论是由[`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata")（使用`iterator=True`或`chunksize`）创建还是手动实例化，都必须作为上下文管理器使用（例如`with`语句）。虽然`close()`方法可用，但不受支持。它不是公共API的一部分，并将在未来的某个时候被删除而没有警告。
- en: '#### Categorical data'
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 分类数据'
- en: '`Categorical` data can be exported to *Stata* data files as value labeled data.
    The exported data consists of the underlying category codes as integer data values
    and the categories as value labels. *Stata* does not have an explicit equivalent
    to a `Categorical` and information about *whether* the variable is ordered is
    lost when exporting.'
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: '`分类`数据可以导出为*Stata*数据文件，作为带有值标签的数据。导出的数据包括底层类别代码作为整数数据值和类别作为值标签。在导出时，*Stata*没有明确的等价`Categorical`，并且关于变量是否有序的信息会丢失。'
- en: Warning
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: '*Stata* only supports string value labels, and so `str` is called on the categories
    when exporting data. Exporting `Categorical` variables with non-string categories
    produces a warning, and can result a loss of information if the `str` representations
    of the categories are not unique.'
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: '*Stata*仅支持字符串值标签，因此在导出数据时会调用`str`。使用非字符串类别导出`Categorical`变量会产生警告，并且如果类别的`str`表示不唯一，则可能导致信息丢失。'
- en: Labeled data can similarly be imported from *Stata* data files as `Categorical`
    variables using the keyword argument `convert_categoricals` (`True` by default).
    The keyword argument `order_categoricals` (`True` by default) determines whether
    imported `Categorical` variables are ordered.
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，可以使用关键字参数`convert_categoricals`（默认为`True`）从*Stata*数据文件中导入带有值标签的`Categorical`变量。关键字参数`order_categoricals`（默认为`True`）确定导入的`Categorical`变量是否有序。
- en: Note
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since `Categorical` variables always use integer data
    types between `-1` and `n-1` where `n` is the number of categories. If the original
    values in the *Stata* data file are required, these can be imported by setting
    `convert_categoricals=False`, which will import original data (but not the variable
    labels). The original values can be matched to the imported categorical data since
    there is a simple mapping between the original *Stata* data values and the category
    codes of imported Categorical variables: missing values are assigned code `-1`,
    and the smallest original value is assigned `0`, the second smallest is assigned
    `1` and so on until the largest original value is assigned the code `n-1`.'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入分类数据时，*Stata* 数据文件中的变量值不会被保留，因为`Categorical`变量始终使用介于`-1`和`n-1`之间的整数数据类型，其中`n`是类别数。如果需要原始值，可以通过设置`convert_categoricals=False`来导入原始数据（但不包括变量标签）。原始值可以与导入的分类数据匹配，因为原始*Stata*数据值与导入的`Categorical`变量的类别代码之间存在简单的映射：缺失值被分配代码`-1`，最小的原始值被分配`0`，第二小的被分配`1`，依此类推，直到最大的原始值被分配代码`n-1`。
- en: Note
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a `Categorical` with string categories for the values that are labeled and numeric
    categories for values with no label.  ## SAS formats'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: '*Stata* 支持部分标记的系列。这些系列对一些数据值有值标签，但并非所有数据值都有。导入部分标记的系列将产生一个具有字符串类别的`Categorical`，对于已标记的值和没有标记的值，将产生数值类别。  ##
    SAS 格式'
- en: The top-level function [`read_sas()`](../reference/api/pandas.read_sas.html#pandas.read_sas
    "pandas.read_sas") can read (but not write) SAS XPORT (.xpt) and SAS7BDAT (.sas7bdat)
    format files.
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层函数[`read_sas()`](../reference/api/pandas.read_sas.html#pandas.read_sas "pandas.read_sas")可以读取（但不能写入）SAS
    XPORT（.xpt）和 SAS7BDAT（.sas7bdat）格式文件。
- en: 'SAS files only contain two value types: ASCII text and floating point values
    (usually 8 bytes but sometimes truncated). For xport files, there is no automatic
    type conversion to integers, dates, or categoricals. For SAS7BDAT files, the format
    codes may allow date variables to be automatically converted to dates. By default
    the whole file is read and returned as a `DataFrame`.'
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: SAS 文件只包含两种值类型：ASCII 文本和浮点值（通常为8字节，但有时被截断）。对于 xport 文件，没有自动将类型转换为整数、日期或分类变量。对于
    SAS7BDAT 文件，格式代码可能允许日期变量自动转换为日期。默认情况下，整个文件被读取并返回为`DataFrame`。
- en: Specify a `chunksize` or use `iterator=True` to obtain reader objects (`XportReader`
    or `SAS7BDATReader`) for incrementally reading the file. The reader objects also
    have attributes that contain additional information about the file and its variables.
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`chunksize`或使用`iterator=True`以获取读取器对象（`XportReader`或`SAS7BDATReader`），以逐步读取文件。读取器对象还具有包含有关文件及其变量的其他信息的属性。
- en: 'Read a SAS7BDAT file:'
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: 读取一个 SAS7BDAT 文件：
- en: '[PRE297]'
  id: totrans-1449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE297]'
- en: 'Obtain an iterator and read an XPORT file 100,000 lines at a time:'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: 获取一个迭代器，并每次读取一个 XPORT 文件的 100,000 行：
- en: '[PRE298]'
  id: totrans-1451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE298]'
- en: The [specification](https://support.sas.com/content/dam/SAS/support/en/technical-papers/record-layout-of-a-sas-version-5-or-6-data-set-in-sas-transport-xport-format.pdf)
    for the xport file format is available from the SAS web site.
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
  zh: 可从 SAS 网站获取 xport 文件格式的[规范](https://support.sas.com/content/dam/SAS/support/en/technical-papers/record-layout-of-a-sas-version-5-or-6-data-set-in-sas-transport-xport-format.pdf)。
- en: 'No official documentation is available for the SAS7BDAT format.  ## SPSS formats'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: '没有关于 SAS7BDAT 格式的官方文档。  ## SPSS 格式'
- en: The top-level function [`read_spss()`](../reference/api/pandas.read_spss.html#pandas.read_spss
    "pandas.read_spss") can read (but not write) SPSS SAV (.sav) and ZSAV (.zsav)
    format files.
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层函数[`read_spss()`](../reference/api/pandas.read_spss.html#pandas.read_spss
    "pandas.read_spss")可以读取（但不能写入）SPSS SAV（.sav）和 ZSAV（.zsav）格式文件。
- en: SPSS files contain column names. By default the whole file is read, categorical
    columns are converted into `pd.Categorical`, and a `DataFrame` with all columns
    is returned.
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: SPSS 文件包含列名。默认情况下，整个文件被读取，分类列被转换为`pd.Categorical`，并返回一个包含所有列的`DataFrame`。
- en: Specify the `usecols` parameter to obtain a subset of columns. Specify `convert_categoricals=False`
    to avoid converting categorical columns into `pd.Categorical`.
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`usecols`参数以获取列的子集。指定`convert_categoricals=False`以避免将分类列转换为`pd.Categorical`。
- en: 'Read an SPSS file:'
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: 读取一个 SPSS 文件：
- en: '[PRE299]'
  id: totrans-1458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE299]'
- en: 'Extract a subset of columns contained in `usecols` from an SPSS file and avoid
    converting categorical columns into `pd.Categorical`:'
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: 从SPSS文件中提取`usecols`中包含的列的子集，并避免将分类列转换为`pd.Categorical`：
- en: '[PRE300]'
  id: totrans-1460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE300]'
- en: 'More information about the SAV and ZSAV file formats is available [here](https://www.ibm.com/docs/en/spss-statistics/22.0.0).  ##
    Other file formats'
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: '有关SAV和ZSAV文件格式的更多信息，请参阅[此处](https://www.ibm.com/docs/en/spss-statistics/22.0.0)。  ##
    其他文件格式'
- en: pandas itself only supports IO with a limited set of file formats that map cleanly
    to its tabular data model. For reading and writing other file formats into and
    from pandas, we recommend these packages from the broader community.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: pandas本身仅支持与其表格数据模型清晰映射的有限一组文件格式的IO。为了将其他文件格式读取和写入pandas，我们建议使用来自更广泛社区的这些软件包。
- en: netCDF
  id: totrans-1463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: netCDF
- en: '[xarray](https://xarray.pydata.org/en/stable/) provides data structures inspired
    by the pandas `DataFrame` for working with multi-dimensional datasets, with a
    focus on the netCDF file format and easy conversion to and from pandas.  ## Performance
    considerations'
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: '[xarray](https://xarray.pydata.org/en/stable/)提供了受到pandas `DataFrame`启发的数据结构，用于处理多维数据集，重点放在netCDF文件格式上，并且易于与pandas之间进行转换。  ##
    性能考虑'
- en: This is an informal comparison of various IO methods, using pandas 0.24.2\.
    Timings are machine dependent and small differences should be ignored.
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对各种IO方法的非正式比较，使用pandas 0.24.2。时间取决于机器，应忽略小差异。
- en: '[PRE301]'
  id: totrans-1466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE301]'
- en: 'The following test functions will be used below to compare the performance
    of several IO methods:'
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 下面将使用以下测试函数来比较几种IO方法的性能：
- en: '[PRE302]'
  id: totrans-1468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE302]'
- en: When writing, the top three functions in terms of speed are `test_feather_write`,
    `test_hdf_fixed_write` and `test_hdf_fixed_write_compress`.
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入时，速度最快的三个函数是`test_feather_write`，`test_hdf_fixed_write`和`test_hdf_fixed_write_compress`。
- en: '[PRE303]'
  id: totrans-1470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE303]'
- en: When reading, the top three functions in terms of speed are `test_feather_read`,
    `test_pickle_read` and `test_hdf_fixed_read`.
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取时，速度最快的三个函数是`test_feather_read`，`test_pickle_read`和`test_hdf_fixed_read`。
- en: '[PRE304]'
  id: totrans-1472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE304]'
- en: The files `test.pkl.compress`, `test.parquet` and `test.feather` took the least
    space on disk (in bytes).
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: 文件`test.pkl.compress`，`test.parquet`和`test.feather`在磁盘上占用的空间最少（以字节为单位）。
- en: '[PRE305]  ## CSV & text files'
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE305]  ## CSV和文本文件'
- en: The workhorse function for reading text files (a.k.a. flat files) is [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"). See the [cookbook](cookbook.html#cookbook-csv) for some advanced
    strategies.
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
  zh: 用于读取文本文件（也称为平面文件）的主要函数是[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")。查看[cookbook](cookbook.html#cookbook-csv)以获取一些高级策略。
- en: Parsing options
  id: totrans-1476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析选项
- en: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    accepts the following common arguments:'
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")接受以下常见参数：'
- en: Basic
  id: totrans-1478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本
- en: filepath_or_buffervarious
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: filepath_or_buffervarious
- en: Either a path to a file (a [`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)"), or `py:py._path.local.LocalPath`), URL (including http,
    ftp, and S3 locations), or any object with a `read()` method (such as an open
    file or [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO "(in
    Python v3.12)")).
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: 要么是文件路径（[`str`](https://docs.python.org/3/library/stdtypes.html#str "(在Python
    v3.12中)")），[`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(在Python v3.12中)")，或`py:py._path.local.LocalPath`），URL（包括http，ftp和S3位置），或具有`read()`方法的任何对象（例如打开的文件或[`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO
    "(在Python v3.12中)")）。
- en: sepstr, defaults to `','` for [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), `\t` for [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: sepstr，默认为`','`用于[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")，`\t`用于[`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
- en: 'Delimiter to use. If sep is `None`, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will be used
    and automatically detect the separator by Python’s builtin sniffer tool, [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)"). In addition, separators longer than 1 character and different
    from `''\s+''` will be interpreted as regular expressions and will also force
    the use of the Python parsing engine. Note that regex delimiters are prone to
    ignoring quoted data. Regex example: `''\\r\\t''`.'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的分隔符。如果sep为`None`，C引擎无法自动检测分隔符，但Python解析引擎可以，这意味着将使用后者，并通过Python的内置sniffer工具[`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)")自动检测分隔符。此外，长于1个字符且不同于`'\s+'`的分隔符将被解释为正则表达式，并且还会强制使用Python解析引擎。请注意，正则表达式分隔符容易忽略带引号的数据。正则表达式示例：`'\\r\\t'`。
- en: delimiterstr, default `None`
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔符字符串，默认为`None`
- en: Alternative argument name for sep.
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
  zh: sep的替代参数名称。
- en: delim_whitespaceboolean, default False
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
  zh: delim_whitespace布尔值，默认为False
- en: Specifies whether or not whitespace (e.g. `' '` or `'\t'`) will be used as the
    delimiter. Equivalent to setting `sep='\s+'`. If this option is set to `True`,
    nothing should be passed in for the `delimiter` parameter.
  id: totrans-1486
  prefs: []
  type: TYPE_NORMAL
  zh: 指定是否使用空格（例如`' '`或`'\t'`）作为分隔符。等效于设置`sep='\s+'`。如果将此选项设置为`True`，则不应为`delimiter`参数传递任何内容。
- en: Column and index locations and names
  id: totrans-1487
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 列和索引位置及名称
- en: headerint or list of ints, default `'infer'`
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
  zh: headerint或int列表，默认为`'infer'`
- en: 'Row number(s) to use as the column names, and the start of the data. Default
    behavior is to infer the column names: if no names are passed the behavior is
    identical to `header=0` and column names are inferred from the first line of the
    file, if column names are passed explicitly then the behavior is identical to
    `header=None`. Explicitly pass `header=0` to be able to replace existing names.'
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: 用作列名和数据起始位置的行号。默认行为是推断列名：如果没有传递名称，则行为与`header=0`相同，并且列名从文件的第一行推断出来，如果显式传递了列名，则行为与`header=None`相同。显式传递`header=0`以能够替换现有名称。
- en: The header can be a list of ints that specify row locations for a MultiIndex
    on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this parameter ignores
    commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes
    the first line of data rather than the first line of the file.
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
  zh: 表头可以是指定列的MultiIndex的行位置列表，例如`[0,1,3]`。未指定的中间行将被跳过（例如，在此示例中跳过了2）。请注意，如果`skip_blank_lines=True`，此参数将忽略注释行和空行，因此`header=0`表示数据的第一行而不是文件的第一行。
- en: namesarray-like, default `None`
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
  zh: names数组样式，默认为`None`
- en: List of column names to use. If file contains no header row, then you should
    explicitly pass `header=None`. Duplicates in this list are not allowed.
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的列名列表。如果文件不包含表头行，则应明确传递`header=None`。不允许在此列表中存在重复项。
- en: index_colint, str, sequence of int / str, or False, optional, default `None`
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: index_colint、str、int/str序列或False，可选，默认为`None`
- en: Column(s) to use as the row labels of the `DataFrame`, either given as string
    name or column index. If a sequence of int / str is given, a MultiIndex is used.
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: 用作`DataFrame`行标签的列，可以是字符串名称或列索引。如果给定int/str序列，则使用MultiIndex。
- en: Note
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`index_col=False` can be used to force pandas to *not* use the first column
    as the index, e.g. when you have a malformed file with delimiters at the end of
    each line.'
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_col=False`可用于强制pandas*不*将第一列用作索引，例如当您有一个每行末尾都有分隔符的格式不正确的文件时。'
- en: The default value of `None` instructs pandas to guess. If the number of fields
    in the column header row is equal to the number of fields in the body of the data
    file, then a default index is used. If it is larger, then the first columns are
    used as index so that the remaining number of fields in the body are equal to
    the number of fields in the header.
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: '`None`的默认值指示pandas进行猜测。如果列头行中的字段数等于数据文件主体中的字段数，则使用默认索引。如果大于，则使用前几列作为索引，以使数据主体中的字段数等于列头中的字段数。'
- en: The first row after the header is used to determine the number of columns, which
    will go into the index. If the subsequent rows contain less columns than the first
    row, they are filled with `NaN`.
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
  zh: 表头后的第一行用于确定列数，这些列将进入索引。如果后续行的列数少于第一行，则用`NaN`填充。
- en: This can be avoided through `usecols`. This ensures that the columns are taken
    as is and the trailing data are ignored.
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: 可通过 `usecols` 避免这种情况。这确保列按原样采取，并且尾随数据被忽略。
- en: usecolslist-like or callable, default `None`
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: usecols 类似列表或可调用函数，默认为 `None`。
- en: Return a subset of the columns. If list-like, all elements must either be positional
    (i.e. integer indices into the document columns) or strings that correspond to
    column names provided either by the user in `names` or inferred from the document
    header row(s). If `names` are given, the document header row(s) are not taken
    into account. For example, a valid list-like `usecols` parameter would be `[0,
    1, 2]` or `['foo', 'bar', 'baz']`.
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
  zh: 返回列的子集。如果类似列表，所有元素必须是位置的（即整数索引到文档列）或与用户在 `names` 中提供的列名对应的字符串，或从文档标题行中推断出的列名。如果给定了
    `names`，则不考虑文档标题行。例如，一个有效的类似列表 `usecols` 参数可以是 `[0, 1, 2]` 或 `['foo', 'bar', 'baz']`。
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate
    a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo',
    'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: 元素顺序被忽略，因此 `usecols=[0, 1]` 与 `[1, 0]` 相同。要从保留元素顺序的 `data` 实例化 DataFrame，请使用
    `pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]` 以 `['foo', 'bar']`
    顺序的列或 `pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]` 以 `['bar', 'foo']`
    顺序。
- en: 'If callable, the callable function will be evaluated against the column names,
    returning names where the callable function evaluates to True:'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可调用，则将对列名评估可调用函数，返回可调用函数评估为 True 的列名：
- en: '[PRE306]'
  id: totrans-1504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE306]'
- en: Using this parameter results in much faster parsing time and lower memory usage
    when using the c engine. The Python engine loads the data first before deciding
    which columns to drop.
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此参数可在使用 c 引擎时获得更快的解析时间和更低的内存使用。Python 引擎在决定要删除哪些列之前会先加载数据。
- en: General parsing configuration
  id: totrans-1506
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通用解析配置
- en: dtypeType name or dict of column -> type, default `None`
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
  zh: dtype 类型名称或列 -> 类型的字典，默认为 `None`。
- en: 'Data type for data or columns. E.g. `{''a'': np.float64, ''b'': np.int32, ''c'':
    ''Int64''}` Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype. If converters are specified, they will be
    applied INSTEAD of dtype conversion.'
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
  zh: '数据或列的数据类型。例如 `{''a'': np.float64, ''b'': np.int32, ''c'': ''Int64''}` 使用 `str`
    或 `object` 与适当的 `na_values` 设置一起使用以保留并不解释 dtype。如果指定了转换器，则将应用转换器而不是 dtype 转换。'
- en: 'New in version 1.5.0: Support for defaultdict was added. Specify a defaultdict
    as input where the default determines the dtype of the columns which are not explicitly
    listed.'
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 1.5.0 中新增功能：支持 defaultdict。指定一个 defaultdict 作为输入，其中默认值确定未明确列出的列的 dtype。
- en: dtype_backend{“numpy_nullable”, “pyarrow”}, defaults to NumPy backed DataFrames
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
  zh: dtype_backend{“numpy_nullable”, “pyarrow”}，默认为 NumPy 支持的 DataFrames。
- en: Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
    nullable dtypes are used for all dtypes that have a nullable implementation when
    “numpy_nullable” is set, pyarrow is used for all dtypes if “pyarrow” is set.
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的 dtype_backend，例如 DataFrame 是否应具有 NumPy 数组，当设置“numpy_nullable”时，所有具有可为空实现的
    dtype 都使用可为空 dtype，如果设置“pyarrow”，则所有 dtype 都使用 pyarrow。
- en: The dtype_backends are still experimential.
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
  zh: dtype_backends 仍处于实验阶段。
- en: New in version 2.0.
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 2.0 中新增。
- en: engine{`'c'`, `'python'`, `'pyarrow'`}
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
  zh: engine{`'c'`, `'python'`, `'pyarrow'`}
- en: Parser engine to use. The C and pyarrow engines are faster, while the python
    engine is currently more feature-complete. Multithreading is currently only supported
    by the pyarrow engine.
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的解析引擎。C 和 pyarrow 引擎更快，而 python 引擎目前更完整。目前只有 pyarrow 引擎支持多线程。
- en: 'New in version 1.4.0: The “pyarrow” engine was added as an *experimental* engine,
    and some features are unsupported, or may not work correctly, with this engine.'
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 1.4.0 中新增功能：添加了“pyarrow”引擎作为*实验性*引擎，某些功能不受支持，或者在此引擎下可能无法正常工作。
- en: convertersdict, default `None`
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器字典，默认为 `None`。
- en: Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在某些列中转换值的函数字典。键可以是整数或列标签。
- en: true_valueslist, default `None`
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
  zh: true_values 列表，默认为 `None`。
- en: Values to consider as `True`.
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: 要视为 `True` 的值。
- en: false_valueslist, default `None`
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
  zh: false_values 列表，默认为 `None`。
- en: Values to consider as `False`.
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
  zh: 要视为 `False` 的值。
- en: skipinitialspaceboolean, default `False`
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: skipinitialspace 布尔值，默认为 `False`。
- en: Skip spaces after delimiter.
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: 在分隔符后跳过空格。
- en: skiprowslist-like or integer, default `None`
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: skiprows 类型为列表或整数，默认为 `None`。
- en: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
    of the file.
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: 要跳过的行号（从0开始）或要跳过的行数（int）文件开头。
- en: 'If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise:'
  id: totrans-1527
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可调用，可调用函数将针对行索引进行评估，如果应跳过该行则返回True，否则返回False：
- en: '[PRE307]'
  id: totrans-1528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE307]'
- en: skipfooterint, default `0`
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: skipfooterint，默认为`0`
- en: Number of lines at bottom of file to skip (unsupported with engine=’c’).
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
  zh: 要跳过文件底部的行数（在engine=’c’下不支持）。
- en: nrowsint, default `None`
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: nrowsint，默认为`None`
- en: Number of rows of file to read. Useful for reading pieces of large files.
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取的文件行数。用于读取大文件的片段。
- en: low_memoryboolean, default `True`
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
  zh: low_memoryboolean，默认为`True`
- en: Internally process the file in chunks, resulting in lower memory use while parsing,
    but possibly mixed type inference. To ensure no mixed types either set `False`,
    or specify the type with the `dtype` parameter. Note that the entire file is read
    into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter
    to return the data in chunks. (Only valid with C parser)
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
  zh: 在块中内部处理文件，从而在解析时降低内存使用，但可能混合类型推断。为确保没有混合类型，要么设置为`False`，要么使用`dtype`参数指定类型。请注意，无论如何整个文件都会读入单个`DataFrame`，使用`chunksize`或`iterator`参数以块返回数据。（仅适用于C解析器）
- en: memory_mapboolean, default False
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
  zh: memory_mapboolean，默认为False
- en: If a filepath is provided for `filepath_or_buffer`, map the file object directly
    onto memory and access the data directly from there. Using this option can improve
    performance because there is no longer any I/O overhead.
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`filepath_or_buffer`提供了文件路径，则将文件对象直接映射到内存并直接从那里访问数据。使用��选项可以提高性能，因为不再有任何I/O开销。
- en: NA and missing data handling
  id: totrans-1537
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NA和缺失数据处理
- en: na_valuesscalar, str, list-like, or dict, default `None`
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
  zh: na_valuesscalar、str、类似列表或字典，默认为`None`
- en: Additional strings to recognize as NA/NaN. If dict passed, specific per-column
    NA values. See [na values const](#io-navaluesconst) below for a list of the values
    interpreted as NaN by default.
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: 附加字符串识别为NA/NaN。如果传递字典，则为每列指定特定的NA值。有关默认解释为NaN的值列表，请参见[na values const](#io-navaluesconst)。
- en: keep_default_naboolean, default `True`
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
  zh: keep_default_naboolean，默认为`True`
- en: 'Whether or not to include the default NaN values when parsing the data. Depending
    on whether `na_values` is passed in, the behavior is as follows:'
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 是否在解析数据时包括默认的NaN值。根据是否传入`na_values`，行为如下：
- en: If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is
    appended to the default NaN values used for parsing.
  id: totrans-1542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`keep_default_na`为`True`，并且指定了`na_values`，则`na_values`将被附加到用于解析的默认NaN值上。
- en: If `keep_default_na` is `True`, and `na_values` are not specified, only the
    default NaN values are used for parsing.
  id: totrans-1543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`keep_default_na`为`True`，并且未指定`na_values`，则仅使用默认的NaN值进行解析。
- en: If `keep_default_na` is `False`, and `na_values` are specified, only the NaN
    values specified `na_values` are used for parsing.
  id: totrans-1544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`keep_default_na`为`False`，并且指定了`na_values`，则仅使用指定的NaN值`na_values`进行解析。
- en: If `keep_default_na` is `False`, and `na_values` are not specified, no strings
    will be parsed as NaN.
  id: totrans-1545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`keep_default_na`为`False`，并且未指定`na_values`，则不会将任何字符串解析为NaN。
- en: Note that if `na_filter` is passed in as `False`, the `keep_default_na` and
    `na_values` parameters will be ignored.
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果将`na_filter`传递为`False`，则将忽略`keep_default_na`和`na_values`参数。
- en: na_filterboolean, default `True`
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
  zh: na_filterboolean，默认为`True`
- en: Detect missing value markers (empty strings and the value of na_values). In
    data without any NAs, passing `na_filter=False` can improve the performance of
    reading a large file.
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
  zh: 检测缺失值标记（空字符串和na_values的值）。在没有任何NA的数据中，传递`na_filter=False`可以提高读取大文件的性能。
- en: verboseboolean, default `False`
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
  zh: verboseboolean，默认为`False`
- en: Indicate number of NA values placed in non-numeric columns.
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
  zh: 指示放置在非数字列中的NA值的数量。
- en: skip_blank_linesboolean, default `True`
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: skip_blank_linesboolean，默认为`True`
- en: If `True`, skip over blank lines rather than interpreting as NaN values.
  id: totrans-1552
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`True`，则跳过空行而不是解释为NaN值。
- en: '#### Datetime handling'
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 日期时间处理'
- en: parse_datesboolean or list of ints or names or list of lists or dict, default
    `False`.
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
  zh: parse_datesboolean或int或名称列表或列表或字典，默认为`False`。
- en: If `True` -> try parsing the index.
  id: totrans-1555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为`True` -> 尝试解析索引。
- en: If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.
  id: totrans-1556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为`[1, 2, 3]` -> 尝试将列1、2、3分别解析为单独的日期列。
- en: If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.
  id: totrans-1557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为`[[1, 3]]` -> 合并列1和3并解析为单个日期列。
- en: 'If `{''foo'': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’.'
  id: totrans-1558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果`{''foo'': [1, 3]}` -> 将列1、3解析为日期并调用结果为‘foo’。'
- en: Note
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A fast-path exists for iso8601-formatted dates.
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个针对iso8601格式日期的快速路径。
- en: infer_datetime_formatboolean, default `False`
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
  zh: infer_datetime_format布尔值，默认为`False`
- en: If `True` and parse_dates is enabled for a column, attempt to infer the datetime
    format to speed up the processing.
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`True`并且为列启用了parse_dates，则尝试推断日期时间格式以加快处理速度。
- en: 'Deprecated since version 2.0.0: A strict version of this argument is now the
    default, passing it has no effect.'
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
  zh: 自版本2.0.0起已弃用：此参数的严格版本现在是默认值，传递它没有任何效果。
- en: keep_date_colboolean, default `False`
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: keep_date_col布尔值，默认为`False`
- en: If `True` and parse_dates specifies combining multiple columns then keep the
    original columns.
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`True`并且parse_dates指定了组合多个列，则保留原始列。
- en: date_parserfunction, default `None`
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
  zh: date_parserfunction，默认为`None`
- en: 'Function to use for converting a sequence of string columns to an array of
    datetime instances. The default uses `dateutil.parser.parser` to do the conversion.
    pandas will try to call date_parser in three different ways, advancing to the
    next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates)
    as arguments; 2) concatenate (row-wise) the string values from the columns defined
    by parse_dates into a single array and pass that; and 3) call date_parser once
    for each row using one or more strings (corresponding to the columns defined by
    parse_dates) as arguments.'
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将一系列字符串列转换为日期时间实例数组的函数。默认使用`dateutil.parser.parser`进行转换。pandas将尝试以三种不同的方式调用date_parser，如果发生异常，则会继续下一个：1)
    将一个或多个数组（由parse_dates定义）作为参数传递；2) 将由parse_dates定义的列中的字符串值（按行）连接成单个数组并传递；3) 对每一行使用一个或多个字符串（对应于由parse_dates定义的列）作为参数调用date_parser。
- en: 'Deprecated since version 2.0.0: Use `date_format` instead, or read in as `object`
    and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.'
  id: totrans-1568
  prefs: []
  type: TYPE_NORMAL
  zh: 自版本2.0.0起已弃用：改用`date_format`，或按`object`读取，然后根据需要应用[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: date_formatstr or dict of column -> format, default `None`
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: date_formatstr或列->格式的字典，默认为`None`
- en: If used in conjunction with `parse_dates`, will parse dates according to this
    format. For anything more complex, please read in as `object` and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: 如果与`parse_dates`一起使用，将根据此格式解析日期。对于更复杂的情况，请按`object`读取，然后根据需要应用[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: New in version 2.0.0.
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
  zh: 自版本2.0.0起新增。
- en: dayfirstboolean, default `False`
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
  zh: dayfirst布尔值，默认为`False`
- en: DD/MM format dates, international and European format.
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: DD/MM格式日期，国际和欧洲格式。
- en: cache_datesboolean, default True
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
  zh: cache_dates布尔值，默认为True
- en: If True, use a cache of unique, converted dates to apply the datetime conversion.
    May produce significant speed-up when parsing duplicate date strings, especially
    ones with timezone offsets.
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`True`，则使用一个唯一的转换日期缓存来应用日期时间转换。在解析重复日期字符串时可能会产生显著的加速，特别是带有时区偏移的日期字符串。
- en: Iteration
  id: totrans-1576
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 迭代
- en: iteratorboolean, default `False`
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
  zh: iterator布尔值，默认为`False`
- en: Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
  zh: 返回用于迭代或使用`get_chunk()`获取块的`TextFileReader`对象。
- en: chunksizeint, default `None`
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: chunksizeint，默认为`None`
- en: Return `TextFileReader` object for iteration. See [iterating and chunking](#io-chunking)
    below.
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
  zh: 返回用于迭代的`TextFileReader`对象。请参阅下面的[迭代和分块](#io-chunking)。
- en: Quoting, compression, and file format
  id: totrans-1581
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 引用、压缩和文件格式
- en: compression{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`,
    `dict`}, default `'infer'`
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩{`'infer'`，`'gzip'`，`'bz2'`，`'zip'`，`'xz'`，`'zstd'`，`None`，`dict`}，默认为`'infer'`
- en: 'For on-the-fly decompression of on-disk data. If ‘infer’, then use gzip, bz2,
    zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in ‘.gz’, ‘.bz2’,
    ‘.zip’, ‘.xz’, ‘.zst’, respectively, and no decompression otherwise. If using
    ‘zip’, the ZIP file must contain only one data file to be read in. Set to `None`
    for no decompression. Can also be a dict with key `''method''` set to one of {`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd''`} and other key-value pairs are forwarded to
    `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`.
    As an example, the following could be passed for faster compression and to create
    a reproducible gzip archive: `compression={''method'': ''gzip'', ''compresslevel'':
    1, ''mtime'': 1}`.'
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
  zh: '用于在磁盘数据上进行即时解压缩。如果‘infer’，则如果`filepath_or_buffer`是以‘.gz’、‘.bz2’、‘.zip’、‘.xz’或‘.zst’结尾的路径，则使用gzip、bz2、zip、xz或zstandard，否则不进行解压缩。如果使用‘zip’，ZIP文件必须只包含一个要读取的数据文件。设置为`None`表示不进行解压缩。也可以是一个字典，其中键为`''method''`，设置为其中一个{`''zip''`、`''gzip''`、`''bz2''`、`''zstd''`}，其他键值对转发到`zipfile.ZipFile`、`gzip.GzipFile`、`bz2.BZ2File`或`zstandard.ZstdDecompressor`。例如，可以传递以下内容以获得更快的压缩和创建可重现的gzip存档：`compression={''method'':
    ''gzip'', ''compresslevel'': 1, ''mtime'': 1}`。'
- en: 'Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’
    to `gzip.open`.'
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本1.2.0中更改：以前的版本将‘gzip’的字典条目转发到`gzip.open`。
- en: thousandsstr, default `None`
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: thousandsstr，默认为`None`
- en: Thousands separator.
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
  zh: 千位分隔符。
- en: decimalstr, default `'.'`
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
  zh: decimalstr，默认为`'.'`
- en: Character to recognize as decimal point. E.g. use `','` for European data.
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: 用作小数点的字符。例如，欧洲数据可以使用`,`。
- en: float_precisionstring, default None
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
  zh: float_precisionstring，默认为None
- en: Specifies which converter the C engine should use for floating-point values.
    The options are `None` for the ordinary converter, `high` for the high-precision
    converter, and `round_trip` for the round-trip converter.
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
  zh: 指定C引擎应使用���个转换器来处理浮点值。选项为`None`表示普通转换器，`high`表示高精度转换器，`round_trip`表示往返转换器。
- en: lineterminatorstr (length 1), default `None`
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
  zh: lineterminatorstr（长度为1），默认为`None`
- en: Character to break file into lines. Only valid with C parser.
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将文件分成行的字符。仅与C解析器有效。
- en: quotecharstr (length 1)
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
  zh: quotecharstr（长度为1）
- en: The character used to denote the start and end of a quoted item. Quoted items
    can include the delimiter and it will be ignored.
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
  zh: 用于表示引用项的开始和结束的字符。引用项可以包括分隔符，它将被忽略。
- en: quotingint or `csv.QUOTE_*` instance, default `0`
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
  zh: quotingint或`csv.QUOTE_*`实例，默认为`0`
- en: Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL`
    (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
  zh: 控制字段引用行为，使用`csv.QUOTE_*`常量之一。使用`QUOTE_MINIMAL`（0）、`QUOTE_ALL`（1）、`QUOTE_NONNUMERIC`（2）或`QUOTE_NONE`（3）中的一个。
- en: doublequoteboolean, default `True`
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
  zh: doublequoteboolean，默认为`True`
- en: When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether
    or not to interpret two consecutive `quotechar` elements **inside** a field as
    a single `quotechar` element.
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: 当指定了`quotechar`并且`quoting`不是`QUOTE_NONE`时，指示是否将字段内连续的两个`quotechar`元素解释为单个`quotechar`元素。
- en: escapecharstr (length 1), default `None`
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: escapecharstr（长度为1），默认为`None`
- en: One-character string used to escape delimiter when quoting is `QUOTE_NONE`.
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在引用为`QUOTE_NONE`时转义分隔符的一个字符字符串。
- en: commentstr, default `None`
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
  zh: commentstr，默认为`None`
- en: Indicates remainder of line should not be parsed. If found at the beginning
    of a line, the line will be ignored altogether. This parameter must be a single
    character. Like empty lines (as long as `skip_blank_lines=True`), fully commented
    lines are ignored by the parameter `header` but not by `skiprows`. For example,
    if `comment='#'`, parsing ‘#empty\na,b,c\n1,2,3’ with `header=0` will result in
    ‘a,b,c’ being treated as the header.
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: 指示不应解析行的其余部分。如果在行的开头找到，整行将被忽略。此参数必须是一个单个字符。像空行一样（只要`skip_blank_lines=True`），完全注释的行由参数`header`忽略，但不由`skiprows`忽略。例如，如果`comment='#'`，使用`header=0`解析‘#empty\na,b,c\n1,2,3’将导致‘a,b,c’被视为标题。
- en: encodingstr, default `None`
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
  zh: encodingstr，默认为`None`
- en: Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python
    standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
  zh: 读取/写入UTF时要使用的编码（例如，`'utf-8'`）。[Python标准编码列表](https://docs.python.org/3/library/codecs.html#standard-encodings)。
- en: dialectstr or [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") instance, default `None`
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
  zh: dialectstr或[`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(在Python v3.12中)")实例，默认为`None`
- en: 'If provided, this parameter will override values (default or not) for the following
    parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`,
    and `quoting`. If it is necessary to override values, a ParserWarning will be
    issued. See [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") documentation for more details.'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供了此参数，它将覆盖以下参数的值（默认或非默认）：`delimiter`、`doublequote`、`escapechar`、`skipinitialspace`、`quotechar`和`quoting`。如果需要覆盖值，将发出ParserWarning。有关更多详细信息，请参阅[`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)")文档。
- en: Error handling
  id: totrans-1607
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误处理
- en: on_bad_lines(‘error’, ‘warn’, ‘skip’), default ‘error’
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
  zh: on_bad_lines（‘error’，‘warn’，‘skip’），默认为‘error’
- en: 'Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: 指定在遇到错误行（字段过多的行）时要执行的操作。允许的值为：
- en: ‘error’, raise an ParserError when a bad line is encountered.
  id: totrans-1610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘error’，在遇到错误行时引发ParserError。
- en: ‘warn’, print a warning when a bad line is encountered and skip that line.
  id: totrans-1611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘warn’，在遇到错误行时打印警告并跳过该行。
- en: ‘skip’, skip bad lines without raising or warning when they are encountered.
  id: totrans-1612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘skip’，在遇到错误行时跳过而不引发或警告。
- en: New in version 1.3.0.
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
  zh: 自版本1.3.0起新增。
- en: '### Specifying column data types'
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: '### 指定列数据类型'
- en: 'You can indicate the data type for the whole `DataFrame` or individual columns:'
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指定整个`DataFrame`或单独的列的数据类型：
- en: '[PRE308]'
  id: totrans-1616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE308]'
- en: Fortunately, pandas offers more than one way to ensure that your column(s) contain
    only one `dtype`. If you’re unfamiliar with these concepts, you can see [here](basics.html#basics-dtypes)
    to learn more about dtypes, and [here](basics.html#basics-object-conversion) to
    learn more about `object` conversion in pandas.
  id: totrans-1617
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，pandas提供了多种方法来确保您的列只包含一个`dtype`。如果您对这些概念不熟悉，您可以在[这里](basics.html#basics-dtypes)了解有关dtype的更多信息，并在[这里](basics.html#basics-object-conversion)了解有关pandas中`object`转换的更多信息。
- en: 'For instance, you can use the `converters` argument of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"):'
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以使用[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")的`converters`参数：
- en: '[PRE309]'
  id: totrans-1619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE309]'
- en: Or you can use the [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") function to coerce the dtypes after reading in the data,
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您可以在读取数据后使用[`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric")函数强制转换dtype，
- en: '[PRE310]'
  id: totrans-1621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE310]'
- en: which will convert all valid parsing to floats, leaving the invalid parsing
    as `NaN`.
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
  zh: 这将将所有有效解析转换为浮点数，将无效解析保留为`NaN`。
- en: Ultimately, how you deal with reading in columns containing mixed dtypes depends
    on your specific needs. In the case above, if you wanted to `NaN` out the data
    anomalies, then [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") is probably your best option. However, if you wanted for
    all the data to be coerced, no matter the type, then using the `converters` argument
    of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    would certainly be worth trying.
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，如何处理包含混合dtype的列取决于您的具体需求。在上面的情况下，如果您想要将数据异常值设为`NaN`，那么[`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric")可能是您最好的选择。然而，如果您希望所有数据都被强制转换，无论类型如何，那么使用[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")的`converters`参数肯定值得一试。
- en: Note
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In some cases, reading in abnormal data with columns containing mixed dtypes
    will result in an inconsistent dataset. If you rely on pandas to infer the dtypes
    of your columns, the parsing engine will go and infer the dtypes for different
    chunks of the data, rather than the whole dataset at once. Consequently, you can
    end up with column(s) with mixed dtypes. For example,
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，读取包含混合dtype的列的异常数据将导致数据集不一致。如果您依赖pandas推断列的dtype，解析引擎将会推断数据的不同块的dtype，而不是一次推断整个数据集的dtype。因此，您可能会得到包含混合dtype的列。例如，
- en: '[PRE311]'
  id: totrans-1626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE311]'
- en: will result with `mixed_df` containing an `int` dtype for certain chunks of
    the column, and `str` for others due to the mixed dtypes from the data that was
    read in. It is important to note that the overall column will be marked with a
    `dtype` of `object`, which is used for columns with mixed dtypes.
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: '将导致`mixed_df`包含某些列的`int` dtype，而由于读取的数据中存在混合dtype，其他列包含`str`。重要的是要注意，整体列将标记为`object`的dtype，用于包含混合dtype的列。 '
- en: Setting `dtype_backend="numpy_nullable"` will result in nullable dtypes for
    every column.
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`dtype_backend="numpy_nullable"`将导致每一列都具有可空的dtype。
- en: '[PRE312]  ### Specifying categorical dtype'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE312]  ### 指定分类dtype'
- en: '`Categorical` columns can be parsed directly by specifying `dtype=''category''`
    or `dtype=CategoricalDtype(categories, ordered)`.'
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过指定`dtype='category'`或`dtype=CategoricalDtype(categories, ordered)`直接解析`Categorical`列。
- en: '[PRE313]'
  id: totrans-1631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE313]'
- en: 'Individual columns can be parsed as a `Categorical` using a dict specification:'
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用字典规范将单独的列解析为`Categorical`：
- en: '[PRE314]'
  id: totrans-1633
  prefs: []
  type: TYPE_PRE
  zh: '[PRE314]'
- en: Specifying `dtype='category'` will result in an unordered `Categorical` whose
    `categories` are the unique values observed in the data. For more control on the
    categories and order, create a `CategoricalDtype` ahead of time, and pass that
    for that column’s `dtype`.
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`dtype='category'`将导致无序的`Categorical`，其`categories`是数据中观察到的唯一值。要对categories和顺序进行更多控制，请提前创建`CategoricalDtype`，并将其传递给该列的`dtype`。
- en: '[PRE315]'
  id: totrans-1635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE315]'
- en: When using `dtype=CategoricalDtype`, “unexpected” values outside of `dtype.categories`
    are treated as missing values.
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dtype=CategoricalDtype`时，`dtype.categories`之外的“意外”值被视为缺失值。
- en: '[PRE316]'
  id: totrans-1637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE316]'
- en: This matches the behavior of `Categorical.set_categories()`.
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: 这与`Categorical.set_categories()`的行为相匹配。
- en: Note
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: With `dtype='category'`, the resulting categories will always be parsed as strings
    (object dtype). If the categories are numeric they can be converted using the
    [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric "pandas.to_numeric")
    function, or as appropriate, another converter such as [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime").
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dtype='category'`，生成的categories将始终被解析为字符串（object dtype）。如果categories是数字，可以使用[`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric")函数进行转换，或者根据需要使用另一个转换器，如[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: When `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all numeric,
    all datetimes, etc.), the conversion is done automatically.
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
  zh: 当`dtype`是具有同质`categories`（全部为数字、全部为日期时间等）的`CategoricalDtype`时，转换会自动完成。
- en: '[PRE317]'
  id: totrans-1642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE317]'
- en: Naming and using columns
  id: totrans-1643
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名和使用列
- en: '#### Handling column names'
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 处理列名'
- en: 'A file may or may not have a header row. pandas assumes the first row should
    be used as the column names:'
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: 文件可能有或没有标题行。pandas假定第一行应该用作列名：
- en: '[PRE318]'
  id: totrans-1646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE318]'
- en: 'By specifying the `names` argument in conjunction with `header` you can indicate
    other names to use and whether or not to throw away the header row (if any):'
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`header`中与`names`参数一起指定，可以指示要使用的其他名称以及是否丢弃标题行（如果有）：
- en: '[PRE319]'
  id: totrans-1648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE319]'
- en: 'If the header is in a row other than the first, pass the row number to `header`.
    This will skip the preceding rows:'
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标题在第一行之外的行中，将行号传递给`header`。这将跳过前面的行：
- en: '[PRE320]'
  id: totrans-1650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE320]'
- en: Note
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Default behavior is to infer the column names: if no names are passed the behavior
    is identical to `header=0` and column names are inferred from the first non-blank
    line of the file, if column names are passed explicitly then the behavior is identical
    to `header=None`.  ### Duplicate names parsing'
  id: totrans-1652
  prefs: []
  type: TYPE_NORMAL
  zh: 默认行为是推断列名：如果没有传递名称，则行为与`header=0`相同，并且列名从文件的第一行开始推断，如果显式传递列名，则行为与`header=None`相同。###
    解析重复名称
- en: 'If the file or header contains duplicate names, pandas will by default distinguish
    between them so as to prevent overwriting data:'
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件或标题包含重复名称，pandas默认会区分它们，以防止覆盖数据：
- en: '[PRE321]'
  id: totrans-1654
  prefs: []
  type: TYPE_PRE
  zh: '[PRE321]'
- en: There is no more duplicate data because duplicate columns ‘X’, …, ‘X’ become
    ‘X’, ‘X.1’, …, ‘X.N’.
  id: totrans-1655
  prefs: []
  type: TYPE_NORMAL
  zh: 不再有重复数据，因为重复列‘X’，…，‘X’变为‘X’，‘X.1’，…，‘X.N’。
- en: '#### Filtering columns (`usecols`)'
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 过滤列（`usecols`）'
- en: 'The `usecols` argument allows you to select any subset of the columns in a
    file, either using the column names, position numbers or a callable:'
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
  zh: '`usecols`参数允许您选择文件中的任何列的子集，可以使用列名称、位置编号或可调用函数：'
- en: '[PRE322]'
  id: totrans-1658
  prefs: []
  type: TYPE_PRE
  zh: '[PRE322]'
- en: 'The `usecols` argument can also be used to specify which columns not to use
    in the final result:'
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: '`usecols`参数也可用于指定在最终结果中不使用哪些列：'
- en: '[PRE323]'
  id: totrans-1660
  prefs: []
  type: TYPE_PRE
  zh: '[PRE323]'
- en: In this case, the callable is specifying that we exclude the “a” and “c” columns
    from the output.
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可调用函数指定我们从输出中排除“a”和“c”列。
- en: Comments and empty lines
  id: totrans-1662
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释和空行
- en: '#### Ignoring line comments and empty lines'
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 忽略行注释和空行'
- en: If the `comment` parameter is specified, then completely commented lines will
    be ignored. By default, completely blank lines will be ignored as well.
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定了`comment`参数，则完全注释的行将被忽略。默认情况下，完全空白行也将被忽略。
- en: '[PRE324]'
  id: totrans-1665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE324]'
- en: 'If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:'
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`skip_blank_lines=False`，则`read_csv`将不会忽略空白行：
- en: '[PRE325]'
  id: totrans-1667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE325]'
- en: Warning
  id: totrans-1668
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: 'The presence of ignored lines might create ambiguities involving line numbers;
    the parameter `header` uses row numbers (ignoring commented/empty lines), while
    `skiprows` uses line numbers (including commented/empty lines):'
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略行的存在可能会导致涉及行号的歧义；参数`header`使用行号（忽略注释/空行），而`skiprows`使用行号（包括注释/空行）：
- en: '[PRE326]'
  id: totrans-1670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE326]'
- en: 'If both `header` and `skiprows` are specified, `header` will be relative to
    the end of `skiprows`. For example:'
  id: totrans-1671
  prefs: []
  type: TYPE_NORMAL
  zh: 如果同时指定了`header`和`skiprows`，`header`将相对于`skiprows`的末尾。例如：
- en: '[PRE327]  #### Comments'
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE327]  #### 注释'
- en: 'Sometimes comments or meta data may be included in a file:'
  id: totrans-1673
  prefs: []
  type: TYPE_NORMAL
  zh: 有时文件中可能包含注释或元数据：
- en: '[PRE328]'
  id: totrans-1674
  prefs: []
  type: TYPE_PRE
  zh: '[PRE328]'
- en: 'By default, the parser includes the comments in the output:'
  id: totrans-1675
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，解析器会在输出中包含注释：
- en: '[PRE329]'
  id: totrans-1676
  prefs: []
  type: TYPE_PRE
  zh: '[PRE329]'
- en: 'We can suppress the comments using the `comment` keyword:'
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`comment`关键字来抑制注释：
- en: '[PRE330]  ### Dealing with Unicode data'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE330]  ### 处理 Unicode 数据'
- en: 'The `encoding` argument should be used for encoded unicode data, which will
    result in byte strings being decoded to unicode in the result:'
  id: totrans-1679
  prefs: []
  type: TYPE_NORMAL
  zh: 应该使用`encoding`参数来处理编码的Unicode数据，这将导致字节字符串在结果中被解码为Unicode：
- en: '[PRE331]'
  id: totrans-1680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE331]'
- en: 'Some formats which encode all characters as multiple bytes, like UTF-16, won’t
    parse correctly at all without specifying the encoding. [Full list of Python standard
    encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).  ###
    Index columns and trailing delimiters'
  id: totrans-1681
  prefs: []
  type: TYPE_NORMAL
  zh: '一些将所有字符编码为多字节的格式，如UTF-16，如果不指定编码，将无法正确解析。 [Python标准编码的完整列表](https://docs.python.org/3/library/codecs.html#standard-encodings)。  ###
    索引列和尾随分隔符'
- en: 'If a file has one more column of data than the number of column names, the
    first column will be used as the `DataFrame`’s row names:'
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件的数据列数比列名多一个，第一列将被用作`DataFrame`的行名：
- en: '[PRE332]'
  id: totrans-1683
  prefs: []
  type: TYPE_PRE
  zh: '[PRE332]'
- en: '[PRE333]'
  id: totrans-1684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE333]'
- en: Ordinarily, you can achieve this behavior using the `index_col` option.
  id: totrans-1685
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您可以使用`index_col`选项来实现此行为。
- en: 'There are some exception cases when a file has been prepared with delimiters
    at the end of each data line, confusing the parser. To explicitly disable the
    index column inference and discard the last column, pass `index_col=False`:'
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
  zh: 当文件在每个数据行末尾都有分隔符时，解析器会产生一些异常情况，导致解析混乱。为了显式禁用索引列推断并丢弃最后一列，传递`index_col=False`：
- en: '[PRE334]'
  id: totrans-1687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE334]'
- en: If a subset of data is being parsed using the `usecols` option, the `index_col`
    specification is based on that subset, not the original data.
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用`usecols`选项解析数据的子集，则`index_col`规范基于该子集，而不是原始数据。
- en: '[PRE335]  ### Date Handling'
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE335]  ### 日期处理'
- en: Specifying date columns
  id: totrans-1690
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指定日期列
- en: To better facilitate working with datetime data, [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") uses the keyword arguments `parse_dates` and `date_format`
    to allow users to specify a variety of columns and date/time formats to turn the
    input text data into `datetime` objects.
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地处理日期时间数据，[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv")使用关键字参数`parse_dates`和`date_format`允许用户指定各种列和日期/时间格式将输入文本数据转换为`datetime`对象。
- en: 'The simplest case is to just pass in `parse_dates=True`:'
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的情况是只需传入`parse_dates=True`：
- en: '[PRE336]'
  id: totrans-1693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE336]'
- en: It is often the case that we may want to store date and time data separately,
    or store various date fields separately. the `parse_dates` keyword can be used
    to specify a combination of columns to parse the dates and/or times from.
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们可能希望将日期和时间数据分开存储，或将各种日期字段分开存储。 `parse_dates`关键字可用于指定要从中解析日期和/或时间的列的组合。
- en: 'You can specify a list of column lists to `parse_dates`, the resulting date
    columns will be prepended to the output (so as to not affect the existing column
    order) and the new column names will be the concatenation of the component column
    names:'
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指定一个列列表的列表给`parse_dates`，生成的日期列将被预置到输出中（以不影响现有列顺序）且新列名将是组件列名的连接：
- en: '[PRE337]'
  id: totrans-1696
  prefs: []
  type: TYPE_PRE
  zh: '[PRE337]'
- en: 'By default the parser removes the component date columns, but you can choose
    to retain them via the `keep_date_col` keyword:'
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，解析器会删除组件日期列，但您可以通过`keep_date_col`关键字选择保留它们：
- en: '[PRE338]'
  id: totrans-1698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE338]'
- en: Note that if you wish to combine multiple columns into a single date column,
    a nested list must be used. In other words, `parse_dates=[1, 2]` indicates that
    the second and third columns should each be parsed as separate date columns while
    `parse_dates=[[1, 2]]` means the two columns should be parsed into a single column.
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您希望将多个列合并为单个日期列，则必须使用嵌套列表。换句话说，`parse_dates=[1, 2]`表示第二列和第三列应分别解析为单独的日期列，而`parse_dates=[[1,
    2]]`表示两列应解析为单个列。
- en: 'You can also use a dict to specify custom name columns:'
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用字典指定自定义名称列：
- en: '[PRE339]'
  id: totrans-1701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE339]'
- en: 'It is important to remember that if multiple text columns are to be parsed
    into a single date column, then a new column is prepended to the data. The `index_col`
    specification is based off of this new set of columns rather than the original
    data columns:'
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，如果要将多个文本列解析为单个日期列，则会在数据前添加一个新列。`index_col`规范是基于这组新列而不是原始数据列：
- en: '[PRE340]'
  id: totrans-1703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE340]'
- en: Note
  id: totrans-1704
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a column or index contains an unparsable date, the entire column or index
    will be returned unaltered as an object data type. For non-standard datetime parsing,
    use [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") after `pd.read_csv`.
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
  zh: 如果列或索引包含无法解析的日期，则整个列或索引将不经更改地返回为对象数据类型。对于非标准日期时间解析，请在`pd.read_csv`后使用[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: Note
  id: totrans-1706
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g
    “2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your
    data to store datetimes in this format, load times will be significantly faster,
    ~20x has been observed.
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
  zh: read_csv 对 iso8601 格式的日期时间字符串有一个快速路���，例如“2000-01-01T00:01:02+00:00”和类似变体。如果您可以安排数据以这种格式存储日期时间，加载时间将显着更快，已观察到约20倍的速度。
- en: 'Deprecated since version 2.2.0: Combining date columns inside read_csv is deprecated.
    Use `pd.to_datetime` on the relevant result columns instead.'
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
  zh: 自版本2.2.0起已弃用：在 read_csv 中合并日期列已弃用。请改为在相关结果列上使用`pd.to_datetime`。
- en: Date parsing functions
  id: totrans-1709
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 日期解析函数
- en: 'Finally, the parser allows you to specify a custom `date_format`. Performance-wise,
    you should try these methods of parsing dates in order:'
  id: totrans-1710
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，解析器允许您指定自定义的`date_format`。从性能角度考虑，您应该按顺序尝试这些日期解析方法：
- en: 'If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or
    `date_format={column_name: "%d/%m/%Y"}`.'
  id: totrans-1711
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '如果知道格式，请使用`date_format`，例如：`date_format="%d/%m/%Y"`或`date_format={column_name:
    "%d/%m/%Y"}`。'
- en: If you different formats for different columns, or want to pass any extra options
    (such as `utc`) to `to_datetime`, then you should read in your data as `object`
    dtype, and then use `to_datetime`.
  id: totrans-1712
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不同列有不同格式，或者想要将任何额外选项（如`utc`）传递给`to_datetime`，则应以`object` dtype 读取数据，然后使用`to_datetime`。
- en: '#### Parsing a CSV with mixed timezones'
  id: totrans-1713
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 解析具有混合时区的 CSV'
- en: pandas cannot natively represent a column or index with mixed timezones. If
    your CSV file contains columns with a mixture of timezones, the default result
    will be an object-dtype column with strings, even with `parse_dates`. To parse
    the mixed-timezone values as a datetime column, read in as `object` dtype and
    then call [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") with `utc=True`.
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 无法本地表示具有混合时区的列或索引。如果您的 CSV 文件包含具有混合时区的列，则默认结果将是一个对象 dtype 列，其中包含字符串，即使使用`parse_dates`也是如此。要将混合时区值解析为日期时间列，请以`object`
    dtype 读取，然后调用[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")并使用`utc=True`。
- en: '[PRE341]  #### Inferring datetime format'
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE341]  #### 推断日期时间格式'
- en: 'Here are some examples of datetime strings that can be guessed (all representing
    December 30th, 2011 at 00:00:00):'
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些可以猜测的日期时间字符串示例（均表示2011年12月30日00:00:00）：
- en: “20111230”
  id: totrans-1717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “20111230”
- en: “2011/12/30”
  id: totrans-1718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “2011/12/30”
- en: “20111230 00:00:00”
  id: totrans-1719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “20111230 00:00:00”
- en: “12/30/2011 00:00:00”
  id: totrans-1720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “12/30/2011 00:00:00”
- en: “30/Dec/2011 00:00:00”
  id: totrans-1721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “30/Dec/2011 00:00:00”
- en: “30/December/2011 00:00:00”
  id: totrans-1722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “30/December/2011 00:00:00”
- en: Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`,
    it will guess “01/12/2011” to be December 1st. With `dayfirst=False` (default)
    it will guess “01/12/2011” to be January 12th.
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，格式推断对`dayfirst`很敏感。使用`dayfirst=True`，它会猜测“01/12/2011”为12月1日。使用`dayfirst=False`（默认），它会猜测“01/12/2011”为1月12日。
- en: If you try to parse a column of date strings, pandas will attempt to guess the
    format from the first non-NaN element, and will then parse the rest of the column
    with that format. If pandas fails to guess the format (for example if your first
    string is `'01 December US/Pacific 2000'`), then a warning will be raised and
    each row will be parsed individually by `dateutil.parser.parse`. The safest way
    to parse dates is to explicitly set `format=`.
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尝试解析日期字符串列，pandas 将尝试从第一个非 NaN 元素猜测格式，然后使用该格式解析列的其余部分。如果 pandas 无法猜测格式（例如，如果您的第一个字符串是`'01
    December US/Pacific 2000'`），则会发出警告，并且每行将通过`dateutil.parser.parse`单独解析。解析日期的最安全方式是显式设置`format=`。
- en: '[PRE342]'
  id: totrans-1725
  prefs: []
  type: TYPE_PRE
  zh: '[PRE342]'
- en: In the case that you have mixed datetime formats within the same column, you
    can pass `format='mixed'`
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
  zh: 如果同一列中有混合的日期时间格式，可以传递`format='mixed'`
- en: '[PRE343]'
  id: totrans-1727
  prefs: []
  type: TYPE_PRE
  zh: '[PRE343]'
- en: 'or, if your datetime formats are all ISO8601 (possibly not identically-formatted):'
  id: totrans-1728
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您的日期时间格式都是 ISO8601（可能不是完全相同的格式）：
- en: '[PRE344]'
  id: totrans-1729
  prefs: []
  type: TYPE_PRE
  zh: '[PRE344]'
- en: International date formats
  id: totrans-1730
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 国际日期格式
- en: 'While US date formats tend to be MM/DD/YYYY, many international formats use
    DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:'
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
  zh: 美国的日期格式通常是 MM/DD/YYYY，而许多国际格式则使用 DD/MM/YYYY。为了方便起见，提供了一个 `dayfirst` 关键字：
- en: '[PRE345]'
  id: totrans-1732
  prefs: []
  type: TYPE_PRE
  zh: '[PRE345]'
- en: Writing CSVs to binary file objects
  id: totrans-1733
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将 CSV 写入二进制文件对象
- en: New in version 1.2.0.
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本 1.2.0 中新增。
- en: '`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary
    mode. In most cases, it is not necessary to specify `mode` as Pandas will auto-detect
    whether the file object is opened in text or binary mode.'
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
  zh: '`df.to_csv(..., mode="wb")` 允许将 CSV 写入以二进制模式打开的文件对象。在大多数情况下，不需要指定 `mode`，因为
    Pandas 将自动检测文件对象是以文本模式还是二进制模式打开的。'
- en: '[PRE346]  ### Specifying method for floating-point conversion'
  id: totrans-1736
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE346]  ### 指定浮点转换方法'
- en: 'The parameter `float_precision` can be specified in order to use a specific
    floating-point converter during parsing with the C engine. The options are the
    ordinary converter, the high-precision converter, and the round-trip converter
    (which is guaranteed to round-trip values after writing to a file). For example:'
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过指定 `float_precision` 参数来使用特定的浮点数转换器在 C 引擎解析时。选项有普通转换器、高精度转换器和往返转换器（保证在写入文件后循环的值）。例如：
- en: '[PRE347]  ### Thousand separators'
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE347]  ### 千位分隔符'
- en: 'For large numbers that have been written with a thousands separator, you can
    set the `thousands` keyword to a string of length 1 so that integers will be parsed
    correctly:'
  id: totrans-1739
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以千位分隔符编写的大数字，您可以将 `thousands` 关键字设置为长度为 1 的字符串，以便正确解析整数：
- en: 'By default, numbers with a thousands separator will be parsed as strings:'
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，带有千位分隔符的数字将被解析为字符串：
- en: '[PRE348]'
  id: totrans-1741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE348]'
- en: 'The `thousands` keyword allows integers to be parsed correctly:'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: '`thousands` 关键字允许正确解析整数：'
- en: '[PRE349]  ### NA values'
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE349]  ### NA 值'
- en: To control which values are parsed as missing values (which are signified by
    `NaN`), specify a string in `na_values`. If you specify a list of strings, then
    all values in it are considered to be missing values. If you specify a number
    (a `float`, like `5.0` or an `integer` like `5`), the corresponding equivalent
    values will also imply a missing value (in this case effectively `[5.0, 5]` are
    recognized as `NaN`).
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制哪些值被解析为缺失值（用 `NaN` 表示），请在 `na_values` 中指定一个字符串。如果您指定了一个字符串列表，那么其中的所有值都将被视为缺失值。如果您指定了一个数字（一个
    `float`，比如 `5.0` 或一个 `integer`，比如 `5`），则相应的等价值也将暗示一个缺失值（在这种情况下，实际上 `[5.0, 5]`
    被认为是 `NaN`）。
- en: To completely override the default values that are recognized as missing, specify
    `keep_default_na=False`.
  id: totrans-1745
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全覆盖默认被识别为缺失的值，请指定 `keep_default_na=False`。
- en: The default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',
    '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN',
    '-NaN', 'nan', '-nan', 'None', '']`.
  id: totrans-1746
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 `NaN` 被识别的值为 `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A',
    'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan',
    'None', '']`。
- en: 'Let us consider some examples:'
  id: totrans-1747
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些例子：
- en: '[PRE350]'
  id: totrans-1748
  prefs: []
  type: TYPE_PRE
  zh: '[PRE350]'
- en: In the example above `5` and `5.0` will be recognized as `NaN`, in addition
    to the defaults. A string will first be interpreted as a numerical `5`, then as
    a `NaN`.
  id: totrans-1749
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，`5` 和 `5.0` 将被识别为 `NaN`，除了默认值。一个字符串首先被解释为数值 `5`，然后作为 `NaN`。
- en: '[PRE351]'
  id: totrans-1750
  prefs: []
  type: TYPE_PRE
  zh: '[PRE351]'
- en: Above, only an empty field will be recognized as `NaN`.
  id: totrans-1751
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，只有一个空字段会被识别为 `NaN`。
- en: '[PRE352]'
  id: totrans-1752
  prefs: []
  type: TYPE_PRE
  zh: '[PRE352]'
- en: Above, both `NA` and `0` as strings are `NaN`.
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，`NA` 和 `0` 都作为字符串是 `NaN`。
- en: '[PRE353]'
  id: totrans-1754
  prefs: []
  type: TYPE_PRE
  zh: '[PRE353]'
- en: 'The default values, in addition to the string `"Nope"` are recognized as `NaN`.  ###
    Infinity'
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
  zh: '默认值除了字符串 `"Nope"` 外，也被识别为 `NaN`。  ### 无穷大'
- en: '`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf`
    as `-np.inf` (negative infinity). These will ignore the case of the value, meaning
    `Inf`, will also be parsed as `np.inf`.  ### Boolean values'
  id: totrans-1756
  prefs: []
  type: TYPE_NORMAL
  zh: '类似 `inf` 的值将被解析为 `np.inf`（正无穷大），而 `-inf` 将被解析为 `-np.inf`（负无穷大）。这些将忽略值的大小写，意味着
    `Inf` 也将被解析为 `np.inf`。  ### 布尔值'
- en: 'The common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as
    boolean. Occasionally you might want to recognize other values as being boolean.
    To do this, use the `true_values` and `false_values` options as follows:'
  id: totrans-1757
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的值 `True`、`False`、`TRUE` 和 `FALSE` 都被识别为布尔值。偶尔你可能想要识别其他值为布尔值。为此，请使用如下所示的 `true_values`
    和 `false_values` 选项：
- en: '[PRE354]  ### Handling “bad” lines'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE354]  ### 处理“坏”行'
- en: 'Some files may have malformed lines with too few fields or too many. Lines
    with too few fields will have NA values filled in the trailing fields. Lines with
    too many fields will raise an error by default:'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文件可能存在字段过少或过多的格式不正确的行。字段过少的行将在尾部字段中填充NA值。字段过多的行将默认引发错误：
- en: '[PRE355]'
  id: totrans-1760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE355]'
- en: 'You can elect to skip bad lines:'
  id: totrans-1761
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择跳过错误行：
- en: '[PRE356]'
  id: totrans-1762
  prefs: []
  type: TYPE_PRE
  zh: '[PRE356]'
- en: New in version 1.4.0.
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
  zh: 版本1.4.0中的新功能。
- en: 'Or pass a callable function to handle the bad line if `engine="python"`. The
    bad line will be a list of strings that was split by the `sep`:'
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在`engine="python"`时传递一个可调用函数来处理错误行。错误行将是由`sep`分割的字符串列表：
- en: '[PRE357]'
  id: totrans-1765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE357]'
- en: Note
  id: totrans-1766
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The callable function will handle only a line with too many fields. Bad lines
    caused by other errors will be silently skipped.
  id: totrans-1767
  prefs: []
  type: TYPE_NORMAL
  zh: 可调用函数仅处理字段过多的行。由其他错误引起的错误行将被静默跳过。
- en: '[PRE358]'
  id: totrans-1768
  prefs: []
  type: TYPE_PRE
  zh: '[PRE358]'
- en: The line was not processed in this case, as a “bad line” here is caused by an
    escape character.
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，该行未被处理，因为这里的“错误行”是由转义字符引起的。
- en: 'You can also use the `usecols` parameter to eliminate extraneous column data
    that appear in some lines but not others:'
  id: totrans-1770
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`usecols`参数消除一些行中出现但其他行中不存在的多余列数据：
- en: '[PRE359]'
  id: totrans-1771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE359]'
- en: In case you want to keep all data including the lines with too many fields,
    you can specify a sufficient number of `names`. This ensures that lines with not
    enough fields are filled with `NaN`.
  id: totrans-1772
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望保留所有数据，包括字段过多的行，可以指定足够数量的`names`。这样可以确保字段不足的行填充为`NaN`。
- en: '[PRE360]  ### Dialect'
  id: totrans-1773
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE360]  ### 方言'
- en: The `dialect` keyword gives greater flexibility in specifying the file format.
    By default it uses the Excel dialect but you can specify either the dialect name
    or a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect "(in
    Python v3.12)") instance.
  id: totrans-1774
  prefs: []
  type: TYPE_NORMAL
  zh: '`dialect`关键字提供了更大的灵活性来指定文件格式。默认情况下使用Excel方言，但您可以指定方言名称或[`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(在Python v3.12中)")实例。'
- en: 'Suppose you had data with unenclosed quotes:'
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据中有未封闭的引号：
- en: '[PRE361]'
  id: totrans-1776
  prefs: []
  type: TYPE_PRE
  zh: '[PRE361]'
- en: By default, `read_csv` uses the Excel dialect and treats the double quote as
    the quote character, which causes it to fail when it finds a newline before it
    finds the closing double quote.
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`read_csv`使用Excel方言，并将双引号视为引号字符，这会导致在找到关闭双引号之前找到换行符时失败。
- en: 'We can get around this using `dialect`:'
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`dialect`来解决这个问题：
- en: '[PRE362]'
  id: totrans-1779
  prefs: []
  type: TYPE_PRE
  zh: '[PRE362]'
- en: 'All of the dialect options can be specified separately by keyword arguments:'
  id: totrans-1780
  prefs: []
  type: TYPE_NORMAL
  zh: 所有方言选项都可以通过关键字参数单独指定：
- en: '[PRE363]'
  id: totrans-1781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE363]'
- en: 'Another common dialect option is `skipinitialspace`, to skip any whitespace
    after a delimiter:'
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的方言选项是`skipinitialspace`，用于跳过分隔符后的任何空格：
- en: '[PRE364]'
  id: totrans-1783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE364]'
- en: 'The parsers make every attempt to “do the right thing” and not be fragile.
    Type inference is a pretty big deal. If a column can be coerced to integer dtype
    without altering the contents, the parser will do so. Any non-numeric columns
    will come through as object dtype as with the rest of pandas objects.  ### Quoting
    and Escape Characters'
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
  zh: '解析器会尽力“做正确的事情”而不会变得脆弱。类型推断是一件很重要的事情。如果可以将列强制转换为整数dtype而不改变内容，则解析器将这样做。任何非数字列将像其他pandas对象一样以object
    dtype传递。  ### 引用和转义字符'
- en: 'Quotes (and other escape characters) in embedded fields can be handled in any
    number of ways. One way is to use backslashes; to properly parse this data, you
    should pass the `escapechar` option:'
  id: totrans-1785
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入字段中的引号（和其他转义字符）可以以多种方式处理。一种方法是使用反斜杠；为了正确解析这些数据，您应该传递`escapechar`选项：
- en: '[PRE365]  ### Files with fixed width columns'
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE365]  ### 具有固定宽度列的文件'
- en: 'While [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") reads delimited data, the [`read_fwf()`](../reference/api/pandas.read_fwf.html#pandas.read_fwf
    "pandas.read_fwf") function works with data files that have known and fixed column
    widths. The function parameters to `read_fwf` are largely the same as `read_csv`
    with two extra parameters, and a different usage of the `delimiter` parameter:'
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")用于读取分隔数据，[`read_fwf()`](../reference/api/pandas.read_fwf.html#pandas.read_fwf
    "pandas.read_fwf")函数用于处理具有已知和固定列宽的数据文件。`read_fwf`的函数参数与`read_csv`基本相同，但有两个额外参数，并且`delimiter`参数的使用方式不同：
- en: '`colspecs`: A list of pairs (tuples) giving the extents of the fixed-width
    fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’
    can be used to instruct the parser to try detecting the column specifications
    from the first 100 rows of the data. Default behavior, if not specified, is to
    infer.'
  id: totrans-1788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colspecs`：一个对给出每行固定宽度字段的范围的一半开放区间（即，[from, to[）的列表（元组）。字符串值‘infer’ 可以用于指示解析器尝试从数据的前
    100 行检测列规格。如果未指定，默认行为是推断。'
- en: '`widths`: A list of field widths which can be used instead of ‘colspecs’ if
    the intervals are contiguous.'
  id: totrans-1789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`widths`：一个字段宽度的列表，可用于代替‘colspecs’，如果间隔是连续的。'
- en: '`delimiter`: Characters to consider as filler characters in the fixed-width
    file. Can be used to specify the filler character of the fields if it is not spaces
    (e.g., ‘~’).'
  id: totrans-1790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delimiter`：在固定宽度文件中视为填充字符的字符。如果字段的填充字符不是空格（例如，‘~’），则可以用它来指定字段的填充字符。'
- en: 'Consider a typical fixed-width data file:'
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个典型的固定宽度数据文件：
- en: '[PRE366]'
  id: totrans-1792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE366]'
- en: 'In order to parse this file into a `DataFrame`, we simply need to supply the
    column specifications to the `read_fwf` function along with the file name:'
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将此文件解析为 `DataFrame`，我们只需提供列规格到 `read_fwf` 函数以及文件名：
- en: '[PRE367]'
  id: totrans-1794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE367]'
- en: 'Note how the parser automatically picks column names X.<column number> when
    `header=None` argument is specified. Alternatively, you can supply just the column
    widths for contiguous columns:'
  id: totrans-1795
  prefs: []
  type: TYPE_NORMAL
  zh: 注意当指定了 `header=None` 参数时，解析器会自动选择列名 X.<列编号>。或者，您可以仅提供连续列的列宽：
- en: '[PRE368]'
  id: totrans-1796
  prefs: []
  type: TYPE_PRE
  zh: '[PRE368]'
- en: The parser will take care of extra white spaces around the columns so it’s ok
    to have extra separation between the columns in the file.
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器会处理列周围的额外空格，因此文件中的列之间有额外的分隔是可以的。
- en: By default, `read_fwf` will try to infer the file’s `colspecs` by using the
    first 100 rows of the file. It can do it only in cases when the columns are aligned
    and correctly separated by the provided `delimiter` (default delimiter is whitespace).
  id: totrans-1798
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`read_fwf` 将尝试通过使用文件的前 100 行推断文件的 `colspecs`。它只能在列对齐且由提供的 `delimiter`（默认分隔符是空格）正确分隔的情况下进行。
- en: '[PRE369]'
  id: totrans-1799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE369]'
- en: '`read_fwf` supports the `dtype` parameter for specifying the types of parsed
    columns to be different from the inferred type.'
  id: totrans-1800
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_fwf` 支持 `dtype` 参数，用于指定解析列的类型与推断类型不同。'
- en: '[PRE370]'
  id: totrans-1801
  prefs: []
  type: TYPE_PRE
  zh: '[PRE370]'
- en: Indexes
  id: totrans-1802
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 索引
- en: Files with an “implicit” index column
  id: totrans-1803
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带有“隐式”索引列的文件
- en: 'Consider a file with one less entry in the header than the number of data column:'
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑文件中标题项比数据列数少一个的情况：
- en: '[PRE371]'
  id: totrans-1805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE371]'
- en: 'In this special case, `read_csv` assumes that the first column is to be used
    as the index of the `DataFrame`:'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特殊情况下，`read_csv` 假设第一列将用作 `DataFrame` 的索引：
- en: '[PRE372]'
  id: totrans-1807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE372]'
- en: 'Note that the dates weren’t automatically parsed. In that case you would need
    to do as before:'
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，日期没有被自动解析。在这种情况下，您需要像以前一样操作：
- en: '[PRE373]'
  id: totrans-1809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE373]'
- en: Reading an index with a `MultiIndex`
  id: totrans-1810
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 `MultiIndex` 读取索引
- en: 'Suppose you have data indexed by two columns:'
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据由两列索引：
- en: '[PRE374]'
  id: totrans-1812
  prefs: []
  type: TYPE_PRE
  zh: '[PRE374]'
- en: 'The `index_col` argument to `read_csv` can take a list of column numbers to
    turn multiple columns into a `MultiIndex` for the index of the returned object:'
  id: totrans-1813
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv` 的 `index_col` 参数可以接受列号列表，将多个列转换为返回对象的索引的 `MultiIndex`：'
- en: '[PRE375]'
  id: totrans-1814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE375]'
- en: '#### Reading columns with a `MultiIndex`'
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 使用 `MultiIndex` 读取列'
- en: By specifying list of row locations for the `header` argument, you can read
    in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the
    intervening rows.
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为 `header` 参数指定行位置列表，您可以读取列的 `MultiIndex`。指定非连续行将跳过介于其间的行。
- en: '[PRE376]'
  id: totrans-1817
  prefs: []
  type: TYPE_PRE
  zh: '[PRE376]'
- en: '`read_csv` is also able to interpret a more common format of multi-columns
    indices.'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv` 还能够解释更常见的多列索引格式。'
- en: '[PRE377]'
  id: totrans-1819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE377]'
- en: Note
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If an `index_col` is not specified (e.g. you don’t have an index, or wrote
    it with `df.to_csv(..., index=False)`, then any `names` on the columns index will
    be *lost*.  ### Automatically “sniffing” the delimiter'
  id: totrans-1821
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未指定 `index_col`（例如，您没有索引，或者用 `df.to_csv(..., index=False)` 写入了它，则列索引上的任何 `names`
    将会 *丢失*。### 自动“嗅探”定界符
- en: '`read_csv` is capable of inferring delimited (not necessarily comma-separated)
    files, as pandas uses the [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)") class of the csv module. For this, you have to specify `sep=None`.'
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
  zh: '`read_csv` 能够推断分隔（不一定是逗号分隔）的文件，因为 pandas 使用 csv 模块的 [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(在 Python v3.12 中)") 类。为此，您必须指定 `sep=None`。'
- en: '[PRE378]  ### Reading multiple files to create a single DataFrame'
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE378]  ### 读取多个文件以创建单个 DataFrame'
- en: 'It’s best to use [`concat()`](../reference/api/pandas.concat.html#pandas.concat
    "pandas.concat") to combine multiple files. See the [cookbook](cookbook.html#cookbook-csv-multiple-files)
    for an example.  ### Iterating through files chunk by chunk'
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
  zh: '最好使用[`concat()`](../reference/api/pandas.concat.html#pandas.concat "pandas.concat")来合并多个文件。查看[cookbook](cookbook.html#cookbook-csv-multiple-files)以获取示例。  ###
    逐块迭代文件'
- en: 'Suppose you wish to iterate through a (potentially very large) file lazily
    rather than reading the entire file into memory, such as the following:'
  id: totrans-1825
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您希望惰性地迭代（可能非常大的）文件，而不是将整个文件读入内存，例如以下内容：
- en: '[PRE379]'
  id: totrans-1826
  prefs: []
  type: TYPE_PRE
  zh: '[PRE379]'
- en: 'By specifying a `chunksize` to `read_csv`, the return value will be an iterable
    object of type `TextFileReader`:'
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`read_csv`中指定`chunksize`，返回值将是一个`TextFileReader`类型的可迭代对象：
- en: '[PRE380]'
  id: totrans-1828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE380]'
- en: 'Changed in version 1.2: `read_csv/json/sas` return a context-manager when iterating
    through a file.'
  id: totrans-1829
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本1.2开始更改：`read_csv/json/sas` 在遍历文件时返回一个上下文管理器。
- en: 'Specifying `iterator=True` will also return the `TextFileReader` object:'
  id: totrans-1830
  prefs: []
  type: TYPE_NORMAL
  zh: 指定`iterator=True`还将返回`TextFileReader`对象：
- en: '[PRE381]'
  id: totrans-1831
  prefs: []
  type: TYPE_PRE
  zh: '[PRE381]'
- en: Specifying the parser engine
  id: totrans-1832
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指定解析引擎
- en: Pandas currently supports three engines, the C engine, the python engine, and
    an experimental pyarrow engine (requires the `pyarrow` package). In general, the
    pyarrow engine is fastest on larger workloads and is equivalent in speed to the
    C engine on most other workloads. The python engine tends to be slower than the
    pyarrow and C engines on most workloads. However, the pyarrow engine is much less
    robust than the C engine, which lacks a few features compared to the Python engine.
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 目前支持三种引擎，C 引擎、Python 引擎和实验性的 pyarrow 引擎（需要`pyarrow`软件包）。一般来说，pyarrow
    引擎在较大的工作负载上速度最快，在大多数其他工作负载上与 C 引擎的速度相当。Python 引擎在大多数工作负载上比 pyarrow 和 C 引擎慢。但是，与
    C 引擎相比，pyarrow 引擎要不那么稳定，缺少一些与 Python 引擎相比的功能。
- en: Where possible, pandas uses the C parser (specified as `engine='c'`), but it
    may fall back to Python if C-unsupported options are specified.
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，pandas 使用 C 解析器（指定为`engine='c'`），但如果指定了不受 C 支持的选项，则可能会退回到 Python。
- en: 'Currently, options unsupported by the C and pyarrow engines include:'
  id: totrans-1835
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，C 和 pyarrow 引擎不支持的选项包括：
- en: '`sep` other than a single character (e.g. regex separators)'
  id: totrans-1836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep` 不是单个字符（例如正则表达式分隔符）'
- en: '`skipfooter`'
  id: totrans-1837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skipfooter`'
- en: '`sep=None` with `delim_whitespace=False`'
  id: totrans-1838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep=None` 与 `delim_whitespace=False`'
- en: Specifying any of the above options will produce a `ParserWarning` unless the
    python engine is selected explicitly using `engine='python'`.
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: 指定上述任何选项将产生一个`ParserWarning`，除非显式选择`engine='python'`来选择 Python 引擎。
- en: 'Options that are unsupported by the pyarrow engine which are not covered by
    the list above include:'
  id: totrans-1840
  prefs: []
  type: TYPE_NORMAL
  zh: pyarrow 引擎不支持的选项，不在上面的列表中包括：
- en: '`float_precision`'
  id: totrans-1841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float_precision`'
- en: '`chunksize`'
  id: totrans-1842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunksize`'
- en: '`comment`'
  id: totrans-1843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`comment`'
- en: '`nrows`'
  id: totrans-1844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nrows`'
- en: '`thousands`'
  id: totrans-1845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thousands`'
- en: '`memory_map`'
  id: totrans-1846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`memory_map`'
- en: '`dialect`'
  id: totrans-1847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dialect`'
- en: '`on_bad_lines`'
  id: totrans-1848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_bad_lines`'
- en: '`delim_whitespace`'
  id: totrans-1849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delim_whitespace`'
- en: '`quoting`'
  id: totrans-1850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quoting`'
- en: '`lineterminator`'
  id: totrans-1851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lineterminator`'
- en: '`converters`'
  id: totrans-1852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`converters`'
- en: '`decimal`'
  id: totrans-1853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decimal`'
- en: '`iterator`'
  id: totrans-1854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterator`'
- en: '`dayfirst`'
  id: totrans-1855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dayfirst`'
- en: '`infer_datetime_format`'
  id: totrans-1856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`infer_datetime_format`'
- en: '`verbose`'
  id: totrans-1857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`'
- en: '`skipinitialspace`'
  id: totrans-1858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skipinitialspace`'
- en: '`low_memory`'
  id: totrans-1859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_memory`'
- en: Specifying these options with `engine='pyarrow'` will raise a `ValueError`.
  id: totrans-1860
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`engine='pyarrow'`指定这些选项将引发`ValueError`。
- en: '### Reading/writing remote files'
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
  zh: '### 读取/写入远程文件'
- en: 'You can pass in a URL to read or write remote files to many of pandas’ IO functions
    - the following example shows reading a CSV file:'
  id: totrans-1862
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以传递一个 URL 给许多 pandas 的 IO 函数来读取或写入远程文件 - 以下示例显示了读取 CSV 文件：
- en: '[PRE382]'
  id: totrans-1863
  prefs: []
  type: TYPE_PRE
  zh: '[PRE382]'
- en: New in version 1.3.0.
  id: totrans-1864
  prefs: []
  type: TYPE_NORMAL
  zh: 版本1.3.0中的新功能。
- en: 'A custom header can be sent alongside HTTP(s) requests by passing a dictionary
    of header key value mappings to the `storage_options` keyword argument as shown
    below:'
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将头键值映射的字典传递给`storage_options`关键字参数来发送自定义标头，如下所示：
- en: '[PRE383]'
  id: totrans-1866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE383]'
- en: 'All URLs which are not local files or HTTP(s) are handled by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/),
    if installed, and its various filesystem implementations (including Amazon S3,
    Google Cloud, SSH, FTP, webHDFS…). Some of these implementations will require
    additional packages to be installed, for example S3 URLs require the [s3fs](https://pypi.org/project/s3fs/)
    library:'
  id: totrans-1867
  prefs: []
  type: TYPE_NORMAL
  zh: 所有不是本地文件或 HTTP(s) 的 URL 都由[fsspec](https://filesystem-spec.readthedocs.io/en/latest/)处理，如果安装了它，以及其各种文件系统实现（包括
    Amazon S3、Google Cloud、SSH、FTP、webHDFS…）。其中一些实现将需要安装其他软件包，例如 S3 URL 需要[s3fs](https://pypi.org/project/s3fs/)库：
- en: '[PRE384]'
  id: totrans-1868
  prefs: []
  type: TYPE_PRE
  zh: '[PRE384]'
- en: When dealing with remote storage systems, you might need extra configuration
    with environment variables or config files in special locations. For example,
    to access data in your S3 bucket, you will need to define credentials in one of
    the several ways listed in the [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/#credentials).
    The same is true for several of the storage backends, and you should follow the
    links at [fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations)
    for implementations built into `fsspec` and [fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)
    for those not included in the main `fsspec` distribution.
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理远程存储系统时，您可能需要在特殊位置的环境变量或配置文件中进行额外配置。例如，要访问您的S3存储桶中的数据，您需要在[S3Fs documentation](https://s3fs.readthedocs.io/en/latest/#credentials)中列出的几种方式之一中定义凭据。对于几个存储后端，情况也是如此，您应该遵循`fsspec`内置的[fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations)和未包含在主`fsspec`分发中的[fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)的链接。
- en: 'You can also pass parameters directly to the backend driver. Since `fsspec`
    does not utilize the `AWS_S3_HOST` environment variable, we can directly define
    a dictionary containing the endpoint_url and pass the object into the storage
    option parameter:'
  id: totrans-1870
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以直接将参数传递给后端驱动程序。由于`fsspec`不使用`AWS_S3_HOST`环境变量，因此我们可以直接定义一个包含endpoint_url的字典，并将对象传递给存储选项参数：
- en: '[PRE385]'
  id: totrans-1871
  prefs: []
  type: TYPE_PRE
  zh: '[PRE385]'
- en: More sample configurations and documentation can be found at [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage).
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
  zh: 更多示例配置和文档可以在[S3Fs documentation](https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage)中找到。
- en: If you do *not* have S3 credentials, you can still access public data by specifying
    an anonymous connection, such as
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有S3凭据，仍然可以通过指定匿名连接来访问公共数据，例如
- en: New in version 1.2.0.
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本1.2.0中。
- en: '[PRE386]'
  id: totrans-1875
  prefs: []
  type: TYPE_PRE
  zh: '[PRE386]'
- en: '`fsspec` also allows complex URLs, for accessing data in compressed archives,
    local caching of files, and more. To locally cache the above example, you would
    modify the call to'
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsspec`还允许使用复杂的URL，用于访问压缩存档中的数据，文件的本地缓存等。要在本地缓存上述示例，您需要修改调用方式为'
- en: '[PRE387]'
  id: totrans-1877
  prefs: []
  type: TYPE_PRE
  zh: '[PRE387]'
- en: where we specify that the “anon” parameter is meant for the “s3” part of the
    implementation, not to the caching implementation. Note that this caches to a
    temporary directory for the duration of the session only, but you can also specify
    a permanent store.
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们指定“anon”参数是针对实现的“s3”部分，而不是缓存实现。请注意，这仅在会话期间缓存到临时目录，但您还可以指定一个永久存储。
- en: Writing out data
  id: totrans-1879
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写出数据
- en: '#### Writing to CSV format'
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 写入到CSV格式'
- en: The `Series` and `DataFrame` objects have an instance method `to_csv` which
    allows storing the contents of the object as a comma-separated-values file. The
    function takes a number of arguments. Only the first is required.
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
  zh: '`Series`和`DataFrame`对象具有一个实例方法`to_csv`，允许将对象的内容存储为逗号分隔值文件。该函数接受多个参数。只需要第一个。'
- en: '`path_or_buf`: A string path to the file to write or a file object. If a file
    object it must be opened with `newline=''''`'
  id: totrans-1882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path_or_buf`: 要写入的文件的字符串路径或文件对象。如果是文件对象，则必须使用`newline=''''`打开。'
- en: '`sep` : Field delimiter for the output file (default “,”)'
  id: totrans-1883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep`: 输出文件的字段分隔符（默认为“,”）'
- en: '`na_rep`: A string representation of a missing value (default ‘’)'
  id: totrans-1884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`na_rep`: 缺失值的字符串表示（默认为‘’）'
- en: '`float_format`: Format string for floating point numbers'
  id: totrans-1885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float_format`: 浮点数的格式字符串'
- en: '`columns`: Columns to write (default None)'
  id: totrans-1886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns`: 写入的列（默认为None）'
- en: '`header`: Whether to write out the column names (default True)'
  id: totrans-1887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header`: 是否写出列名（默认为True）'
- en: '`index`: whether to write row (index) names (default True)'
  id: totrans-1888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`: 是否写入行（索引）名称（默认为True）'
- en: '`index_label`: Column label(s) for index column(s) if desired. If None (default),
    and `header` and `index` are True, then the index names are used. (A sequence
    should be given if the `DataFrame` uses MultiIndex).'
  id: totrans-1889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_label`: 如果需要，用于索引列的列标签。如果为None（默认值），并且`header`和`index`为True，则使用索引名称。（如果`DataFrame`使用MultiIndex，则应给出一个序列）。'
- en: '`mode` : Python write mode, default ‘w’'
  id: totrans-1890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`: Python写入模式，默认为‘w’'
- en: '`encoding`: a string representing the encoding to use if the contents are non-ASCII,
    for Python versions prior to 3'
  id: totrans-1891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding`: 表示要使用的编码的字符串，如果内容为非ASCII字符，对于Python版本3之前'
- en: '`lineterminator`: Character sequence denoting line end (default `os.linesep`)'
  id: totrans-1892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lineterminator`: 表示行尾的字符序列（默认为`os.linesep`）'
- en: '`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).
    Note that if you have set a `float_format` then floats are converted to strings
    and csv.QUOTE_NONNUMERIC will treat them as non-numeric'
  id: totrans-1893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quoting`：设置引用规则，如 csv 模块中的设置（默认为 csv.QUOTE_MINIMAL）。注意，如果您设置了一个 `float_format`，那么浮点数将被转换为字符串，并且
    csv.QUOTE_NONNUMERIC 将把它们视为非数值。'
- en: '`quotechar`: Character used to quote fields (default ‘”’)'
  id: totrans-1894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quotechar`：用于引用字段的字符（默认为 ‘”’）。'
- en: '`doublequote`: Control quoting of `quotechar` in fields (default True)'
  id: totrans-1895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doublequote`：控制在字段中引用 `quotechar`（默认为 True）。'
- en: '`escapechar`: Character used to escape `sep` and `quotechar` when appropriate
    (default None)'
  id: totrans-1896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`escapechar`：用于在适当时候转义 `sep` 和 `quotechar` 的字符（默认为 None）。'
- en: '`chunksize`: Number of rows to write at a time'
  id: totrans-1897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunksize`：一次写入的行数。'
- en: '`date_format`: Format string for datetime objects'
  id: totrans-1898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date_format`：datetime 对象的格式字符串。'
- en: Writing a formatted string
  id: totrans-1899
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写入一个格式化字符串
- en: 'The `DataFrame` object has an instance method `to_string` which allows control
    over the string representation of the object. All arguments are optional:'
  id: totrans-1900
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame` 对象有一个实例方法 `to_string`，它允许控制对象的字符串表示。所有参数都是可选的：'
- en: '`buf` default None, for example a StringIO object'
  id: totrans-1901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`buf` 默认为 None，例如一个 StringIO 对象。'
- en: '`columns` default None, which columns to write'
  id: totrans-1902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`columns` 默认为 None，要写入的列。'
- en: '`col_space` default None, minimum width of each column.'
  id: totrans-1903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`col_space` 默认为 None，每列的最小宽度。'
- en: '`na_rep` default `NaN`, representation of NA value'
  id: totrans-1904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`na_rep` 默认为 `NaN`，表示 NA 值。'
- en: '`formatters` default None, a dictionary (by column) of functions each of which
    takes a single argument and returns a formatted string'
  id: totrans-1905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`formatters` 默认为 None，一个字典（按列）的函数，每个函数接受一个参数并返回一个格式化的字符串。'
- en: '`float_format` default None, a function which takes a single (float) argument
    and returns a formatted string; to be applied to floats in the `DataFrame`.'
  id: totrans-1906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float_format` 默认为 None，一个函数，它接受一个单一（浮点）参数并返回一个格式化的字符串；应用于 `DataFrame` 中的浮点数。'
- en: '`sparsify` default True, set to False for a `DataFrame` with a hierarchical
    index to print every MultiIndex key at each row.'
  id: totrans-1907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparsify` 默认为 True，对于具有分层索引的 `DataFrame`，设置为 False 以在每一行打印每个 MultiIndex 键。'
- en: '`index_names` default True, will print the names of the indices'
  id: totrans-1908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_names` 默认为 True，将打印索引的名称。'
- en: '`index` default True, will print the index (ie, row labels)'
  id: totrans-1909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index` 默认为 True，将打印索引（即，行标签）。'
- en: '`header` default True, will print the column labels'
  id: totrans-1910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header` 默认为 True，将打印列标签。'
- en: '`justify` default `left`, will print column headers left- or right-justified'
  id: totrans-1911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`justify` 默认为 `left`，将列标题左对齐或右对齐。'
- en: The `Series` object also has a `to_string` method, but with only the `buf`,
    `na_rep`, `float_format` arguments. There is also a `length` argument which, if
    set to `True`, will additionally output the length of the Series.
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: '`Series` 对象也有一个 `to_string` 方法，但只有 `buf`、`na_rep`、`float_format` 参数。还有一个 `length`
    参数，如果设置为 `True`，还会输出 Series 的长度。'
- en: Parsing options
  id: totrans-1913
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析选项
- en: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    accepts the following common arguments:'
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
  zh: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    接受以下常见参数：'
- en: Basic
  id: totrans-1915
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础
- en: filepath_or_buffervarious
  id: totrans-1916
  prefs: []
  type: TYPE_NORMAL
  zh: filepath_or_buffervarious
- en: Either a path to a file (a [`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)"), or `py:py._path.local.LocalPath`), URL (including http,
    ftp, and S3 locations), or any object with a `read()` method (such as an open
    file or [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO "(in
    Python v3.12)")).
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
  zh: 要么是文件路径（[`str`](https://docs.python.org/3/library/stdtypes.html#str "(在 Python
    v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(在 Python v3.12)") 或 `py:py._path.local.LocalPath`)，URL（包括 http、ftp 和 S3 位置），或具有
    `read()` 方法的任何对象（例如打开的文件或 [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO
    "(在 Python v3.12)")）。
- en: sepstr, defaults to `','` for [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), `\t` for [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
  zh: sepstr，默认为 [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") 的 `','`，`\t` 为 [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")。
- en: 'Delimiter to use. If sep is `None`, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will be used
    and automatically detect the separator by Python’s builtin sniffer tool, [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)"). In addition, separators longer than 1 character and different
    from `''\s+''` will be interpreted as regular expressions and will also force
    the use of the Python parsing engine. Note that regex delimiters are prone to
    ignoring quoted data. Regex example: `''\\r\\t''`.'
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的分隔符。如果sep为`None`，C引擎无法自动检测分隔符，但Python解析引擎可以，这意味着将使用后者，并通过Python的内置嗅探工具[`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(在Python v3.12中)")自动检测分隔符。此外，长度大于1个字符且不同于`'\s+'`的分隔符将被解释为正则表达式，并且还会强制使用Python解析引擎。请注意，正则表达式分隔符容易忽略带引号的数据。正则表达式示例：`'\\r\\t'`。
- en: delimiterstr, default `None`
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔符字符串，默认为`None`
- en: Alternative argument name for sep.
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: sep的替代参数名称。
- en: delim_whitespaceboolean, default False
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: delim_whitespace布尔值，默认为False
- en: Specifies whether or not whitespace (e.g. `' '` or `'\t'`) will be used as the
    delimiter. Equivalent to setting `sep='\s+'`. If this option is set to `True`,
    nothing should be passed in for the `delimiter` parameter.
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
  zh: 指定是否使用空格（例如`' '`或`'\t'`）作为分隔符。等效于设置`sep='\s+'`。如果将此选项设置为`True`，则不应为`delimiter`参数传递任何内容。
- en: Column and index locations and names
  id: totrans-1924
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 列和索引位置及名称
- en: headerint or list of ints, default `'infer'`
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
  zh: header整数或整数列表，默认为`'infer'`
- en: 'Row number(s) to use as the column names, and the start of the data. Default
    behavior is to infer the column names: if no names are passed the behavior is
    identical to `header=0` and column names are inferred from the first line of the
    file, if column names are passed explicitly then the behavior is identical to
    `header=None`. Explicitly pass `header=0` to be able to replace existing names.'
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: 用作列名和数据起始位置的行号。默认行为是推断列名：如果没有传递名称，则行为与`header=0`相同，并且列名从文件的第一行推断出来，如果明确传递了列名，则行为与`header=None`相同。明确传递`header=0`以能够替换现有名称。
- en: The header can be a list of ints that specify row locations for a MultiIndex
    on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this parameter ignores
    commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes
    the first line of data rather than the first line of the file.
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
  zh: 标题可以是指定列的MultiIndex的行位置的整数列表，例如`[0,1,3]`。未指定的中间行将被跳过（例如，在此示例中跳过2）。请注意，如果`skip_blank_lines=True`，此参数会忽略注释行和空行，因此header=0表示数据的第一行而不是文件的第一行。
- en: namesarray-like, default `None`
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
  zh: names类似数组，默认为`None`
- en: List of column names to use. If file contains no header row, then you should
    explicitly pass `header=None`. Duplicates in this list are not allowed.
  id: totrans-1929
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的列名列表。如果文件不包含标题行，则应明确传递`header=None`。不允许在此列表中存在重复项。
- en: index_colint, str, sequence of int / str, or False, optional, default `None`
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
  zh: index_col整数、字符串、int/str序列或False，可选，默认为`None`
- en: Column(s) to use as the row labels of the `DataFrame`, either given as string
    name or column index. If a sequence of int / str is given, a MultiIndex is used.
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: 作为`DataFrame`行标签使用的列。可以作为字符串名称或列索引给出。如果给出int / str序列，则将使用MultiIndex。
- en: Note
  id: totrans-1932
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`index_col=False` can be used to force pandas to *not* use the first column
    as the index, e.g. when you have a malformed file with delimiters at the end of
    each line.'
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_col=False`可用于强制pandas*不*将第一列用作索引，例如当您有一个在每行末尾带有分隔符的格式不正确的文件时。'
- en: The default value of `None` instructs pandas to guess. If the number of fields
    in the column header row is equal to the number of fields in the body of the data
    file, then a default index is used. If it is larger, then the first columns are
    used as index so that the remaining number of fields in the body are equal to
    the number of fields in the header.
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值为`None`指示pandas进行猜测。如果列标题行中的字段数等于数据文件主体中的字段数，则使用默认索引。如果大于，则使用前几列作为索引，以使数据主体中的剩余字段数等于标题中的字段数。
- en: The first row after the header is used to determine the number of columns, which
    will go into the index. If the subsequent rows contain less columns than the first
    row, they are filled with `NaN`.
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
  zh: 标题后的第一行用于确定将进入索引的列数。如果后续行包含的列少于第一行，则用`NaN`填充。
- en: This can be avoided through `usecols`. This ensures that the columns are taken
    as is and the trailing data are ignored.
  id: totrans-1936
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过`usecols`来避免。这确保列按原样取出，并且忽略尾随数据。
- en: usecolslist-like or callable, default `None`
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
  zh: usecols类似列表或可调用，默认为`None`
- en: Return a subset of the columns. If list-like, all elements must either be positional
    (i.e. integer indices into the document columns) or strings that correspond to
    column names provided either by the user in `names` or inferred from the document
    header row(s). If `names` are given, the document header row(s) are not taken
    into account. For example, a valid list-like `usecols` parameter would be `[0,
    1, 2]` or `['foo', 'bar', 'baz']`.
  id: totrans-1938
  prefs: []
  type: TYPE_NORMAL
  zh: 返回列的子集。如果类似列表，则所有元素必须是位置（即整数索引到文档列）或字符串，对应于用户在`names`中提供的列名称或从文档标题行推断出的列名称。如果给出了`names`，则不考虑文档标题行。例如，一个有效的类似列表`usecols`参数将是`[0,
    1, 2]`或`['foo', 'bar', 'baz']`。
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate
    a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo',
    'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
  id: totrans-1939
  prefs: []
  type: TYPE_NORMAL
  zh: 元素顺序被忽略，因此`usecols=[0, 1]`与`[1, 0]`相同。要从具有元素顺序的`data`实例化DataFrame，请使用`pd.read_csv(data,
    usecols=['foo', 'bar'])[['foo', 'bar']]`以`['foo', 'bar']`顺序的列或`pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]`以`['bar', 'foo']`顺序的列。
- en: 'If callable, the callable function will be evaluated against the column names,
    returning names where the callable function evaluates to True:'
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是可调用的，将对列名求值可调用函数，返回使可调用函数求值为True的名称：
- en: '[PRE388]'
  id: totrans-1941
  prefs: []
  type: TYPE_PRE
  zh: '[PRE388]'
- en: Using this parameter results in much faster parsing time and lower memory usage
    when using the c engine. The Python engine loads the data first before deciding
    which columns to drop.
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此参数会导致使用c引擎时解析时间更快，内存使用量更低。 Python引擎首先加载数据，然后再决定要删除哪些列。
- en: General parsing configuration
  id: totrans-1943
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通用解析配置
- en: dtypeType name or dict of column -> type, default `None`
  id: totrans-1944
  prefs: []
  type: TYPE_NORMAL
  zh: dtype类型名称或列->类型的字典，默认为`None`
- en: 'Data type for data or columns. E.g. `{''a'': np.float64, ''b'': np.int32, ''c'':
    ''Int64''}` Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype. If converters are specified, they will be
    applied INSTEAD of dtype conversion.'
  id: totrans-1945
  prefs: []
  type: TYPE_NORMAL
  zh: '数据或列的数据类型。例如`{''a'': np.float64, ''b'': np.int32, ''c'': ''Int64''}`使用`str`或`object`以及合适的`na_values`设置来保留并不解释dtype。如果指定了转换器，则将应用于dtype转换而不是。'
- en: 'New in version 1.5.0: Support for defaultdict was added. Specify a defaultdict
    as input where the default determines the dtype of the columns which are not explicitly
    listed.'
  id: totrans-1946
  prefs: []
  type: TYPE_NORMAL
  zh: 1.5.0版中的新功能：添加了对defaultdict的支持。在输入中指定defaultdict，其中默认值确定未明确列出的列的dtype。
- en: dtype_backend{“numpy_nullable”, “pyarrow”}, defaults to NumPy backed DataFrames
  id: totrans-1947
  prefs: []
  type: TYPE_NORMAL
  zh: dtype_backend{“numpy_nullable”, “pyarrow”}，默认为NumPy后端的DataFrame
- en: Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
    nullable dtypes are used for all dtypes that have a nullable implementation when
    “numpy_nullable” is set, pyarrow is used for all dtypes if “pyarrow” is set.
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的dtype_backend，例如DataFrame应该具有NumPy数组，当设置为“numpy_nullable”时，所有具有可空实现的dtype都使用可空dtype，如果设置为“pyarrow”，则使用pyarrow。
- en: The dtype_backends are still experimential.
  id: totrans-1949
  prefs: []
  type: TYPE_NORMAL
  zh: dtype_backends仍然是实验性的。
- en: New in version 2.0.
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
  zh: 2.0版中的新功能。
- en: engine{`'c'`, `'python'`, `'pyarrow'`}
  id: totrans-1951
  prefs: []
  type: TYPE_NORMAL
  zh: engine{`'c'`, `'python'`, `'pyarrow'`}
- en: Parser engine to use. The C and pyarrow engines are faster, while the python
    engine is currently more feature-complete. Multithreading is currently only supported
    by the pyarrow engine.
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的解析引擎。 C和pyarrow引擎速度更快，而Python引擎目前更完整。当前仅pyarrow引擎支持多线程。
- en: 'New in version 1.4.0: The “pyarrow” engine was added as an *experimental* engine,
    and some features are unsupported, or may not work correctly, with this engine.'
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
  zh: 1.4.0版中的新功能：添加了“pyarrow”引擎作为*实验性*引擎，并且某些功能不受支持，或者可能在此引擎下无法正常工作。
- en: convertersdict, default `None`
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器字典，默认为`None`
- en: Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.
  id: totrans-1955
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在某些列中转换值的函数字典。键可以是整数或列标签。
- en: true_valueslist, default `None`
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: true_values列表，默认为`None`
- en: Values to consider as `True`.
  id: totrans-1957
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑为`True`的值。
- en: false_valueslist, default `None`
  id: totrans-1958
  prefs: []
  type: TYPE_NORMAL
  zh: false_values列表，默认为`None`
- en: Values to consider as `False`.
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑为`False`的值。
- en: skipinitialspaceboolean, default `False`
  id: totrans-1960
  prefs: []
  type: TYPE_NORMAL
  zh: skipinitialspace布尔值，默认为`False`
- en: Skip spaces after delimiter.
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: 在分隔符后跳过空格。
- en: skiprowslist-like or integer, default `None`
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
  zh: skiprows类似列表或整数，默认为`None`
- en: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
    of the file.
  id: totrans-1963
  prefs: []
  type: TYPE_NORMAL
  zh: 要跳过的行号（从 0 开始）或文件开头要跳过的行数（整数）。
- en: 'If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise:'
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可调用，则将对行索引评估可调用函数，如果应跳过该行，则返回 True，否则返回 False：
- en: '[PRE389]'
  id: totrans-1965
  prefs: []
  type: TYPE_PRE
  zh: '[PRE389]'
- en: skipfooterint, default `0`
  id: totrans-1966
  prefs: []
  type: TYPE_NORMAL
  zh: skipfooter 整数，默认为 `0`
- en: Number of lines at bottom of file to skip (unsupported with engine=’c’).
  id: totrans-1967
  prefs: []
  type: TYPE_NORMAL
  zh: 要跳过文件底部的行数（在引擎为 'c' 时不支持）。
- en: nrowsint, default `None`
  id: totrans-1968
  prefs: []
  type: TYPE_NORMAL
  zh: nrows 整数，默认为 `None`
- en: Number of rows of file to read. Useful for reading pieces of large files.
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取的文件行数。用于读取大文件的片段。
- en: low_memoryboolean, default `True`
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
  zh: low_memory 布尔值，默认为 `True`
- en: Internally process the file in chunks, resulting in lower memory use while parsing,
    but possibly mixed type inference. To ensure no mixed types either set `False`,
    or specify the type with the `dtype` parameter. Note that the entire file is read
    into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter
    to return the data in chunks. (Only valid with C parser)
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析文件时以块的方式内部处理，从而在解析时降低内存使用，但可能导致混合类型推断。要确保没有混合类型，可以设置 `False`，或者使用 `dtype`
    参数指定类型。请注意，无论如何，整个文件都会读入一个单独的 `DataFrame`，使用 `chunksize` 或 `iterator` 参数返回数据块。(仅与
    C 解析器有效)
- en: memory_mapboolean, default False
  id: totrans-1972
  prefs: []
  type: TYPE_NORMAL
  zh: memory_map 布尔值，默认为 False
- en: If a filepath is provided for `filepath_or_buffer`, map the file object directly
    onto memory and access the data directly from there. Using this option can improve
    performance because there is no longer any I/O overhead.
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为 `filepath_or_buffer` 提供了文件路径，则直接将文件对象映射到内存中，并直接从那里访问数据。使用此选项可以提高性能，因为不再有
    I/O 开销。
- en: NA and missing data handling
  id: totrans-1974
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NA 和缺失数据处理
- en: na_valuesscalar, str, list-like, or dict, default `None`
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: na_values 标量、字符串、类似列表或字典，默认为 `None`
- en: Additional strings to recognize as NA/NaN. If dict passed, specific per-column
    NA values. See [na values const](#io-navaluesconst) below for a list of the values
    interpreted as NaN by default.
  id: totrans-1976
  prefs: []
  type: TYPE_NORMAL
  zh: 附加字符串以识别为 NA/NaN。如果传入了字典，则为每列指定特定的 NA 值。请参阅下面的 [na values const](#io-navaluesconst)
    列表，了解默认情况下解释为 NaN 的值。
- en: keep_default_naboolean, default `True`
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: keep_default_na 布尔值，默认为 `True`
- en: 'Whether or not to include the default NaN values when parsing the data. Depending
    on whether `na_values` is passed in, the behavior is as follows:'
  id: totrans-1978
  prefs: []
  type: TYPE_NORMAL
  zh: 解析数据时是否包括默认的 NaN 值。根据是否传入了 `na_values`，行为如下：
- en: If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is
    appended to the default NaN values used for parsing.
  id: totrans-1979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `keep_default_na` 为 `True`，并且指定了 `na_values`，则将 `na_values` 追加到用于解析的默认 NaN
    值中。
- en: If `keep_default_na` is `True`, and `na_values` are not specified, only the
    default NaN values are used for parsing.
  id: totrans-1980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `keep_default_na` 为 `True`，并且未指定 `na_values`，则仅使用默认的 NaN 值进行解析。
- en: If `keep_default_na` is `False`, and `na_values` are specified, only the NaN
    values specified `na_values` are used for parsing.
  id: totrans-1981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `keep_default_na` 为 `False`，并且指定了 `na_values`，则仅使用指定的 NaN 值 `na_values` 进行解析。
- en: If `keep_default_na` is `False`, and `na_values` are not specified, no strings
    will be parsed as NaN.
  id: totrans-1982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `keep_default_na` 为 `False`，并且未指定 `na_values`，则不会将任何字符串解析为 NaN。
- en: Note that if `na_filter` is passed in as `False`, the `keep_default_na` and
    `na_values` parameters will be ignored.
  id: totrans-1983
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果将 `na_filter` 传入为 `False`，则将忽略 `keep_default_na` 和 `na_values` 参数。
- en: na_filterboolean, default `True`
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: na_filter 布尔值，默认为 `True`
- en: Detect missing value markers (empty strings and the value of na_values). In
    data without any NAs, passing `na_filter=False` can improve the performance of
    reading a large file.
  id: totrans-1985
  prefs: []
  type: TYPE_NORMAL
  zh: 检测缺失值标记（空字符串和 na_values 的值）。在没有任何 NA 的数据中，传入 `na_filter=False` 可以提高读取大文件的性能。
- en: verboseboolean, default `False`
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
  zh: verbose 布尔值，默认为 `False`
- en: Indicate number of NA values placed in non-numeric columns.
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
  zh: 在非数字列中指示放置的 NA 值的数量。
- en: skip_blank_linesboolean, default `True`
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
  zh: skip_blank_lines 布尔值，默认为 `True`
- en: If `True`, skip over blank lines rather than interpreting as NaN values.
  id: totrans-1989
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为 `True`，则跳过空行而不是解释为 NaN 值。
- en: '#### Datetime handling'
  id: totrans-1990
  prefs: []
  type: TYPE_NORMAL
  zh: '#### 日期时间处理'
- en: parse_datesboolean or list of ints or names or list of lists or dict, default
    `False`.
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
  zh: parse_dates 布尔值或整数列表或名称列表或列表列表或字典，默认为 `False`。
- en: If `True` -> try parsing the index.
  id: totrans-1992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果为 `True` -> 尝试解析索引。
- en: If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.
  id: totrans-1993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `[1, 2, 3]` -> 尝试将列 1、2、3 分别解析为单独的日期列。
- en: If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.
  id: totrans-1994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `[[1, 3]]` -> 组合列 1 和 3 并解析为单个日期列。
- en: 'If `{''foo'': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’.'
  id: totrans-1995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果 `{''foo'': [1, 3]}` -> 解析列 1、3 为日期，并将结果命名为 ''foo''。'
- en: Note
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A fast-path exists for iso8601-formatted dates.
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
- en: infer_datetime_formatboolean, default `False`
  id: totrans-1998
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates is enabled for a column, attempt to infer the datetime
    format to speed up the processing.
  id: totrans-1999
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: A strict version of this argument is now the
    default, passing it has no effect.'
  id: totrans-2000
  prefs: []
  type: TYPE_NORMAL
- en: keep_date_colboolean, default `False`
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates specifies combining multiple columns then keep the
    original columns.
  id: totrans-2002
  prefs: []
  type: TYPE_NORMAL
- en: date_parserfunction, default `None`
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
- en: 'Function to use for converting a sequence of string columns to an array of
    datetime instances. The default uses `dateutil.parser.parser` to do the conversion.
    pandas will try to call date_parser in three different ways, advancing to the
    next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates)
    as arguments; 2) concatenate (row-wise) the string values from the columns defined
    by parse_dates into a single array and pass that; and 3) call date_parser once
    for each row using one or more strings (corresponding to the columns defined by
    parse_dates) as arguments.'
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: Use `date_format` instead, or read in as `object`
    and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.'
  id: totrans-2005
  prefs: []
  type: TYPE_NORMAL
- en: date_formatstr or dict of column -> format, default `None`
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
- en: If used in conjunction with `parse_dates`, will parse dates according to this
    format. For anything more complex, please read in as `object` and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.
  id: totrans-2007
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.0.
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
- en: dayfirstboolean, default `False`
  id: totrans-2009
  prefs: []
  type: TYPE_NORMAL
- en: DD/MM format dates, international and European format.
  id: totrans-2010
  prefs: []
  type: TYPE_NORMAL
- en: cache_datesboolean, default True
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
- en: If True, use a cache of unique, converted dates to apply the datetime conversion.
    May produce significant speed-up when parsing duplicate date strings, especially
    ones with timezone offsets.
  id: totrans-2012
  prefs: []
  type: TYPE_NORMAL
- en: Iteration
  id: totrans-2013
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: iteratorboolean, default `False`
  id: totrans-2014
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.
  id: totrans-2015
  prefs: []
  type: TYPE_NORMAL
- en: chunksizeint, default `None`
  id: totrans-2016
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration. See [iterating and chunking](#io-chunking)
    below.
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
- en: Quoting, compression, and file format
  id: totrans-2018
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: compression{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`,
    `dict`}, default `'infer'`
  id: totrans-2019
  prefs: []
  type: TYPE_NORMAL
- en: 'For on-the-fly decompression of on-disk data. If ‘infer’, then use gzip, bz2,
    zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in ‘.gz’, ‘.bz2’,
    ‘.zip’, ‘.xz’, ‘.zst’, respectively, and no decompression otherwise. If using
    ‘zip’, the ZIP file must contain only one data file to be read in. Set to `None`
    for no decompression. Can also be a dict with key `''method''` set to one of {`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd''`} and other key-value pairs are forwarded to
    `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`.
    As an example, the following could be passed for faster compression and to create
    a reproducible gzip archive: `compression={''method'': ''gzip'', ''compresslevel'':
    1, ''mtime'': 1}`.'
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
  zh: '用于在磁盘上数据的即时解压缩。如果为‘infer’，则如果`filepath_or_buffer`是以‘.gz’、‘.bz2’、‘.zip’、‘.xz’、‘.zst’结尾的路径，则使用gzip、bz2、zip、xz或zstandard，否则不进行解压缩。如果使用‘zip’，ZIP文件必须只包含一个要读取的数据文件。设置为`None`表示不解压缩。也可以是一个字典，其中键为`''method''`，设置为其中之一{`''zip''`、`''gzip''`、`''bz2''`、`''zstd''`}，其他键值对将被转发到`zipfile.ZipFile`、`gzip.GzipFile`、`bz2.BZ2File`或`zstandard.ZstdDecompressor`。例如，可以传递以下内容以进行更快的压缩并创建可重现的gzip存档：`compression={''method'':
    ''gzip'', ''compresslevel'': 1, ''mtime'': 1}`。'
- en: 'Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’
    to `gzip.open`.'
  id: totrans-2021
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本1.2.0中更改：以前的版本将‘gzip’的字典条目转发给`gzip.open`。
- en: thousandsstr, default `None`
  id: totrans-2022
  prefs: []
  type: TYPE_NORMAL
  zh: thousandsstr，默认为`None`
- en: Thousands separator.
  id: totrans-2023
  prefs: []
  type: TYPE_NORMAL
  zh: 千位分隔符。
- en: decimalstr, default `'.'`
  id: totrans-2024
  prefs: []
  type: TYPE_NORMAL
  zh: decimalstr，默认为`'.'`
- en: Character to recognize as decimal point. E.g. use `','` for European data.
  id: totrans-2025
  prefs: []
  type: TYPE_NORMAL
  zh: 用作小数点的字符。例如，对于欧洲数据，使用`','`。
- en: float_precisionstring, default None
  id: totrans-2026
  prefs: []
  type: TYPE_NORMAL
  zh: float_precisionstring，默认为None
- en: Specifies which converter the C engine should use for floating-point values.
    The options are `None` for the ordinary converter, `high` for the high-precision
    converter, and `round_trip` for the round-trip converter.
  id: totrans-2027
  prefs: []
  type: TYPE_NORMAL
  zh: 指定C引擎应使用哪个转换器处理浮点值。选项为`None`表示普通转换器，`high`表示高精度转换器，`round_trip`表示往返转换器。
- en: lineterminatorstr (length 1), default `None`
  id: totrans-2028
  prefs: []
  type: TYPE_NORMAL
  zh: lineterminatorstr（长度为1），默认为`None`
- en: Character to break file into lines. Only valid with C parser.
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将文件分成行的字符。仅与C解析器有效。
- en: quotecharstr (length 1)
  id: totrans-2030
  prefs: []
  type: TYPE_NORMAL
  zh: quotecharstr（长度为1）
- en: The character used to denote the start and end of a quoted item. Quoted items
    can include the delimiter and it will be ignored.
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
  zh: 用于表示引用项的开始和结束的字符。引用项可以包括分隔符，它将被忽略。
- en: quotingint or `csv.QUOTE_*` instance, default `0`
  id: totrans-2032
  prefs: []
  type: TYPE_NORMAL
  zh: quotingint或`csv.QUOTE_*`实例，默认为`0`
- en: Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL`
    (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).
  id: totrans-2033
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`csv.QUOTE_*`常量控制字段引用行为。使用`QUOTE_MINIMAL`（0）、`QUOTE_ALL`（1）、`QUOTE_NONNUMERIC`（2）或`QUOTE_NONE`（3）中的一个。
- en: doublequoteboolean, default `True`
  id: totrans-2034
  prefs: []
  type: TYPE_NORMAL
  zh: doublequoteboolean，默认为`True`
- en: When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether
    or not to interpret two consecutive `quotechar` elements **inside** a field as
    a single `quotechar` element.
  id: totrans-2035
  prefs: []
  type: TYPE_NORMAL
  zh: 当指定`quotechar`并且`quoting`不是`QUOTE_NONE`时，指示是否将字段内两个连续的`quotechar`元素解释为单个`quotechar`元素。
- en: escapecharstr (length 1), default `None`
  id: totrans-2036
  prefs: []
  type: TYPE_NORMAL
  zh: escapecharstr（长度为1），默认为`None`
- en: One-character string used to escape delimiter when quoting is `QUOTE_NONE`.
  id: totrans-2037
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在引用为`QUOTE_NONE`时转义分隔符的单字符字符串。
- en: commentstr, default `None`
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
  zh: commentstr，默认为`None`
- en: Indicates remainder of line should not be parsed. If found at the beginning
    of a line, the line will be ignored altogether. This parameter must be a single
    character. Like empty lines (as long as `skip_blank_lines=True`), fully commented
    lines are ignored by the parameter `header` but not by `skiprows`. For example,
    if `comment='#'`, parsing ‘#empty\na,b,c\n1,2,3’ with `header=0` will result in
    ‘a,b,c’ being treated as the header.
  id: totrans-2039
  prefs: []
  type: TYPE_NORMAL
  zh: 表示剩余的行不应被解析。如果在行的开头找到，整行将被完全忽略。此参数必须是一个单个字符。与空行一样（只要`skip_blank_lines=True`），完全注释的行由参数`header`忽略，但不由`skiprows`忽略。例如，如果`comment='#'`，使用`header=0`解析‘#empty\na,b,c\n1,2,3’将导致‘a,b,c’被视为标题。
- en: encodingstr, default `None`
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
  zh: encodingstr，默认为`None`
- en: Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python
    standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: 读取/写入UTF时要使用的编码（例如，`'utf-8'`）。[Python标准编码列表](https://docs.python.org/3/library/codecs.html#standard-encodings)。
- en: dialectstr or [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") instance, default `None`
  id: totrans-2042
  prefs: []
  type: TYPE_NORMAL
  zh: dialectstr或[`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(在Python v3.12中)")实例，默认为`None`
- en: 'If provided, this parameter will override values (default or not) for the following
    parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`,
    and `quoting`. If it is necessary to override values, a ParserWarning will be
    issued. See [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") documentation for more details.'
  id: totrans-2043
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供了此参数，将覆盖以下参数的值（默认或非默认）：`delimiter`、`doublequote`、`escapechar`、`skipinitialspace`、`quotechar`
    和 `quoting`。如果需要覆盖值，将发出一个 ParserWarning。更多详情请参阅[`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)")文档。
- en: Error handling
  id: totrans-2044
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误处理
- en: on_bad_lines(‘error’, ‘warn’, ‘skip’), default ‘error’
  id: totrans-2045
  prefs: []
  type: TYPE_NORMAL
  zh: on_bad_lines（‘error’、‘warn’、‘skip’），默认为 ‘error’
- en: 'Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :'
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
  zh: 指定在遇到坏行（字段过多的行）时要执行的操作。允许的值为：
- en: ‘error’, raise an ParserError when a bad line is encountered.
  id: totrans-2047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘error’，遇到坏行时引发 ParserError。
- en: ‘warn’, print a warning when a bad line is encountered and skip that line.
  id: totrans-2048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘warn’，遇到坏行时打印警告并跳过该行。
- en: ‘skip’, skip bad lines without raising or warning when they are encountered.
  id: totrans-2049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘skip’，遇到坏行时跳过而不引发或警告。
- en: New in version 1.3.0.
  id: totrans-2050
  prefs: []
  type: TYPE_NORMAL
  zh: 1.3.0 版本中的新功能。
- en: Basic
  id: totrans-2051
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本
- en: filepath_or_buffervarious
  id: totrans-2052
  prefs: []
  type: TYPE_NORMAL
  zh: filepath_or_buffervarious
- en: Either a path to a file (a [`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)"), or `py:py._path.local.LocalPath`), URL (including http,
    ftp, and S3 locations), or any object with a `read()` method (such as an open
    file or [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO "(in
    Python v3.12)")).
  id: totrans-2053
  prefs: []
  type: TYPE_NORMAL
  zh: 可以是文件路径（一个[`str`](https://docs.python.org/3/library/stdtypes.html#str "(in Python
    v3.12)")、[`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)")，或 `py:py._path.local.LocalPath`），URL（包括 http、ftp 和 S3 地址），或任何具有
    `read()` 方法的对象（例如打开的文件或 [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO
    "(in Python v3.12)")）。
- en: sepstr, defaults to `','` for [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), `\t` for [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
  id: totrans-2054
  prefs: []
  type: TYPE_NORMAL
  zh: sepstr，默认为 [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") 的 `','`，[`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table") 的 `\t`
- en: 'Delimiter to use. If sep is `None`, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will be used
    and automatically detect the separator by Python’s builtin sniffer tool, [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)"). In addition, separators longer than 1 character and different
    from `''\s+''` will be interpreted as regular expressions and will also force
    the use of the Python parsing engine. Note that regex delimiters are prone to
    ignoring quoted data. Regex example: `''\\r\\t''`.'
  id: totrans-2055
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的分隔符。如果 sep 为 `None`，C 引擎无法自动检测分隔符，但 Python 解析引擎可以，这意味着将使用后者，并通过 Python 的内置嗅探工具
    [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer "(in Python
    v3.12)") 自动检测分隔符。此外，长度大于 1 且不同于 `'\s+'` 的分隔符将被解释为正则表达式，并且还将强制使用 Python 解析引擎。请注意，正则表达式分隔符容易忽略带引号的数据。正则表达式示例：`'\\r\\t'`。
- en: delimiterstr, default `None`
  id: totrans-2056
  prefs: []
  type: TYPE_NORMAL
  zh: delimiterstr，默认为 `None`
- en: Alternative argument name for sep.
  id: totrans-2057
  prefs: []
  type: TYPE_NORMAL
  zh: sep 的替代参数名称。
- en: delim_whitespaceboolean, default False
  id: totrans-2058
  prefs: []
  type: TYPE_NORMAL
  zh: delim_whitespaceboolean，默认为 False
- en: Specifies whether or not whitespace (e.g. `' '` or `'\t'`) will be used as the
    delimiter. Equivalent to setting `sep='\s+'`. If this option is set to `True`,
    nothing should be passed in for the `delimiter` parameter.
  id: totrans-2059
  prefs: []
  type: TYPE_NORMAL
  zh: 指定是否使用空格（例如 `' '` 或 `'\t'`）作为分隔符。等同于设置 `sep='\s+'`。如果将此选项设置为 `True`，则不应为 `delimiter`
    参数传递任何内容。
- en: Column and index locations and names
  id: totrans-2060
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 列和索引位置和名称
- en: headerint or list of ints, default `'infer'`
  id: totrans-2061
  prefs: []
  type: TYPE_NORMAL
  zh: headerint 或 int 列表，默认为 `'infer'`
- en: 'Row number(s) to use as the column names, and the start of the data. Default
    behavior is to infer the column names: if no names are passed the behavior is
    identical to `header=0` and column names are inferred from the first line of the
    file, if column names are passed explicitly then the behavior is identical to
    `header=None`. Explicitly pass `header=0` to be able to replace existing names.'
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
  zh: 用作列名和数据起始位置的行号， 默认行为是推断列名：如果没有传递名称，则行为与 `header=0` 相同，并且列名从文件的第一行推断出来，如果显式传递了列名，则行为与
    `header=None` 相同。显式传递 `header=0` 以替换现有名称。
- en: The header can be a list of ints that specify row locations for a MultiIndex
    on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this parameter ignores
    commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes
    the first line of data rather than the first line of the file.
  id: totrans-2063
  prefs: []
  type: TYPE_NORMAL
- en: namesarray-like, default `None`
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
- en: List of column names to use. If file contains no header row, then you should
    explicitly pass `header=None`. Duplicates in this list are not allowed.
  id: totrans-2065
  prefs: []
  type: TYPE_NORMAL
- en: index_colint, str, sequence of int / str, or False, optional, default `None`
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
- en: Column(s) to use as the row labels of the `DataFrame`, either given as string
    name or column index. If a sequence of int / str is given, a MultiIndex is used.
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2068
  prefs: []
  type: TYPE_NORMAL
- en: '`index_col=False` can be used to force pandas to *not* use the first column
    as the index, e.g. when you have a malformed file with delimiters at the end of
    each line.'
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `None` instructs pandas to guess. If the number of fields
    in the column header row is equal to the number of fields in the body of the data
    file, then a default index is used. If it is larger, then the first columns are
    used as index so that the remaining number of fields in the body are equal to
    the number of fields in the header.
  id: totrans-2070
  prefs: []
  type: TYPE_NORMAL
- en: The first row after the header is used to determine the number of columns, which
    will go into the index. If the subsequent rows contain less columns than the first
    row, they are filled with `NaN`.
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
- en: This can be avoided through `usecols`. This ensures that the columns are taken
    as is and the trailing data are ignored.
  id: totrans-2072
  prefs: []
  type: TYPE_NORMAL
- en: usecolslist-like or callable, default `None`
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
- en: Return a subset of the columns. If list-like, all elements must either be positional
    (i.e. integer indices into the document columns) or strings that correspond to
    column names provided either by the user in `names` or inferred from the document
    header row(s). If `names` are given, the document header row(s) are not taken
    into account. For example, a valid list-like `usecols` parameter would be `[0,
    1, 2]` or `['foo', 'bar', 'baz']`.
  id: totrans-2074
  prefs: []
  type: TYPE_NORMAL
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate
    a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo',
    'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the column names,
    returning names where the callable function evaluates to True:'
  id: totrans-2076
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE390]'
  id: totrans-2077
  prefs: []
  type: TYPE_PRE
  zh: '[PRE390]'
- en: Using this parameter results in much faster parsing time and lower memory usage
    when using the c engine. The Python engine loads the data first before deciding
    which columns to drop.
  id: totrans-2078
  prefs: []
  type: TYPE_NORMAL
- en: General parsing configuration
  id: totrans-2079
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: dtypeType name or dict of column -> type, default `None`
  id: totrans-2080
  prefs: []
  type: TYPE_NORMAL
- en: 'Data type for data or columns. E.g. `{''a'': np.float64, ''b'': np.int32, ''c'':
    ''Int64''}` Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype. If converters are specified, they will be
    applied INSTEAD of dtype conversion.'
  id: totrans-2081
  prefs: []
  type: TYPE_NORMAL
  zh: '数据或列的数据类型。例如`{''a'': np.float64, ''b'': np.int32, ''c'': ''Int64''}` 使用`str`或`object`与适当的`na_values`设置一起使用以保留并不解释dtype。如果指定了转换器，则将应用转换器而不是dtype转换。'
- en: 'New in version 1.5.0: Support for defaultdict was added. Specify a defaultdict
    as input where the default determines the dtype of the columns which are not explicitly
    listed.'
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本1.5.0中添加了对defaultdict的支持。指定一个defaultdict作为输入，其中默认值确定未明确列出的列的dtype。
- en: dtype_backend{“numpy_nullable”, “pyarrow”}, defaults to NumPy backed DataFrames
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
  zh: dtype_backend{“numpy_nullable”、“pyarrow”}，默认为NumPy支持的DataFrames
- en: Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
    nullable dtypes are used for all dtypes that have a nullable implementation when
    “numpy_nullable” is set, pyarrow is used for all dtypes if “pyarrow” is set.
  id: totrans-2084
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哪种dtype后端，例如DataFrame是否应具有NumPy数组，当设置“numpy_nullable”时，所有具有可空实现的dtype都将使用可空dtype，如果设置“pyarrow”，则所有dtype都将使用pyarrow。
- en: The dtype_backends are still experimential.
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
  zh: dtype_backends仍处于实验阶段。
- en: New in version 2.0.
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本2.0中添加。
- en: engine{`'c'`, `'python'`, `'pyarrow'`}
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
  zh: engine{`'c'`、`'python'`、`'pyarrow'`}
- en: Parser engine to use. The C and pyarrow engines are faster, while the python
    engine is currently more feature-complete. Multithreading is currently only supported
    by the pyarrow engine.
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的解析引擎。C和pyarrow引擎更快，而python引擎目前更完整。目前仅pyarrow引擎支持多线程。
- en: 'New in version 1.4.0: The “pyarrow” engine was added as an *experimental* engine,
    and some features are unsupported, or may not work correctly, with this engine.'
  id: totrans-2089
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本1.4.0中添加了“pyarrow”引擎作为*实验性*引擎，并且某些功能不受支持，或者可能无法正确工作，使用此引擎。
- en: convertersdict, default `None`
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器字典，默认为`None`
- en: Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
  zh: 用于转换某些列中值的函数字典。键可以是整数或列标签。
- en: true_valueslist, default `None`
  id: totrans-2092
  prefs: []
  type: TYPE_NORMAL
  zh: true_values列表，默认为`None`
- en: Values to consider as `True`.
  id: totrans-2093
  prefs: []
  type: TYPE_NORMAL
  zh: 要视为`True`的值。
- en: false_valueslist, default `None`
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
  zh: false_values列表，默认为`None`
- en: Values to consider as `False`.
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
  zh: 要视为`False`的值。
- en: skipinitialspaceboolean, default `False`
  id: totrans-2096
  prefs: []
  type: TYPE_NORMAL
  zh: skipinitialspace布尔值，默认为`False`
- en: Skip spaces after delimiter.
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
  zh: 在分隔符后跳过空格。
- en: skiprowslist-like or integer, default `None`
  id: totrans-2098
  prefs: []
  type: TYPE_NORMAL
  zh: skiprows类似列表或整数，默认为`None`
- en: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
    of the file.
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
  zh: 要跳过的行号（从0开始索引）或文件开头要跳过的行数（int）。
- en: 'If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise:'
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可调用，将对行索引评估可调用函数，如果应跳过该行则返回True，否则返回False：
- en: '[PRE391]'
  id: totrans-2101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE391]'
- en: skipfooterint, default `0`
  id: totrans-2102
  prefs: []
  type: TYPE_NORMAL
  zh: skipfooter整数，默认为`0`
- en: Number of lines at bottom of file to skip (unsupported with engine=’c’).
  id: totrans-2103
  prefs: []
  type: TYPE_NORMAL
  zh: 要跳过文件底部的行数（与engine=’c’不兼容）。
- en: nrowsint, default `None`
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
  zh: nrows整数，默认为`None`
- en: Number of rows of file to read. Useful for reading pieces of large files.
  id: totrans-2105
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取的文件行数。用于读取大文件的片段。
- en: low_memoryboolean, default `True`
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
  zh: low_memory布尔值，默认为`True`
- en: Internally process the file in chunks, resulting in lower memory use while parsing,
    but possibly mixed type inference. To ensure no mixed types either set `False`,
    or specify the type with the `dtype` parameter. Note that the entire file is read
    into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter
    to return the data in chunks. (Only valid with C parser)
  id: totrans-2107
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部以块的形式处理文件，从而在解析时使用更少的内存，但可能会混合类型推断。为了确保没有混合类型，要么设置为`False`，要么使用`dtype`参数指定类型。请注意，无论如何整个文件都会被读入单个`DataFrame`中，使用`chunksize`或`iterator`参数以块返回数据。（仅适用于C解析器）
- en: memory_mapboolean, default False
  id: totrans-2108
  prefs: []
  type: TYPE_NORMAL
  zh: memory_map布尔值，默认为False
- en: If a filepath is provided for `filepath_or_buffer`, map the file object directly
    onto memory and access the data directly from there. Using this option can improve
    performance because there is no longer any I/O overhead.
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为`filepath_or_buffer`提供了文件路径，则直接将文件对象映射到内存并直接从那里访问数据。使用此选项可以提高性能，因为不再有任何I/O开销。
- en: NA and missing data handling
  id: totrans-2110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NA和缺失数据处理
- en: na_valuesscalar, str, list-like, or dict, default `None`
  id: totrans-2111
  prefs: []
  type: TYPE_NORMAL
  zh: na_valuesscalar、str、类似列表或字典，默认为`None`
- en: Additional strings to recognize as NA/NaN. If dict passed, specific per-column
    NA values. See [na values const](#io-navaluesconst) below for a list of the values
    interpreted as NaN by default.
  id: totrans-2112
  prefs: []
  type: TYPE_NORMAL
  zh: 附加字符串以识别为NA/NaN。如果传递了字典，则特定于每列的NA值。请参阅下面的[na values const](#io-navaluesconst)以获取默认情况下解释为NaN的值列表。
- en: keep_default_naboolean, default `True`
  id: totrans-2113
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether or not to include the default NaN values when parsing the data. Depending
    on whether `na_values` is passed in, the behavior is as follows:'
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is
    appended to the default NaN values used for parsing.
  id: totrans-2115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are not specified, only the
    default NaN values are used for parsing.
  id: totrans-2116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are specified, only the NaN
    values specified `na_values` are used for parsing.
  id: totrans-2117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are not specified, no strings
    will be parsed as NaN.
  id: totrans-2118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if `na_filter` is passed in as `False`, the `keep_default_na` and
    `na_values` parameters will be ignored.
  id: totrans-2119
  prefs: []
  type: TYPE_NORMAL
- en: na_filterboolean, default `True`
  id: totrans-2120
  prefs: []
  type: TYPE_NORMAL
- en: Detect missing value markers (empty strings and the value of na_values). In
    data without any NAs, passing `na_filter=False` can improve the performance of
    reading a large file.
  id: totrans-2121
  prefs: []
  type: TYPE_NORMAL
- en: verboseboolean, default `False`
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
- en: Indicate number of NA values placed in non-numeric columns.
  id: totrans-2123
  prefs: []
  type: TYPE_NORMAL
- en: skip_blank_linesboolean, default `True`
  id: totrans-2124
  prefs: []
  type: TYPE_NORMAL
- en: If `True`, skip over blank lines rather than interpreting as NaN values.
  id: totrans-2125
  prefs: []
  type: TYPE_NORMAL
- en: '#### Datetime handling'
  id: totrans-2126
  prefs: []
  type: TYPE_NORMAL
- en: parse_datesboolean or list of ints or names or list of lists or dict, default
    `False`.
  id: totrans-2127
  prefs: []
  type: TYPE_NORMAL
- en: If `True` -> try parsing the index.
  id: totrans-2128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.
  id: totrans-2129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.
  id: totrans-2130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `{''foo'': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’.'
  id: totrans-2131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-2132
  prefs: []
  type: TYPE_NORMAL
- en: A fast-path exists for iso8601-formatted dates.
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
- en: infer_datetime_formatboolean, default `False`
  id: totrans-2134
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates is enabled for a column, attempt to infer the datetime
    format to speed up the processing.
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: A strict version of this argument is now the
    default, passing it has no effect.'
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
- en: keep_date_colboolean, default `False`
  id: totrans-2137
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates specifies combining multiple columns then keep the
    original columns.
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
- en: date_parserfunction, default `None`
  id: totrans-2139
  prefs: []
  type: TYPE_NORMAL
- en: 'Function to use for converting a sequence of string columns to an array of
    datetime instances. The default uses `dateutil.parser.parser` to do the conversion.
    pandas will try to call date_parser in three different ways, advancing to the
    next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates)
    as arguments; 2) concatenate (row-wise) the string values from the columns defined
    by parse_dates into a single array and pass that; and 3) call date_parser once
    for each row using one or more strings (corresponding to the columns defined by
    parse_dates) as arguments.'
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: Use `date_format` instead, or read in as `object`
    and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.'
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
- en: date_formatstr or dict of column -> format, default `None`
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
- en: If used in conjunction with `parse_dates`, will parse dates according to this
    format. For anything more complex, please read in as `object` and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.
  id: totrans-2143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果与`parse_dates`一起使用，将根据此格式解析日期。对于更复杂的情况，请将其读取为`object`，然后根据需要应用[`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime")。
- en: New in version 2.0.0.
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.0.0版本中新增。
- en: dayfirstboolean, default `False`
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
  zh: dayfirstboolean，默认为`False`
- en: DD/MM format dates, international and European format.
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
  zh: DD/MM格式的日期，国际和欧洲格式。
- en: cache_datesboolean, default True
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
  zh: cache_datesboolean，默认为True
- en: If True, use a cache of unique, converted dates to apply the datetime conversion.
    May produce significant speed-up when parsing duplicate date strings, especially
    ones with timezone offsets.
  id: totrans-2148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为True，则使用唯一的转换日期缓存应用日期时间转换。在解析重复日期字符串时可能会产生显著加速，特别是带有时区偏移的日期字符串。
- en: Iteration
  id: totrans-2149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 迭代
- en: iteratorboolean, default `False`
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
  zh: iteratorboolean，默认为`False`
- en: Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
  zh: 返回用于迭代或获取块的`TextFileReader`对象。
- en: chunksizeint, default `None`
  id: totrans-2152
  prefs: []
  type: TYPE_NORMAL
  zh: chunksizeint，默认为`None`
- en: Return `TextFileReader` object for iteration. See [iterating and chunking](#io-chunking)
    below.
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
  zh: 返回用于迭代的`TextFileReader`对象。请参见下面的[迭代和分块](#io-chunking)。
- en: Quoting, compression, and file format
  id: totrans-2154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 引用、压缩和文件格式
- en: compression{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`,
    `dict`}, default `'infer'`
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: compression{`'infer'`、`'gzip'`、`'bz2'`、`'zip'`、`'xz'`、`'zstd'`、`None`、`dict`}，默认为`'infer'`
- en: 'For on-the-fly decompression of on-disk data. If ‘infer’, then use gzip, bz2,
    zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in ‘.gz’, ‘.bz2’,
    ‘.zip’, ‘.xz’, ‘.zst’, respectively, and no decompression otherwise. If using
    ‘zip’, the ZIP file must contain only one data file to be read in. Set to `None`
    for no decompression. Can also be a dict with key `''method''` set to one of {`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd''`} and other key-value pairs are forwarded to
    `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`.
    As an example, the following could be passed for faster compression and to create
    a reproducible gzip archive: `compression={''method'': ''gzip'', ''compresslevel'':
    1, ''mtime'': 1}`.'
  id: totrans-2156
  prefs: []
  type: TYPE_NORMAL
  zh: '用于在磁盘上的数据进行即时解压缩。如果为''infer''，则如果`filepath_or_buffer`是以''.gz''、''.bz2''、''.zip''、''.xz''、''.zst''结尾的路径，则使用gzip、bz2、zip、xz或zstandard，否则不进行解压缩。如果使用''zip''，ZIP文件必须只包含一个要读取的数据文件。设置为`None`表示不进行解压缩。也可以是一个字典，其中键为''method''，设置为其中之一{''zip''、''gzip''、''bz2''、''zstd''}，其他键值对将转发给`zipfile.ZipFile`、`gzip.GzipFile`、`bz2.BZ2File`或`zstandard.ZstdDecompressor`。例如，可以传递以下内容以获得更快的压缩和创建可重现的gzip存档：`compression={''method'':
    ''gzip'', ''compresslevel'': 1, ''mtime'': 1}`。'
- en: 'Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’
    to `gzip.open`.'
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
  zh: 在1.2.0版本中更改：以前的版本将'gzip'的字典条目转发给`gzip.open`。
- en: thousandsstr, default `None`
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: thousandsstr，默认为`None`
- en: Thousands separator.
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
  zh: 千位分隔符。
- en: decimalstr, default `'.'`
  id: totrans-2160
  prefs: []
  type: TYPE_NORMAL
  zh: decimalstr，默认为`'.'`
- en: Character to recognize as decimal point. E.g. use `','` for European data.
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: 用作十进制点的字符。例如，对于欧洲数据，请使用`','`。
- en: float_precisionstring, default None
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
  zh: float_precisionstring，默认为None
- en: Specifies which converter the C engine should use for floating-point values.
    The options are `None` for the ordinary converter, `high` for the high-precision
    converter, and `round_trip` for the round-trip converter.
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
  zh: 指定C引擎应使用哪个转换器来处理浮点值。选项为`None`表示普通转换器，`high`表示高精度转换器，`round_trip`表示往返转换器。
- en: lineterminatorstr (length 1), default `None`
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
  zh: lineterminatorstr（长度为1），默认为`None`
- en: Character to break file into lines. Only valid with C parser.
  id: totrans-2165
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将文件分成行的字符。仅与C解析器有效。
- en: quotecharstr (length 1)
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
  zh: quotecharstr（长度为1）
- en: The character used to denote the start and end of a quoted item. Quoted items
    can include the delimiter and it will be ignored.
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
  zh: 用于表示引号项的开始和结束的字符。引号项可以包括分隔符，它将被忽略。
- en: quotingint or `csv.QUOTE_*` instance, default `0`
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
  zh: quotingint或`csv.QUOTE_*`实例，默认为`0`
- en: Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL`
    (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
  zh: 控制字段引用行为的`csv.QUOTE_*`常量。使用`QUOTE_MINIMAL`（0）、`QUOTE_ALL`（1）、`QUOTE_NONNUMERIC`（2）或`QUOTE_NONE`（3）中的一个。
- en: doublequoteboolean, default `True`
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
  zh: doublequoteboolean，默认为`True`
- en: When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether
    or not to interpret two consecutive `quotechar` elements **inside** a field as
    a single `quotechar` element.
  id: totrans-2171
  prefs: []
  type: TYPE_NORMAL
  zh: 当指定了`quotechar`并且`quoting`不是`QUOTE_NONE`时，指示是否将字段内连续的两个`quotechar`元素解释为单个`quotechar`元素。
- en: escapecharstr (length 1), default `None`
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
  zh: escapecharstr（长度为1），默认为`None`
- en: One-character string used to escape delimiter when quoting is `QUOTE_NONE`.
  id: totrans-2173
  prefs: []
  type: TYPE_NORMAL
- en: commentstr, default `None`
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
- en: Indicates remainder of line should not be parsed. If found at the beginning
    of a line, the line will be ignored altogether. This parameter must be a single
    character. Like empty lines (as long as `skip_blank_lines=True`), fully commented
    lines are ignored by the parameter `header` but not by `skiprows`. For example,
    if `comment='#'`, parsing ‘#empty\na,b,c\n1,2,3’ with `header=0` will result in
    ‘a,b,c’ being treated as the header.
  id: totrans-2175
  prefs: []
  type: TYPE_NORMAL
- en: encodingstr, default `None`
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
- en: Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python
    standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
- en: dialectstr or [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") instance, default `None`
  id: totrans-2178
  prefs: []
  type: TYPE_NORMAL
- en: 'If provided, this parameter will override values (default or not) for the following
    parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`,
    and `quoting`. If it is necessary to override values, a ParserWarning will be
    issued. See [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") documentation for more details.'
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  id: totrans-2180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: on_bad_lines(‘error’, ‘warn’, ‘skip’), default ‘error’
  id: totrans-2181
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :'
  id: totrans-2182
  prefs: []
  type: TYPE_NORMAL
- en: ‘error’, raise an ParserError when a bad line is encountered.
  id: totrans-2183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘warn’, print a warning when a bad line is encountered and skip that line.
  id: totrans-2184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘skip’, skip bad lines without raising or warning when they are encountered.
  id: totrans-2185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  id: totrans-2186
  prefs: []
  type: TYPE_NORMAL
- en: '### Specifying column data types'
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
- en: 'You can indicate the data type for the whole `DataFrame` or individual columns:'
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE392]'
  id: totrans-2189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE392]'
- en: Fortunately, pandas offers more than one way to ensure that your column(s) contain
    only one `dtype`. If you’re unfamiliar with these concepts, you can see [here](basics.html#basics-dtypes)
    to learn more about dtypes, and [here](basics.html#basics-object-conversion) to
    learn more about `object` conversion in pandas.
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, you can use the `converters` argument of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"):'
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE393]'
  id: totrans-2192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE393]'
- en: Or you can use the [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") function to coerce the dtypes after reading in the data,
  id: totrans-2193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE394]'
  id: totrans-2194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE394]'
- en: which will convert all valid parsing to floats, leaving the invalid parsing
    as `NaN`.
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, how you deal with reading in columns containing mixed dtypes depends
    on your specific needs. In the case above, if you wanted to `NaN` out the data
    anomalies, then [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") is probably your best option. However, if you wanted for
    all the data to be coerced, no matter the type, then using the `converters` argument
    of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    would certainly be worth trying.
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, reading in abnormal data with columns containing mixed dtypes
    will result in an inconsistent dataset. If you rely on pandas to infer the dtypes
    of your columns, the parsing engine will go and infer the dtypes for different
    chunks of the data, rather than the whole dataset at once. Consequently, you can
    end up with column(s) with mixed dtypes. For example,
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE395]'
  id: totrans-2199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE395]'
- en: will result with `mixed_df` containing an `int` dtype for certain chunks of
    the column, and `str` for others due to the mixed dtypes from the data that was
    read in. It is important to note that the overall column will be marked with a
    `dtype` of `object`, which is used for columns with mixed dtypes.
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
- en: Setting `dtype_backend="numpy_nullable"` will result in nullable dtypes for
    every column.
  id: totrans-2201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE396]'
  id: totrans-2202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE396]'
- en: '### Specifying categorical dtype'
  id: totrans-2203
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` columns can be parsed directly by specifying `dtype=''category''`
    or `dtype=CategoricalDtype(categories, ordered)`.'
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE397]'
  id: totrans-2205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE397]'
- en: 'Individual columns can be parsed as a `Categorical` using a dict specification:'
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE398]'
  id: totrans-2207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE398]'
- en: Specifying `dtype='category'` will result in an unordered `Categorical` whose
    `categories` are the unique values observed in the data. For more control on the
    categories and order, create a `CategoricalDtype` ahead of time, and pass that
    for that column’s `dtype`.
  id: totrans-2208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE399]'
  id: totrans-2209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE399]'
- en: When using `dtype=CategoricalDtype`, “unexpected” values outside of `dtype.categories`
    are treated as missing values.
  id: totrans-2210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE400]'
  id: totrans-2211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE400]'
- en: This matches the behavior of `Categorical.set_categories()`.
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
- en: With `dtype='category'`, the resulting categories will always be parsed as strings
    (object dtype). If the categories are numeric they can be converted using the
    [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric "pandas.to_numeric")
    function, or as appropriate, another converter such as [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime").
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
- en: When `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all numeric,
    all datetimes, etc.), the conversion is done automatically.
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE401]'
  id: totrans-2216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE401]'
- en: Naming and using columns
  id: totrans-2217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Handling column names'
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
- en: 'A file may or may not have a header row. pandas assumes the first row should
    be used as the column names:'
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE402]'
  id: totrans-2220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE402]'
- en: 'By specifying the `names` argument in conjunction with `header` you can indicate
    other names to use and whether or not to throw away the header row (if any):'
  id: totrans-2221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE403]'
  id: totrans-2222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE403]'
- en: 'If the header is in a row other than the first, pass the row number to `header`.
    This will skip the preceding rows:'
  id: totrans-2223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE404]'
  id: totrans-2224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE404]'
- en: Note
  id: totrans-2225
  prefs: []
  type: TYPE_NORMAL
- en: 'Default behavior is to infer the column names: if no names are passed the behavior
    is identical to `header=0` and column names are inferred from the first non-blank
    line of the file, if column names are passed explicitly then the behavior is identical
    to `header=None`.  #### Handling column names'
  id: totrans-2226
  prefs: []
  type: TYPE_NORMAL
- en: 'A file may or may not have a header row. pandas assumes the first row should
    be used as the column names:'
  id: totrans-2227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE405]'
  id: totrans-2228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE405]'
- en: 'By specifying the `names` argument in conjunction with `header` you can indicate
    other names to use and whether or not to throw away the header row (if any):'
  id: totrans-2229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE406]'
  id: totrans-2230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE406]'
- en: 'If the header is in a row other than the first, pass the row number to `header`.
    This will skip the preceding rows:'
  id: totrans-2231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE407]'
  id: totrans-2232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE407]'
- en: Note
  id: totrans-2233
  prefs: []
  type: TYPE_NORMAL
- en: 'Default behavior is to infer the column names: if no names are passed the behavior
    is identical to `header=0` and column names are inferred from the first non-blank
    line of the file, if column names are passed explicitly then the behavior is identical
    to `header=None`.'
  id: totrans-2234
  prefs: []
  type: TYPE_NORMAL
- en: '### Duplicate names parsing'
  id: totrans-2235
  prefs: []
  type: TYPE_NORMAL
- en: 'If the file or header contains duplicate names, pandas will by default distinguish
    between them so as to prevent overwriting data:'
  id: totrans-2236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE408]'
  id: totrans-2237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE408]'
- en: There is no more duplicate data because duplicate columns ‘X’, …, ‘X’ become
    ‘X’, ‘X.1’, …, ‘X.N’.
  id: totrans-2238
  prefs: []
  type: TYPE_NORMAL
- en: '#### Filtering columns (`usecols`)'
  id: totrans-2239
  prefs: []
  type: TYPE_NORMAL
- en: 'The `usecols` argument allows you to select any subset of the columns in a
    file, either using the column names, position numbers or a callable:'
  id: totrans-2240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE409]'
  id: totrans-2241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE409]'
- en: 'The `usecols` argument can also be used to specify which columns not to use
    in the final result:'
  id: totrans-2242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE410]'
  id: totrans-2243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE410]'
- en: 'In this case, the callable is specifying that we exclude the “a” and “c” columns
    from the output.  #### Filtering columns (`usecols`)'
  id: totrans-2244
  prefs: []
  type: TYPE_NORMAL
- en: 'The `usecols` argument allows you to select any subset of the columns in a
    file, either using the column names, position numbers or a callable:'
  id: totrans-2245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE411]'
  id: totrans-2246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE411]'
- en: 'The `usecols` argument can also be used to specify which columns not to use
    in the final result:'
  id: totrans-2247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE412]'
  id: totrans-2248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE412]'
- en: In this case, the callable is specifying that we exclude the “a” and “c” columns
    from the output.
  id: totrans-2249
  prefs: []
  type: TYPE_NORMAL
- en: Comments and empty lines
  id: totrans-2250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Ignoring line comments and empty lines'
  id: totrans-2251
  prefs: []
  type: TYPE_NORMAL
- en: If the `comment` parameter is specified, then completely commented lines will
    be ignored. By default, completely blank lines will be ignored as well.
  id: totrans-2252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE413]'
  id: totrans-2253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE413]'
- en: 'If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:'
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE414]'
  id: totrans-2255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE414]'
- en: Warning
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
- en: 'The presence of ignored lines might create ambiguities involving line numbers;
    the parameter `header` uses row numbers (ignoring commented/empty lines), while
    `skiprows` uses line numbers (including commented/empty lines):'
  id: totrans-2257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE415]'
  id: totrans-2258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE415]'
- en: 'If both `header` and `skiprows` are specified, `header` will be relative to
    the end of `skiprows`. For example:'
  id: totrans-2259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE416]  #### Comments'
  id: totrans-2260
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes comments or meta data may be included in a file:'
  id: totrans-2261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE417]'
  id: totrans-2262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE417]'
- en: 'By default, the parser includes the comments in the output:'
  id: totrans-2263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE418]'
  id: totrans-2264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE418]'
- en: 'We can suppress the comments using the `comment` keyword:'
  id: totrans-2265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE419]  #### Ignoring line comments and empty lines'
  id: totrans-2266
  prefs: []
  type: TYPE_NORMAL
- en: If the `comment` parameter is specified, then completely commented lines will
    be ignored. By default, completely blank lines will be ignored as well.
  id: totrans-2267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE420]'
  id: totrans-2268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE420]'
- en: 'If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:'
  id: totrans-2269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE421]'
  id: totrans-2270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE421]'
- en: Warning
  id: totrans-2271
  prefs: []
  type: TYPE_NORMAL
- en: 'The presence of ignored lines might create ambiguities involving line numbers;
    the parameter `header` uses row numbers (ignoring commented/empty lines), while
    `skiprows` uses line numbers (including commented/empty lines):'
  id: totrans-2272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE422]'
  id: totrans-2273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE422]'
- en: 'If both `header` and `skiprows` are specified, `header` will be relative to
    the end of `skiprows`. For example:'
  id: totrans-2274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE423]'
  id: totrans-2275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE423]'
- en: '#### Comments'
  id: totrans-2276
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes comments or meta data may be included in a file:'
  id: totrans-2277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE424]'
  id: totrans-2278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE424]'
- en: 'By default, the parser includes the comments in the output:'
  id: totrans-2279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE425]'
  id: totrans-2280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE425]'
- en: 'We can suppress the comments using the `comment` keyword:'
  id: totrans-2281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE426]'
  id: totrans-2282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE426]'
- en: '### Dealing with Unicode data'
  id: totrans-2283
  prefs: []
  type: TYPE_NORMAL
- en: 'The `encoding` argument should be used for encoded unicode data, which will
    result in byte strings being decoded to unicode in the result:'
  id: totrans-2284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE427]'
  id: totrans-2285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE427]'
- en: Some formats which encode all characters as multiple bytes, like UTF-16, won’t
    parse correctly at all without specifying the encoding. [Full list of Python standard
    encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  id: totrans-2286
  prefs: []
  type: TYPE_NORMAL
- en: '### Index columns and trailing delimiters'
  id: totrans-2287
  prefs: []
  type: TYPE_NORMAL
- en: 'If a file has one more column of data than the number of column names, the
    first column will be used as the `DataFrame`’s row names:'
  id: totrans-2288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE428]'
  id: totrans-2289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE428]'
- en: '[PRE429]'
  id: totrans-2290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE429]'
- en: Ordinarily, you can achieve this behavior using the `index_col` option.
  id: totrans-2291
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some exception cases when a file has been prepared with delimiters
    at the end of each data line, confusing the parser. To explicitly disable the
    index column inference and discard the last column, pass `index_col=False`:'
  id: totrans-2292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE430]'
  id: totrans-2293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE430]'
- en: If a subset of data is being parsed using the `usecols` option, the `index_col`
    specification is based on that subset, not the original data.
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE431]'
  id: totrans-2295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE431]'
- en: '### Date Handling'
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
- en: Specifying date columns
  id: totrans-2297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To better facilitate working with datetime data, [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") uses the keyword arguments `parse_dates` and `date_format`
    to allow users to specify a variety of columns and date/time formats to turn the
    input text data into `datetime` objects.
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest case is to just pass in `parse_dates=True`:'
  id: totrans-2299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE432]'
  id: totrans-2300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE432]'
- en: It is often the case that we may want to store date and time data separately,
    or store various date fields separately. the `parse_dates` keyword can be used
    to specify a combination of columns to parse the dates and/or times from.
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a list of column lists to `parse_dates`, the resulting date
    columns will be prepended to the output (so as to not affect the existing column
    order) and the new column names will be the concatenation of the component column
    names:'
  id: totrans-2302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE433]'
  id: totrans-2303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE433]'
- en: 'By default the parser removes the component date columns, but you can choose
    to retain them via the `keep_date_col` keyword:'
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE434]'
  id: totrans-2305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE434]'
- en: Note that if you wish to combine multiple columns into a single date column,
    a nested list must be used. In other words, `parse_dates=[1, 2]` indicates that
    the second and third columns should each be parsed as separate date columns while
    `parse_dates=[[1, 2]]` means the two columns should be parsed into a single column.
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use a dict to specify custom name columns:'
  id: totrans-2307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE435]'
  id: totrans-2308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE435]'
- en: 'It is important to remember that if multiple text columns are to be parsed
    into a single date column, then a new column is prepended to the data. The `index_col`
    specification is based off of this new set of columns rather than the original
    data columns:'
  id: totrans-2309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE436]'
  id: totrans-2310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE436]'
- en: Note
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
- en: If a column or index contains an unparsable date, the entire column or index
    will be returned unaltered as an object data type. For non-standard datetime parsing,
    use [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") after `pd.read_csv`.
  id: totrans-2312
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2313
  prefs: []
  type: TYPE_NORMAL
- en: read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g
    “2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your
    data to store datetimes in this format, load times will be significantly faster,
    ~20x has been observed.
  id: totrans-2314
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.2.0: Combining date columns inside read_csv is deprecated.
    Use `pd.to_datetime` on the relevant result columns instead.'
  id: totrans-2315
  prefs: []
  type: TYPE_NORMAL
- en: Date parsing functions
  id: totrans-2316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, the parser allows you to specify a custom `date_format`. Performance-wise,
    you should try these methods of parsing dates in order:'
  id: totrans-2317
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or
    `date_format={column_name: "%d/%m/%Y"}`.'
  id: totrans-2318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you different formats for different columns, or want to pass any extra options
    (such as `utc`) to `to_datetime`, then you should read in your data as `object`
    dtype, and then use `to_datetime`.
  id: totrans-2319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### Parsing a CSV with mixed timezones'
  id: totrans-2320
  prefs: []
  type: TYPE_NORMAL
- en: pandas cannot natively represent a column or index with mixed timezones. If
    your CSV file contains columns with a mixture of timezones, the default result
    will be an object-dtype column with strings, even with `parse_dates`. To parse
    the mixed-timezone values as a datetime column, read in as `object` dtype and
    then call [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") with `utc=True`.
  id: totrans-2321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE437]  #### Inferring datetime format'
  id: totrans-2322
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of datetime strings that can be guessed (all representing
    December 30th, 2011 at 00:00:00):'
  id: totrans-2323
  prefs: []
  type: TYPE_NORMAL
- en: “20111230”
  id: totrans-2324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “2011/12/30”
  id: totrans-2325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “20111230 00:00:00”
  id: totrans-2326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “12/30/2011 00:00:00”
  id: totrans-2327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/Dec/2011 00:00:00”
  id: totrans-2328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/December/2011 00:00:00”
  id: totrans-2329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`,
    it will guess “01/12/2011” to be December 1st. With `dayfirst=False` (default)
    it will guess “01/12/2011” to be January 12th.
  id: totrans-2330
  prefs: []
  type: TYPE_NORMAL
- en: If you try to parse a column of date strings, pandas will attempt to guess the
    format from the first non-NaN element, and will then parse the rest of the column
    with that format. If pandas fails to guess the format (for example if your first
    string is `'01 December US/Pacific 2000'`), then a warning will be raised and
    each row will be parsed individually by `dateutil.parser.parse`. The safest way
    to parse dates is to explicitly set `format=`.
  id: totrans-2331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE438]'
  id: totrans-2332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE438]'
- en: In the case that you have mixed datetime formats within the same column, you
    can pass `format='mixed'`
  id: totrans-2333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE439]'
  id: totrans-2334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE439]'
- en: 'or, if your datetime formats are all ISO8601 (possibly not identically-formatted):'
  id: totrans-2335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE440]'
  id: totrans-2336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE440]'
- en: International date formats
  id: totrans-2337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While US date formats tend to be MM/DD/YYYY, many international formats use
    DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:'
  id: totrans-2338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE441]'
  id: totrans-2339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE441]'
- en: Writing CSVs to binary file objects
  id: totrans-2340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  id: totrans-2341
  prefs: []
  type: TYPE_NORMAL
- en: '`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary
    mode. In most cases, it is not necessary to specify `mode` as Pandas will auto-detect
    whether the file object is opened in text or binary mode.'
  id: totrans-2342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE442]'
  id: totrans-2343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE442]'
- en: Specifying date columns
  id: totrans-2344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To better facilitate working with datetime data, [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") uses the keyword arguments `parse_dates` and `date_format`
    to allow users to specify a variety of columns and date/time formats to turn the
    input text data into `datetime` objects.
  id: totrans-2345
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest case is to just pass in `parse_dates=True`:'
  id: totrans-2346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE443]'
  id: totrans-2347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE443]'
- en: It is often the case that we may want to store date and time data separately,
    or store various date fields separately. the `parse_dates` keyword can be used
    to specify a combination of columns to parse the dates and/or times from.
  id: totrans-2348
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a list of column lists to `parse_dates`, the resulting date
    columns will be prepended to the output (so as to not affect the existing column
    order) and the new column names will be the concatenation of the component column
    names:'
  id: totrans-2349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE444]'
  id: totrans-2350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE444]'
- en: 'By default the parser removes the component date columns, but you can choose
    to retain them via the `keep_date_col` keyword:'
  id: totrans-2351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE445]'
  id: totrans-2352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE445]'
- en: Note that if you wish to combine multiple columns into a single date column,
    a nested list must be used. In other words, `parse_dates=[1, 2]` indicates that
    the second and third columns should each be parsed as separate date columns while
    `parse_dates=[[1, 2]]` means the two columns should be parsed into a single column.
  id: totrans-2353
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use a dict to specify custom name columns:'
  id: totrans-2354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE446]'
  id: totrans-2355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE446]'
- en: 'It is important to remember that if multiple text columns are to be parsed
    into a single date column, then a new column is prepended to the data. The `index_col`
    specification is based off of this new set of columns rather than the original
    data columns:'
  id: totrans-2356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE447]'
  id: totrans-2357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE447]'
- en: Note
  id: totrans-2358
  prefs: []
  type: TYPE_NORMAL
- en: If a column or index contains an unparsable date, the entire column or index
    will be returned unaltered as an object data type. For non-standard datetime parsing,
    use [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") after `pd.read_csv`.
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2360
  prefs: []
  type: TYPE_NORMAL
- en: read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g
    “2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your
    data to store datetimes in this format, load times will be significantly faster,
    ~20x has been observed.
  id: totrans-2361
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.2.0: Combining date columns inside read_csv is deprecated.
    Use `pd.to_datetime` on the relevant result columns instead.'
  id: totrans-2362
  prefs: []
  type: TYPE_NORMAL
- en: Date parsing functions
  id: totrans-2363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, the parser allows you to specify a custom `date_format`. Performance-wise,
    you should try these methods of parsing dates in order:'
  id: totrans-2364
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or
    `date_format={column_name: "%d/%m/%Y"}`.'
  id: totrans-2365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you different formats for different columns, or want to pass any extra options
    (such as `utc`) to `to_datetime`, then you should read in your data as `object`
    dtype, and then use `to_datetime`.
  id: totrans-2366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### Parsing a CSV with mixed timezones'
  id: totrans-2367
  prefs: []
  type: TYPE_NORMAL
- en: pandas cannot natively represent a column or index with mixed timezones. If
    your CSV file contains columns with a mixture of timezones, the default result
    will be an object-dtype column with strings, even with `parse_dates`. To parse
    the mixed-timezone values as a datetime column, read in as `object` dtype and
    then call [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") with `utc=True`.
  id: totrans-2368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE448]'
  id: totrans-2369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE448]'
- en: '#### Inferring datetime format'
  id: totrans-2370
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of datetime strings that can be guessed (all representing
    December 30th, 2011 at 00:00:00):'
  id: totrans-2371
  prefs: []
  type: TYPE_NORMAL
- en: “20111230”
  id: totrans-2372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “2011/12/30”
  id: totrans-2373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “20111230 00:00:00”
  id: totrans-2374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “12/30/2011 00:00:00”
  id: totrans-2375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/Dec/2011 00:00:00”
  id: totrans-2376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/December/2011 00:00:00”
  id: totrans-2377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`,
    it will guess “01/12/2011” to be December 1st. With `dayfirst=False` (default)
    it will guess “01/12/2011” to be January 12th.
  id: totrans-2378
  prefs: []
  type: TYPE_NORMAL
- en: If you try to parse a column of date strings, pandas will attempt to guess the
    format from the first non-NaN element, and will then parse the rest of the column
    with that format. If pandas fails to guess the format (for example if your first
    string is `'01 December US/Pacific 2000'`), then a warning will be raised and
    each row will be parsed individually by `dateutil.parser.parse`. The safest way
    to parse dates is to explicitly set `format=`.
  id: totrans-2379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE449]'
  id: totrans-2380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE449]'
- en: In the case that you have mixed datetime formats within the same column, you
    can pass `format='mixed'`
  id: totrans-2381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE450]'
  id: totrans-2382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE450]'
- en: 'or, if your datetime formats are all ISO8601 (possibly not identically-formatted):'
  id: totrans-2383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE451]'
  id: totrans-2384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE451]'
- en: International date formats
  id: totrans-2385
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While US date formats tend to be MM/DD/YYYY, many international formats use
    DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:'
  id: totrans-2386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE452]'
  id: totrans-2387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE452]'
- en: Writing CSVs to binary file objects
  id: totrans-2388
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  id: totrans-2389
  prefs: []
  type: TYPE_NORMAL
- en: '`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary
    mode. In most cases, it is not necessary to specify `mode` as Pandas will auto-detect
    whether the file object is opened in text or binary mode.'
  id: totrans-2390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE453]'
  id: totrans-2391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE453]'
- en: '### Specifying method for floating-point conversion'
  id: totrans-2392
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `float_precision` can be specified in order to use a specific
    floating-point converter during parsing with the C engine. The options are the
    ordinary converter, the high-precision converter, and the round-trip converter
    (which is guaranteed to round-trip values after writing to a file). For example:'
  id: totrans-2393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE454]'
  id: totrans-2394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE454]'
- en: '### Thousand separators'
  id: totrans-2395
  prefs: []
  type: TYPE_NORMAL
- en: 'For large numbers that have been written with a thousands separator, you can
    set the `thousands` keyword to a string of length 1 so that integers will be parsed
    correctly:'
  id: totrans-2396
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, numbers with a thousands separator will be parsed as strings:'
  id: totrans-2397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE455]'
  id: totrans-2398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE455]'
- en: 'The `thousands` keyword allows integers to be parsed correctly:'
  id: totrans-2399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE456]'
  id: totrans-2400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE456]'
- en: '### NA values'
  id: totrans-2401
  prefs: []
  type: TYPE_NORMAL
- en: To control which values are parsed as missing values (which are signified by
    `NaN`), specify a string in `na_values`. If you specify a list of strings, then
    all values in it are considered to be missing values. If you specify a number
    (a `float`, like `5.0` or an `integer` like `5`), the corresponding equivalent
    values will also imply a missing value (in this case effectively `[5.0, 5]` are
    recognized as `NaN`).
  id: totrans-2402
  prefs: []
  type: TYPE_NORMAL
- en: To completely override the default values that are recognized as missing, specify
    `keep_default_na=False`.
  id: totrans-2403
  prefs: []
  type: TYPE_NORMAL
- en: The default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',
    '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN',
    '-NaN', 'nan', '-nan', 'None', '']`.
  id: totrans-2404
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider some examples:'
  id: totrans-2405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE457]'
  id: totrans-2406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE457]'
- en: In the example above `5` and `5.0` will be recognized as `NaN`, in addition
    to the defaults. A string will first be interpreted as a numerical `5`, then as
    a `NaN`.
  id: totrans-2407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE458]'
  id: totrans-2408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE458]'
- en: Above, only an empty field will be recognized as `NaN`.
  id: totrans-2409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE459]'
  id: totrans-2410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE459]'
- en: Above, both `NA` and `0` as strings are `NaN`.
  id: totrans-2411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE460]'
  id: totrans-2412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE460]'
- en: The default values, in addition to the string `"Nope"` are recognized as `NaN`.
  id: totrans-2413
  prefs: []
  type: TYPE_NORMAL
- en: '### Infinity'
  id: totrans-2414
  prefs: []
  type: TYPE_NORMAL
- en: '`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf`
    as `-np.inf` (negative infinity). These will ignore the case of the value, meaning
    `Inf`, will also be parsed as `np.inf`.'
  id: totrans-2415
  prefs: []
  type: TYPE_NORMAL
- en: '### Boolean values'
  id: totrans-2416
  prefs: []
  type: TYPE_NORMAL
- en: 'The common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as
    boolean. Occasionally you might want to recognize other values as being boolean.
    To do this, use the `true_values` and `false_values` options as follows:'
  id: totrans-2417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE461]'
  id: totrans-2418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE461]'
- en: '### Handling “bad” lines'
  id: totrans-2419
  prefs: []
  type: TYPE_NORMAL
- en: 'Some files may have malformed lines with too few fields or too many. Lines
    with too few fields will have NA values filled in the trailing fields. Lines with
    too many fields will raise an error by default:'
  id: totrans-2420
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE462]'
  id: totrans-2421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE462]'
- en: 'You can elect to skip bad lines:'
  id: totrans-2422
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE463]'
  id: totrans-2423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE463]'
- en: New in version 1.4.0.
  id: totrans-2424
  prefs: []
  type: TYPE_NORMAL
- en: 'Or pass a callable function to handle the bad line if `engine="python"`. The
    bad line will be a list of strings that was split by the `sep`:'
  id: totrans-2425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE464]'
  id: totrans-2426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE464]'
- en: Note
  id: totrans-2427
  prefs: []
  type: TYPE_NORMAL
- en: The callable function will handle only a line with too many fields. Bad lines
    caused by other errors will be silently skipped.
  id: totrans-2428
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE465]'
  id: totrans-2429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE465]'
- en: The line was not processed in this case, as a “bad line” here is caused by an
    escape character.
  id: totrans-2430
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `usecols` parameter to eliminate extraneous column data
    that appear in some lines but not others:'
  id: totrans-2431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE466]'
  id: totrans-2432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE466]'
- en: In case you want to keep all data including the lines with too many fields,
    you can specify a sufficient number of `names`. This ensures that lines with not
    enough fields are filled with `NaN`.
  id: totrans-2433
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE467]'
  id: totrans-2434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE467]'
- en: '### Dialect'
  id: totrans-2435
  prefs: []
  type: TYPE_NORMAL
- en: The `dialect` keyword gives greater flexibility in specifying the file format.
    By default it uses the Excel dialect but you can specify either the dialect name
    or a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect "(in
    Python v3.12)") instance.
  id: totrans-2436
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you had data with unenclosed quotes:'
  id: totrans-2437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE468]'
  id: totrans-2438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE468]'
- en: By default, `read_csv` uses the Excel dialect and treats the double quote as
    the quote character, which causes it to fail when it finds a newline before it
    finds the closing double quote.
  id: totrans-2439
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get around this using `dialect`:'
  id: totrans-2440
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE469]'
  id: totrans-2441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE469]'
- en: 'All of the dialect options can be specified separately by keyword arguments:'
  id: totrans-2442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE470]'
  id: totrans-2443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE470]'
- en: 'Another common dialect option is `skipinitialspace`, to skip any whitespace
    after a delimiter:'
  id: totrans-2444
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE471]'
  id: totrans-2445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE471]'
- en: The parsers make every attempt to “do the right thing” and not be fragile. Type
    inference is a pretty big deal. If a column can be coerced to integer dtype without
    altering the contents, the parser will do so. Any non-numeric columns will come
    through as object dtype as with the rest of pandas objects.
  id: totrans-2446
  prefs: []
  type: TYPE_NORMAL
- en: '### Quoting and Escape Characters'
  id: totrans-2447
  prefs: []
  type: TYPE_NORMAL
- en: 'Quotes (and other escape characters) in embedded fields can be handled in any
    number of ways. One way is to use backslashes; to properly parse this data, you
    should pass the `escapechar` option:'
  id: totrans-2448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE472]'
  id: totrans-2449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE472]'
- en: '### Files with fixed width columns'
  id: totrans-2450
  prefs: []
  type: TYPE_NORMAL
- en: 'While [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") reads delimited data, the [`read_fwf()`](../reference/api/pandas.read_fwf.html#pandas.read_fwf
    "pandas.read_fwf") function works with data files that have known and fixed column
    widths. The function parameters to `read_fwf` are largely the same as `read_csv`
    with two extra parameters, and a different usage of the `delimiter` parameter:'
  id: totrans-2451
  prefs: []
  type: TYPE_NORMAL
- en: '`colspecs`: A list of pairs (tuples) giving the extents of the fixed-width
    fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’
    can be used to instruct the parser to try detecting the column specifications
    from the first 100 rows of the data. Default behavior, if not specified, is to
    infer.'
  id: totrans-2452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`widths`: A list of field widths which can be used instead of ‘colspecs’ if
    the intervals are contiguous.'
  id: totrans-2453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delimiter`: Characters to consider as filler characters in the fixed-width
    file. Can be used to specify the filler character of the fields if it is not spaces
    (e.g., ‘~’).'
  id: totrans-2454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider a typical fixed-width data file:'
  id: totrans-2455
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE473]'
  id: totrans-2456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE473]'
- en: 'In order to parse this file into a `DataFrame`, we simply need to supply the
    column specifications to the `read_fwf` function along with the file name:'
  id: totrans-2457
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE474]'
  id: totrans-2458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE474]'
- en: 'Note how the parser automatically picks column names X.<column number> when
    `header=None` argument is specified. Alternatively, you can supply just the column
    widths for contiguous columns:'
  id: totrans-2459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE475]'
  id: totrans-2460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE475]'
- en: The parser will take care of extra white spaces around the columns so it’s ok
    to have extra separation between the columns in the file.
  id: totrans-2461
  prefs: []
  type: TYPE_NORMAL
- en: By default, `read_fwf` will try to infer the file’s `colspecs` by using the
    first 100 rows of the file. It can do it only in cases when the columns are aligned
    and correctly separated by the provided `delimiter` (default delimiter is whitespace).
  id: totrans-2462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE476]'
  id: totrans-2463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE476]'
- en: '`read_fwf` supports the `dtype` parameter for specifying the types of parsed
    columns to be different from the inferred type.'
  id: totrans-2464
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE477]'
  id: totrans-2465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE477]'
- en: Indexes
  id: totrans-2466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Files with an “implicit” index column
  id: totrans-2467
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a file with one less entry in the header than the number of data column:'
  id: totrans-2468
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE478]'
  id: totrans-2469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE478]'
- en: 'In this special case, `read_csv` assumes that the first column is to be used
    as the index of the `DataFrame`:'
  id: totrans-2470
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE479]'
  id: totrans-2471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE479]'
- en: 'Note that the dates weren’t automatically parsed. In that case you would need
    to do as before:'
  id: totrans-2472
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE480]'
  id: totrans-2473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE480]'
- en: Reading an index with a `MultiIndex`
  id: totrans-2474
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you have data indexed by two columns:'
  id: totrans-2475
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE481]'
  id: totrans-2476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE481]'
- en: 'The `index_col` argument to `read_csv` can take a list of column numbers to
    turn multiple columns into a `MultiIndex` for the index of the returned object:'
  id: totrans-2477
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE482]'
  id: totrans-2478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE482]'
- en: '#### Reading columns with a `MultiIndex`'
  id: totrans-2479
  prefs: []
  type: TYPE_NORMAL
- en: By specifying list of row locations for the `header` argument, you can read
    in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the
    intervening rows.
  id: totrans-2480
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE483]'
  id: totrans-2481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE483]'
- en: '`read_csv` is also able to interpret a more common format of multi-columns
    indices.'
  id: totrans-2482
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE484]'
  id: totrans-2483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE484]'
- en: Note
  id: totrans-2484
  prefs: []
  type: TYPE_NORMAL
- en: If an `index_col` is not specified (e.g. you don’t have an index, or wrote it
    with `df.to_csv(..., index=False)`, then any `names` on the columns index will
    be *lost*.
  id: totrans-2485
  prefs: []
  type: TYPE_NORMAL
- en: Files with an “implicit” index column
  id: totrans-2486
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a file with one less entry in the header than the number of data column:'
  id: totrans-2487
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE485]'
  id: totrans-2488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE485]'
- en: 'In this special case, `read_csv` assumes that the first column is to be used
    as the index of the `DataFrame`:'
  id: totrans-2489
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE486]'
  id: totrans-2490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE486]'
- en: 'Note that the dates weren’t automatically parsed. In that case you would need
    to do as before:'
  id: totrans-2491
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE487]'
  id: totrans-2492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE487]'
- en: Reading an index with a `MultiIndex`
  id: totrans-2493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you have data indexed by two columns:'
  id: totrans-2494
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE488]'
  id: totrans-2495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE488]'
- en: 'The `index_col` argument to `read_csv` can take a list of column numbers to
    turn multiple columns into a `MultiIndex` for the index of the returned object:'
  id: totrans-2496
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE489]'
  id: totrans-2497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE489]'
- en: '#### Reading columns with a `MultiIndex`'
  id: totrans-2498
  prefs: []
  type: TYPE_NORMAL
- en: By specifying list of row locations for the `header` argument, you can read
    in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the
    intervening rows.
  id: totrans-2499
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE490]'
  id: totrans-2500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE490]'
- en: '`read_csv` is also able to interpret a more common format of multi-columns
    indices.'
  id: totrans-2501
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE491]'
  id: totrans-2502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE491]'
- en: Note
  id: totrans-2503
  prefs: []
  type: TYPE_NORMAL
- en: If an `index_col` is not specified (e.g. you don’t have an index, or wrote it
    with `df.to_csv(..., index=False)`, then any `names` on the columns index will
    be *lost*.
  id: totrans-2504
  prefs: []
  type: TYPE_NORMAL
- en: '### Automatically “sniffing” the delimiter'
  id: totrans-2505
  prefs: []
  type: TYPE_NORMAL
- en: '`read_csv` is capable of inferring delimited (not necessarily comma-separated)
    files, as pandas uses the [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)") class of the csv module. For this, you have to specify `sep=None`.'
  id: totrans-2506
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE492]'
  id: totrans-2507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE492]'
- en: '### Reading multiple files to create a single DataFrame'
  id: totrans-2508
  prefs: []
  type: TYPE_NORMAL
- en: It’s best to use [`concat()`](../reference/api/pandas.concat.html#pandas.concat
    "pandas.concat") to combine multiple files. See the [cookbook](cookbook.html#cookbook-csv-multiple-files)
    for an example.
  id: totrans-2509
  prefs: []
  type: TYPE_NORMAL
- en: '### Iterating through files chunk by chunk'
  id: totrans-2510
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you wish to iterate through a (potentially very large) file lazily
    rather than reading the entire file into memory, such as the following:'
  id: totrans-2511
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE493]'
  id: totrans-2512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE493]'
- en: 'By specifying a `chunksize` to `read_csv`, the return value will be an iterable
    object of type `TextFileReader`:'
  id: totrans-2513
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE494]'
  id: totrans-2514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE494]'
- en: 'Changed in version 1.2: `read_csv/json/sas` return a context-manager when iterating
    through a file.'
  id: totrans-2515
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifying `iterator=True` will also return the `TextFileReader` object:'
  id: totrans-2516
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE495]'
  id: totrans-2517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE495]'
- en: Specifying the parser engine
  id: totrans-2518
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pandas currently supports three engines, the C engine, the python engine, and
    an experimental pyarrow engine (requires the `pyarrow` package). In general, the
    pyarrow engine is fastest on larger workloads and is equivalent in speed to the
    C engine on most other workloads. The python engine tends to be slower than the
    pyarrow and C engines on most workloads. However, the pyarrow engine is much less
    robust than the C engine, which lacks a few features compared to the Python engine.
  id: totrans-2519
  prefs: []
  type: TYPE_NORMAL
- en: Where possible, pandas uses the C parser (specified as `engine='c'`), but it
    may fall back to Python if C-unsupported options are specified.
  id: totrans-2520
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, options unsupported by the C and pyarrow engines include:'
  id: totrans-2521
  prefs: []
  type: TYPE_NORMAL
- en: '`sep` other than a single character (e.g. regex separators)'
  id: totrans-2522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipfooter`'
  id: totrans-2523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep=None` with `delim_whitespace=False`'
  id: totrans-2524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying any of the above options will produce a `ParserWarning` unless the
    python engine is selected explicitly using `engine='python'`.
  id: totrans-2525
  prefs: []
  type: TYPE_NORMAL
- en: 'Options that are unsupported by the pyarrow engine which are not covered by
    the list above include:'
  id: totrans-2526
  prefs: []
  type: TYPE_NORMAL
- en: '`float_precision`'
  id: totrans-2527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`'
  id: totrans-2528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comment`'
  id: totrans-2529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nrows`'
  id: totrans-2530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thousands`'
  id: totrans-2531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_map`'
  id: totrans-2532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dialect`'
  id: totrans-2533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`on_bad_lines`'
  id: totrans-2534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delim_whitespace`'
  id: totrans-2535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`'
  id: totrans-2536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`'
  id: totrans-2537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`converters`'
  id: totrans-2538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decimal`'
  id: totrans-2539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iterator`'
  id: totrans-2540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dayfirst`'
  id: totrans-2541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`infer_datetime_format`'
  id: totrans-2542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose`'
  id: totrans-2543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipinitialspace`'
  id: totrans-2544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_memory`'
  id: totrans-2545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying these options with `engine='pyarrow'` will raise a `ValueError`.
  id: totrans-2546
  prefs: []
  type: TYPE_NORMAL
- en: '### Reading/writing remote files'
  id: totrans-2547
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pass in a URL to read or write remote files to many of pandas’ IO functions
    - the following example shows reading a CSV file:'
  id: totrans-2548
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE496]'
  id: totrans-2549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE496]'
- en: New in version 1.3.0.
  id: totrans-2550
  prefs: []
  type: TYPE_NORMAL
- en: 'A custom header can be sent alongside HTTP(s) requests by passing a dictionary
    of header key value mappings to the `storage_options` keyword argument as shown
    below:'
  id: totrans-2551
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE497]'
  id: totrans-2552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE497]'
- en: 'All URLs which are not local files or HTTP(s) are handled by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/),
    if installed, and its various filesystem implementations (including Amazon S3,
    Google Cloud, SSH, FTP, webHDFS…). Some of these implementations will require
    additional packages to be installed, for example S3 URLs require the [s3fs](https://pypi.org/project/s3fs/)
    library:'
  id: totrans-2553
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE498]'
  id: totrans-2554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE498]'
- en: When dealing with remote storage systems, you might need extra configuration
    with environment variables or config files in special locations. For example,
    to access data in your S3 bucket, you will need to define credentials in one of
    the several ways listed in the [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/#credentials).
    The same is true for several of the storage backends, and you should follow the
    links at [fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations)
    for implementations built into `fsspec` and [fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)
    for those not included in the main `fsspec` distribution.
  id: totrans-2555
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also pass parameters directly to the backend driver. Since `fsspec`
    does not utilize the `AWS_S3_HOST` environment variable, we can directly define
    a dictionary containing the endpoint_url and pass the object into the storage
    option parameter:'
  id: totrans-2556
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE499]'
  id: totrans-2557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE499]'
- en: More sample configurations and documentation can be found at [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage).
  id: totrans-2558
  prefs: []
  type: TYPE_NORMAL
- en: If you do *not* have S3 credentials, you can still access public data by specifying
    an anonymous connection, such as
  id: totrans-2559
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  id: totrans-2560
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE500]'
  id: totrans-2561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE500]'
- en: '`fsspec` also allows complex URLs, for accessing data in compressed archives,
    local caching of files, and more. To locally cache the above example, you would
    modify the call to'
  id: totrans-2562
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE501]'
  id: totrans-2563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE501]'
- en: where we specify that the “anon” parameter is meant for the “s3” part of the
    implementation, not to the caching implementation. Note that this caches to a
    temporary directory for the duration of the session only, but you can also specify
    a permanent store.
  id: totrans-2564
  prefs: []
  type: TYPE_NORMAL
- en: Writing out data
  id: totrans-2565
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Writing to CSV format'
  id: totrans-2566
  prefs: []
  type: TYPE_NORMAL
- en: The `Series` and `DataFrame` objects have an instance method `to_csv` which
    allows storing the contents of the object as a comma-separated-values file. The
    function takes a number of arguments. Only the first is required.
  id: totrans-2567
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf`: A string path to the file to write or a file object. If a file
    object it must be opened with `newline=''''`'
  id: totrans-2568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep` : Field delimiter for the output file (default “,”)'
  id: totrans-2569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep`: A string representation of a missing value (default ‘’)'
  id: totrans-2570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format`: Format string for floating point numbers'
  id: totrans-2571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: Columns to write (default None)'
  id: totrans-2572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header`: Whether to write out the column names (default True)'
  id: totrans-2573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index`: whether to write row (index) names (default True)'
  id: totrans-2574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_label`: Column label(s) for index column(s) if desired. If None (default),
    and `header` and `index` are True, then the index names are used. (A sequence
    should be given if the `DataFrame` uses MultiIndex).'
  id: totrans-2575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : Python write mode, default ‘w’'
  id: totrans-2576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding`: a string representing the encoding to use if the contents are non-ASCII,
    for Python versions prior to 3'
  id: totrans-2577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`: Character sequence denoting line end (default `os.linesep`)'
  id: totrans-2578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).
    Note that if you have set a `float_format` then floats are converted to strings
    and csv.QUOTE_NONNUMERIC will treat them as non-numeric'
  id: totrans-2579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quotechar`: Character used to quote fields (default ‘”’)'
  id: totrans-2580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doublequote`: Control quoting of `quotechar` in fields (default True)'
  id: totrans-2581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`escapechar`: Character used to escape `sep` and `quotechar` when appropriate
    (default None)'
  id: totrans-2582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`: Number of rows to write at a time'
  id: totrans-2583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_format`: Format string for datetime objects'
  id: totrans-2584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a formatted string
  id: totrans-2585
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `DataFrame` object has an instance method `to_string` which allows control
    over the string representation of the object. All arguments are optional:'
  id: totrans-2586
  prefs: []
  type: TYPE_NORMAL
- en: '`buf` default None, for example a StringIO object'
  id: totrans-2587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` default None, which columns to write'
  id: totrans-2588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col_space` default None, minimum width of each column.'
  id: totrans-2589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep` default `NaN`, representation of NA value'
  id: totrans-2590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formatters` default None, a dictionary (by column) of functions each of which
    takes a single argument and returns a formatted string'
  id: totrans-2591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format` default None, a function which takes a single (float) argument
    and returns a formatted string; to be applied to floats in the `DataFrame`.'
  id: totrans-2592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparsify` default True, set to False for a `DataFrame` with a hierarchical
    index to print every MultiIndex key at each row.'
  id: totrans-2593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_names` default True, will print the names of the indices'
  id: totrans-2594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` default True, will print the index (ie, row labels)'
  id: totrans-2595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header` default True, will print the column labels'
  id: totrans-2596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`justify` default `left`, will print column headers left- or right-justified'
  id: totrans-2597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Series` object also has a `to_string` method, but with only the `buf`,
    `na_rep`, `float_format` arguments. There is also a `length` argument which, if
    set to `True`, will additionally output the length of the Series.
  id: totrans-2598
  prefs: []
  type: TYPE_NORMAL
- en: '#### Writing to CSV format'
  id: totrans-2599
  prefs: []
  type: TYPE_NORMAL
- en: The `Series` and `DataFrame` objects have an instance method `to_csv` which
    allows storing the contents of the object as a comma-separated-values file. The
    function takes a number of arguments. Only the first is required.
  id: totrans-2600
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf`: A string path to the file to write or a file object. If a file
    object it must be opened with `newline=''''`'
  id: totrans-2601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep` : Field delimiter for the output file (default “,”)'
  id: totrans-2602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep`: A string representation of a missing value (default ‘’)'
  id: totrans-2603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format`: Format string for floating point numbers'
  id: totrans-2604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: Columns to write (default None)'
  id: totrans-2605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header`: Whether to write out the column names (default True)'
  id: totrans-2606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index`: whether to write row (index) names (default True)'
  id: totrans-2607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_label`: Column label(s) for index column(s) if desired. If None (default),
    and `header` and `index` are True, then the index names are used. (A sequence
    should be given if the `DataFrame` uses MultiIndex).'
  id: totrans-2608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : Python write mode, default ‘w’'
  id: totrans-2609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding`: a string representing the encoding to use if the contents are non-ASCII,
    for Python versions prior to 3'
  id: totrans-2610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`: Character sequence denoting line end (default `os.linesep`)'
  id: totrans-2611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).
    Note that if you have set a `float_format` then floats are converted to strings
    and csv.QUOTE_NONNUMERIC will treat them as non-numeric'
  id: totrans-2612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quotechar`: Character used to quote fields (default ‘”’)'
  id: totrans-2613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doublequote`: Control quoting of `quotechar` in fields (default True)'
  id: totrans-2614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`escapechar`: Character used to escape `sep` and `quotechar` when appropriate
    (default None)'
  id: totrans-2615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`: Number of rows to write at a time'
  id: totrans-2616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_format`: Format string for datetime objects'
  id: totrans-2617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a formatted string
  id: totrans-2618
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `DataFrame` object has an instance method `to_string` which allows control
    over the string representation of the object. All arguments are optional:'
  id: totrans-2619
  prefs: []
  type: TYPE_NORMAL
- en: '`buf` default None, for example a StringIO object'
  id: totrans-2620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` default None, which columns to write'
  id: totrans-2621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col_space` default None, minimum width of each column.'
  id: totrans-2622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep` default `NaN`, representation of NA value'
  id: totrans-2623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formatters` default None, a dictionary (by column) of functions each of which
    takes a single argument and returns a formatted string'
  id: totrans-2624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format` default None, a function which takes a single (float) argument
    and returns a formatted string; to be applied to floats in the `DataFrame`.'
  id: totrans-2625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparsify` default True, set to False for a `DataFrame` with a hierarchical
    index to print every MultiIndex key at each row.'
  id: totrans-2626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_names` default True, will print the names of the indices'
  id: totrans-2627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` default True, will print the index (ie, row labels)'
  id: totrans-2628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header` default True, will print the column labels'
  id: totrans-2629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`justify` default `left`, will print column headers left- or right-justified'
  id: totrans-2630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Series` object also has a `to_string` method, but with only the `buf`,
    `na_rep`, `float_format` arguments. There is also a `length` argument which, if
    set to `True`, will additionally output the length of the Series.
  id: totrans-2631
  prefs: []
  type: TYPE_NORMAL
- en: '## JSON'
  id: totrans-2632
  prefs: []
  type: TYPE_NORMAL
- en: Read and write `JSON` format files and strings.
  id: totrans-2633
  prefs: []
  type: TYPE_NORMAL
- en: '### Writing JSON'
  id: totrans-2634
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Series` or `DataFrame` can be converted to a valid JSON string. Use `to_json`
    with optional parameters:'
  id: totrans-2635
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` : the pathname or buffer to write the output. This can be `None`
    in which case a JSON string is returned.'
  id: totrans-2636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  id: totrans-2637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Series`:'
  id: totrans-2638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  id: totrans-2639
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  id: totrans-2640
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`:'
  id: totrans-2641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  id: totrans-2642
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  id: totrans-2643
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  id: totrans-2644
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  id: totrans-2645
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  id: totrans-2646
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  id: totrans-2647
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  id: totrans-2648
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  id: totrans-2649
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  id: totrans-2650
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`date_format` : string, type of date conversion, ‘epoch’ for timestamp, ‘iso’
    for ISO8601.'
  id: totrans-2651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`double_precision` : The number of decimal places to use when encoding floating
    point values, default 10.'
  id: totrans-2652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_ascii` : force encoded string to be ASCII, default True.'
  id: totrans-2653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : The time unit to encode to, governs timestamp and ISO8601 precision.
    One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds
    respectively. Default ‘ms’.'
  id: totrans-2654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_handler` : The handler to call if an object cannot otherwise be converted
    to a suitable format for JSON. Takes a single argument, which is the object to
    convert, and returns a serializable object.'
  id: totrans-2655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : If `records` orient, then will write each record per line as json.'
  id: totrans-2656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : string, writer mode when writing to path. ‘w’ for write, ‘a’ for append.
    Default ‘w’'
  id: totrans-2657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note `NaN`’s, `NaT`’s and `None` will be converted to `null` and `datetime`
    objects will be converted based on the `date_format` and `date_unit` parameters.
  id: totrans-2658
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE502]'
  id: totrans-2659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE502]'
- en: Orient options
  id: totrans-2660
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a number of different options for the format of the resulting JSON
    file / string. Consider the following `DataFrame` and `Series`:'
  id: totrans-2661
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE503]'
  id: totrans-2662
  prefs: []
  type: TYPE_PRE
  zh: '[PRE503]'
- en: '**Column oriented** (the default for `DataFrame`) serializes the data as nested
    JSON objects with column labels acting as the primary index:'
  id: totrans-2663
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE504]'
  id: totrans-2664
  prefs: []
  type: TYPE_PRE
  zh: '[PRE504]'
- en: '**Index oriented** (the default for `Series`) similar to column oriented but
    the index labels are now primary:'
  id: totrans-2665
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE505]'
  id: totrans-2666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE505]'
- en: '**Record oriented** serializes the data to a JSON array of column -> value
    records, index labels are not included. This is useful for passing `DataFrame`
    data to plotting libraries, for example the JavaScript library `d3.js`:'
  id: totrans-2667
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE506]'
  id: totrans-2668
  prefs: []
  type: TYPE_PRE
  zh: '[PRE506]'
- en: '**Value oriented** is a bare-bones option which serializes to nested JSON arrays
    of values only, column and index labels are not included:'
  id: totrans-2669
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE507]'
  id: totrans-2670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE507]'
- en: '**Split oriented** serializes to a JSON object containing separate entries
    for values, index and columns. Name is also included for `Series`:'
  id: totrans-2671
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE508]'
  id: totrans-2672
  prefs: []
  type: TYPE_PRE
  zh: '[PRE508]'
- en: '**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/),
    allowing for the preservation of metadata including but not limited to dtypes
    and index names.'
  id: totrans-2673
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2674
  prefs: []
  type: TYPE_NORMAL
- en: Any orient option that encodes to a JSON object will not preserve the ordering
    of index and column labels during round-trip serialization. If you wish to preserve
    label ordering use the `split` option as it uses ordered containers.
  id: totrans-2675
  prefs: []
  type: TYPE_NORMAL
- en: Date handling
  id: totrans-2676
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Writing in ISO date format:'
  id: totrans-2677
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE509]'
  id: totrans-2678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE509]'
- en: 'Writing in ISO date format, with microseconds:'
  id: totrans-2679
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE510]'
  id: totrans-2680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE510]'
- en: 'Epoch timestamps, in seconds:'
  id: totrans-2681
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE511]'
  id: totrans-2682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE511]'
- en: 'Writing to a file, with a date index and a date column:'
  id: totrans-2683
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE512]'
  id: totrans-2684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE512]'
- en: Fallback behavior
  id: totrans-2685
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the JSON serializer cannot handle the container contents directly it will
    fall back in the following manner:'
  id: totrans-2686
  prefs: []
  type: TYPE_NORMAL
- en: if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,
    if provided, will be called for each value, otherwise an exception is raised.
  id: totrans-2687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'if an object is unsupported it will attempt the following:'
  id: totrans-2688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check if the object has defined a `toDict` method and call it. A `toDict` method
    should return a `dict` which will then be JSON serialized.
  id: totrans-2689
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2690
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2691
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: invoke the `default_handler` if one was provided.
  id: totrans-2692
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2693
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2694
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: convert the object to a `dict` by traversing its contents. However this will
    often fail with an `OverflowError` or give unexpected results.
  id: totrans-2695
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general the best approach for unsupported objects or dtypes is to provide
    a `default_handler`. For example:'
  id: totrans-2696
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE513]'
  id: totrans-2697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE513]'
- en: 'can be dealt with by specifying a simple `default_handler`:'
  id: totrans-2698
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE514]  ### Reading JSON'
  id: totrans-2699
  prefs: []
  type: TYPE_NORMAL
- en: Reading a JSON string to pandas object can take a number of parameters. The
    parser will try to parse a `DataFrame` if `typ` is not supplied or is `None`.
    To explicitly force `Series` parsing, pass `typ=series`
  id: totrans-2700
  prefs: []
  type: TYPE_NORMAL
- en: '`filepath_or_buffer` : a **VALID** JSON string or file handle / StringIO. The
    string could be a URL. Valid URL schemes include http, ftp, S3, and file. For
    file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json'
  id: totrans-2701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typ` : type of object to recover (series or frame), default ‘frame’'
  id: totrans-2702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  id: totrans-2703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Series :'
  id: totrans-2704
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  id: totrans-2705
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  id: totrans-2706
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame
  id: totrans-2707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  id: totrans-2708
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  id: totrans-2709
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  id: totrans-2710
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  id: totrans-2711
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  id: totrans-2712
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  id: totrans-2713
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  id: totrans-2714
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  id: totrans-2715
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  id: totrans-2716
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`dtype` : if True, infer dtypes, if a dict of column to dtype, then use those,
    if `False`, then don’t infer dtypes at all, default is True, apply only to the
    data.'
  id: totrans-2717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_axes` : boolean, try to convert the axes to the proper dtypes, default
    is `True`'
  id: totrans-2718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_dates` : a list of columns to parse for dates; If `True`, then try
    to parse date-like columns, default is `True`.'
  id: totrans-2719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_default_dates` : boolean, default `True`. If parsing dates, then parse
    the default date-like columns.'
  id: totrans-2720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`precise_float` : boolean, default `False`. Set to enable usage of higher precision
    (strtod) function when decoding string to double values. Default (`False`) is
    to use fast but less precise builtin functionality.'
  id: totrans-2721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : string, the timestamp unit to detect if converting dates. Default
    None. By default the timestamp precision will be detected, if this is not desired
    then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force timestamp precision to seconds,
    milliseconds, microseconds or nanoseconds respectively.'
  id: totrans-2722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : reads file as one json object per line.'
  id: totrans-2723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding` : The encoding to use to decode py3 bytes.'
  id: totrans-2724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize` : when used in combination with `lines=True`, return a `pandas.api.typing.JsonReader`
    which reads in `chunksize` lines per iteration.'
  id: totrans-2725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`engine`: Either `"ujson"`, the built-in JSON parser, or `"pyarrow"` which
    dispatches to pyarrow’s `pyarrow.json.read_json`. The `"pyarrow"` is only available
    when `lines=True`'
  id: totrans-2726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parser will raise one of `ValueError/TypeError/AssertionError` if the JSON
    is not parseable.
  id: totrans-2727
  prefs: []
  type: TYPE_NORMAL
- en: If a non-default `orient` was used when encoding to JSON be sure to pass the
    same option here so that decoding produces sensible results, see [Orient Options](#orient-options)
    for an overview.
  id: totrans-2728
  prefs: []
  type: TYPE_NORMAL
- en: Data conversion
  id: totrans-2729
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will
    try to parse the axes, and all of the data into appropriate types, including dates.
    If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes`
    should only be set to `False` if you need to preserve string-like numbers (e.g.
    ‘1’, ‘2’) in an axes.
  id: totrans-2730
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2731
  prefs: []
  type: TYPE_NORMAL
- en: 'Large integer values may be converted to dates if `convert_dates=True` and
    the data and / or column labels appear ‘date-like’. The exact threshold depends
    on the `date_unit` specified. ‘date-like’ means that the column label meets one
    of the following criteria:'
  id: totrans-2732
  prefs: []
  type: TYPE_NORMAL
- en: it ends with `'_at'`
  id: totrans-2733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it ends with `'_time'`
  id: totrans-2734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it begins with `'timestamp'`
  id: totrans-2735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'modified'`
  id: totrans-2736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'date'`
  id: totrans-2737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  id: totrans-2738
  prefs: []
  type: TYPE_NORMAL
- en: 'When reading JSON data, automatic coercing into dtypes has some quirks:'
  id: totrans-2739
  prefs: []
  type: TYPE_NORMAL
- en: an index can be reconstructed in a different order from serialization, that
    is, the returned order is not guaranteed to be the same as before serialization
  id: totrans-2740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a column that was `float` data will be converted to `integer` if it can be done
    safely, e.g. a column of `1.`
  id: totrans-2741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bool columns will be converted to `integer` on reconstruction
  id: totrans-2742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus there are times where you may want to specify specific dtypes via the `dtype`
    keyword argument.
  id: totrans-2743
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading from a JSON string:'
  id: totrans-2744
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE515]'
  id: totrans-2745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE515]'
- en: 'Reading from a file:'
  id: totrans-2746
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE516]'
  id: totrans-2747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE516]'
- en: 'Don’t convert any data (but still convert axes and dates):'
  id: totrans-2748
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE517]'
  id: totrans-2749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE517]'
- en: 'Specify dtypes for conversion:'
  id: totrans-2750
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE518]'
  id: totrans-2751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE518]'
- en: 'Preserve string indices:'
  id: totrans-2752
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE519]'
  id: totrans-2753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE519]'
- en: 'Dates written in nanoseconds need to be read back in nanoseconds:'
  id: totrans-2754
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE520]'
  id: totrans-2755
  prefs: []
  type: TYPE_PRE
  zh: '[PRE520]'
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  id: totrans-2756
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE521]  ### Normalization'
  id: totrans-2757
  prefs: []
  type: TYPE_NORMAL
- en: pandas provides a utility function to take a dict or list of dicts and *normalize*
    this semi-structured data into a flat table.
  id: totrans-2758
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE522]'
  id: totrans-2759
  prefs: []
  type: TYPE_PRE
  zh: '[PRE522]'
- en: '[PRE523]'
  id: totrans-2760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE523]'
- en: The max_level parameter provides more control over which level to end normalization.
    With max_level=1 the following snippet normalizes until 1st nesting level of the
    provided dict.
  id: totrans-2761
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE524]  ### Line delimited json'
  id: totrans-2762
  prefs: []
  type: TYPE_NORMAL
- en: pandas is able to read and write line-delimited json files that are common in
    data processing pipelines using Hadoop or Spark.
  id: totrans-2763
  prefs: []
  type: TYPE_NORMAL
- en: For line-delimited json files, pandas can also return an iterator which reads
    in `chunksize` lines at a time. This can be useful for large files or to read
    from a stream.
  id: totrans-2764
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE525]'
  id: totrans-2765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE525]'
- en: Line-limited json can also be read using the pyarrow reader by specifying `engine="pyarrow"`.
  id: totrans-2766
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE526]'
  id: totrans-2767
  prefs: []
  type: TYPE_PRE
  zh: '[PRE526]'
- en: 'New in version 2.0.0.  ### Table schema'
  id: totrans-2768
  prefs: []
  type: TYPE_NORMAL
- en: '[Table Schema](https://specs.frictionlessdata.io/table-schema/) is a spec for
    describing tabular datasets as a JSON object. The JSON includes information on
    the field names, types, and other attributes. You can use the orient `table` to
    build a JSON string with two fields, `schema` and `data`.'
  id: totrans-2769
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE527]'
  id: totrans-2770
  prefs: []
  type: TYPE_PRE
  zh: '[PRE527]'
- en: The `schema` field contains the `fields` key, which itself contains a list of
    column name to type pairs, including the `Index` or `MultiIndex` (see below for
    a list of types). The `schema` field also contains a `primaryKey` field if the
    (Multi)index is unique.
  id: totrans-2771
  prefs: []
  type: TYPE_NORMAL
- en: The second field, `data`, contains the serialized data with the `records` orient.
    The index is included, and any datetimes are ISO 8601 formatted, as required by
    the Table Schema spec.
  id: totrans-2772
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of types supported are described in the Table Schema spec. This
    table shows the mapping from pandas types:'
  id: totrans-2773
  prefs: []
  type: TYPE_NORMAL
- en: '| pandas type | Table Schema type |'
  id: totrans-2774
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-2775
  prefs: []
  type: TYPE_TB
- en: '| int64 | integer |'
  id: totrans-2776
  prefs: []
  type: TYPE_TB
- en: '| float64 | number |'
  id: totrans-2777
  prefs: []
  type: TYPE_TB
- en: '| bool | boolean |'
  id: totrans-2778
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | datetime |'
  id: totrans-2779
  prefs: []
  type: TYPE_TB
- en: '| timedelta64[ns] | duration |'
  id: totrans-2780
  prefs: []
  type: TYPE_TB
- en: '| categorical | any |'
  id: totrans-2781
  prefs: []
  type: TYPE_TB
- en: '| object | str |'
  id: totrans-2782
  prefs: []
  type: TYPE_TB
- en: 'A few notes on the generated table schema:'
  id: totrans-2783
  prefs: []
  type: TYPE_NORMAL
- en: The `schema` object contains a `pandas_version` field. This contains the version
    of pandas’ dialect of the schema, and will be incremented with each revision.
  id: totrans-2784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All dates are converted to UTC when serializing. Even timezone naive values,
    which are treated as UTC with an offset of 0.
  id: totrans-2785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE528]'
  id: totrans-2786
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE528]'
- en: datetimes with a timezone (before serializing), include an additional field
    `tz` with the time zone name (e.g. `'US/Central'`).
  id: totrans-2787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE529]'
  id: totrans-2788
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE529]'
- en: Periods are converted to timestamps before serialization, and so have the same
    behavior of being converted to UTC. In addition, periods will contain and additional
    field `freq` with the period’s frequency, e.g. `'A-DEC'`.
  id: totrans-2789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE530]'
  id: totrans-2790
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE530]'
- en: 'Categoricals use the `any` type and an `enum` constraint listing the set of
    possible values. Additionally, an `ordered` field is included:'
  id: totrans-2791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE531]'
  id: totrans-2792
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE531]'
- en: 'A `primaryKey` field, containing an array of labels, is included *if the index
    is unique*:'
  id: totrans-2793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE532]'
  id: totrans-2794
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE532]'
- en: 'The `primaryKey` behavior is the same with MultiIndexes, but in this case the
    `primaryKey` is an array:'
  id: totrans-2795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE533]'
  id: totrans-2796
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE533]'
- en: 'The default naming roughly follows these rules:'
  id: totrans-2797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For series, the `object.name` is used. If that’s none, then the name is `values`
  id: totrans-2798
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2799
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2800
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `DataFrames`, the stringified version of the column name is used
  id: totrans-2801
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2802
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2803
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `Index` (not `MultiIndex`), `index.name` is used, with a fallback to `index`
    if that is None.
  id: totrans-2804
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2805
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2806
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `MultiIndex`, `mi.names` is used. If any level has no name, then `level_<i>`
    is used.
  id: totrans-2807
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_json` also accepts `orient=''table''` as an argument. This allows for
    the preservation of metadata such as dtypes and index names in a round-trippable
    manner.'
  id: totrans-2808
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE534]'
  id: totrans-2809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE534]'
- en: Please note that the literal string ‘index’ as the name of an [`Index`](../reference/api/pandas.Index.html#pandas.Index
    "pandas.Index") is not round-trippable, nor are any names beginning with `'level_'`
    within a [`MultiIndex`](../reference/api/pandas.MultiIndex.html#pandas.MultiIndex
    "pandas.MultiIndex"). These are used by default in [`DataFrame.to_json()`](../reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json
    "pandas.DataFrame.to_json") to indicate missing values and the subsequent read
    cannot distinguish the intent.
  id: totrans-2810
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE535]'
  id: totrans-2811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE535]'
- en: When using `orient='table'` along with user-defined `ExtensionArray`, the generated
    schema will contain an additional `extDtype` key in the respective `fields` element.
    This extra key is not standard but does enable JSON roundtrips for extension types
    (e.g. `read_json(df.to_json(orient="table"), orient="table")`).
  id: totrans-2812
  prefs: []
  type: TYPE_NORMAL
- en: 'The `extDtype` key carries the name of the extension, if you have properly
    registered the `ExtensionDtype`, pandas will use said name to perform a lookup
    into the registry and re-convert the serialized data into your custom dtype.  ###
    Writing JSON'
  id: totrans-2813
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Series` or `DataFrame` can be converted to a valid JSON string. Use `to_json`
    with optional parameters:'
  id: totrans-2814
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` : the pathname or buffer to write the output. This can be `None`
    in which case a JSON string is returned.'
  id: totrans-2815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  id: totrans-2816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Series`:'
  id: totrans-2817
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  id: totrans-2818
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  id: totrans-2819
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`:'
  id: totrans-2820
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  id: totrans-2821
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  id: totrans-2822
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  id: totrans-2823
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  id: totrans-2824
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  id: totrans-2825
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  id: totrans-2826
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  id: totrans-2827
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  id: totrans-2828
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  id: totrans-2829
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`date_format` : string, type of date conversion, ‘epoch’ for timestamp, ‘iso’
    for ISO8601.'
  id: totrans-2830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`double_precision` : The number of decimal places to use when encoding floating
    point values, default 10.'
  id: totrans-2831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_ascii` : force encoded string to be ASCII, default True.'
  id: totrans-2832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : The time unit to encode to, governs timestamp and ISO8601 precision.
    One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds
    respectively. Default ‘ms’.'
  id: totrans-2833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_handler` : The handler to call if an object cannot otherwise be converted
    to a suitable format for JSON. Takes a single argument, which is the object to
    convert, and returns a serializable object.'
  id: totrans-2834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : If `records` orient, then will write each record per line as json.'
  id: totrans-2835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : string, writer mode when writing to path. ‘w’ for write, ‘a’ for append.
    Default ‘w’'
  id: totrans-2836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note `NaN`’s, `NaT`’s and `None` will be converted to `null` and `datetime`
    objects will be converted based on the `date_format` and `date_unit` parameters.
  id: totrans-2837
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE536]'
  id: totrans-2838
  prefs: []
  type: TYPE_PRE
  zh: '[PRE536]'
- en: Orient options
  id: totrans-2839
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a number of different options for the format of the resulting JSON
    file / string. Consider the following `DataFrame` and `Series`:'
  id: totrans-2840
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE537]'
  id: totrans-2841
  prefs: []
  type: TYPE_PRE
  zh: '[PRE537]'
- en: '**Column oriented** (the default for `DataFrame`) serializes the data as nested
    JSON objects with column labels acting as the primary index:'
  id: totrans-2842
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE538]'
  id: totrans-2843
  prefs: []
  type: TYPE_PRE
  zh: '[PRE538]'
- en: '**Index oriented** (the default for `Series`) similar to column oriented but
    the index labels are now primary:'
  id: totrans-2844
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE539]'
  id: totrans-2845
  prefs: []
  type: TYPE_PRE
  zh: '[PRE539]'
- en: '**Record oriented** serializes the data to a JSON array of column -> value
    records, index labels are not included. This is useful for passing `DataFrame`
    data to plotting libraries, for example the JavaScript library `d3.js`:'
  id: totrans-2846
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE540]'
  id: totrans-2847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE540]'
- en: '**Value oriented** is a bare-bones option which serializes to nested JSON arrays
    of values only, column and index labels are not included:'
  id: totrans-2848
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE541]'
  id: totrans-2849
  prefs: []
  type: TYPE_PRE
  zh: '[PRE541]'
- en: '**Split oriented** serializes to a JSON object containing separate entries
    for values, index and columns. Name is also included for `Series`:'
  id: totrans-2850
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE542]'
  id: totrans-2851
  prefs: []
  type: TYPE_PRE
  zh: '[PRE542]'
- en: '**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/),
    allowing for the preservation of metadata including but not limited to dtypes
    and index names.'
  id: totrans-2852
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2853
  prefs: []
  type: TYPE_NORMAL
- en: Any orient option that encodes to a JSON object will not preserve the ordering
    of index and column labels during round-trip serialization. If you wish to preserve
    label ordering use the `split` option as it uses ordered containers.
  id: totrans-2854
  prefs: []
  type: TYPE_NORMAL
- en: Date handling
  id: totrans-2855
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Writing in ISO date format:'
  id: totrans-2856
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE543]'
  id: totrans-2857
  prefs: []
  type: TYPE_PRE
  zh: '[PRE543]'
- en: 'Writing in ISO date format, with microseconds:'
  id: totrans-2858
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE544]'
  id: totrans-2859
  prefs: []
  type: TYPE_PRE
  zh: '[PRE544]'
- en: 'Epoch timestamps, in seconds:'
  id: totrans-2860
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE545]'
  id: totrans-2861
  prefs: []
  type: TYPE_PRE
  zh: '[PRE545]'
- en: 'Writing to a file, with a date index and a date column:'
  id: totrans-2862
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE546]'
  id: totrans-2863
  prefs: []
  type: TYPE_PRE
  zh: '[PRE546]'
- en: Fallback behavior
  id: totrans-2864
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the JSON serializer cannot handle the container contents directly it will
    fall back in the following manner:'
  id: totrans-2865
  prefs: []
  type: TYPE_NORMAL
- en: if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,
    if provided, will be called for each value, otherwise an exception is raised.
  id: totrans-2866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'if an object is unsupported it will attempt the following:'
  id: totrans-2867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check if the object has defined a `toDict` method and call it. A `toDict` method
    should return a `dict` which will then be JSON serialized.
  id: totrans-2868
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2869
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2870
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: invoke the `default_handler` if one was provided.
  id: totrans-2871
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2872
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2873
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: convert the object to a `dict` by traversing its contents. However this will
    often fail with an `OverflowError` or give unexpected results.
  id: totrans-2874
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general the best approach for unsupported objects or dtypes is to provide
    a `default_handler`. For example:'
  id: totrans-2875
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE547]'
  id: totrans-2876
  prefs: []
  type: TYPE_PRE
  zh: '[PRE547]'
- en: 'can be dealt with by specifying a simple `default_handler`:'
  id: totrans-2877
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE548]'
  id: totrans-2878
  prefs: []
  type: TYPE_PRE
  zh: '[PRE548]'
- en: Orient options
  id: totrans-2879
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a number of different options for the format of the resulting JSON
    file / string. Consider the following `DataFrame` and `Series`:'
  id: totrans-2880
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE549]'
  id: totrans-2881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE549]'
- en: '**Column oriented** (the default for `DataFrame`) serializes the data as nested
    JSON objects with column labels acting as the primary index:'
  id: totrans-2882
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE550]'
  id: totrans-2883
  prefs: []
  type: TYPE_PRE
  zh: '[PRE550]'
- en: '**Index oriented** (the default for `Series`) similar to column oriented but
    the index labels are now primary:'
  id: totrans-2884
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE551]'
  id: totrans-2885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE551]'
- en: '**Record oriented** serializes the data to a JSON array of column -> value
    records, index labels are not included. This is useful for passing `DataFrame`
    data to plotting libraries, for example the JavaScript library `d3.js`:'
  id: totrans-2886
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE552]'
  id: totrans-2887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE552]'
- en: '**Value oriented** is a bare-bones option which serializes to nested JSON arrays
    of values only, column and index labels are not included:'
  id: totrans-2888
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE553]'
  id: totrans-2889
  prefs: []
  type: TYPE_PRE
  zh: '[PRE553]'
- en: '**Split oriented** serializes to a JSON object containing separate entries
    for values, index and columns. Name is also included for `Series`:'
  id: totrans-2890
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE554]'
  id: totrans-2891
  prefs: []
  type: TYPE_PRE
  zh: '[PRE554]'
- en: '**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/),
    allowing for the preservation of metadata including but not limited to dtypes
    and index names.'
  id: totrans-2892
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2893
  prefs: []
  type: TYPE_NORMAL
- en: Any orient option that encodes to a JSON object will not preserve the ordering
    of index and column labels during round-trip serialization. If you wish to preserve
    label ordering use the `split` option as it uses ordered containers.
  id: totrans-2894
  prefs: []
  type: TYPE_NORMAL
- en: Date handling
  id: totrans-2895
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Writing in ISO date format:'
  id: totrans-2896
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE555]'
  id: totrans-2897
  prefs: []
  type: TYPE_PRE
  zh: '[PRE555]'
- en: 'Writing in ISO date format, with microseconds:'
  id: totrans-2898
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE556]'
  id: totrans-2899
  prefs: []
  type: TYPE_PRE
  zh: '[PRE556]'
- en: 'Epoch timestamps, in seconds:'
  id: totrans-2900
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE557]'
  id: totrans-2901
  prefs: []
  type: TYPE_PRE
  zh: '[PRE557]'
- en: 'Writing to a file, with a date index and a date column:'
  id: totrans-2902
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE558]'
  id: totrans-2903
  prefs: []
  type: TYPE_PRE
  zh: '[PRE558]'
- en: Fallback behavior
  id: totrans-2904
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the JSON serializer cannot handle the container contents directly it will
    fall back in the following manner:'
  id: totrans-2905
  prefs: []
  type: TYPE_NORMAL
- en: if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,
    if provided, will be called for each value, otherwise an exception is raised.
  id: totrans-2906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'if an object is unsupported it will attempt the following:'
  id: totrans-2907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check if the object has defined a `toDict` method and call it. A `toDict` method
    should return a `dict` which will then be JSON serialized.
  id: totrans-2908
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2909
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2910
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: invoke the `default_handler` if one was provided.
  id: totrans-2911
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-2912
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-2913
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: convert the object to a `dict` by traversing its contents. However this will
    often fail with an `OverflowError` or give unexpected results.
  id: totrans-2914
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general the best approach for unsupported objects or dtypes is to provide
    a `default_handler`. For example:'
  id: totrans-2915
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE559]'
  id: totrans-2916
  prefs: []
  type: TYPE_PRE
  zh: '[PRE559]'
- en: 'can be dealt with by specifying a simple `default_handler`:'
  id: totrans-2917
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE560]'
  id: totrans-2918
  prefs: []
  type: TYPE_PRE
  zh: '[PRE560]'
- en: '### Reading JSON'
  id: totrans-2919
  prefs: []
  type: TYPE_NORMAL
- en: Reading a JSON string to pandas object can take a number of parameters. The
    parser will try to parse a `DataFrame` if `typ` is not supplied or is `None`.
    To explicitly force `Series` parsing, pass `typ=series`
  id: totrans-2920
  prefs: []
  type: TYPE_NORMAL
- en: '`filepath_or_buffer` : a **VALID** JSON string or file handle / StringIO. The
    string could be a URL. Valid URL schemes include http, ftp, S3, and file. For
    file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json'
  id: totrans-2921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typ` : type of object to recover (series or frame), default ‘frame’'
  id: totrans-2922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  id: totrans-2923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Series :'
  id: totrans-2924
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  id: totrans-2925
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  id: totrans-2926
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame
  id: totrans-2927
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  id: totrans-2928
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  id: totrans-2929
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  id: totrans-2930
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  id: totrans-2931
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  id: totrans-2932
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  id: totrans-2933
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  id: totrans-2934
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  id: totrans-2935
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  id: totrans-2936
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`dtype` : if True, infer dtypes, if a dict of column to dtype, then use those,
    if `False`, then don’t infer dtypes at all, default is True, apply only to the
    data.'
  id: totrans-2937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_axes` : boolean, try to convert the axes to the proper dtypes, default
    is `True`'
  id: totrans-2938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_dates` : a list of columns to parse for dates; If `True`, then try
    to parse date-like columns, default is `True`.'
  id: totrans-2939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_default_dates` : boolean, default `True`. If parsing dates, then parse
    the default date-like columns.'
  id: totrans-2940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`precise_float` : boolean, default `False`. Set to enable usage of higher precision
    (strtod) function when decoding string to double values. Default (`False`) is
    to use fast but less precise builtin functionality.'
  id: totrans-2941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : string, the timestamp unit to detect if converting dates. Default
    None. By default the timestamp precision will be detected, if this is not desired
    then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force timestamp precision to seconds,
    milliseconds, microseconds or nanoseconds respectively.'
  id: totrans-2942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : reads file as one json object per line.'
  id: totrans-2943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding` : The encoding to use to decode py3 bytes.'
  id: totrans-2944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize` : when used in combination with `lines=True`, return a `pandas.api.typing.JsonReader`
    which reads in `chunksize` lines per iteration.'
  id: totrans-2945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`engine`: Either `"ujson"`, the built-in JSON parser, or `"pyarrow"` which
    dispatches to pyarrow’s `pyarrow.json.read_json`. The `"pyarrow"` is only available
    when `lines=True`'
  id: totrans-2946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parser will raise one of `ValueError/TypeError/AssertionError` if the JSON
    is not parseable.
  id: totrans-2947
  prefs: []
  type: TYPE_NORMAL
- en: If a non-default `orient` was used when encoding to JSON be sure to pass the
    same option here so that decoding produces sensible results, see [Orient Options](#orient-options)
    for an overview.
  id: totrans-2948
  prefs: []
  type: TYPE_NORMAL
- en: Data conversion
  id: totrans-2949
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will
    try to parse the axes, and all of the data into appropriate types, including dates.
    If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes`
    should only be set to `False` if you need to preserve string-like numbers (e.g.
    ‘1’, ‘2’) in an axes.
  id: totrans-2950
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2951
  prefs: []
  type: TYPE_NORMAL
- en: 'Large integer values may be converted to dates if `convert_dates=True` and
    the data and / or column labels appear ‘date-like’. The exact threshold depends
    on the `date_unit` specified. ‘date-like’ means that the column label meets one
    of the following criteria:'
  id: totrans-2952
  prefs: []
  type: TYPE_NORMAL
- en: it ends with `'_at'`
  id: totrans-2953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it ends with `'_time'`
  id: totrans-2954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it begins with `'timestamp'`
  id: totrans-2955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'modified'`
  id: totrans-2956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'date'`
  id: totrans-2957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  id: totrans-2958
  prefs: []
  type: TYPE_NORMAL
- en: 'When reading JSON data, automatic coercing into dtypes has some quirks:'
  id: totrans-2959
  prefs: []
  type: TYPE_NORMAL
- en: an index can be reconstructed in a different order from serialization, that
    is, the returned order is not guaranteed to be the same as before serialization
  id: totrans-2960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a column that was `float` data will be converted to `integer` if it can be done
    safely, e.g. a column of `1.`
  id: totrans-2961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bool columns will be converted to `integer` on reconstruction
  id: totrans-2962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus there are times where you may want to specify specific dtypes via the `dtype`
    keyword argument.
  id: totrans-2963
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading from a JSON string:'
  id: totrans-2964
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE561]'
  id: totrans-2965
  prefs: []
  type: TYPE_PRE
  zh: '[PRE561]'
- en: 'Reading from a file:'
  id: totrans-2966
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE562]'
  id: totrans-2967
  prefs: []
  type: TYPE_PRE
  zh: '[PRE562]'
- en: 'Don’t convert any data (but still convert axes and dates):'
  id: totrans-2968
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE563]'
  id: totrans-2969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE563]'
- en: 'Specify dtypes for conversion:'
  id: totrans-2970
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE564]'
  id: totrans-2971
  prefs: []
  type: TYPE_PRE
  zh: '[PRE564]'
- en: 'Preserve string indices:'
  id: totrans-2972
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE565]'
  id: totrans-2973
  prefs: []
  type: TYPE_PRE
  zh: '[PRE565]'
- en: 'Dates written in nanoseconds need to be read back in nanoseconds:'
  id: totrans-2974
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE566]'
  id: totrans-2975
  prefs: []
  type: TYPE_PRE
  zh: '[PRE566]'
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  id: totrans-2976
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE567]'
  id: totrans-2977
  prefs: []
  type: TYPE_PRE
  zh: '[PRE567]'
- en: Data conversion
  id: totrans-2978
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will
    try to parse the axes, and all of the data into appropriate types, including dates.
    If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes`
    should only be set to `False` if you need to preserve string-like numbers (e.g.
    ‘1’, ‘2’) in an axes.
  id: totrans-2979
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2980
  prefs: []
  type: TYPE_NORMAL
- en: 'Large integer values may be converted to dates if `convert_dates=True` and
    the data and / or column labels appear ‘date-like’. The exact threshold depends
    on the `date_unit` specified. ‘date-like’ means that the column label meets one
    of the following criteria:'
  id: totrans-2981
  prefs: []
  type: TYPE_NORMAL
- en: it ends with `'_at'`
  id: totrans-2982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it ends with `'_time'`
  id: totrans-2983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it begins with `'timestamp'`
  id: totrans-2984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'modified'`
  id: totrans-2985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'date'`
  id: totrans-2986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  id: totrans-2987
  prefs: []
  type: TYPE_NORMAL
- en: 'When reading JSON data, automatic coercing into dtypes has some quirks:'
  id: totrans-2988
  prefs: []
  type: TYPE_NORMAL
- en: an index can be reconstructed in a different order from serialization, that
    is, the returned order is not guaranteed to be the same as before serialization
  id: totrans-2989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a column that was `float` data will be converted to `integer` if it can be done
    safely, e.g. a column of `1.`
  id: totrans-2990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bool columns will be converted to `integer` on reconstruction
  id: totrans-2991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus there are times where you may want to specify specific dtypes via the `dtype`
    keyword argument.
  id: totrans-2992
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading from a JSON string:'
  id: totrans-2993
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE568]'
  id: totrans-2994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE568]'
- en: 'Reading from a file:'
  id: totrans-2995
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE569]'
  id: totrans-2996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE569]'
- en: 'Don’t convert any data (but still convert axes and dates):'
  id: totrans-2997
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE570]'
  id: totrans-2998
  prefs: []
  type: TYPE_PRE
  zh: '[PRE570]'
- en: 'Specify dtypes for conversion:'
  id: totrans-2999
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE571]'
  id: totrans-3000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE571]'
- en: 'Preserve string indices:'
  id: totrans-3001
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE572]'
  id: totrans-3002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE572]'
- en: 'Dates written in nanoseconds need to be read back in nanoseconds:'
  id: totrans-3003
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE573]'
  id: totrans-3004
  prefs: []
  type: TYPE_PRE
  zh: '[PRE573]'
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  id: totrans-3005
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE574]'
  id: totrans-3006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE574]'
- en: '### Normalization'
  id: totrans-3007
  prefs: []
  type: TYPE_NORMAL
- en: pandas provides a utility function to take a dict or list of dicts and *normalize*
    this semi-structured data into a flat table.
  id: totrans-3008
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE575]'
  id: totrans-3009
  prefs: []
  type: TYPE_PRE
  zh: '[PRE575]'
- en: '[PRE576]'
  id: totrans-3010
  prefs: []
  type: TYPE_PRE
  zh: '[PRE576]'
- en: The max_level parameter provides more control over which level to end normalization.
    With max_level=1 the following snippet normalizes until 1st nesting level of the
    provided dict.
  id: totrans-3011
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE577]'
  id: totrans-3012
  prefs: []
  type: TYPE_PRE
  zh: '[PRE577]'
- en: '### Line delimited json'
  id: totrans-3013
  prefs: []
  type: TYPE_NORMAL
- en: pandas is able to read and write line-delimited json files that are common in
    data processing pipelines using Hadoop or Spark.
  id: totrans-3014
  prefs: []
  type: TYPE_NORMAL
- en: For line-delimited json files, pandas can also return an iterator which reads
    in `chunksize` lines at a time. This can be useful for large files or to read
    from a stream.
  id: totrans-3015
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE578]'
  id: totrans-3016
  prefs: []
  type: TYPE_PRE
  zh: '[PRE578]'
- en: Line-limited json can also be read using the pyarrow reader by specifying `engine="pyarrow"`.
  id: totrans-3017
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE579]'
  id: totrans-3018
  prefs: []
  type: TYPE_PRE
  zh: '[PRE579]'
- en: New in version 2.0.0.
  id: totrans-3019
  prefs: []
  type: TYPE_NORMAL
- en: '### Table schema'
  id: totrans-3020
  prefs: []
  type: TYPE_NORMAL
- en: '[Table Schema](https://specs.frictionlessdata.io/table-schema/) is a spec for
    describing tabular datasets as a JSON object. The JSON includes information on
    the field names, types, and other attributes. You can use the orient `table` to
    build a JSON string with two fields, `schema` and `data`.'
  id: totrans-3021
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE580]'
  id: totrans-3022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE580]'
- en: The `schema` field contains the `fields` key, which itself contains a list of
    column name to type pairs, including the `Index` or `MultiIndex` (see below for
    a list of types). The `schema` field also contains a `primaryKey` field if the
    (Multi)index is unique.
  id: totrans-3023
  prefs: []
  type: TYPE_NORMAL
- en: The second field, `data`, contains the serialized data with the `records` orient.
    The index is included, and any datetimes are ISO 8601 formatted, as required by
    the Table Schema spec.
  id: totrans-3024
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of types supported are described in the Table Schema spec. This
    table shows the mapping from pandas types:'
  id: totrans-3025
  prefs: []
  type: TYPE_NORMAL
- en: '| pandas type | Table Schema type |'
  id: totrans-3026
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-3027
  prefs: []
  type: TYPE_TB
- en: '| int64 | integer |'
  id: totrans-3028
  prefs: []
  type: TYPE_TB
- en: '| float64 | number |'
  id: totrans-3029
  prefs: []
  type: TYPE_TB
- en: '| bool | boolean |'
  id: totrans-3030
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | datetime |'
  id: totrans-3031
  prefs: []
  type: TYPE_TB
- en: '| timedelta64[ns] | duration |'
  id: totrans-3032
  prefs: []
  type: TYPE_TB
- en: '| categorical | any |'
  id: totrans-3033
  prefs: []
  type: TYPE_TB
- en: '| object | str |'
  id: totrans-3034
  prefs: []
  type: TYPE_TB
- en: 'A few notes on the generated table schema:'
  id: totrans-3035
  prefs: []
  type: TYPE_NORMAL
- en: The `schema` object contains a `pandas_version` field. This contains the version
    of pandas’ dialect of the schema, and will be incremented with each revision.
  id: totrans-3036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All dates are converted to UTC when serializing. Even timezone naive values,
    which are treated as UTC with an offset of 0.
  id: totrans-3037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE581]'
  id: totrans-3038
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE581]'
- en: datetimes with a timezone (before serializing), include an additional field
    `tz` with the time zone name (e.g. `'US/Central'`).
  id: totrans-3039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE582]'
  id: totrans-3040
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE582]'
- en: Periods are converted to timestamps before serialization, and so have the same
    behavior of being converted to UTC. In addition, periods will contain and additional
    field `freq` with the period’s frequency, e.g. `'A-DEC'`.
  id: totrans-3041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE583]'
  id: totrans-3042
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE583]'
- en: 'Categoricals use the `any` type and an `enum` constraint listing the set of
    possible values. Additionally, an `ordered` field is included:'
  id: totrans-3043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE584]'
  id: totrans-3044
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE584]'
- en: 'A `primaryKey` field, containing an array of labels, is included *if the index
    is unique*:'
  id: totrans-3045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE585]'
  id: totrans-3046
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE585]'
- en: 'The `primaryKey` behavior is the same with MultiIndexes, but in this case the
    `primaryKey` is an array:'
  id: totrans-3047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE586]'
  id: totrans-3048
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE586]'
- en: 'The default naming roughly follows these rules:'
  id: totrans-3049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For series, the `object.name` is used. If that’s none, then the name is `values`
  id: totrans-3050
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3051
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3052
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `DataFrames`, the stringified version of the column name is used
  id: totrans-3053
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3054
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3055
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `Index` (not `MultiIndex`), `index.name` is used, with a fallback to `index`
    if that is None.
  id: totrans-3056
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3057
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3058
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `MultiIndex`, `mi.names` is used. If any level has no name, then `level_<i>`
    is used.
  id: totrans-3059
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_json` also accepts `orient=''table''` as an argument. This allows for
    the preservation of metadata such as dtypes and index names in a round-trippable
    manner.'
  id: totrans-3060
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE587]'
  id: totrans-3061
  prefs: []
  type: TYPE_PRE
  zh: '[PRE587]'
- en: Please note that the literal string ‘index’ as the name of an [`Index`](../reference/api/pandas.Index.html#pandas.Index
    "pandas.Index") is not round-trippable, nor are any names beginning with `'level_'`
    within a [`MultiIndex`](../reference/api/pandas.MultiIndex.html#pandas.MultiIndex
    "pandas.MultiIndex"). These are used by default in [`DataFrame.to_json()`](../reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json
    "pandas.DataFrame.to_json") to indicate missing values and the subsequent read
    cannot distinguish the intent.
  id: totrans-3062
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE588]'
  id: totrans-3063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE588]'
- en: When using `orient='table'` along with user-defined `ExtensionArray`, the generated
    schema will contain an additional `extDtype` key in the respective `fields` element.
    This extra key is not standard but does enable JSON roundtrips for extension types
    (e.g. `read_json(df.to_json(orient="table"), orient="table")`).
  id: totrans-3064
  prefs: []
  type: TYPE_NORMAL
- en: The `extDtype` key carries the name of the extension, if you have properly registered
    the `ExtensionDtype`, pandas will use said name to perform a lookup into the registry
    and re-convert the serialized data into your custom dtype.
  id: totrans-3065
  prefs: []
  type: TYPE_NORMAL
- en: HTML
  id: totrans-3066
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### Reading HTML content'
  id: totrans-3067
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3068
  prefs: []
  type: TYPE_NORMAL
- en: We **highly encourage** you to read the [HTML Table Parsing gotchas](#io-html-gotchas)
    below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.
  id: totrans-3069
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_html()` function can accept an HTML string/file/URL and
    will parse HTML tables into list of pandas `DataFrames`. Let’s look at a few examples.
  id: totrans-3070
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3071
  prefs: []
  type: TYPE_NORMAL
- en: '`read_html` returns a `list` of `DataFrame` objects, even if there is only
    a single table contained in the HTML content.'
  id: totrans-3072
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL with no options:'
  id: totrans-3073
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE589]'
  id: totrans-3074
  prefs: []
  type: TYPE_PRE
  zh: '[PRE589]'
- en: Note
  id: totrans-3075
  prefs: []
  type: TYPE_NORMAL
- en: The data from the above URL changes every Monday so the resulting data above
    may be slightly different.
  id: totrans-3076
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL while passing headers alongside the HTTP request:'
  id: totrans-3077
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE590]'
  id: totrans-3078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE590]'
- en: Note
  id: totrans-3079
  prefs: []
  type: TYPE_NORMAL
- en: We see above that the headers we passed are reflected in the HTTP request.
  id: totrans-3080
  prefs: []
  type: TYPE_NORMAL
- en: 'Read in the content of the file from the above URL and pass it to `read_html`
    as a string:'
  id: totrans-3081
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE591]'
  id: totrans-3082
  prefs: []
  type: TYPE_PRE
  zh: '[PRE591]'
- en: 'You can even pass in an instance of `StringIO` if you so desire:'
  id: totrans-3083
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE592]'
  id: totrans-3084
  prefs: []
  type: TYPE_PRE
  zh: '[PRE592]'
- en: Note
  id: totrans-3085
  prefs: []
  type: TYPE_NORMAL
- en: The following examples are not run by the IPython evaluator due to the fact
    that having so many network-accessing functions slows down the documentation build.
    If you spot an error or an example that doesn’t run, please do not hesitate to
    report it over on [pandas GitHub issues page](https://github.com/pandas-dev/pandas/issues).
  id: totrans-3086
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL and match a table that contains specific text:'
  id: totrans-3087
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE593]'
  id: totrans-3088
  prefs: []
  type: TYPE_PRE
  zh: '[PRE593]'
- en: Specify a header row (by default `<th>` or `<td>` elements located within a
    `<thead>` are used to form the column index, if multiple rows are contained within
    `<thead>` then a MultiIndex is created); if specified, the header row is taken
    from the data minus the parsed header elements (`<th>` elements).
  id: totrans-3089
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE594]'
  id: totrans-3090
  prefs: []
  type: TYPE_PRE
  zh: '[PRE594]'
- en: 'Specify an index column:'
  id: totrans-3091
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE595]'
  id: totrans-3092
  prefs: []
  type: TYPE_PRE
  zh: '[PRE595]'
- en: 'Specify a number of rows to skip:'
  id: totrans-3093
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE596]'
  id: totrans-3094
  prefs: []
  type: TYPE_PRE
  zh: '[PRE596]'
- en: 'Specify a number of rows to skip using a list (`range` works as well):'
  id: totrans-3095
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE597]'
  id: totrans-3096
  prefs: []
  type: TYPE_PRE
  zh: '[PRE597]'
- en: 'Specify an HTML attribute:'
  id: totrans-3097
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE598]'
  id: totrans-3098
  prefs: []
  type: TYPE_PRE
  zh: '[PRE598]'
- en: 'Specify values that should be converted to NaN:'
  id: totrans-3099
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE599]'
  id: totrans-3100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE599]'
- en: 'Specify whether to keep the default set of NaN values:'
  id: totrans-3101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE600]'
  id: totrans-3102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE600]'
- en: Specify converters for columns. This is useful for numerical text data that
    has leading zeros. By default columns that are numerical are cast to numeric types
    and the leading zeros are lost. To avoid this, we can convert these columns to
    strings.
  id: totrans-3103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE601]'
  id: totrans-3104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE601]'
- en: 'Use some combination of the above:'
  id: totrans-3105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE602]'
  id: totrans-3106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE602]'
- en: 'Read in pandas `to_html` output (with some loss of floating point precision):'
  id: totrans-3107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE603]'
  id: totrans-3108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE603]'
- en: 'The `lxml` backend will raise an error on a failed parse if that is the only
    parser you provide. If you only have a single parser you can provide just a string,
    but it is considered good practice to pass a list with one string if, for example,
    the function expects a sequence of strings. You may use:'
  id: totrans-3109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE604]'
  id: totrans-3110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE604]'
- en: 'Or you could pass `flavor=''lxml''` without a list:'
  id: totrans-3111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE605]'
  id: totrans-3112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE605]'
- en: However, if you have bs4 and html5lib installed and pass `None` or `['lxml',
    'bs4']` then the parse will most likely succeed. Note that *as soon as a parse
    succeeds, the function will return*.
  id: totrans-3113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE606]'
  id: totrans-3114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE606]'
- en: Links can be extracted from cells along with the text using `extract_links="all"`.
  id: totrans-3115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE607]'
  id: totrans-3116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE607]'
- en: 'New in version 1.5.0.  ### Writing to HTML files'
  id: totrans-3117
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_html` which renders the contents
    of the `DataFrame` as an HTML table. The function arguments are as in the method
    `to_string` described above.'
  id: totrans-3118
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3119
  prefs: []
  type: TYPE_NORMAL
- en: Not all of the possible options for `DataFrame.to_html` are shown here for brevity’s
    sake. See `DataFrame.to_html()` for the full set of options.
  id: totrans-3120
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3121
  prefs: []
  type: TYPE_NORMAL
- en: In an HTML-rendering supported environment like a Jupyter Notebook, `display(HTML(...))``
    will render the raw HTML into the environment.
  id: totrans-3122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE608]'
  id: totrans-3123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE608]'
- en: 'The `columns` argument will limit the columns shown:'
  id: totrans-3124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE609]'
  id: totrans-3125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE609]'
- en: '`float_format` takes a Python callable to control the precision of floating
    point values:'
  id: totrans-3126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE610]'
  id: totrans-3127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE610]'
- en: '`bold_rows` will make the row labels bold by default, but you can turn that
    off:'
  id: totrans-3128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE611]'
  id: totrans-3129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE611]'
- en: The `classes` argument provides the ability to give the resulting HTML table
    CSS classes. Note that these classes are *appended* to the existing `'dataframe'`
    class.
  id: totrans-3130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE612]'
  id: totrans-3131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE612]'
- en: The `render_links` argument provides the ability to add hyperlinks to cells
    that contain URLs.
  id: totrans-3132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE613]'
  id: totrans-3133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE613]'
- en: Finally, the `escape` argument allows you to control whether the “<”, “>” and
    “&” characters escaped in the resulting HTML (by default it is `True`). So to
    get the HTML without escaped characters pass `escape=False`
  id: totrans-3134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE614]'
  id: totrans-3135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE614]'
- en: 'Escaped:'
  id: totrans-3136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE615]'
  id: totrans-3137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE615]'
- en: 'Not escaped:'
  id: totrans-3138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE616]'
  id: totrans-3139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE616]'
- en: Note
  id: totrans-3140
  prefs: []
  type: TYPE_NORMAL
- en: 'Some browsers may not show a difference in the rendering of the previous two
    HTML tables.  ### HTML Table Parsing Gotchas'
  id: totrans-3141
  prefs: []
  type: TYPE_NORMAL
- en: There are some versioning issues surrounding the libraries that are used to
    parse HTML tables in the top-level pandas io function `read_html`.
  id: totrans-3142
  prefs: []
  type: TYPE_NORMAL
- en: '**Issues with** [**lxml**](https://lxml.de)'
  id: totrans-3143
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  id: totrans-3144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) is very fast.'
  id: totrans-3145
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3146
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3147
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) requires Cython to install correctly.'
  id: totrans-3148
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  id: totrans-3149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) does *not* make any guarantees about the results
    of its parse *unless* it is given [**strictly valid markup**](https://validator.w3.org/docs/help.html#validation_basics).'
  id: totrans-3150
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3151
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3152
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: In light of the above, we have chosen to allow you, the user, to use the [**lxml**](https://lxml.de)
    backend, but **this backend will use** [**html5lib**](https://github.com/html5lib/html5lib-python)
    if [**lxml**](https://lxml.de) fails to parse
  id: totrans-3153
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3154
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3155
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: It is therefore *highly recommended* that you install both [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    and [**html5lib**](https://github.com/html5lib/html5lib-python), so that you will
    still get a valid result (provided everything else is valid) even if [**lxml**](https://lxml.de)
    fails.
  id: totrans-3156
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**lxml**](https://lxml.de) **as a backend**'
  id: totrans-3157
  prefs: []
  type: TYPE_NORMAL
- en: The above issues hold here as well since [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    is essentially just a wrapper around a parser backend.
  id: totrans-3158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**html5lib**](https://github.com/html5lib/html5lib-python) **as a backend**'
  id: totrans-3159
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  id: totrans-3160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is far more lenient
    than [**lxml**](https://lxml.de) and consequently deals with *real-life markup*
    in a much saner way rather than just, e.g., dropping an element without notifying
    you.'
  id: totrans-3161
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3162
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3163
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) *generates valid
    HTML5 markup from invalid markup automatically*. This is extremely important for
    parsing HTML tables, since it guarantees a valid document. However, that does
    NOT mean that it is “correct”, since the process of fixing markup does not have
    a single definition.'
  id: totrans-3164
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3165
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3166
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is pure Python
    and requires no additional build steps beyond its own installation.'
  id: totrans-3167
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  id: totrans-3168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The biggest drawback to using [**html5lib**](https://github.com/html5lib/html5lib-python)
    is that it is slow as molasses. However consider the fact that many tables on
    the web are not big enough for the parsing algorithm runtime to matter. It is
    more likely that the bottleneck will be in the process of reading the raw text
    from the URL over the web, i.e., IO (input-output). For very large tables, this
    might not be true.  ### Reading HTML content'
  id: totrans-3169
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3170
  prefs: []
  type: TYPE_NORMAL
- en: We **highly encourage** you to read the [HTML Table Parsing gotchas](#io-html-gotchas)
    below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.
  id: totrans-3171
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_html()` function can accept an HTML string/file/URL and
    will parse HTML tables into list of pandas `DataFrames`. Let’s look at a few examples.
  id: totrans-3172
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3173
  prefs: []
  type: TYPE_NORMAL
- en: '`read_html` returns a `list` of `DataFrame` objects, even if there is only
    a single table contained in the HTML content.'
  id: totrans-3174
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL with no options:'
  id: totrans-3175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE617]'
  id: totrans-3176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE617]'
- en: Note
  id: totrans-3177
  prefs: []
  type: TYPE_NORMAL
- en: The data from the above URL changes every Monday so the resulting data above
    may be slightly different.
  id: totrans-3178
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL while passing headers alongside the HTTP request:'
  id: totrans-3179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE618]'
  id: totrans-3180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE618]'
- en: Note
  id: totrans-3181
  prefs: []
  type: TYPE_NORMAL
- en: We see above that the headers we passed are reflected in the HTTP request.
  id: totrans-3182
  prefs: []
  type: TYPE_NORMAL
- en: 'Read in the content of the file from the above URL and pass it to `read_html`
    as a string:'
  id: totrans-3183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE619]'
  id: totrans-3184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE619]'
- en: 'You can even pass in an instance of `StringIO` if you so desire:'
  id: totrans-3185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE620]'
  id: totrans-3186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE620]'
- en: Note
  id: totrans-3187
  prefs: []
  type: TYPE_NORMAL
- en: The following examples are not run by the IPython evaluator due to the fact
    that having so many network-accessing functions slows down the documentation build.
    If you spot an error or an example that doesn’t run, please do not hesitate to
    report it over on [pandas GitHub issues page](https://github.com/pandas-dev/pandas/issues).
  id: totrans-3188
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL and match a table that contains specific text:'
  id: totrans-3189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE621]'
  id: totrans-3190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE621]'
- en: Specify a header row (by default `<th>` or `<td>` elements located within a
    `<thead>` are used to form the column index, if multiple rows are contained within
    `<thead>` then a MultiIndex is created); if specified, the header row is taken
    from the data minus the parsed header elements (`<th>` elements).
  id: totrans-3191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE622]'
  id: totrans-3192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE622]'
- en: 'Specify an index column:'
  id: totrans-3193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE623]'
  id: totrans-3194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE623]'
- en: 'Specify a number of rows to skip:'
  id: totrans-3195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE624]'
  id: totrans-3196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE624]'
- en: 'Specify a number of rows to skip using a list (`range` works as well):'
  id: totrans-3197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE625]'
  id: totrans-3198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE625]'
- en: 'Specify an HTML attribute:'
  id: totrans-3199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE626]'
  id: totrans-3200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE626]'
- en: 'Specify values that should be converted to NaN:'
  id: totrans-3201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE627]'
  id: totrans-3202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE627]'
- en: 'Specify whether to keep the default set of NaN values:'
  id: totrans-3203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE628]'
  id: totrans-3204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE628]'
- en: Specify converters for columns. This is useful for numerical text data that
    has leading zeros. By default columns that are numerical are cast to numeric types
    and the leading zeros are lost. To avoid this, we can convert these columns to
    strings.
  id: totrans-3205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE629]'
  id: totrans-3206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE629]'
- en: 'Use some combination of the above:'
  id: totrans-3207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE630]'
  id: totrans-3208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE630]'
- en: 'Read in pandas `to_html` output (with some loss of floating point precision):'
  id: totrans-3209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE631]'
  id: totrans-3210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE631]'
- en: 'The `lxml` backend will raise an error on a failed parse if that is the only
    parser you provide. If you only have a single parser you can provide just a string,
    but it is considered good practice to pass a list with one string if, for example,
    the function expects a sequence of strings. You may use:'
  id: totrans-3211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE632]'
  id: totrans-3212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE632]'
- en: 'Or you could pass `flavor=''lxml''` without a list:'
  id: totrans-3213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE633]'
  id: totrans-3214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE633]'
- en: However, if you have bs4 and html5lib installed and pass `None` or `['lxml',
    'bs4']` then the parse will most likely succeed. Note that *as soon as a parse
    succeeds, the function will return*.
  id: totrans-3215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE634]'
  id: totrans-3216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE634]'
- en: Links can be extracted from cells along with the text using `extract_links="all"`.
  id: totrans-3217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE635]'
  id: totrans-3218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE635]'
- en: New in version 1.5.0.
  id: totrans-3219
  prefs: []
  type: TYPE_NORMAL
- en: '### Writing to HTML files'
  id: totrans-3220
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_html` which renders the contents
    of the `DataFrame` as an HTML table. The function arguments are as in the method
    `to_string` described above.'
  id: totrans-3221
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3222
  prefs: []
  type: TYPE_NORMAL
- en: Not all of the possible options for `DataFrame.to_html` are shown here for brevity’s
    sake. See `DataFrame.to_html()` for the full set of options.
  id: totrans-3223
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3224
  prefs: []
  type: TYPE_NORMAL
- en: In an HTML-rendering supported environment like a Jupyter Notebook, `display(HTML(...))``
    will render the raw HTML into the environment.
  id: totrans-3225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE636]'
  id: totrans-3226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE636]'
- en: 'The `columns` argument will limit the columns shown:'
  id: totrans-3227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE637]'
  id: totrans-3228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE637]'
- en: '`float_format` takes a Python callable to control the precision of floating
    point values:'
  id: totrans-3229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE638]'
  id: totrans-3230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE638]'
- en: '`bold_rows` will make the row labels bold by default, but you can turn that
    off:'
  id: totrans-3231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE639]'
  id: totrans-3232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE639]'
- en: The `classes` argument provides the ability to give the resulting HTML table
    CSS classes. Note that these classes are *appended* to the existing `'dataframe'`
    class.
  id: totrans-3233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE640]'
  id: totrans-3234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE640]'
- en: The `render_links` argument provides the ability to add hyperlinks to cells
    that contain URLs.
  id: totrans-3235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE641]'
  id: totrans-3236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE641]'
- en: Finally, the `escape` argument allows you to control whether the “<”, “>” and
    “&” characters escaped in the resulting HTML (by default it is `True`). So to
    get the HTML without escaped characters pass `escape=False`
  id: totrans-3237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE642]'
  id: totrans-3238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE642]'
- en: 'Escaped:'
  id: totrans-3239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE643]'
  id: totrans-3240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE643]'
- en: 'Not escaped:'
  id: totrans-3241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE644]'
  id: totrans-3242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE644]'
- en: Note
  id: totrans-3243
  prefs: []
  type: TYPE_NORMAL
- en: Some browsers may not show a difference in the rendering of the previous two
    HTML tables.
  id: totrans-3244
  prefs: []
  type: TYPE_NORMAL
- en: '### HTML Table Parsing Gotchas'
  id: totrans-3245
  prefs: []
  type: TYPE_NORMAL
- en: There are some versioning issues surrounding the libraries that are used to
    parse HTML tables in the top-level pandas io function `read_html`.
  id: totrans-3246
  prefs: []
  type: TYPE_NORMAL
- en: '**Issues with** [**lxml**](https://lxml.de)'
  id: totrans-3247
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  id: totrans-3248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) is very fast.'
  id: totrans-3249
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3250
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3251
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) requires Cython to install correctly.'
  id: totrans-3252
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  id: totrans-3253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) does *not* make any guarantees about the results
    of its parse *unless* it is given [**strictly valid markup**](https://validator.w3.org/docs/help.html#validation_basics).'
  id: totrans-3254
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3255
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3256
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: In light of the above, we have chosen to allow you, the user, to use the [**lxml**](https://lxml.de)
    backend, but **this backend will use** [**html5lib**](https://github.com/html5lib/html5lib-python)
    if [**lxml**](https://lxml.de) fails to parse
  id: totrans-3257
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3258
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3259
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: It is therefore *highly recommended* that you install both [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    and [**html5lib**](https://github.com/html5lib/html5lib-python), so that you will
    still get a valid result (provided everything else is valid) even if [**lxml**](https://lxml.de)
    fails.
  id: totrans-3260
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**lxml**](https://lxml.de) **as a backend**'
  id: totrans-3261
  prefs: []
  type: TYPE_NORMAL
- en: The above issues hold here as well since [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    is essentially just a wrapper around a parser backend.
  id: totrans-3262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**html5lib**](https://github.com/html5lib/html5lib-python) **as a backend**'
  id: totrans-3263
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  id: totrans-3264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is far more lenient
    than [**lxml**](https://lxml.de) and consequently deals with *real-life markup*
    in a much saner way rather than just, e.g., dropping an element without notifying
    you.'
  id: totrans-3265
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3266
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3267
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) *generates valid
    HTML5 markup from invalid markup automatically*. This is extremely important for
    parsing HTML tables, since it guarantees a valid document. However, that does
    NOT mean that it is “correct”, since the process of fixing markup does not have
    a single definition.'
  id: totrans-3268
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  id: totrans-3269
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  id: totrans-3270
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is pure Python
    and requires no additional build steps beyond its own installation.'
  id: totrans-3271
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  id: totrans-3272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The biggest drawback to using [**html5lib**](https://github.com/html5lib/html5lib-python)
    is that it is slow as molasses. However consider the fact that many tables on
    the web are not big enough for the parsing algorithm runtime to matter. It is
    more likely that the bottleneck will be in the process of reading the raw text
    from the URL over the web, i.e., IO (input-output). For very large tables, this
    might not be true.
  id: totrans-3273
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '## LaTeX'
  id: totrans-3274
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  id: totrans-3275
  prefs: []
  type: TYPE_NORMAL
- en: Currently there are no methods to read from LaTeX, only output methods.
  id: totrans-3276
  prefs: []
  type: TYPE_NORMAL
- en: Writing to LaTeX files
  id: totrans-3277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  id: totrans-3278
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame *and* Styler objects currently have a `to_latex` method. We recommend
    using the [Styler.to_latex()](../reference/api/pandas.io.formats.style.Styler.to_latex.html)
    method over [DataFrame.to_latex()](../reference/api/pandas.DataFrame.to_latex.html)
    due to the former’s greater flexibility with conditional styling, and the latter’s
    possible future deprecation.
  id: totrans-3279
  prefs: []
  type: TYPE_NORMAL
- en: Review the documentation for [Styler.to_latex](../reference/api/pandas.io.formats.style.Styler.to_latex.html),
    which gives examples of conditional styling and explains the operation of its
    keyword arguments.
  id: totrans-3280
  prefs: []
  type: TYPE_NORMAL
- en: For simple application the following pattern is sufficient.
  id: totrans-3281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE645]'
  id: totrans-3282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE645]'
- en: To format values before output, chain the [Styler.format](../reference/api/pandas.io.formats.style.Styler.format.html)
    method.
  id: totrans-3283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE646]'
  id: totrans-3284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE646]'
- en: Writing to LaTeX files
  id: totrans-3285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  id: totrans-3286
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame *and* Styler objects currently have a `to_latex` method. We recommend
    using the [Styler.to_latex()](../reference/api/pandas.io.formats.style.Styler.to_latex.html)
    method over [DataFrame.to_latex()](../reference/api/pandas.DataFrame.to_latex.html)
    due to the former’s greater flexibility with conditional styling, and the latter’s
    possible future deprecation.
  id: totrans-3287
  prefs: []
  type: TYPE_NORMAL
- en: Review the documentation for [Styler.to_latex](../reference/api/pandas.io.formats.style.Styler.to_latex.html),
    which gives examples of conditional styling and explains the operation of its
    keyword arguments.
  id: totrans-3288
  prefs: []
  type: TYPE_NORMAL
- en: For simple application the following pattern is sufficient.
  id: totrans-3289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE647]'
  id: totrans-3290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE647]'
- en: To format values before output, chain the [Styler.format](../reference/api/pandas.io.formats.style.Styler.format.html)
    method.
  id: totrans-3291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE648]'
  id: totrans-3292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE648]'
- en: XML
  id: totrans-3293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### Reading XML'
  id: totrans-3294
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  id: totrans-3295
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_xml()` function can accept an XML string/file/URL and will
    parse nodes and attributes into a pandas `DataFrame`.
  id: totrans-3296
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3297
  prefs: []
  type: TYPE_NORMAL
- en: Since there is no standard XML structure where design types can vary in many
    ways, `read_xml` works best with flatter, shallow versions. If an XML document
    is deeply nested, use the `stylesheet` feature to transform XML into a flatter
    version.
  id: totrans-3298
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  id: totrans-3299
  prefs: []
  type: TYPE_NORMAL
- en: 'Read an XML string:'
  id: totrans-3300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE649]'
  id: totrans-3301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE649]'
- en: 'Read a URL with no options:'
  id: totrans-3302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE650]'
  id: totrans-3303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE650]'
- en: 'Read in the content of the “books.xml” file and pass it to `read_xml` as a
    string:'
  id: totrans-3304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE651]'
  id: totrans-3305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE651]'
- en: 'Read in the content of the “books.xml” as instance of `StringIO` or `BytesIO`
    and pass it to `read_xml`:'
  id: totrans-3306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE652]'
  id: totrans-3307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE652]'
- en: '[PRE653]'
  id: totrans-3308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE653]'
- en: 'Even read XML from AWS S3 buckets such as NIH NCBI PMC Article Datasets providing
    Biomedical and Life Science Jorurnals:'
  id: totrans-3309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE654]'
  id: totrans-3310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE654]'
- en: 'With [lxml](https://lxml.de) as default `parser`, you access the full-featured
    XML library that extends Python’s ElementTree API. One powerful tool is ability
    to query nodes selectively or conditionally with more expressive XPath:'
  id: totrans-3311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE655]'
  id: totrans-3312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE655]'
- en: 'Specify only elements or only attributes to parse:'
  id: totrans-3313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE656]'
  id: totrans-3314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE656]'
- en: '[PRE657]'
  id: totrans-3315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE657]'
- en: XML documents can have namespaces with prefixes and default namespaces without
    prefixes both of which are denoted with a special attribute `xmlns`. In order
    to parse by node under a namespace context, `xpath` must reference a prefix.
  id: totrans-3316
  prefs: []
  type: TYPE_NORMAL
- en: For example, below XML contains a namespace with prefix, `doc`, and URI at `https://example.com`.
    In order to parse `doc:row` nodes, `namespaces` must be used.
  id: totrans-3317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE658]'
  id: totrans-3318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE658]'
- en: Similarly, an XML document can have a default namespace without prefix. Failing
    to assign a temporary prefix will return no nodes and raise a `ValueError`. But
    assigning *any* temporary name to correct URI allows parsing by nodes.
  id: totrans-3319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE659]'
  id: totrans-3320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE659]'
- en: However, if XPath does not reference node names such as default, `/*`, then
    `namespaces` is not required.
  id: totrans-3321
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3322
  prefs: []
  type: TYPE_NORMAL
- en: Since `xpath` identifies the parent of content to be parsed, only immediate
    desendants which include child nodes or current attributes are parsed. Therefore,
    `read_xml` will not parse the text of grandchildren or other descendants and will
    not parse attributes of any descendant. To retrieve lower level content, adjust
    xpath to lower level. For example,
  id: totrans-3323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE660]'
  id: totrans-3324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE660]'
- en: shows the attribute `sides` on `shape` element was not parsed as expected since
    this attribute resides on the child of `row` element and not `row` element itself.
    In other words, `sides` attribute is a grandchild level descendant of `row` element.
    However, the `xpath` targets `row` element which covers only its children and
    attributes.
  id: totrans-3325
  prefs: []
  type: TYPE_NORMAL
- en: With [lxml](https://lxml.de) as parser, you can flatten nested XML documents
    with an XSLT script which also can be string/file/URL types. As background, [XSLT](https://www.w3.org/TR/xslt/)
    is a special-purpose language written in a special XML file that can transform
    original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using
    an XSLT processor.
  id: totrans-3326
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider this somewhat nested structure of Chicago “L” Rides where
    station and rides elements encapsulate data in their own sections. With below
    XSLT, `lxml` can transform original nested document into a flatter output (as
    shown below for demonstration) for easier parse into `DataFrame`:'
  id: totrans-3327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE661]'
  id: totrans-3328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE661]'
- en: For very large XML files that can range in hundreds of megabytes to gigabytes,
    [`pandas.read_xml()`](../reference/api/pandas.read_xml.html#pandas.read_xml "pandas.read_xml")
    supports parsing such sizeable files using [lxml’s iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk)
    and [etree’s iterparse](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse)
    which are memory-efficient methods to iterate through an XML tree and extract
    specific elements and attributes. without holding entire tree in memory.
  id: totrans-3329
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.5.0.
  id: totrans-3330
  prefs: []
  type: TYPE_NORMAL
- en: To use this feature, you must pass a physical XML file path into `read_xml`
    and use the `iterparse` argument. Files should not be compressed or point to online
    sources but stored on local disk. Also, `iterparse` should be a dictionary where
    the key is the repeating nodes in document (which become the rows) and the value
    is a list of any element or attribute that is a descendant (i.e., child, grandchild)
    of repeating node. Since XPath is not used in this method, descendants do not
    need to share same relationship with one another. Below shows example of reading
    in Wikipedia’s very large (12 GB+) latest article data dump.
  id: totrans-3331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE662]  ### Writing XML'
  id: totrans-3332
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  id: totrans-3333
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_xml` which renders the contents
    of the `DataFrame` as an XML document.'
  id: totrans-3334
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3335
  prefs: []
  type: TYPE_NORMAL
- en: This method does not support special properties of XML including DTD, CData,
    XSD schemas, processing instructions, comments, and others. Only namespaces at
    the root level is supported. However, `stylesheet` allows design changes after
    initial output.
  id: totrans-3336
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  id: totrans-3337
  prefs: []
  type: TYPE_NORMAL
- en: 'Write an XML without options:'
  id: totrans-3338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE663]'
  id: totrans-3339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE663]'
- en: 'Write an XML with new root and row name:'
  id: totrans-3340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE664]'
  id: totrans-3341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE664]'
- en: 'Write an attribute-centric XML:'
  id: totrans-3342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE665]'
  id: totrans-3343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE665]'
- en: 'Write a mix of elements and attributes:'
  id: totrans-3344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE666]'
  id: totrans-3345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE666]'
- en: 'Any `DataFrames` with hierarchical columns will be flattened for XML element
    names with levels delimited by underscores:'
  id: totrans-3346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE667]'
  id: totrans-3347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE667]'
- en: 'Write an XML with default namespace:'
  id: totrans-3348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE668]'
  id: totrans-3349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE668]'
- en: 'Write an XML with namespace prefix:'
  id: totrans-3350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE669]'
  id: totrans-3351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE669]'
- en: 'Write an XML without declaration or pretty print:'
  id: totrans-3352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE670]'
  id: totrans-3353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE670]'
- en: 'Write an XML and transform with stylesheet:'
  id: totrans-3354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE671]'
  id: totrans-3355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE671]'
- en: XML Final Notes
  id: totrans-3356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All XML documents adhere to [W3C specifications](https://www.w3.org/TR/xml/).
    Both `etree` and `lxml` parsers will fail to parse any markup document that is
    not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document
    unless it follows XHTML specs. However, other popular markup types including KML,
    XAML, RSS, MusicML, MathML are compliant [XML schemas](https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas).
  id: totrans-3357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For above reason, if your application builds XML prior to pandas operations,
    use appropriate DOM libraries like `etree` and `lxml` to build the necessary document
    and not by string concatenation or regex adjustments. Always remember XML is a
    *special* text file with markup rules.
  id: totrans-3358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With very large XML files (several hundred MBs to GBs), XPath and XSLT can become
    memory-intensive operations. Be sure to have enough available RAM for reading
    and writing to large XML files (roughly about 5 times the size of text).
  id: totrans-3359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because XSLT is a programming language, use it with caution since such scripts
    can pose a security risk in your environment and can run large or infinite recursive
    operations. Always test scripts on small fragments before full run.
  id: totrans-3360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [etree](https://docs.python.org/3/library/xml.etree.elementtree.html) parser
    supports all functionality of both `read_xml` and `to_xml` except for complex
    XPath and any XSLT. Though limited in features, `etree` is still a reliable and
    capable parser and tree builder. Its performance may trail `lxml` to a certain
    degree for larger files but relatively unnoticeable on small to medium size files.
  id: totrans-3361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### Reading XML'
  id: totrans-3362
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  id: totrans-3363
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_xml()` function can accept an XML string/file/URL and will
    parse nodes and attributes into a pandas `DataFrame`.
  id: totrans-3364
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3365
  prefs: []
  type: TYPE_NORMAL
- en: Since there is no standard XML structure where design types can vary in many
    ways, `read_xml` works best with flatter, shallow versions. If an XML document
    is deeply nested, use the `stylesheet` feature to transform XML into a flatter
    version.
  id: totrans-3366
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  id: totrans-3367
  prefs: []
  type: TYPE_NORMAL
- en: 'Read an XML string:'
  id: totrans-3368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE672]'
  id: totrans-3369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE672]'
- en: 'Read a URL with no options:'
  id: totrans-3370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE673]'
  id: totrans-3371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE673]'
- en: 'Read in the content of the “books.xml” file and pass it to `read_xml` as a
    string:'
  id: totrans-3372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE674]'
  id: totrans-3373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE674]'
- en: 'Read in the content of the “books.xml” as instance of `StringIO` or `BytesIO`
    and pass it to `read_xml`:'
  id: totrans-3374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE675]'
  id: totrans-3375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE675]'
- en: '[PRE676]'
  id: totrans-3376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE676]'
- en: 'Even read XML from AWS S3 buckets such as NIH NCBI PMC Article Datasets providing
    Biomedical and Life Science Jorurnals:'
  id: totrans-3377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE677]'
  id: totrans-3378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE677]'
- en: 'With [lxml](https://lxml.de) as default `parser`, you access the full-featured
    XML library that extends Python’s ElementTree API. One powerful tool is ability
    to query nodes selectively or conditionally with more expressive XPath:'
  id: totrans-3379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE678]'
  id: totrans-3380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE678]'
- en: 'Specify only elements or only attributes to parse:'
  id: totrans-3381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE679]'
  id: totrans-3382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE679]'
- en: '[PRE680]'
  id: totrans-3383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE680]'
- en: XML documents can have namespaces with prefixes and default namespaces without
    prefixes both of which are denoted with a special attribute `xmlns`. In order
    to parse by node under a namespace context, `xpath` must reference a prefix.
  id: totrans-3384
  prefs: []
  type: TYPE_NORMAL
- en: For example, below XML contains a namespace with prefix, `doc`, and URI at `https://example.com`.
    In order to parse `doc:row` nodes, `namespaces` must be used.
  id: totrans-3385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE681]'
  id: totrans-3386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE681]'
- en: Similarly, an XML document can have a default namespace without prefix. Failing
    to assign a temporary prefix will return no nodes and raise a `ValueError`. But
    assigning *any* temporary name to correct URI allows parsing by nodes.
  id: totrans-3387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE682]'
  id: totrans-3388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE682]'
- en: However, if XPath does not reference node names such as default, `/*`, then
    `namespaces` is not required.
  id: totrans-3389
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3390
  prefs: []
  type: TYPE_NORMAL
- en: Since `xpath` identifies the parent of content to be parsed, only immediate
    desendants which include child nodes or current attributes are parsed. Therefore,
    `read_xml` will not parse the text of grandchildren or other descendants and will
    not parse attributes of any descendant. To retrieve lower level content, adjust
    xpath to lower level. For example,
  id: totrans-3391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE683]'
  id: totrans-3392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE683]'
- en: shows the attribute `sides` on `shape` element was not parsed as expected since
    this attribute resides on the child of `row` element and not `row` element itself.
    In other words, `sides` attribute is a grandchild level descendant of `row` element.
    However, the `xpath` targets `row` element which covers only its children and
    attributes.
  id: totrans-3393
  prefs: []
  type: TYPE_NORMAL
- en: With [lxml](https://lxml.de) as parser, you can flatten nested XML documents
    with an XSLT script which also can be string/file/URL types. As background, [XSLT](https://www.w3.org/TR/xslt/)
    is a special-purpose language written in a special XML file that can transform
    original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using
    an XSLT processor.
  id: totrans-3394
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider this somewhat nested structure of Chicago “L” Rides where
    station and rides elements encapsulate data in their own sections. With below
    XSLT, `lxml` can transform original nested document into a flatter output (as
    shown below for demonstration) for easier parse into `DataFrame`:'
  id: totrans-3395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE684]'
  id: totrans-3396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE684]'
- en: For very large XML files that can range in hundreds of megabytes to gigabytes,
    [`pandas.read_xml()`](../reference/api/pandas.read_xml.html#pandas.read_xml "pandas.read_xml")
    supports parsing such sizeable files using [lxml’s iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk)
    and [etree’s iterparse](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse)
    which are memory-efficient methods to iterate through an XML tree and extract
    specific elements and attributes. without holding entire tree in memory.
  id: totrans-3397
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.5.0.
  id: totrans-3398
  prefs: []
  type: TYPE_NORMAL
- en: To use this feature, you must pass a physical XML file path into `read_xml`
    and use the `iterparse` argument. Files should not be compressed or point to online
    sources but stored on local disk. Also, `iterparse` should be a dictionary where
    the key is the repeating nodes in document (which become the rows) and the value
    is a list of any element or attribute that is a descendant (i.e., child, grandchild)
    of repeating node. Since XPath is not used in this method, descendants do not
    need to share same relationship with one another. Below shows example of reading
    in Wikipedia’s very large (12 GB+) latest article data dump.
  id: totrans-3399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE685]'
  id: totrans-3400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE685]'
- en: '### Writing XML'
  id: totrans-3401
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  id: totrans-3402
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_xml` which renders the contents
    of the `DataFrame` as an XML document.'
  id: totrans-3403
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3404
  prefs: []
  type: TYPE_NORMAL
- en: This method does not support special properties of XML including DTD, CData,
    XSD schemas, processing instructions, comments, and others. Only namespaces at
    the root level is supported. However, `stylesheet` allows design changes after
    initial output.
  id: totrans-3405
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  id: totrans-3406
  prefs: []
  type: TYPE_NORMAL
- en: 'Write an XML without options:'
  id: totrans-3407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE686]'
  id: totrans-3408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE686]'
- en: 'Write an XML with new root and row name:'
  id: totrans-3409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE687]'
  id: totrans-3410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE687]'
- en: 'Write an attribute-centric XML:'
  id: totrans-3411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE688]'
  id: totrans-3412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE688]'
- en: 'Write a mix of elements and attributes:'
  id: totrans-3413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE689]'
  id: totrans-3414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE689]'
- en: 'Any `DataFrames` with hierarchical columns will be flattened for XML element
    names with levels delimited by underscores:'
  id: totrans-3415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE690]'
  id: totrans-3416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE690]'
- en: 'Write an XML with default namespace:'
  id: totrans-3417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE691]'
  id: totrans-3418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE691]'
- en: 'Write an XML with namespace prefix:'
  id: totrans-3419
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE692]'
  id: totrans-3420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE692]'
- en: 'Write an XML without declaration or pretty print:'
  id: totrans-3421
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE693]'
  id: totrans-3422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE693]'
- en: 'Write an XML and transform with stylesheet:'
  id: totrans-3423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE694]'
  id: totrans-3424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE694]'
- en: XML Final Notes
  id: totrans-3425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All XML documents adhere to [W3C specifications](https://www.w3.org/TR/xml/).
    Both `etree` and `lxml` parsers will fail to parse any markup document that is
    not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document
    unless it follows XHTML specs. However, other popular markup types including KML,
    XAML, RSS, MusicML, MathML are compliant [XML schemas](https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas).
  id: totrans-3426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For above reason, if your application builds XML prior to pandas operations,
    use appropriate DOM libraries like `etree` and `lxml` to build the necessary document
    and not by string concatenation or regex adjustments. Always remember XML is a
    *special* text file with markup rules.
  id: totrans-3427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With very large XML files (several hundred MBs to GBs), XPath and XSLT can become
    memory-intensive operations. Be sure to have enough available RAM for reading
    and writing to large XML files (roughly about 5 times the size of text).
  id: totrans-3428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because XSLT is a programming language, use it with caution since such scripts
    can pose a security risk in your environment and can run large or infinite recursive
    operations. Always test scripts on small fragments before full run.
  id: totrans-3429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [etree](https://docs.python.org/3/library/xml.etree.elementtree.html) parser
    supports all functionality of both `read_xml` and `to_xml` except for complex
    XPath and any XSLT. Though limited in features, `etree` is still a reliable and
    capable parser and tree builder. Its performance may trail `lxml` to a certain
    degree for larger files but relatively unnoticeable on small to medium size files.
  id: totrans-3430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '## Excel files'
  id: totrans-3431
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read Excel 2007+ (`.xlsx`) files using the `openpyxl`
    Python module. Excel 2003 (`.xls`) files can be read using `xlrd`. Binary Excel
    (`.xlsb`) files can be read using `pyxlsb`. All formats can be read using [calamine](#io-calamine)
    engine. The [`to_excel()`](../reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel
    "pandas.DataFrame.to_excel") instance method is used for saving a `DataFrame`
    to Excel. Generally the semantics are similar to working with [csv](#io-read-csv-table)
    data. See the [cookbook](cookbook.html#cookbook-excel) for some advanced strategies.
  id: totrans-3432
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3433
  prefs: []
  type: TYPE_NORMAL
- en: 'When `engine=None`, the following logic will be used to determine the engine:'
  id: totrans-3434
  prefs: []
  type: TYPE_NORMAL
- en: If `path_or_buffer` is an OpenDocument format (.odf, .ods, .odt), then [odf](https://pypi.org/project/odfpy/)
    will be used.
  id: totrans-3435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise if `path_or_buffer` is an xls format, `xlrd` will be used.
  id: totrans-3436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise if `path_or_buffer` is in xlsb format, `pyxlsb` will be used.
  id: totrans-3437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise `openpyxl` will be used.
  id: totrans-3438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### Reading Excel files'
  id: totrans-3439
  prefs: []
  type: TYPE_NORMAL
- en: In the most basic use-case, `read_excel` takes a path to an Excel file, and
    the `sheet_name` indicating which sheet to parse.
  id: totrans-3440
  prefs: []
  type: TYPE_NORMAL
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  id: totrans-3441
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.load_workbook()` to read
    in (`.xlsx`) and (`.xlsm`) files.
  id: totrans-3442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlrd, pandas is using `xlrd.open_workbook()` to read in (`.xls`)
    files.
  id: totrans-3443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine pyxlsb, pandas is using `pyxlsb.open_workbook()` to read in (`.xlsb`)
    files.
  id: totrans-3444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.load()` to read in (`.ods`)
    files.
  id: totrans-3445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine calamine, pandas is using `python_calamine.load_workbook()` to
    read in (`.xlsx`), (`.xlsm`), (`.xls`), (`.xlsb`), (`.ods`) files.
  id: totrans-3446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE695]'
  id: totrans-3447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE695]'
- en: '#### `ExcelFile` class'
  id: totrans-3448
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate working with multiple sheets from the same file, the `ExcelFile`
    class can be used to wrap the file and can be passed into `read_excel` There will
    be a performance benefit for reading multiple sheets as the file is read into
    memory only once.
  id: totrans-3449
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE696]'
  id: totrans-3450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE696]'
- en: The `ExcelFile` class can also be used as a context manager.
  id: totrans-3451
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE697]'
  id: totrans-3452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE697]'
- en: The `sheet_names` property will generate a list of the sheet names in the file.
  id: totrans-3453
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary use-case for an `ExcelFile` is parsing multiple sheets with different
    parameters:'
  id: totrans-3454
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE698]'
  id: totrans-3455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE698]'
- en: Note that if the same parsing parameters are used for all sheets, a list of
    sheet names can simply be passed to `read_excel` with no loss in performance.
  id: totrans-3456
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE699]'
  id: totrans-3457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE699]'
- en: '`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.
    This allows the user to control how the excel file is read. For example, sheets
    can be loaded on demand by calling `xlrd.open_workbook()` with `on_demand=True`.'
  id: totrans-3458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE700]  #### Specifying sheets'
  id: totrans-3459
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3460
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.
  id: totrans-3461
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3462
  prefs: []
  type: TYPE_NORMAL
- en: An ExcelFile’s attribute `sheet_names` provides access to a list of sheets.
  id: totrans-3463
  prefs: []
  type: TYPE_NORMAL
- en: The arguments `sheet_name` allows specifying the sheet or sheets to read.
  id: totrans-3464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default value for `sheet_name` is 0, indicating to read the first sheet
  id: totrans-3465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a string to refer to the name of a particular sheet in the workbook.
  id: totrans-3466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass an integer to refer to the index of a sheet. Indices follow Python convention,
    beginning at 0.
  id: totrans-3467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a list of either strings or integers, to return a dictionary of specified
    sheets.
  id: totrans-3468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a `None` to return a dictionary of all available sheets.
  id: totrans-3469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE701]'
  id: totrans-3470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE701]'
- en: 'Using the sheet index:'
  id: totrans-3471
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE702]'
  id: totrans-3472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE702]'
- en: 'Using all default values:'
  id: totrans-3473
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE703]'
  id: totrans-3474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE703]'
- en: 'Using None to get all sheets:'
  id: totrans-3475
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE704]'
  id: totrans-3476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE704]'
- en: 'Using a list to get multiple sheets:'
  id: totrans-3477
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE705]'
  id: totrans-3478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE705]'
- en: '`read_excel` can read more than one sheet, by setting `sheet_name` to either
    a list of sheet names, a list of sheet positions, or `None` to read all sheets.
    Sheets can be specified by sheet index or sheet name, using an integer or string,
    respectively.  #### Reading a `MultiIndex`'
  id: totrans-3479
  prefs: []
  type: TYPE_NORMAL
- en: '`read_excel` can read a `MultiIndex` index, by passing a list of columns to
    `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If
    either the `index` or `columns` have serialized level names those will be read
    in as well by specifying the rows/columns that make up the levels.'
  id: totrans-3480
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to read in a `MultiIndex` index without names:'
  id: totrans-3481
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE706]'
  id: totrans-3482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE706]'
- en: If the index has level names, they will parsed as well, using the same parameters.
  id: totrans-3483
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE707]'
  id: totrans-3484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE707]'
- en: 'If the source file has both `MultiIndex` index and columns, lists specifying
    each should be passed to `index_col` and `header`:'
  id: totrans-3485
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE708]'
  id: totrans-3486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE708]'
- en: Missing values in columns specified in `index_col` will be forward filled to
    allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward
    filling the missing values use `set_index` after reading the data instead of `index_col`.
  id: totrans-3487
  prefs: []
  type: TYPE_NORMAL
- en: Parsing specific columns
  id: totrans-3488
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is often the case that users will insert columns to do temporary computations
    in Excel and you may not want to read in those columns. `read_excel` takes a `usecols`
    keyword to allow you to specify a subset of columns to parse.
  id: totrans-3489
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a comma-delimited set of Excel columns and ranges as a string:'
  id: totrans-3490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE709]'
  id: totrans-3491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE709]'
- en: If `usecols` is a list of integers, then it is assumed to be the file column
    indices to be parsed.
  id: totrans-3492
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE710]'
  id: totrans-3493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE710]'
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.
  id: totrans-3494
  prefs: []
  type: TYPE_NORMAL
- en: 'If `usecols` is a list of strings, it is assumed that each string corresponds
    to a column name provided either by the user in `names` or inferred from the document
    header row(s). Those strings define which columns will be parsed:'
  id: totrans-3495
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE711]'
  id: totrans-3496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE711]'
- en: Element order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',
    'baz']`.
  id: totrans-3497
  prefs: []
  type: TYPE_NORMAL
- en: If `usecols` is callable, the callable function will be evaluated against the
    column names, returning names where the callable function evaluates to `True`.
  id: totrans-3498
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE712]'
  id: totrans-3499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE712]'
- en: Parsing dates
  id: totrans-3500
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Datetime-like values are normally automatically converted to the appropriate
    dtype when reading the excel file. But if you have a column of strings that *look*
    like dates (but are not actually formatted as dates in excel), you can use the
    `parse_dates` keyword to parse those strings to datetimes:'
  id: totrans-3501
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE713]'
  id: totrans-3502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE713]'
- en: Cell converters
  id: totrans-3503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is possible to transform the contents of Excel cells via the `converters`
    option. For instance, to convert a column to boolean:'
  id: totrans-3504
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE714]'
  id: totrans-3505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE714]'
- en: 'This options handles missing values and treats exceptions in the converters
    as missing data. Transformations are applied cell by cell rather than to the column
    as a whole, so the array dtype is not guaranteed. For instance, a column of integers
    with missing values cannot be transformed to an array with integer dtype, because
    NaN is strictly a float. You can manually mask missing data to recover integer
    dtype:'
  id: totrans-3506
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE715]'
  id: totrans-3507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE715]'
- en: Dtype specifications
  id: totrans-3508
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to converters, the type for an entire column can be specified
    using the `dtype` keyword, which takes a dictionary mapping column names to types.
    To interpret data with no type inference, use the type `str` or `object`.
  id: totrans-3509
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE716]  ### Writing Excel files'
  id: totrans-3510
  prefs: []
  type: TYPE_NORMAL
- en: Writing Excel files to disk
  id: totrans-3511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To write a `DataFrame` object to a sheet of an Excel file, you can use the
    `to_excel` instance method. The arguments are largely the same as `to_csv` described
    above, the first argument being the name of the excel file, and the optional second
    argument the name of the sheet to which the `DataFrame` should be written. For
    example:'
  id: totrans-3512
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE717]'
  id: totrans-3513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE717]'
- en: Files with a `.xlsx` extension will be written using `xlsxwriter` (if available)
    or `openpyxl`.
  id: totrans-3514
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataFrame` will be written in a way that tries to mimic the REPL output.
    The `index_label` will be placed in the second row instead of the first. You can
    place it in the first row by setting the `merge_cells` option in `to_excel()`
    to `False`:'
  id: totrans-3515
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE718]'
  id: totrans-3516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE718]'
- en: In order to write separate `DataFrames` to separate sheets in a single Excel
    file, one can pass an `ExcelWriter`.
  id: totrans-3517
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE719]'
  id: totrans-3518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE719]'
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  id: totrans-3519
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.Workbook()` to create a new
    sheet and `openpyxl.load_workbook()` to append data to an existing sheet. The
    openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  id: totrans-3520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlsxwriter, pandas is using `xlsxwriter.Workbook()` to write
    to (`.xlsx`) files.
  id: totrans-3521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.OpenDocumentSpreadsheet()`
    to write to (`.ods`) files.
  id: totrans-3522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Excel files to memory
  id: totrans-3523
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: pandas supports writing Excel files to buffer-like objects such as `StringIO`
    or `BytesIO` using `ExcelWriter`.
  id: totrans-3524
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE720]'
  id: totrans-3525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE720]'
- en: Note
  id: totrans-3526
  prefs: []
  type: TYPE_NORMAL
- en: '`engine` is optional but recommended. Setting the engine determines the version
    of workbook produced. Setting `engine=''xlrd''` will produce an Excel 2003-format
    workbook (xls). Using either `''openpyxl''` or `''xlsxwriter''` will produce an
    Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook
    is produced.  ### Excel writer engines'
  id: totrans-3527
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas chooses an Excel writer via two methods:'
  id: totrans-3528
  prefs: []
  type: TYPE_NORMAL
- en: the `engine` keyword argument
  id: totrans-3529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the filename extension (via the default specified in config options)
  id: totrans-3530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, pandas uses the [XlsxWriter](https://xlsxwriter.readthedocs.io)
    for `.xlsx`, [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsm`. If you
    have multiple engines installed, you can set the default engine through [setting
    the config options](options.html#options) `io.excel.xlsx.writer` and `io.excel.xls.writer`.
    pandas will fall back on [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsx`
    files if [Xlsxwriter](https://xlsxwriter.readthedocs.io) is not available.
  id: totrans-3531
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify which writer you want to use, you can pass an engine keyword argument
    to `to_excel` and to `ExcelWriter`. The built-in engines are:'
  id: totrans-3532
  prefs: []
  type: TYPE_NORMAL
- en: '`openpyxl`: version 2.4 or higher is required'
  id: totrans-3533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xlsxwriter`'
  id: totrans-3534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE721]  ### Style and formatting'
  id: totrans-3535
  prefs: []
  type: TYPE_NORMAL
- en: The look and feel of Excel worksheets created from pandas can be modified using
    the following parameters on the `DataFrame`’s `to_excel` method.
  id: totrans-3536
  prefs: []
  type: TYPE_NORMAL
- en: '`float_format` : Format string for floating point numbers (default `None`).'
  id: totrans-3537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_panes` : A tuple of two integers representing the bottommost row and
    rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will
    freeze the first row and first column (default `None`).'
  id: totrans-3538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the [Xlsxwriter](https://xlsxwriter.readthedocs.io) engine provides many
    options for controlling the format of an Excel worksheet created with the `to_excel`
    method. Excellent examples can be found in the [Xlsxwriter](https://xlsxwriter.readthedocs.io)
    documentation here: [https://xlsxwriter.readthedocs.io/working_with_pandas.html](https://xlsxwriter.readthedocs.io/working_with_pandas.html)  ###
    Reading Excel files'
  id: totrans-3539
  prefs: []
  type: TYPE_NORMAL
- en: In the most basic use-case, `read_excel` takes a path to an Excel file, and
    the `sheet_name` indicating which sheet to parse.
  id: totrans-3540
  prefs: []
  type: TYPE_NORMAL
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  id: totrans-3541
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.load_workbook()` to read
    in (`.xlsx`) and (`.xlsm`) files.
  id: totrans-3542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlrd, pandas is using `xlrd.open_workbook()` to read in (`.xls`)
    files.
  id: totrans-3543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine pyxlsb, pandas is using `pyxlsb.open_workbook()` to read in (`.xlsb`)
    files.
  id: totrans-3544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.load()` to read in (`.ods`)
    files.
  id: totrans-3545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine calamine, pandas is using `python_calamine.load_workbook()` to
    read in (`.xlsx`), (`.xlsm`), (`.xls`), (`.xlsb`), (`.ods`) files.
  id: totrans-3546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE722]'
  id: totrans-3547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE722]'
- en: '#### `ExcelFile` class'
  id: totrans-3548
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate working with multiple sheets from the same file, the `ExcelFile`
    class can be used to wrap the file and can be passed into `read_excel` There will
    be a performance benefit for reading multiple sheets as the file is read into
    memory only once.
  id: totrans-3549
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE723]'
  id: totrans-3550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE723]'
- en: The `ExcelFile` class can also be used as a context manager.
  id: totrans-3551
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE724]'
  id: totrans-3552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE724]'
- en: The `sheet_names` property will generate a list of the sheet names in the file.
  id: totrans-3553
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary use-case for an `ExcelFile` is parsing multiple sheets with different
    parameters:'
  id: totrans-3554
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE725]'
  id: totrans-3555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE725]'
- en: Note that if the same parsing parameters are used for all sheets, a list of
    sheet names can simply be passed to `read_excel` with no loss in performance.
  id: totrans-3556
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE726]'
  id: totrans-3557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE726]'
- en: '`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.
    This allows the user to control how the excel file is read. For example, sheets
    can be loaded on demand by calling `xlrd.open_workbook()` with `on_demand=True`.'
  id: totrans-3558
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE727]  #### Specifying sheets'
  id: totrans-3559
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3560
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.
  id: totrans-3561
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3562
  prefs: []
  type: TYPE_NORMAL
- en: An ExcelFile’s attribute `sheet_names` provides access to a list of sheets.
  id: totrans-3563
  prefs: []
  type: TYPE_NORMAL
- en: The arguments `sheet_name` allows specifying the sheet or sheets to read.
  id: totrans-3564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default value for `sheet_name` is 0, indicating to read the first sheet
  id: totrans-3565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a string to refer to the name of a particular sheet in the workbook.
  id: totrans-3566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass an integer to refer to the index of a sheet. Indices follow Python convention,
    beginning at 0.
  id: totrans-3567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a list of either strings or integers, to return a dictionary of specified
    sheets.
  id: totrans-3568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a `None` to return a dictionary of all available sheets.
  id: totrans-3569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE728]'
  id: totrans-3570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE728]'
- en: 'Using the sheet index:'
  id: totrans-3571
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE729]'
  id: totrans-3572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE729]'
- en: 'Using all default values:'
  id: totrans-3573
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE730]'
  id: totrans-3574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE730]'
- en: 'Using None to get all sheets:'
  id: totrans-3575
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE731]'
  id: totrans-3576
  prefs: []
  type: TYPE_PRE
  zh: '[PRE731]'
- en: 'Using a list to get multiple sheets:'
  id: totrans-3577
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE732]'
  id: totrans-3578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE732]'
- en: '`read_excel` can read more than one sheet, by setting `sheet_name` to either
    a list of sheet names, a list of sheet positions, or `None` to read all sheets.
    Sheets can be specified by sheet index or sheet name, using an integer or string,
    respectively.  #### Reading a `MultiIndex`'
  id: totrans-3579
  prefs: []
  type: TYPE_NORMAL
- en: '`read_excel` can read a `MultiIndex` index, by passing a list of columns to
    `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If
    either the `index` or `columns` have serialized level names those will be read
    in as well by specifying the rows/columns that make up the levels.'
  id: totrans-3580
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to read in a `MultiIndex` index without names:'
  id: totrans-3581
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE733]'
  id: totrans-3582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE733]'
- en: If the index has level names, they will parsed as well, using the same parameters.
  id: totrans-3583
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE734]'
  id: totrans-3584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE734]'
- en: 'If the source file has both `MultiIndex` index and columns, lists specifying
    each should be passed to `index_col` and `header`:'
  id: totrans-3585
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE735]'
  id: totrans-3586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE735]'
- en: Missing values in columns specified in `index_col` will be forward filled to
    allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward
    filling the missing values use `set_index` after reading the data instead of `index_col`.
  id: totrans-3587
  prefs: []
  type: TYPE_NORMAL
- en: Parsing specific columns
  id: totrans-3588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is often the case that users will insert columns to do temporary computations
    in Excel and you may not want to read in those columns. `read_excel` takes a `usecols`
    keyword to allow you to specify a subset of columns to parse.
  id: totrans-3589
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a comma-delimited set of Excel columns and ranges as a string:'
  id: totrans-3590
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE736]'
  id: totrans-3591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE736]'
- en: If `usecols` is a list of integers, then it is assumed to be the file column
    indices to be parsed.
  id: totrans-3592
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE737]'
  id: totrans-3593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE737]'
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.
  id: totrans-3594
  prefs: []
  type: TYPE_NORMAL
- en: 'If `usecols` is a list of strings, it is assumed that each string corresponds
    to a column name provided either by the user in `names` or inferred from the document
    header row(s). Those strings define which columns will be parsed:'
  id: totrans-3595
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE738]'
  id: totrans-3596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE738]'
- en: Element order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',
    'baz']`.
  id: totrans-3597
  prefs: []
  type: TYPE_NORMAL
- en: If `usecols` is callable, the callable function will be evaluated against the
    column names, returning names where the callable function evaluates to `True`.
  id: totrans-3598
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE739]'
  id: totrans-3599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE739]'
- en: Parsing dates
  id: totrans-3600
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Datetime-like values are normally automatically converted to the appropriate
    dtype when reading the excel file. But if you have a column of strings that *look*
    like dates (but are not actually formatted as dates in excel), you can use the
    `parse_dates` keyword to parse those strings to datetimes:'
  id: totrans-3601
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE740]'
  id: totrans-3602
  prefs: []
  type: TYPE_PRE
  zh: '[PRE740]'
- en: Cell converters
  id: totrans-3603
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is possible to transform the contents of Excel cells via the `converters`
    option. For instance, to convert a column to boolean:'
  id: totrans-3604
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE741]'
  id: totrans-3605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE741]'
- en: 'This options handles missing values and treats exceptions in the converters
    as missing data. Transformations are applied cell by cell rather than to the column
    as a whole, so the array dtype is not guaranteed. For instance, a column of integers
    with missing values cannot be transformed to an array with integer dtype, because
    NaN is strictly a float. You can manually mask missing data to recover integer
    dtype:'
  id: totrans-3606
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE742]'
  id: totrans-3607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE742]'
- en: Dtype specifications
  id: totrans-3608
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to converters, the type for an entire column can be specified
    using the `dtype` keyword, which takes a dictionary mapping column names to types.
    To interpret data with no type inference, use the type `str` or `object`.
  id: totrans-3609
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE743]'
  id: totrans-3610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE743]'
- en: '#### `ExcelFile` class'
  id: totrans-3611
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate working with multiple sheets from the same file, the `ExcelFile`
    class can be used to wrap the file and can be passed into `read_excel` There will
    be a performance benefit for reading multiple sheets as the file is read into
    memory only once.
  id: totrans-3612
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE744]'
  id: totrans-3613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE744]'
- en: The `ExcelFile` class can also be used as a context manager.
  id: totrans-3614
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE745]'
  id: totrans-3615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE745]'
- en: The `sheet_names` property will generate a list of the sheet names in the file.
  id: totrans-3616
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary use-case for an `ExcelFile` is parsing multiple sheets with different
    parameters:'
  id: totrans-3617
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE746]'
  id: totrans-3618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE746]'
- en: Note that if the same parsing parameters are used for all sheets, a list of
    sheet names can simply be passed to `read_excel` with no loss in performance.
  id: totrans-3619
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE747]'
  id: totrans-3620
  prefs: []
  type: TYPE_PRE
  zh: '[PRE747]'
- en: '`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.
    This allows the user to control how the excel file is read. For example, sheets
    can be loaded on demand by calling `xlrd.open_workbook()` with `on_demand=True`.'
  id: totrans-3621
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE748]'
  id: totrans-3622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE748]'
- en: '#### Specifying sheets'
  id: totrans-3623
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3624
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.
  id: totrans-3625
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3626
  prefs: []
  type: TYPE_NORMAL
- en: An ExcelFile’s attribute `sheet_names` provides access to a list of sheets.
  id: totrans-3627
  prefs: []
  type: TYPE_NORMAL
- en: The arguments `sheet_name` allows specifying the sheet or sheets to read.
  id: totrans-3628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default value for `sheet_name` is 0, indicating to read the first sheet
  id: totrans-3629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a string to refer to the name of a particular sheet in the workbook.
  id: totrans-3630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass an integer to refer to the index of a sheet. Indices follow Python convention,
    beginning at 0.
  id: totrans-3631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a list of either strings or integers, to return a dictionary of specified
    sheets.
  id: totrans-3632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a `None` to return a dictionary of all available sheets.
  id: totrans-3633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE749]'
  id: totrans-3634
  prefs: []
  type: TYPE_PRE
  zh: '[PRE749]'
- en: 'Using the sheet index:'
  id: totrans-3635
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE750]'
  id: totrans-3636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE750]'
- en: 'Using all default values:'
  id: totrans-3637
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE751]'
  id: totrans-3638
  prefs: []
  type: TYPE_PRE
  zh: '[PRE751]'
- en: 'Using None to get all sheets:'
  id: totrans-3639
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE752]'
  id: totrans-3640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE752]'
- en: 'Using a list to get multiple sheets:'
  id: totrans-3641
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE753]'
  id: totrans-3642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE753]'
- en: '`read_excel` can read more than one sheet, by setting `sheet_name` to either
    a list of sheet names, a list of sheet positions, or `None` to read all sheets.
    Sheets can be specified by sheet index or sheet name, using an integer or string,
    respectively.'
  id: totrans-3643
  prefs: []
  type: TYPE_NORMAL
- en: '#### Reading a `MultiIndex`'
  id: totrans-3644
  prefs: []
  type: TYPE_NORMAL
- en: '`read_excel` can read a `MultiIndex` index, by passing a list of columns to
    `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If
    either the `index` or `columns` have serialized level names those will be read
    in as well by specifying the rows/columns that make up the levels.'
  id: totrans-3645
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to read in a `MultiIndex` index without names:'
  id: totrans-3646
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE754]'
  id: totrans-3647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE754]'
- en: If the index has level names, they will parsed as well, using the same parameters.
  id: totrans-3648
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE755]'
  id: totrans-3649
  prefs: []
  type: TYPE_PRE
  zh: '[PRE755]'
- en: 'If the source file has both `MultiIndex` index and columns, lists specifying
    each should be passed to `index_col` and `header`:'
  id: totrans-3650
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE756]'
  id: totrans-3651
  prefs: []
  type: TYPE_PRE
  zh: '[PRE756]'
- en: Missing values in columns specified in `index_col` will be forward filled to
    allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward
    filling the missing values use `set_index` after reading the data instead of `index_col`.
  id: totrans-3652
  prefs: []
  type: TYPE_NORMAL
- en: Parsing specific columns
  id: totrans-3653
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is often the case that users will insert columns to do temporary computations
    in Excel and you may not want to read in those columns. `read_excel` takes a `usecols`
    keyword to allow you to specify a subset of columns to parse.
  id: totrans-3654
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a comma-delimited set of Excel columns and ranges as a string:'
  id: totrans-3655
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE757]'
  id: totrans-3656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE757]'
- en: If `usecols` is a list of integers, then it is assumed to be the file column
    indices to be parsed.
  id: totrans-3657
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE758]'
  id: totrans-3658
  prefs: []
  type: TYPE_PRE
  zh: '[PRE758]'
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.
  id: totrans-3659
  prefs: []
  type: TYPE_NORMAL
- en: 'If `usecols` is a list of strings, it is assumed that each string corresponds
    to a column name provided either by the user in `names` or inferred from the document
    header row(s). Those strings define which columns will be parsed:'
  id: totrans-3660
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE759]'
  id: totrans-3661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE759]'
- en: Element order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',
    'baz']`.
  id: totrans-3662
  prefs: []
  type: TYPE_NORMAL
- en: If `usecols` is callable, the callable function will be evaluated against the
    column names, returning names where the callable function evaluates to `True`.
  id: totrans-3663
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE760]'
  id: totrans-3664
  prefs: []
  type: TYPE_PRE
  zh: '[PRE760]'
- en: Parsing dates
  id: totrans-3665
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Datetime-like values are normally automatically converted to the appropriate
    dtype when reading the excel file. But if you have a column of strings that *look*
    like dates (but are not actually formatted as dates in excel), you can use the
    `parse_dates` keyword to parse those strings to datetimes:'
  id: totrans-3666
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE761]'
  id: totrans-3667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE761]'
- en: Cell converters
  id: totrans-3668
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is possible to transform the contents of Excel cells via the `converters`
    option. For instance, to convert a column to boolean:'
  id: totrans-3669
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE762]'
  id: totrans-3670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE762]'
- en: 'This options handles missing values and treats exceptions in the converters
    as missing data. Transformations are applied cell by cell rather than to the column
    as a whole, so the array dtype is not guaranteed. For instance, a column of integers
    with missing values cannot be transformed to an array with integer dtype, because
    NaN is strictly a float. You can manually mask missing data to recover integer
    dtype:'
  id: totrans-3671
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE763]'
  id: totrans-3672
  prefs: []
  type: TYPE_PRE
  zh: '[PRE763]'
- en: Dtype specifications
  id: totrans-3673
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to converters, the type for an entire column can be specified
    using the `dtype` keyword, which takes a dictionary mapping column names to types.
    To interpret data with no type inference, use the type `str` or `object`.
  id: totrans-3674
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE764]'
  id: totrans-3675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE764]'
- en: '### Writing Excel files'
  id: totrans-3676
  prefs: []
  type: TYPE_NORMAL
- en: Writing Excel files to disk
  id: totrans-3677
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To write a `DataFrame` object to a sheet of an Excel file, you can use the
    `to_excel` instance method. The arguments are largely the same as `to_csv` described
    above, the first argument being the name of the excel file, and the optional second
    argument the name of the sheet to which the `DataFrame` should be written. For
    example:'
  id: totrans-3678
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE765]'
  id: totrans-3679
  prefs: []
  type: TYPE_PRE
  zh: '[PRE765]'
- en: Files with a `.xlsx` extension will be written using `xlsxwriter` (if available)
    or `openpyxl`.
  id: totrans-3680
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataFrame` will be written in a way that tries to mimic the REPL output.
    The `index_label` will be placed in the second row instead of the first. You can
    place it in the first row by setting the `merge_cells` option in `to_excel()`
    to `False`:'
  id: totrans-3681
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE766]'
  id: totrans-3682
  prefs: []
  type: TYPE_PRE
  zh: '[PRE766]'
- en: In order to write separate `DataFrames` to separate sheets in a single Excel
    file, one can pass an `ExcelWriter`.
  id: totrans-3683
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE767]'
  id: totrans-3684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE767]'
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  id: totrans-3685
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.Workbook()` to create a new
    sheet and `openpyxl.load_workbook()` to append data to an existing sheet. The
    openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  id: totrans-3686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlsxwriter, pandas is using `xlsxwriter.Workbook()` to write
    to (`.xlsx`) files.
  id: totrans-3687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.OpenDocumentSpreadsheet()`
    to write to (`.ods`) files.
  id: totrans-3688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Excel files to memory
  id: totrans-3689
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: pandas supports writing Excel files to buffer-like objects such as `StringIO`
    or `BytesIO` using `ExcelWriter`.
  id: totrans-3690
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE768]'
  id: totrans-3691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE768]'
- en: Note
  id: totrans-3692
  prefs: []
  type: TYPE_NORMAL
- en: '`engine` is optional but recommended. Setting the engine determines the version
    of workbook produced. Setting `engine=''xlrd''` will produce an Excel 2003-format
    workbook (xls). Using either `''openpyxl''` or `''xlsxwriter''` will produce an
    Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook
    is produced.'
  id: totrans-3693
  prefs: []
  type: TYPE_NORMAL
- en: Writing Excel files to disk
  id: totrans-3694
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To write a `DataFrame` object to a sheet of an Excel file, you can use the
    `to_excel` instance method. The arguments are largely the same as `to_csv` described
    above, the first argument being the name of the excel file, and the optional second
    argument the name of the sheet to which the `DataFrame` should be written. For
    example:'
  id: totrans-3695
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE769]'
  id: totrans-3696
  prefs: []
  type: TYPE_PRE
  zh: '[PRE769]'
- en: Files with a `.xlsx` extension will be written using `xlsxwriter` (if available)
    or `openpyxl`.
  id: totrans-3697
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataFrame` will be written in a way that tries to mimic the REPL output.
    The `index_label` will be placed in the second row instead of the first. You can
    place it in the first row by setting the `merge_cells` option in `to_excel()`
    to `False`:'
  id: totrans-3698
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE770]'
  id: totrans-3699
  prefs: []
  type: TYPE_PRE
  zh: '[PRE770]'
- en: In order to write separate `DataFrames` to separate sheets in a single Excel
    file, one can pass an `ExcelWriter`.
  id: totrans-3700
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE771]'
  id: totrans-3701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE771]'
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  id: totrans-3702
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.Workbook()` to create a new
    sheet and `openpyxl.load_workbook()` to append data to an existing sheet. The
    openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  id: totrans-3703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlsxwriter, pandas is using `xlsxwriter.Workbook()` to write
    to (`.xlsx`) files.
  id: totrans-3704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.OpenDocumentSpreadsheet()`
    to write to (`.ods`) files.
  id: totrans-3705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Excel files to memory
  id: totrans-3706
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: pandas supports writing Excel files to buffer-like objects such as `StringIO`
    or `BytesIO` using `ExcelWriter`.
  id: totrans-3707
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE772]'
  id: totrans-3708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE772]'
- en: Note
  id: totrans-3709
  prefs: []
  type: TYPE_NORMAL
- en: '`engine` is optional but recommended. Setting the engine determines the version
    of workbook produced. Setting `engine=''xlrd''` will produce an Excel 2003-format
    workbook (xls). Using either `''openpyxl''` or `''xlsxwriter''` will produce an
    Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook
    is produced.'
  id: totrans-3710
  prefs: []
  type: TYPE_NORMAL
- en: '### Excel writer engines'
  id: totrans-3711
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas chooses an Excel writer via two methods:'
  id: totrans-3712
  prefs: []
  type: TYPE_NORMAL
- en: the `engine` keyword argument
  id: totrans-3713
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the filename extension (via the default specified in config options)
  id: totrans-3714
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, pandas uses the [XlsxWriter](https://xlsxwriter.readthedocs.io)
    for `.xlsx`, [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsm`. If you
    have multiple engines installed, you can set the default engine through [setting
    the config options](options.html#options) `io.excel.xlsx.writer` and `io.excel.xls.writer`.
    pandas will fall back on [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsx`
    files if [Xlsxwriter](https://xlsxwriter.readthedocs.io) is not available.
  id: totrans-3715
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify which writer you want to use, you can pass an engine keyword argument
    to `to_excel` and to `ExcelWriter`. The built-in engines are:'
  id: totrans-3716
  prefs: []
  type: TYPE_NORMAL
- en: '`openpyxl`: version 2.4 or higher is required'
  id: totrans-3717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xlsxwriter`'
  id: totrans-3718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE773]'
  id: totrans-3719
  prefs: []
  type: TYPE_PRE
  zh: '[PRE773]'
- en: '### Style and formatting'
  id: totrans-3720
  prefs: []
  type: TYPE_NORMAL
- en: The look and feel of Excel worksheets created from pandas can be modified using
    the following parameters on the `DataFrame`’s `to_excel` method.
  id: totrans-3721
  prefs: []
  type: TYPE_NORMAL
- en: '`float_format` : Format string for floating point numbers (default `None`).'
  id: totrans-3722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_panes` : A tuple of two integers representing the bottommost row and
    rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will
    freeze the first row and first column (default `None`).'
  id: totrans-3723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the [Xlsxwriter](https://xlsxwriter.readthedocs.io) engine provides many
    options for controlling the format of an Excel worksheet created with the `to_excel`
    method. Excellent examples can be found in the [Xlsxwriter](https://xlsxwriter.readthedocs.io)
    documentation here: [https://xlsxwriter.readthedocs.io/working_with_pandas.html](https://xlsxwriter.readthedocs.io/working_with_pandas.html)'
  id: totrans-3724
  prefs: []
  type: TYPE_NORMAL
- en: '## OpenDocument Spreadsheets'
  id: totrans-3725
  prefs: []
  type: TYPE_NORMAL
- en: The io methods for [Excel files](#excel-files) also support reading and writing
    OpenDocument spreadsheets using the [odfpy](https://pypi.org/project/odfpy/) module.
    The semantics and features for reading and writing OpenDocument spreadsheets match
    what can be done for [Excel files](#excel-files) using `engine='odf'`. The optional
    dependency ‘odfpy’ needs to be installed.
  id: totrans-3726
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read OpenDocument spreadsheets
  id: totrans-3727
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE774]'
  id: totrans-3728
  prefs: []
  type: TYPE_PRE
  zh: '[PRE774]'
- en: Similarly, the `to_excel()` method can write OpenDocument spreadsheets
  id: totrans-3729
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE775]'
  id: totrans-3730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE775]'
- en: '## Binary Excel (.xlsb) files'
  id: totrans-3731
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can also read binary Excel files using the `pyxlsb`
    module. The semantics and features for reading binary Excel files mostly match
    what can be done for [Excel files](#excel-files) using `engine='pyxlsb'`. `pyxlsb`
    does not recognize datetime types in files and will return floats instead (you
    can use [calamine](#io-calamine) if you need recognize datetime types).
  id: totrans-3732
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE776]'
  id: totrans-3733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE776]'
- en: Note
  id: totrans-3734
  prefs: []
  type: TYPE_NORMAL
- en: Currently pandas only supports *reading* binary Excel files. Writing is not
    implemented.
  id: totrans-3735
  prefs: []
  type: TYPE_NORMAL
- en: '## Calamine (Excel and ODS files)'
  id: totrans-3736
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read Excel file (`.xlsx`, `.xlsm`, `.xls`, `.xlsb`)
    and OpenDocument spreadsheets (`.ods`) using the `python-calamine` module. This
    module is a binding for Rust library [calamine](https://crates.io/crates/calamine)
    and is faster than other engines in most cases. The optional dependency ‘python-calamine’
    needs to be installed.
  id: totrans-3737
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE777]'
  id: totrans-3738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE777]'
- en: '## Clipboard'
  id: totrans-3739
  prefs: []
  type: TYPE_NORMAL
- en: 'A handy way to grab data is to use the `read_clipboard()` method, which takes
    the contents of the clipboard buffer and passes them to the `read_csv` method.
    For instance, you can copy the following text to the clipboard (CTRL-C on many
    operating systems):'
  id: totrans-3740
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE778]'
  id: totrans-3741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE778]'
- en: 'And then import the data directly to a `DataFrame` by calling:'
  id: totrans-3742
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE779]'
  id: totrans-3743
  prefs: []
  type: TYPE_PRE
  zh: '[PRE779]'
- en: The `to_clipboard` method can be used to write the contents of a `DataFrame`
    to the clipboard. Following which you can paste the clipboard contents into other
    applications (CTRL-V on many operating systems). Here we illustrate writing a
    `DataFrame` into clipboard and reading it back.
  id: totrans-3744
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE780]'
  id: totrans-3745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE780]'
- en: We can see that we got the same content back, which we had earlier written to
    the clipboard.
  id: totrans-3746
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3747
  prefs: []
  type: TYPE_NORMAL
- en: You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to
    use these methods.
  id: totrans-3748
  prefs: []
  type: TYPE_NORMAL
- en: '## Pickling'
  id: totrans-3749
  prefs: []
  type: TYPE_NORMAL
- en: All pandas objects are equipped with `to_pickle` methods which use Python’s
    `cPickle` module to save data structures to disk using the pickle format.
  id: totrans-3750
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE781]'
  id: totrans-3751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE781]'
- en: 'The `read_pickle` function in the `pandas` namespace can be used to load any
    pickled pandas object (or any other pickled object) from file:'
  id: totrans-3752
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE782]'
  id: totrans-3753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE782]'
- en: Warning
  id: totrans-3754
  prefs: []
  type: TYPE_NORMAL
- en: Loading pickled data received from untrusted sources can be unsafe.
  id: totrans-3755
  prefs: []
  type: TYPE_NORMAL
- en: 'See: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)'
  id: totrans-3756
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3757
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle") is only guaranteed backwards compatible back to a few minor
    release.'
  id: totrans-3758
  prefs: []
  type: TYPE_NORMAL
- en: '### Compressed pickle files'
  id: totrans-3759
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle"), [`DataFrame.to_pickle()`](../reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle
    "pandas.DataFrame.to_pickle") and [`Series.to_pickle()`](../reference/api/pandas.Series.to_pickle.html#pandas.Series.to_pickle
    "pandas.Series.to_pickle") can read and write compressed pickle files. The compression
    types of `gzip`, `bz2`, `xz`, `zstd` are supported for reading and writing. The
    `zip` file format only supports reading and must contain only one data file to
    be read.'
  id: totrans-3760
  prefs: []
  type: TYPE_NORMAL
- en: The compression type can be an explicit parameter or be inferred from the file
    extension. If ‘infer’, then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename
    ends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.
  id: totrans-3761
  prefs: []
  type: TYPE_NORMAL
- en: The compression parameter can also be a `dict` in order to pass options to the
    compression protocol. It must have a `'method'` key set to the name of the compression
    protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`, `'xz'`, `'zstd'`}.
    All other key-value pairs are passed to the underlying compression library.
  id: totrans-3762
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE783]'
  id: totrans-3763
  prefs: []
  type: TYPE_PRE
  zh: '[PRE783]'
- en: 'Using an explicit compression type:'
  id: totrans-3764
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE784]'
  id: totrans-3765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE784]'
- en: 'Inferring compression type from the extension:'
  id: totrans-3766
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE785]'
  id: totrans-3767
  prefs: []
  type: TYPE_PRE
  zh: '[PRE785]'
- en: 'The default is to ‘infer’:'
  id: totrans-3768
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE786]'
  id: totrans-3769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE786]'
- en: 'Passing options to the compression protocol in order to speed up compression:'
  id: totrans-3770
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE787]  ### Compressed pickle files'
  id: totrans-3771
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle"), [`DataFrame.to_pickle()`](../reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle
    "pandas.DataFrame.to_pickle") and [`Series.to_pickle()`](../reference/api/pandas.Series.to_pickle.html#pandas.Series.to_pickle
    "pandas.Series.to_pickle") can read and write compressed pickle files. The compression
    types of `gzip`, `bz2`, `xz`, `zstd` are supported for reading and writing. The
    `zip` file format only supports reading and must contain only one data file to
    be read.'
  id: totrans-3772
  prefs: []
  type: TYPE_NORMAL
- en: The compression type can be an explicit parameter or be inferred from the file
    extension. If ‘infer’, then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename
    ends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.
  id: totrans-3773
  prefs: []
  type: TYPE_NORMAL
- en: The compression parameter can also be a `dict` in order to pass options to the
    compression protocol. It must have a `'method'` key set to the name of the compression
    protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`, `'xz'`, `'zstd'`}.
    All other key-value pairs are passed to the underlying compression library.
  id: totrans-3774
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE788]'
  id: totrans-3775
  prefs: []
  type: TYPE_PRE
  zh: '[PRE788]'
- en: 'Using an explicit compression type:'
  id: totrans-3776
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE789]'
  id: totrans-3777
  prefs: []
  type: TYPE_PRE
  zh: '[PRE789]'
- en: 'Inferring compression type from the extension:'
  id: totrans-3778
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE790]'
  id: totrans-3779
  prefs: []
  type: TYPE_PRE
  zh: '[PRE790]'
- en: 'The default is to ‘infer’:'
  id: totrans-3780
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE791]'
  id: totrans-3781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE791]'
- en: 'Passing options to the compression protocol in order to speed up compression:'
  id: totrans-3782
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE792]'
  id: totrans-3783
  prefs: []
  type: TYPE_PRE
  zh: '[PRE792]'
- en: '## msgpack'
  id: totrans-3784
  prefs: []
  type: TYPE_NORMAL
- en: pandas support for `msgpack` has been removed in version 1.0.0\. It is recommended
    to use [pickle](#io-pickle) instead.
  id: totrans-3785
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can also the Arrow IPC serialization format for on-the-wire
    transmission of pandas objects. For documentation on pyarrow, see [here](https://arrow.apache.org/docs/python/ipc.html).
  id: totrans-3786
  prefs: []
  type: TYPE_NORMAL
- en: '## HDF5 (PyTables)'
  id: totrans-3787
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is a dict-like object which reads and writes pandas using the high
    performance HDF5 format using the excellent [PyTables](https://www.pytables.org/)
    library. See the [cookbook](cookbook.html#cookbook-hdf) for some advanced strategies'
  id: totrans-3788
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3789
  prefs: []
  type: TYPE_NORMAL
- en: pandas uses PyTables for reading and writing HDF5 files, which allows serializing
    object-dtype data with pickle. Loading pickled data received from untrusted sources
    can be unsafe.
  id: totrans-3790
  prefs: []
  type: TYPE_NORMAL
- en: 'See: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)
    for more.'
  id: totrans-3791
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE793]'
  id: totrans-3792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE793]'
- en: 'Objects can be written to the file just like adding key-value pairs to a dict:'
  id: totrans-3793
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE794]'
  id: totrans-3794
  prefs: []
  type: TYPE_PRE
  zh: '[PRE794]'
- en: 'In a current or later Python session, you can retrieve stored objects:'
  id: totrans-3795
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE795]'
  id: totrans-3796
  prefs: []
  type: TYPE_PRE
  zh: '[PRE795]'
- en: 'Deletion of the object specified by the key:'
  id: totrans-3797
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE796]'
  id: totrans-3798
  prefs: []
  type: TYPE_PRE
  zh: '[PRE796]'
- en: 'Closing a Store and using a context manager:'
  id: totrans-3799
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE797]'
  id: totrans-3800
  prefs: []
  type: TYPE_PRE
  zh: '[PRE797]'
- en: Read/write API
  id: totrans-3801
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf`
    for writing, similar to how `read_csv` and `to_csv` work.'
  id: totrans-3802
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE798]'
  id: totrans-3803
  prefs: []
  type: TYPE_PRE
  zh: '[PRE798]'
- en: HDFStore will by default not drop rows that are all missing. This behavior can
    be changed by setting `dropna=True`.
  id: totrans-3804
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE799]'
  id: totrans-3805
  prefs: []
  type: TYPE_PRE
  zh: '[PRE799]'
- en: '### Fixed format'
  id: totrans-3806
  prefs: []
  type: TYPE_NORMAL
- en: The examples above show storing using `put`, which write the HDF5 to `PyTables`
    in a fixed array format, called the `fixed` format. These types of stores are
    **not** appendable once written (though you can simply remove them and rewrite).
    Nor are they **queryable**; they must be retrieved in their entirety. They also
    do not support dataframes with non-unique column names. The `fixed` format stores
    offer very fast writing and slightly faster reading than `table` stores. This
    format is specified by default when using `put` or `to_hdf` or by `format='fixed'`
    or `format='f'`.
  id: totrans-3807
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3808
  prefs: []
  type: TYPE_NORMAL
- en: 'A `fixed` format will raise a `TypeError` if you try to retrieve using a `where`:'
  id: totrans-3809
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE800]  ### Table format'
  id: totrans-3810
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` supports another `PyTables` format on disk, the `table` format.
    Conceptually a `table` is shaped very much like a DataFrame, with rows and columns.
    A `table` may be appended to in the same or other sessions. In addition, delete
    and query type operations are supported. This format is specified by `format=''table''`
    or `format=''t''` to `append` or `put` or `to_hdf`.'
  id: totrans-3811
  prefs: []
  type: TYPE_NORMAL
- en: This format can be set as an option as well `pd.set_option('io.hdf.default_format','table')`
    to enable `put/append/to_hdf` to by default store in the `table` format.
  id: totrans-3812
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE801]'
  id: totrans-3813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE801]'
- en: Note
  id: totrans-3814
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also create a `table` by passing `format=''table''` or `format=''t''`
    to a `put` operation.  ### Hierarchical keys'
  id: totrans-3815
  prefs: []
  type: TYPE_NORMAL
- en: Keys to a store can be specified as a string. These can be in a hierarchical
    path-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of
    sub-stores (or `Groups` in PyTables parlance). Keys can be specified without the
    leading ‘/’ and are **always** absolute (e.g. ‘foo’ refers to ‘/foo’). Removal
    operations can remove everything in the sub-store and **below**, so be *careful*.
  id: totrans-3816
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE802]'
  id: totrans-3817
  prefs: []
  type: TYPE_PRE
  zh: '[PRE802]'
- en: You can walk through the group hierarchy using the `walk` method which will
    yield a tuple for each group key along with the relative keys of its contents.
  id: totrans-3818
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE803]'
  id: totrans-3819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE803]'
- en: Warning
  id: totrans-3820
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical keys cannot be retrieved as dotted (attribute) access as described
    above for items stored under the root node.
  id: totrans-3821
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE804]'
  id: totrans-3822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE804]'
- en: '[PRE805]'
  id: totrans-3823
  prefs: []
  type: TYPE_PRE
  zh: '[PRE805]'
- en: 'Instead, use explicit string based keys:'
  id: totrans-3824
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE806]  ### Storing types'
  id: totrans-3825
  prefs: []
  type: TYPE_NORMAL
- en: Storing mixed types in a table
  id: totrans-3826
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing mixed-dtype data is supported. Strings are stored as a fixed-width using
    the maximum size of the appended column. Subsequent attempts at appending longer
    strings will raise a `ValueError`.
  id: totrans-3827
  prefs: []
  type: TYPE_NORMAL
- en: 'Passing `min_itemsize={`values`: size}` as a parameter to append will set a
    larger minimum for the string columns. Storing `floats, strings, ints, bools,
    datetime64` are currently supported. For string columns, passing `nan_rep = ''nan''`
    to append will change the default nan representation on disk (which converts to/from
    `np.nan`), this defaults to `nan`.'
  id: totrans-3828
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE807]'
  id: totrans-3829
  prefs: []
  type: TYPE_PRE
  zh: '[PRE807]'
- en: Storing MultiIndex DataFrames
  id: totrans-3830
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting
    from homogeneous index `DataFrames`.
  id: totrans-3831
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE808]'
  id: totrans-3832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE808]'
- en: Note
  id: totrans-3833
  prefs: []
  type: TYPE_NORMAL
- en: 'The `index` keyword is reserved and cannot be use as a level name.  ### Querying'
  id: totrans-3834
  prefs: []
  type: TYPE_NORMAL
- en: Querying a table
  id: totrans-3835
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`select` and `delete` operations have an optional criterion that can be specified
    to select/delete only a subset of the data. This allows one to have a very large
    on-disk table and retrieve only a portion of the data.'
  id: totrans-3836
  prefs: []
  type: TYPE_NORMAL
- en: A query is specified using the `Term` class under the hood, as a boolean expression.
  id: totrans-3837
  prefs: []
  type: TYPE_NORMAL
- en: '`index` and `columns` are supported indexers of `DataFrames`.'
  id: totrans-3838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `data_columns` are specified, these can be used as additional indexers.
  id: totrans-3839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: level name in a MultiIndex, with default name `level_0`, `level_1`, … if not
    provided.
  id: totrans-3840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valid comparison operators are:'
  id: totrans-3841
  prefs: []
  type: TYPE_NORMAL
- en: '`=, ==, !=, >, >=, <, <=`'
  id: totrans-3842
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid boolean expressions are combined with:'
  id: totrans-3843
  prefs: []
  type: TYPE_NORMAL
- en: '`|` : or'
  id: totrans-3844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`&` : and'
  id: totrans-3845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(` and `)` : for grouping'
  id: totrans-3846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are similar to how boolean expressions are used in pandas for indexing.
  id: totrans-3847
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3848
  prefs: []
  type: TYPE_NORMAL
- en: '`=` will be automatically expanded to the comparison operator `==`'
  id: totrans-3849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~` is the not operator, but can only be used in very limited circumstances'
  id: totrans-3850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a list/tuple of expressions is passed they will be combined via `&`
  id: totrans-3851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are valid expressions:'
  id: totrans-3852
  prefs: []
  type: TYPE_NORMAL
- en: '`''index >= date''`'
  id: totrans-3853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns = [''A'', ''D'']"`'
  id: totrans-3854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns in [''A'', ''D'']"`'
  id: totrans-3855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns = A''`'
  id: totrans-3856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns == A''`'
  id: totrans-3857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"~(columns = [''A'', ''B''])"`'
  id: totrans-3858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''index > df.index[3] & string = "bar"''`'
  id: totrans-3859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
  id: totrans-3860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"ts >= Timestamp(''2012-02-01'')"`'
  id: totrans-3861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"major_axis>=20130101"`'
  id: totrans-3862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `indexers` are on the left-hand side of the sub-expression:'
  id: totrans-3863
  prefs: []
  type: TYPE_NORMAL
- en: '`columns`, `major_axis`, `ts`'
  id: totrans-3864
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-hand side of the sub-expression (after a comparison operator) can
    be:'
  id: totrans-3865
  prefs: []
  type: TYPE_NORMAL
- en: functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  id: totrans-3866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strings, e.g. `"bar"`
  id: totrans-3867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: date-like, e.g. `20130101`, or `"20130101"`
  id: totrans-3868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lists, e.g. `"['A', 'B']"`
  id: totrans-3869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variables that are defined in the local names space, e.g. `date`
  id: totrans-3870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-3871
  prefs: []
  type: TYPE_NORMAL
- en: Passing a string to a query by interpolating it into the query expression is
    not recommended. Simply assign the string of interest to a variable and use that
    variable in an expression. For example, do this
  id: totrans-3872
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE809]'
  id: totrans-3873
  prefs: []
  type: TYPE_PRE
  zh: '[PRE809]'
- en: instead of this
  id: totrans-3874
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE810]'
  id: totrans-3875
  prefs: []
  type: TYPE_PRE
  zh: '[PRE810]'
- en: The latter will **not** work and will raise a `SyntaxError`.Note that there’s
    a single quote followed by a double quote in the `string` variable.
  id: totrans-3876
  prefs: []
  type: TYPE_NORMAL
- en: If you *must* interpolate, use the `'%r'` format specifier
  id: totrans-3877
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE811]'
  id: totrans-3878
  prefs: []
  type: TYPE_PRE
  zh: '[PRE811]'
- en: which will quote `string`.
  id: totrans-3879
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  id: totrans-3880
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE812]'
  id: totrans-3881
  prefs: []
  type: TYPE_PRE
  zh: '[PRE812]'
- en: Use boolean expressions, with in-line function evaluation.
  id: totrans-3882
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE813]'
  id: totrans-3883
  prefs: []
  type: TYPE_PRE
  zh: '[PRE813]'
- en: Use inline column reference.
  id: totrans-3884
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE814]'
  id: totrans-3885
  prefs: []
  type: TYPE_PRE
  zh: '[PRE814]'
- en: 'The `columns` keyword can be supplied to select a list of columns to be returned,
    this is equivalent to passing a `''columns=list_of_columns_to_filter''`:'
  id: totrans-3886
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE815]'
  id: totrans-3887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE815]'
- en: '`start` and `stop` parameters can be specified to limit the total search space.
    These are in terms of the total number of rows in a table.'
  id: totrans-3888
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3889
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `ValueError` if the query expression has an unknown variable
    reference. Usually this means that you are trying to select on a column that is
    **not** a data_column.'
  id: totrans-3890
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `SyntaxError` if the query expression is not valid.'
  id: totrans-3891
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query timedelta64[ns]'
  id: totrans-3892
  prefs: []
  type: TYPE_NORMAL
- en: 'You can store and query using the `timedelta64[ns]` type. Terms can be specified
    in the format: `<float>(<unit>)`, where float may be signed (and fractional),
    and unit can be `D,s,ms,us,ns` for the timedelta. Here’s an example:'
  id: totrans-3893
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE816]  #### Query MultiIndex'
  id: totrans-3894
  prefs: []
  type: TYPE_NORMAL
- en: Selecting from a `MultiIndex` can be achieved by using the name of the level.
  id: totrans-3895
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE817]'
  id: totrans-3896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE817]'
- en: If the `MultiIndex` levels names are `None`, the levels are automatically made
    available via the `level_n` keyword with `n` the level of the `MultiIndex` you
    want to select from.
  id: totrans-3897
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE818]'
  id: totrans-3898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE818]'
- en: Indexing
  id: totrans-3899
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can create/modify an index for a table with `create_table_index` after data
    is already in the table (after and `append/put` operation). Creating a table index
    is **highly** encouraged. This will speed your queries a great deal when you use
    a `select` with the indexed dimension as the `where`.
  id: totrans-3900
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3901
  prefs: []
  type: TYPE_NORMAL
- en: Indexes are automagically created on the indexables and any data columns you
    specify. This behavior can be turned off by passing `index=False` to `append`.
  id: totrans-3902
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE819]'
  id: totrans-3903
  prefs: []
  type: TYPE_PRE
  zh: '[PRE819]'
- en: Oftentimes when appending large amounts of data to a store, it is useful to
    turn off index creation for each append, then recreate at the end.
  id: totrans-3904
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE820]'
  id: totrans-3905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE820]'
- en: Then create the index when finished appending.
  id: totrans-3906
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE821]'
  id: totrans-3907
  prefs: []
  type: TYPE_PRE
  zh: '[PRE821]'
- en: See [here](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)
    for how to create a completely-sorted-index (CSI) on an existing store.
  id: totrans-3908
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query via data columns'
  id: totrans-3909
  prefs: []
  type: TYPE_NORMAL
- en: You can designate (and index) certain columns that you want to be able to perform
    queries (other than the `indexable` columns, which you can always query). For
    instance say you want to perform this common operation, on-disk, and return just
    the frame that matches this query. You can specify `data_columns = True` to force
    all columns to be `data_columns`.
  id: totrans-3910
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE822]'
  id: totrans-3911
  prefs: []
  type: TYPE_PRE
  zh: '[PRE822]'
- en: There is some performance degradation by making lots of columns into `data columns`,
    so it is up to the user to designate these. In addition, you cannot change data
    columns (nor indexables) after the first append/put operation (Of course you can
    simply read in the data and create a new table!).
  id: totrans-3912
  prefs: []
  type: TYPE_NORMAL
- en: Iterator
  id: totrans-3913
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and
    `select_as_multiple` to return an iterator on the results. The default is 50,000
    rows returned in a chunk.
  id: totrans-3914
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE823]'
  id: totrans-3915
  prefs: []
  type: TYPE_PRE
  zh: '[PRE823]'
- en: Note
  id: totrans-3916
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the iterator with `read_hdf` which will open, then automatically
    close the store when finished iterating.
  id: totrans-3917
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE824]'
  id: totrans-3918
  prefs: []
  type: TYPE_PRE
  zh: '[PRE824]'
- en: Note, that the chunksize keyword applies to the **source** rows. So if you are
    doing a query, then the chunksize will subdivide the total rows in the table and
    the query applied, returning an iterator on potentially unequal sized chunks.
  id: totrans-3919
  prefs: []
  type: TYPE_NORMAL
- en: Here is a recipe for generating a query and using it to create equal sized return
    chunks.
  id: totrans-3920
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE825]'
  id: totrans-3921
  prefs: []
  type: TYPE_PRE
  zh: '[PRE825]'
- en: Advanced queries
  id: totrans-3922
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Select a single column
  id: totrans-3923
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  id: totrans-3924
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE826]'
  id: totrans-3925
  prefs: []
  type: TYPE_PRE
  zh: '[PRE826]'
- en: '##### Selecting coordinates'
  id: totrans-3926
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  id: totrans-3927
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE827]  ##### Selecting using a where mask'
  id: totrans-3928
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  id: totrans-3929
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE828]'
  id: totrans-3930
  prefs: []
  type: TYPE_PRE
  zh: '[PRE828]'
- en: Storer object
  id: totrans-3931
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  id: totrans-3932
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE829]'
  id: totrans-3933
  prefs: []
  type: TYPE_PRE
  zh: '[PRE829]'
- en: Multiple table queries
  id: totrans-3934
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The methods `append_to_multiple` and `select_as_multiple` can perform appending/selecting
    from multiple tables at once. The idea is to have one table (call it the selector
    table) that you index most/all of the columns, and perform your queries. The other
    table(s) are data tables with an index matching the selector table’s index. You
    can then perform a very fast query on the selector table, yet get lots of data
    back. This method is similar to having a very wide table, but enables more efficient
    queries.
  id: totrans-3935
  prefs: []
  type: TYPE_NORMAL
- en: The `append_to_multiple` method splits a given single DataFrame into multiple
    tables according to `d`, a dictionary that maps the table names to a list of ‘columns’
    you want in that table. If `None` is used in place of a list, that table will
    have the remaining unspecified columns of the given DataFrame. The argument `selector`
    defines which table is the selector table (which you can make queries from). The
    argument `dropna` will drop rows from the input `DataFrame` to ensure tables are
    synchronized. This means that if a row for one of the tables being written to
    is entirely `np.nan`, that row will be dropped from all tables.
  id: totrans-3936
  prefs: []
  type: TYPE_NORMAL
- en: If `dropna` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
    Remember that entirely `np.Nan` rows are not written to the HDFStore, so if you
    choose to call `dropna=False`, some tables may have more rows than others, and
    therefore `select_as_multiple` may not work or it may return unexpected results.
  id: totrans-3937
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE830]'
  id: totrans-3938
  prefs: []
  type: TYPE_PRE
  zh: '[PRE830]'
- en: Delete from a table
  id: totrans-3939
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can delete from a table selectively by specifying a `where`. In deleting
    rows, it is important to understand the `PyTables` deletes rows by erasing the
    rows, then **moving** the following data. Thus deleting can potentially be a very
    expensive operation depending on the orientation of your data. To get optimal
    performance, it’s worthwhile to have the dimension you are deleting be the first
    of the `indexables`.
  id: totrans-3940
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is ordered (on the disk) in terms of the `indexables`. Here’s a simple
    use case. You store panel-type data, with dates in the `major_axis` and ids in
    the `minor_axis`. The data is then interleaved like this:'
  id: totrans-3941
  prefs: []
  type: TYPE_NORMAL
- en: date_1
  id: totrans-3942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  id: totrans-3943
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_2
  id: totrans-3944
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  id: totrans-3945
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  id: totrans-3946
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: date_2
  id: totrans-3947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  id: totrans-3948
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  id: totrans-3949
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  id: totrans-3950
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be clear that a delete operation on the `major_axis` will be fairly
    quick, as one chunk is removed, then the following data moved. On the other hand
    a delete operation on the `minor_axis` will be very expensive. In this case it
    would almost certainly be faster to rewrite the table using a `where` that selects
    all but the missing data.
  id: totrans-3951
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3952
  prefs: []
  type: TYPE_NORMAL
- en: Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files automatically.
    Thus, repeatedly deleting (or removing nodes) and adding again, **WILL TEND TO
    INCREASE THE FILE SIZE**.
  id: totrans-3953
  prefs: []
  type: TYPE_NORMAL
- en: To *repack and clean* the file, use [ptrepack](#io-hdf5-ptrepack).
  id: totrans-3954
  prefs: []
  type: TYPE_NORMAL
- en: '### Notes & caveats'
  id: totrans-3955
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  id: totrans-3956
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`PyTables` allows the stored data to be compressed. This applies to all kinds
    of stores, not just tables. Two parameters are used to control compression: `complevel`
    and `complib`.'
  id: totrans-3957
  prefs: []
  type: TYPE_NORMAL
- en: '`complevel` specifies if and how hard data is to be compressed. `complevel=0`
    and `complevel=None` disables compression and `0<complevel<10` enables compression.'
  id: totrans-3958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`complib` specifies which compression library to use. If nothing is specified
    the default library `zlib` is used. A compression library usually optimizes for
    either good compression rates or speed and the results will depend on the type
    of data. Which type of compression to choose depends on your specific needs and
    data. The list of supported compression libraries:'
  id: totrans-3959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[zlib](https://zlib.net/): The default compression library. A classic in terms
    of compression, achieves good compression rates but is somewhat slow.'
  id: totrans-3960
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lzo](https://www.oberhumer.com/opensource/lzo/): Fast compression and decompression.'
  id: totrans-3961
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bzip2](https://sourceware.org/bzip2/): Good compression rates.'
  id: totrans-3962
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc](https://www.blosc.org/): Fast compression and decompression.'
  id: totrans-3963
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for alternative blosc compressors:'
  id: totrans-3964
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[blosc:blosclz](https://www.blosc.org/) This is the default compressor for
    `blosc`'
  id: totrans-3965
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html): A compact, very
    popular and fast compressor.'
  id: totrans-3966
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html): A tweaked version
    of LZ4, produces better compression ratios at the expense of speed.'
  id: totrans-3967
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:snappy](https://google.github.io/snappy/): A popular compressor used
    in many places.'
  id: totrans-3968
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zlib](https://zlib.net/): A classic; somewhat slower than the previous
    ones, but achieving better compression ratios.'
  id: totrans-3969
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zstd](https://facebook.github.io/zstd/): An extremely well balanced
    codec; it provides the best compression ratios among the others above, and at
    reasonably fast speed.'
  id: totrans-3970
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `complib` is defined as something other than the listed libraries a `ValueError`
    exception is issued.
  id: totrans-3971
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-3972
  prefs: []
  type: TYPE_NORMAL
- en: If the library specified with the `complib` option is missing on your platform,
    compression defaults to `zlib` without further ado.
  id: totrans-3973
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable compression for all objects within the file:'
  id: totrans-3974
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE831]'
  id: totrans-3975
  prefs: []
  type: TYPE_PRE
  zh: '[PRE831]'
- en: 'Or on-the-fly compression (this only applies to tables) in stores where compression
    is not enabled:'
  id: totrans-3976
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE832]'
  id: totrans-3977
  prefs: []
  type: TYPE_PRE
  zh: '[PRE832]'
- en: '#### ptrepack'
  id: totrans-3978
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` offers better write performance when tables are compressed after
    they are written, as opposed to turning on compression at the very beginning.
    You can use the supplied `PyTables` utility `ptrepack`. In addition, `ptrepack`
    can change compression levels after the fact.'
  id: totrans-3979
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE833]'
  id: totrans-3980
  prefs: []
  type: TYPE_PRE
  zh: '[PRE833]'
- en: 'Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow you to
    reuse previously deleted space. Alternatively, one can simply remove the file
    and write again, or use the `copy` method.  #### Caveats'
  id: totrans-3981
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3982
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is **not-threadsafe for writing**. The underlying `PyTables` only
    supports concurrent reads (via threading or processes). If you need reading and
    writing *at the same time*, you need to serialize these operations in a single
    thread in a single process. You will corrupt your data otherwise. See the ([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397)) for more information.'
  id: totrans-3983
  prefs: []
  type: TYPE_NORMAL
- en: If you use locks to manage write access between multiple processes, you may
    want to use [`fsync()`](https://docs.python.org/3/library/os.html#os.fsync "(in
    Python v3.12)") before releasing write locks. For convenience you can use `store.flush(fsync=True)`
    to do this for you.
  id: totrans-3984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a `table` is created columns (DataFrame) are fixed; only exactly the same
    columns can be appended
  id: totrans-3985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not necessarily
    equal across timezone versions. So if data is localized to a specific timezone
    in the HDFStore using one version of a timezone library and that data is updated
    with another version, the data will be converted to UTC since these timezones
    are not considered equal. Either use the same version of timezone library or use
    `tz_convert` with the updated timezone definition.
  id: totrans-3986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3987
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` will show a `NaturalNameWarning` if a column name cannot be used
    as an attribute selector. *Natural* identifiers contain only letters, numbers,
    and underscores, and may not begin with a number. Other identifiers cannot be
    used in a `where` clause and are generally a bad idea.  ### DataTypes'
  id: totrans-3988
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` will map an object dtype to the `PyTables` underlying dtype. This
    means the following types are known to work:'
  id: totrans-3989
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Represents missing values |'
  id: totrans-3990
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-3991
  prefs: []
  type: TYPE_TB
- en: '| floating : `float64, float32, float16` | `np.nan` |'
  id: totrans-3992
  prefs: []
  type: TYPE_TB
- en: '| integer : `int64, int32, int8, uint64,uint32, uint8` |  |'
  id: totrans-3993
  prefs: []
  type: TYPE_TB
- en: '| boolean |  |'
  id: totrans-3994
  prefs: []
  type: TYPE_TB
- en: '| `datetime64[ns]` | `NaT` |'
  id: totrans-3995
  prefs: []
  type: TYPE_TB
- en: '| `timedelta64[ns]` | `NaT` |'
  id: totrans-3996
  prefs: []
  type: TYPE_TB
- en: '| categorical : see the section below |  |'
  id: totrans-3997
  prefs: []
  type: TYPE_TB
- en: '| object : `strings` | `np.nan` |'
  id: totrans-3998
  prefs: []
  type: TYPE_TB
- en: '`unicode` columns are not supported, and **WILL FAIL**.'
  id: totrans-3999
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  id: totrans-4000
  prefs: []
  type: TYPE_NORMAL
- en: You can write data that contains `category` dtypes to a `HDFStore`. Queries
    work the same as if it was an object array. However, the `category` dtyped data
    is stored in a more efficient manner.
  id: totrans-4001
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE834]'
  id: totrans-4002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE834]'
- en: String columns
  id: totrans-4003
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**min_itemsize**'
  id: totrans-4004
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation of `HDFStore` uses a fixed column width (itemsize)
    for string columns. A string column itemsize is calculated as the maximum of the
    length of data (for that column) that is passed to the `HDFStore`, **in the first
    append**. Subsequent appends, may introduce a string for a column **larger** than
    the column can hold, an Exception will be raised (otherwise you could have a silent
    truncation of these columns, leading to loss of information). In the future we
    may relax this and allow a user-specified truncation to occur.
  id: totrans-4005
  prefs: []
  type: TYPE_NORMAL
- en: Pass `min_itemsize` on the first table creation to a-priori specify the minimum
    length of a particular string column. `min_itemsize` can be an integer, or a dict
    mapping a column name to an integer. You can pass `values` as a key to allow all
    *indexables* or *data_columns* to have this min_itemsize.
  id: totrans-4006
  prefs: []
  type: TYPE_NORMAL
- en: Passing a `min_itemsize` dict will cause all passed columns to be created as
    *data_columns* automatically.
  id: totrans-4007
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4008
  prefs: []
  type: TYPE_NORMAL
- en: If you are not passing any `data_columns`, then the `min_itemsize` will be the
    maximum of the length of any string passed
  id: totrans-4009
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE835]'
  id: totrans-4010
  prefs: []
  type: TYPE_PRE
  zh: '[PRE835]'
- en: '**nan_rep**'
  id: totrans-4011
  prefs: []
  type: TYPE_NORMAL
- en: String columns will serialize a `np.nan` (a missing value) with the `nan_rep`
    string representation. This defaults to the string value `nan`. You could inadvertently
    turn an actual `nan` value into a missing value.
  id: totrans-4012
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE836]'
  id: totrans-4013
  prefs: []
  type: TYPE_PRE
  zh: '[PRE836]'
- en: Performance
  id: totrans-4014
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`tables` format come with a writing performance penalty as compared to `fixed`
    stores. The benefit is the ability to append/delete and query (potentially very
    large amounts of data). Write times are generally longer as compared with regular
    stores. Query times can be quite fast, especially on an indexed axis.'
  id: totrans-4015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `chunksize=<int>` to `append`, specifying the write chunksize (default
    is 50000). This will significantly lower your memory usage on writing.
  id: totrans-4016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `expectedrows=<int>` to the first `append`, to set the TOTAL number
    of rows that `PyTables` will expect. This will optimize read/write performance.
  id: totrans-4017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate rows can be written to tables, but are filtered out in selection (with
    the last items being selected; thus a table is unique on major, minor pairs)
  id: totrans-4018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `PerformanceWarning` will be raised if you are attempting to store types that
    will be pickled by PyTables (rather than stored as endemic types). See [Here](https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190)
    for more information and some solutions.
  id: totrans-4019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read/write API
  id: totrans-4020
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf`
    for writing, similar to how `read_csv` and `to_csv` work.'
  id: totrans-4021
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE837]'
  id: totrans-4022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE837]'
- en: HDFStore will by default not drop rows that are all missing. This behavior can
    be changed by setting `dropna=True`.
  id: totrans-4023
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE838]'
  id: totrans-4024
  prefs: []
  type: TYPE_PRE
  zh: '[PRE838]'
- en: '### Fixed format'
  id: totrans-4025
  prefs: []
  type: TYPE_NORMAL
- en: The examples above show storing using `put`, which write the HDF5 to `PyTables`
    in a fixed array format, called the `fixed` format. These types of stores are
    **not** appendable once written (though you can simply remove them and rewrite).
    Nor are they **queryable**; they must be retrieved in their entirety. They also
    do not support dataframes with non-unique column names. The `fixed` format stores
    offer very fast writing and slightly faster reading than `table` stores. This
    format is specified by default when using `put` or `to_hdf` or by `format='fixed'`
    or `format='f'`.
  id: totrans-4026
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4027
  prefs: []
  type: TYPE_NORMAL
- en: 'A `fixed` format will raise a `TypeError` if you try to retrieve using a `where`:'
  id: totrans-4028
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE839]'
  id: totrans-4029
  prefs: []
  type: TYPE_PRE
  zh: '[PRE839]'
- en: '### Table format'
  id: totrans-4030
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` supports another `PyTables` format on disk, the `table` format.
    Conceptually a `table` is shaped very much like a DataFrame, with rows and columns.
    A `table` may be appended to in the same or other sessions. In addition, delete
    and query type operations are supported. This format is specified by `format=''table''`
    or `format=''t''` to `append` or `put` or `to_hdf`.'
  id: totrans-4031
  prefs: []
  type: TYPE_NORMAL
- en: This format can be set as an option as well `pd.set_option('io.hdf.default_format','table')`
    to enable `put/append/to_hdf` to by default store in the `table` format.
  id: totrans-4032
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE840]'
  id: totrans-4033
  prefs: []
  type: TYPE_PRE
  zh: '[PRE840]'
- en: Note
  id: totrans-4034
  prefs: []
  type: TYPE_NORMAL
- en: You can also create a `table` by passing `format='table'` or `format='t'` to
    a `put` operation.
  id: totrans-4035
  prefs: []
  type: TYPE_NORMAL
- en: '### Hierarchical keys'
  id: totrans-4036
  prefs: []
  type: TYPE_NORMAL
- en: Keys to a store can be specified as a string. These can be in a hierarchical
    path-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of
    sub-stores (or `Groups` in PyTables parlance). Keys can be specified without the
    leading ‘/’ and are **always** absolute (e.g. ‘foo’ refers to ‘/foo’). Removal
    operations can remove everything in the sub-store and **below**, so be *careful*.
  id: totrans-4037
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE841]'
  id: totrans-4038
  prefs: []
  type: TYPE_PRE
  zh: '[PRE841]'
- en: You can walk through the group hierarchy using the `walk` method which will
    yield a tuple for each group key along with the relative keys of its contents.
  id: totrans-4039
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE842]'
  id: totrans-4040
  prefs: []
  type: TYPE_PRE
  zh: '[PRE842]'
- en: Warning
  id: totrans-4041
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical keys cannot be retrieved as dotted (attribute) access as described
    above for items stored under the root node.
  id: totrans-4042
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE843]'
  id: totrans-4043
  prefs: []
  type: TYPE_PRE
  zh: '[PRE843]'
- en: '[PRE844]'
  id: totrans-4044
  prefs: []
  type: TYPE_PRE
  zh: '[PRE844]'
- en: 'Instead, use explicit string based keys:'
  id: totrans-4045
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE845]'
  id: totrans-4046
  prefs: []
  type: TYPE_PRE
  zh: '[PRE845]'
- en: '### Storing types'
  id: totrans-4047
  prefs: []
  type: TYPE_NORMAL
- en: Storing mixed types in a table
  id: totrans-4048
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing mixed-dtype data is supported. Strings are stored as a fixed-width using
    the maximum size of the appended column. Subsequent attempts at appending longer
    strings will raise a `ValueError`.
  id: totrans-4049
  prefs: []
  type: TYPE_NORMAL
- en: 'Passing `min_itemsize={`values`: size}` as a parameter to append will set a
    larger minimum for the string columns. Storing `floats, strings, ints, bools,
    datetime64` are currently supported. For string columns, passing `nan_rep = ''nan''`
    to append will change the default nan representation on disk (which converts to/from
    `np.nan`), this defaults to `nan`.'
  id: totrans-4050
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE846]'
  id: totrans-4051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE846]'
- en: Storing MultiIndex DataFrames
  id: totrans-4052
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting
    from homogeneous index `DataFrames`.
  id: totrans-4053
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE847]'
  id: totrans-4054
  prefs: []
  type: TYPE_PRE
  zh: '[PRE847]'
- en: Note
  id: totrans-4055
  prefs: []
  type: TYPE_NORMAL
- en: The `index` keyword is reserved and cannot be use as a level name.
  id: totrans-4056
  prefs: []
  type: TYPE_NORMAL
- en: Storing mixed types in a table
  id: totrans-4057
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing mixed-dtype data is supported. Strings are stored as a fixed-width using
    the maximum size of the appended column. Subsequent attempts at appending longer
    strings will raise a `ValueError`.
  id: totrans-4058
  prefs: []
  type: TYPE_NORMAL
- en: 'Passing `min_itemsize={`values`: size}` as a parameter to append will set a
    larger minimum for the string columns. Storing `floats, strings, ints, bools,
    datetime64` are currently supported. For string columns, passing `nan_rep = ''nan''`
    to append will change the default nan representation on disk (which converts to/from
    `np.nan`), this defaults to `nan`.'
  id: totrans-4059
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE848]'
  id: totrans-4060
  prefs: []
  type: TYPE_PRE
  zh: '[PRE848]'
- en: Storing MultiIndex DataFrames
  id: totrans-4061
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting
    from homogeneous index `DataFrames`.
  id: totrans-4062
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE849]'
  id: totrans-4063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE849]'
- en: Note
  id: totrans-4064
  prefs: []
  type: TYPE_NORMAL
- en: The `index` keyword is reserved and cannot be use as a level name.
  id: totrans-4065
  prefs: []
  type: TYPE_NORMAL
- en: '### Querying'
  id: totrans-4066
  prefs: []
  type: TYPE_NORMAL
- en: Querying a table
  id: totrans-4067
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`select` and `delete` operations have an optional criterion that can be specified
    to select/delete only a subset of the data. This allows one to have a very large
    on-disk table and retrieve only a portion of the data.'
  id: totrans-4068
  prefs: []
  type: TYPE_NORMAL
- en: A query is specified using the `Term` class under the hood, as a boolean expression.
  id: totrans-4069
  prefs: []
  type: TYPE_NORMAL
- en: '`index` and `columns` are supported indexers of `DataFrames`.'
  id: totrans-4070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `data_columns` are specified, these can be used as additional indexers.
  id: totrans-4071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: level name in a MultiIndex, with default name `level_0`, `level_1`, … if not
    provided.
  id: totrans-4072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valid comparison operators are:'
  id: totrans-4073
  prefs: []
  type: TYPE_NORMAL
- en: '`=, ==, !=, >, >=, <, <=`'
  id: totrans-4074
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid boolean expressions are combined with:'
  id: totrans-4075
  prefs: []
  type: TYPE_NORMAL
- en: '`|` : or'
  id: totrans-4076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`&` : and'
  id: totrans-4077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(` and `)` : for grouping'
  id: totrans-4078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are similar to how boolean expressions are used in pandas for indexing.
  id: totrans-4079
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4080
  prefs: []
  type: TYPE_NORMAL
- en: '`=` will be automatically expanded to the comparison operator `==`'
  id: totrans-4081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~` is the not operator, but can only be used in very limited circumstances'
  id: totrans-4082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a list/tuple of expressions is passed they will be combined via `&`
  id: totrans-4083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are valid expressions:'
  id: totrans-4084
  prefs: []
  type: TYPE_NORMAL
- en: '`''index >= date''`'
  id: totrans-4085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns = [''A'', ''D'']"`'
  id: totrans-4086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns in [''A'', ''D'']"`'
  id: totrans-4087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns = A''`'
  id: totrans-4088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns == A''`'
  id: totrans-4089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"~(columns = [''A'', ''B''])"`'
  id: totrans-4090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''index > df.index[3] & string = "bar"''`'
  id: totrans-4091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
  id: totrans-4092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"ts >= Timestamp(''2012-02-01'')"`'
  id: totrans-4093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"major_axis>=20130101"`'
  id: totrans-4094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `indexers` are on the left-hand side of the sub-expression:'
  id: totrans-4095
  prefs: []
  type: TYPE_NORMAL
- en: '`columns`, `major_axis`, `ts`'
  id: totrans-4096
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-hand side of the sub-expression (after a comparison operator) can
    be:'
  id: totrans-4097
  prefs: []
  type: TYPE_NORMAL
- en: functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  id: totrans-4098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strings, e.g. `"bar"`
  id: totrans-4099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: date-like, e.g. `20130101`, or `"20130101"`
  id: totrans-4100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lists, e.g. `"['A', 'B']"`
  id: totrans-4101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variables that are defined in the local names space, e.g. `date`
  id: totrans-4102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-4103
  prefs: []
  type: TYPE_NORMAL
- en: Passing a string to a query by interpolating it into the query expression is
    not recommended. Simply assign the string of interest to a variable and use that
    variable in an expression. For example, do this
  id: totrans-4104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE850]'
  id: totrans-4105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE850]'
- en: instead of this
  id: totrans-4106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE851]'
  id: totrans-4107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE851]'
- en: The latter will **not** work and will raise a `SyntaxError`.Note that there’s
    a single quote followed by a double quote in the `string` variable.
  id: totrans-4108
  prefs: []
  type: TYPE_NORMAL
- en: If you *must* interpolate, use the `'%r'` format specifier
  id: totrans-4109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE852]'
  id: totrans-4110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE852]'
- en: which will quote `string`.
  id: totrans-4111
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  id: totrans-4112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE853]'
  id: totrans-4113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE853]'
- en: Use boolean expressions, with in-line function evaluation.
  id: totrans-4114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE854]'
  id: totrans-4115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE854]'
- en: Use inline column reference.
  id: totrans-4116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE855]'
  id: totrans-4117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE855]'
- en: 'The `columns` keyword can be supplied to select a list of columns to be returned,
    this is equivalent to passing a `''columns=list_of_columns_to_filter''`:'
  id: totrans-4118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE856]'
  id: totrans-4119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE856]'
- en: '`start` and `stop` parameters can be specified to limit the total search space.
    These are in terms of the total number of rows in a table.'
  id: totrans-4120
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4121
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `ValueError` if the query expression has an unknown variable
    reference. Usually this means that you are trying to select on a column that is
    **not** a data_column.'
  id: totrans-4122
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `SyntaxError` if the query expression is not valid.'
  id: totrans-4123
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query timedelta64[ns]'
  id: totrans-4124
  prefs: []
  type: TYPE_NORMAL
- en: 'You can store and query using the `timedelta64[ns]` type. Terms can be specified
    in the format: `<float>(<unit>)`, where float may be signed (and fractional),
    and unit can be `D,s,ms,us,ns` for the timedelta. Here’s an example:'
  id: totrans-4125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE857]  #### Query MultiIndex'
  id: totrans-4126
  prefs: []
  type: TYPE_NORMAL
- en: Selecting from a `MultiIndex` can be achieved by using the name of the level.
  id: totrans-4127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE858]'
  id: totrans-4128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE858]'
- en: If the `MultiIndex` levels names are `None`, the levels are automatically made
    available via the `level_n` keyword with `n` the level of the `MultiIndex` you
    want to select from.
  id: totrans-4129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE859]'
  id: totrans-4130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE859]'
- en: Indexing
  id: totrans-4131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can create/modify an index for a table with `create_table_index` after data
    is already in the table (after and `append/put` operation). Creating a table index
    is **highly** encouraged. This will speed your queries a great deal when you use
    a `select` with the indexed dimension as the `where`.
  id: totrans-4132
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4133
  prefs: []
  type: TYPE_NORMAL
- en: Indexes are automagically created on the indexables and any data columns you
    specify. This behavior can be turned off by passing `index=False` to `append`.
  id: totrans-4134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE860]'
  id: totrans-4135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE860]'
- en: Oftentimes when appending large amounts of data to a store, it is useful to
    turn off index creation for each append, then recreate at the end.
  id: totrans-4136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE861]'
  id: totrans-4137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE861]'
- en: Then create the index when finished appending.
  id: totrans-4138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE862]'
  id: totrans-4139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE862]'
- en: See [here](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)
    for how to create a completely-sorted-index (CSI) on an existing store.
  id: totrans-4140
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query via data columns'
  id: totrans-4141
  prefs: []
  type: TYPE_NORMAL
- en: You can designate (and index) certain columns that you want to be able to perform
    queries (other than the `indexable` columns, which you can always query). For
    instance say you want to perform this common operation, on-disk, and return just
    the frame that matches this query. You can specify `data_columns = True` to force
    all columns to be `data_columns`.
  id: totrans-4142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE863]'
  id: totrans-4143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE863]'
- en: There is some performance degradation by making lots of columns into `data columns`,
    so it is up to the user to designate these. In addition, you cannot change data
    columns (nor indexables) after the first append/put operation (Of course you can
    simply read in the data and create a new table!).
  id: totrans-4144
  prefs: []
  type: TYPE_NORMAL
- en: Iterator
  id: totrans-4145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and
    `select_as_multiple` to return an iterator on the results. The default is 50,000
    rows returned in a chunk.
  id: totrans-4146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE864]'
  id: totrans-4147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE864]'
- en: Note
  id: totrans-4148
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the iterator with `read_hdf` which will open, then automatically
    close the store when finished iterating.
  id: totrans-4149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE865]'
  id: totrans-4150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE865]'
- en: Note, that the chunksize keyword applies to the **source** rows. So if you are
    doing a query, then the chunksize will subdivide the total rows in the table and
    the query applied, returning an iterator on potentially unequal sized chunks.
  id: totrans-4151
  prefs: []
  type: TYPE_NORMAL
- en: Here is a recipe for generating a query and using it to create equal sized return
    chunks.
  id: totrans-4152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE866]'
  id: totrans-4153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE866]'
- en: Advanced queries
  id: totrans-4154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Select a single column
  id: totrans-4155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  id: totrans-4156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE867]'
  id: totrans-4157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE867]'
- en: '##### Selecting coordinates'
  id: totrans-4158
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  id: totrans-4159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE868]  ##### Selecting using a where mask'
  id: totrans-4160
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  id: totrans-4161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE869]'
  id: totrans-4162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE869]'
- en: Storer object
  id: totrans-4163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  id: totrans-4164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE870]'
  id: totrans-4165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE870]'
- en: Multiple table queries
  id: totrans-4166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The methods `append_to_multiple` and `select_as_multiple` can perform appending/selecting
    from multiple tables at once. The idea is to have one table (call it the selector
    table) that you index most/all of the columns, and perform your queries. The other
    table(s) are data tables with an index matching the selector table’s index. You
    can then perform a very fast query on the selector table, yet get lots of data
    back. This method is similar to having a very wide table, but enables more efficient
    queries.
  id: totrans-4167
  prefs: []
  type: TYPE_NORMAL
- en: The `append_to_multiple` method splits a given single DataFrame into multiple
    tables according to `d`, a dictionary that maps the table names to a list of ‘columns’
    you want in that table. If `None` is used in place of a list, that table will
    have the remaining unspecified columns of the given DataFrame. The argument `selector`
    defines which table is the selector table (which you can make queries from). The
    argument `dropna` will drop rows from the input `DataFrame` to ensure tables are
    synchronized. This means that if a row for one of the tables being written to
    is entirely `np.nan`, that row will be dropped from all tables.
  id: totrans-4168
  prefs: []
  type: TYPE_NORMAL
- en: If `dropna` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
    Remember that entirely `np.Nan` rows are not written to the HDFStore, so if you
    choose to call `dropna=False`, some tables may have more rows than others, and
    therefore `select_as_multiple` may not work or it may return unexpected results.
  id: totrans-4169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE871]'
  id: totrans-4170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE871]'
- en: Querying a table
  id: totrans-4171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`select` and `delete` operations have an optional criterion that can be specified
    to select/delete only a subset of the data. This allows one to have a very large
    on-disk table and retrieve only a portion of the data.'
  id: totrans-4172
  prefs: []
  type: TYPE_NORMAL
- en: A query is specified using the `Term` class under the hood, as a boolean expression.
  id: totrans-4173
  prefs: []
  type: TYPE_NORMAL
- en: '`index` and `columns` are supported indexers of `DataFrames`.'
  id: totrans-4174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `data_columns` are specified, these can be used as additional indexers.
  id: totrans-4175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: level name in a MultiIndex, with default name `level_0`, `level_1`, … if not
    provided.
  id: totrans-4176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valid comparison operators are:'
  id: totrans-4177
  prefs: []
  type: TYPE_NORMAL
- en: '`=, ==, !=, >, >=, <, <=`'
  id: totrans-4178
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid boolean expressions are combined with:'
  id: totrans-4179
  prefs: []
  type: TYPE_NORMAL
- en: '`|` : or'
  id: totrans-4180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`&` : and'
  id: totrans-4181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(` and `)` : for grouping'
  id: totrans-4182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are similar to how boolean expressions are used in pandas for indexing.
  id: totrans-4183
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4184
  prefs: []
  type: TYPE_NORMAL
- en: '`=` will be automatically expanded to the comparison operator `==`'
  id: totrans-4185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~` is the not operator, but can only be used in very limited circumstances'
  id: totrans-4186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a list/tuple of expressions is passed they will be combined via `&`
  id: totrans-4187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are valid expressions:'
  id: totrans-4188
  prefs: []
  type: TYPE_NORMAL
- en: '`''index >= date''`'
  id: totrans-4189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns = [''A'', ''D'']"`'
  id: totrans-4190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns in [''A'', ''D'']"`'
  id: totrans-4191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns = A''`'
  id: totrans-4192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns == A''`'
  id: totrans-4193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"~(columns = [''A'', ''B''])"`'
  id: totrans-4194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''index > df.index[3] & string = "bar"''`'
  id: totrans-4195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
  id: totrans-4196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"ts >= Timestamp(''2012-02-01'')"`'
  id: totrans-4197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"major_axis>=20130101"`'
  id: totrans-4198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `indexers` are on the left-hand side of the sub-expression:'
  id: totrans-4199
  prefs: []
  type: TYPE_NORMAL
- en: '`columns`, `major_axis`, `ts`'
  id: totrans-4200
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-hand side of the sub-expression (after a comparison operator) can
    be:'
  id: totrans-4201
  prefs: []
  type: TYPE_NORMAL
- en: functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  id: totrans-4202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strings, e.g. `"bar"`
  id: totrans-4203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: date-like, e.g. `20130101`, or `"20130101"`
  id: totrans-4204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lists, e.g. `"['A', 'B']"`
  id: totrans-4205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variables that are defined in the local names space, e.g. `date`
  id: totrans-4206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-4207
  prefs: []
  type: TYPE_NORMAL
- en: Passing a string to a query by interpolating it into the query expression is
    not recommended. Simply assign the string of interest to a variable and use that
    variable in an expression. For example, do this
  id: totrans-4208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE872]'
  id: totrans-4209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE872]'
- en: instead of this
  id: totrans-4210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE873]'
  id: totrans-4211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE873]'
- en: The latter will **not** work and will raise a `SyntaxError`.Note that there’s
    a single quote followed by a double quote in the `string` variable.
  id: totrans-4212
  prefs: []
  type: TYPE_NORMAL
- en: If you *must* interpolate, use the `'%r'` format specifier
  id: totrans-4213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE874]'
  id: totrans-4214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE874]'
- en: which will quote `string`.
  id: totrans-4215
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  id: totrans-4216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE875]'
  id: totrans-4217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE875]'
- en: Use boolean expressions, with in-line function evaluation.
  id: totrans-4218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE876]'
  id: totrans-4219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE876]'
- en: Use inline column reference.
  id: totrans-4220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE877]'
  id: totrans-4221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE877]'
- en: 'The `columns` keyword can be supplied to select a list of columns to be returned,
    this is equivalent to passing a `''columns=list_of_columns_to_filter''`:'
  id: totrans-4222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE878]'
  id: totrans-4223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE878]'
- en: '`start` and `stop` parameters can be specified to limit the total search space.
    These are in terms of the total number of rows in a table.'
  id: totrans-4224
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4225
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `ValueError` if the query expression has an unknown variable
    reference. Usually this means that you are trying to select on a column that is
    **not** a data_column.'
  id: totrans-4226
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `SyntaxError` if the query expression is not valid.'
  id: totrans-4227
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query timedelta64[ns]'
  id: totrans-4228
  prefs: []
  type: TYPE_NORMAL
- en: 'You can store and query using the `timedelta64[ns]` type. Terms can be specified
    in the format: `<float>(<unit>)`, where float may be signed (and fractional),
    and unit can be `D,s,ms,us,ns` for the timedelta. Here’s an example:'
  id: totrans-4229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE879]'
  id: totrans-4230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE879]'
- en: '#### Query MultiIndex'
  id: totrans-4231
  prefs: []
  type: TYPE_NORMAL
- en: Selecting from a `MultiIndex` can be achieved by using the name of the level.
  id: totrans-4232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE880]'
  id: totrans-4233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE880]'
- en: If the `MultiIndex` levels names are `None`, the levels are automatically made
    available via the `level_n` keyword with `n` the level of the `MultiIndex` you
    want to select from.
  id: totrans-4234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE881]'
  id: totrans-4235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE881]'
- en: Indexing
  id: totrans-4236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can create/modify an index for a table with `create_table_index` after data
    is already in the table (after and `append/put` operation). Creating a table index
    is **highly** encouraged. This will speed your queries a great deal when you use
    a `select` with the indexed dimension as the `where`.
  id: totrans-4237
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4238
  prefs: []
  type: TYPE_NORMAL
- en: Indexes are automagically created on the indexables and any data columns you
    specify. This behavior can be turned off by passing `index=False` to `append`.
  id: totrans-4239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE882]'
  id: totrans-4240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE882]'
- en: Oftentimes when appending large amounts of data to a store, it is useful to
    turn off index creation for each append, then recreate at the end.
  id: totrans-4241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE883]'
  id: totrans-4242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE883]'
- en: Then create the index when finished appending.
  id: totrans-4243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE884]'
  id: totrans-4244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE884]'
- en: See [here](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)
    for how to create a completely-sorted-index (CSI) on an existing store.
  id: totrans-4245
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query via data columns'
  id: totrans-4246
  prefs: []
  type: TYPE_NORMAL
- en: You can designate (and index) certain columns that you want to be able to perform
    queries (other than the `indexable` columns, which you can always query). For
    instance say you want to perform this common operation, on-disk, and return just
    the frame that matches this query. You can specify `data_columns = True` to force
    all columns to be `data_columns`.
  id: totrans-4247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE885]'
  id: totrans-4248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE885]'
- en: There is some performance degradation by making lots of columns into `data columns`,
    so it is up to the user to designate these. In addition, you cannot change data
    columns (nor indexables) after the first append/put operation (Of course you can
    simply read in the data and create a new table!).
  id: totrans-4249
  prefs: []
  type: TYPE_NORMAL
- en: Iterator
  id: totrans-4250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and
    `select_as_multiple` to return an iterator on the results. The default is 50,000
    rows returned in a chunk.
  id: totrans-4251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE886]'
  id: totrans-4252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE886]'
- en: Note
  id: totrans-4253
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the iterator with `read_hdf` which will open, then automatically
    close the store when finished iterating.
  id: totrans-4254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE887]'
  id: totrans-4255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE887]'
- en: Note, that the chunksize keyword applies to the **source** rows. So if you are
    doing a query, then the chunksize will subdivide the total rows in the table and
    the query applied, returning an iterator on potentially unequal sized chunks.
  id: totrans-4256
  prefs: []
  type: TYPE_NORMAL
- en: Here is a recipe for generating a query and using it to create equal sized return
    chunks.
  id: totrans-4257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE888]'
  id: totrans-4258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE888]'
- en: Advanced queries
  id: totrans-4259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Select a single column
  id: totrans-4260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  id: totrans-4261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE889]'
  id: totrans-4262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE889]'
- en: '##### Selecting coordinates'
  id: totrans-4263
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  id: totrans-4264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE890]  ##### Selecting using a where mask'
  id: totrans-4265
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  id: totrans-4266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE891]'
  id: totrans-4267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE891]'
- en: Storer object
  id: totrans-4268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  id: totrans-4269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE892]'
  id: totrans-4270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE892]'
- en: Select a single column
  id: totrans-4271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  id: totrans-4272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE893]'
  id: totrans-4273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE893]'
- en: '##### Selecting coordinates'
  id: totrans-4274
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  id: totrans-4275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE894]'
  id: totrans-4276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE894]'
- en: '##### Selecting using a where mask'
  id: totrans-4277
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  id: totrans-4278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE895]'
  id: totrans-4279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE895]'
- en: Storer object
  id: totrans-4280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  id: totrans-4281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE896]'
  id: totrans-4282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE896]'
- en: Multiple table queries
  id: totrans-4283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The methods `append_to_multiple` and `select_as_multiple` can perform appending/selecting
    from multiple tables at once. The idea is to have one table (call it the selector
    table) that you index most/all of the columns, and perform your queries. The other
    table(s) are data tables with an index matching the selector table’s index. You
    can then perform a very fast query on the selector table, yet get lots of data
    back. This method is similar to having a very wide table, but enables more efficient
    queries.
  id: totrans-4284
  prefs: []
  type: TYPE_NORMAL
- en: The `append_to_multiple` method splits a given single DataFrame into multiple
    tables according to `d`, a dictionary that maps the table names to a list of ‘columns’
    you want in that table. If `None` is used in place of a list, that table will
    have the remaining unspecified columns of the given DataFrame. The argument `selector`
    defines which table is the selector table (which you can make queries from). The
    argument `dropna` will drop rows from the input `DataFrame` to ensure tables are
    synchronized. This means that if a row for one of the tables being written to
    is entirely `np.nan`, that row will be dropped from all tables.
  id: totrans-4285
  prefs: []
  type: TYPE_NORMAL
- en: If `dropna` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
    Remember that entirely `np.Nan` rows are not written to the HDFStore, so if you
    choose to call `dropna=False`, some tables may have more rows than others, and
    therefore `select_as_multiple` may not work or it may return unexpected results.
  id: totrans-4286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE897]'
  id: totrans-4287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE897]'
- en: Delete from a table
  id: totrans-4288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can delete from a table selectively by specifying a `where`. In deleting
    rows, it is important to understand the `PyTables` deletes rows by erasing the
    rows, then **moving** the following data. Thus deleting can potentially be a very
    expensive operation depending on the orientation of your data. To get optimal
    performance, it’s worthwhile to have the dimension you are deleting be the first
    of the `indexables`.
  id: totrans-4289
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is ordered (on the disk) in terms of the `indexables`. Here’s a simple
    use case. You store panel-type data, with dates in the `major_axis` and ids in
    the `minor_axis`. The data is then interleaved like this:'
  id: totrans-4290
  prefs: []
  type: TYPE_NORMAL
- en: date_1
  id: totrans-4291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  id: totrans-4292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_2
  id: totrans-4293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  id: totrans-4294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  id: totrans-4295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: date_2
  id: totrans-4296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  id: totrans-4297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  id: totrans-4298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  id: totrans-4299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be clear that a delete operation on the `major_axis` will be fairly
    quick, as one chunk is removed, then the following data moved. On the other hand
    a delete operation on the `minor_axis` will be very expensive. In this case it
    would almost certainly be faster to rewrite the table using a `where` that selects
    all but the missing data.
  id: totrans-4300
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4301
  prefs: []
  type: TYPE_NORMAL
- en: Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files automatically.
    Thus, repeatedly deleting (or removing nodes) and adding again, **WILL TEND TO
    INCREASE THE FILE SIZE**.
  id: totrans-4302
  prefs: []
  type: TYPE_NORMAL
- en: To *repack and clean* the file, use [ptrepack](#io-hdf5-ptrepack).
  id: totrans-4303
  prefs: []
  type: TYPE_NORMAL
- en: '### Notes & caveats'
  id: totrans-4304
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  id: totrans-4305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`PyTables` allows the stored data to be compressed. This applies to all kinds
    of stores, not just tables. Two parameters are used to control compression: `complevel`
    and `complib`.'
  id: totrans-4306
  prefs: []
  type: TYPE_NORMAL
- en: '`complevel` specifies if and how hard data is to be compressed. `complevel=0`
    and `complevel=None` disables compression and `0<complevel<10` enables compression.'
  id: totrans-4307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`complib` specifies which compression library to use. If nothing is specified
    the default library `zlib` is used. A compression library usually optimizes for
    either good compression rates or speed and the results will depend on the type
    of data. Which type of compression to choose depends on your specific needs and
    data. The list of supported compression libraries:'
  id: totrans-4308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[zlib](https://zlib.net/): The default compression library. A classic in terms
    of compression, achieves good compression rates but is somewhat slow.'
  id: totrans-4309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lzo](https://www.oberhumer.com/opensource/lzo/): Fast compression and decompression.'
  id: totrans-4310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bzip2](https://sourceware.org/bzip2/): Good compression rates.'
  id: totrans-4311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc](https://www.blosc.org/): Fast compression and decompression.'
  id: totrans-4312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for alternative blosc compressors:'
  id: totrans-4313
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[blosc:blosclz](https://www.blosc.org/) This is the default compressor for
    `blosc`'
  id: totrans-4314
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html): A compact, very
    popular and fast compressor.'
  id: totrans-4315
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html): A tweaked version
    of LZ4, produces better compression ratios at the expense of speed.'
  id: totrans-4316
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:snappy](https://google.github.io/snappy/): A popular compressor used
    in many places.'
  id: totrans-4317
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zlib](https://zlib.net/): A classic; somewhat slower than the previous
    ones, but achieving better compression ratios.'
  id: totrans-4318
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zstd](https://facebook.github.io/zstd/): An extremely well balanced
    codec; it provides the best compression ratios among the others above, and at
    reasonably fast speed.'
  id: totrans-4319
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `complib` is defined as something other than the listed libraries a `ValueError`
    exception is issued.
  id: totrans-4320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-4321
  prefs: []
  type: TYPE_NORMAL
- en: If the library specified with the `complib` option is missing on your platform,
    compression defaults to `zlib` without further ado.
  id: totrans-4322
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable compression for all objects within the file:'
  id: totrans-4323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE898]'
  id: totrans-4324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE898]'
- en: 'Or on-the-fly compression (this only applies to tables) in stores where compression
    is not enabled:'
  id: totrans-4325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE899]'
  id: totrans-4326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE899]'
- en: '#### ptrepack'
  id: totrans-4327
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` offers better write performance when tables are compressed after
    they are written, as opposed to turning on compression at the very beginning.
    You can use the supplied `PyTables` utility `ptrepack`. In addition, `ptrepack`
    can change compression levels after the fact.'
  id: totrans-4328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE900]'
  id: totrans-4329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE900]'
- en: 'Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow you to
    reuse previously deleted space. Alternatively, one can simply remove the file
    and write again, or use the `copy` method.  #### Caveats'
  id: totrans-4330
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4331
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is **not-threadsafe for writing**. The underlying `PyTables` only
    supports concurrent reads (via threading or processes). If you need reading and
    writing *at the same time*, you need to serialize these operations in a single
    thread in a single process. You will corrupt your data otherwise. See the ([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397)) for more information.'
  id: totrans-4332
  prefs: []
  type: TYPE_NORMAL
- en: If you use locks to manage write access between multiple processes, you may
    want to use [`fsync()`](https://docs.python.org/3/library/os.html#os.fsync "(in
    Python v3.12)") before releasing write locks. For convenience you can use `store.flush(fsync=True)`
    to do this for you.
  id: totrans-4333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a `table` is created columns (DataFrame) are fixed; only exactly the same
    columns can be appended
  id: totrans-4334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not necessarily
    equal across timezone versions. So if data is localized to a specific timezone
    in the HDFStore using one version of a timezone library and that data is updated
    with another version, the data will be converted to UTC since these timezones
    are not considered equal. Either use the same version of timezone library or use
    `tz_convert` with the updated timezone definition.
  id: totrans-4335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4336
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` will show a `NaturalNameWarning` if a column name cannot be used
    as an attribute selector. *Natural* identifiers contain only letters, numbers,
    and underscores, and may not begin with a number. Other identifiers cannot be
    used in a `where` clause and are generally a bad idea.'
  id: totrans-4337
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  id: totrans-4338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`PyTables` allows the stored data to be compressed. This applies to all kinds
    of stores, not just tables. Two parameters are used to control compression: `complevel`
    and `complib`.'
  id: totrans-4339
  prefs: []
  type: TYPE_NORMAL
- en: '`complevel` specifies if and how hard data is to be compressed. `complevel=0`
    and `complevel=None` disables compression and `0<complevel<10` enables compression.'
  id: totrans-4340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`complib` specifies which compression library to use. If nothing is specified
    the default library `zlib` is used. A compression library usually optimizes for
    either good compression rates or speed and the results will depend on the type
    of data. Which type of compression to choose depends on your specific needs and
    data. The list of supported compression libraries:'
  id: totrans-4341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[zlib](https://zlib.net/): The default compression library. A classic in terms
    of compression, achieves good compression rates but is somewhat slow.'
  id: totrans-4342
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lzo](https://www.oberhumer.com/opensource/lzo/): Fast compression and decompression.'
  id: totrans-4343
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bzip2](https://sourceware.org/bzip2/): Good compression rates.'
  id: totrans-4344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc](https://www.blosc.org/): Fast compression and decompression.'
  id: totrans-4345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for alternative blosc compressors:'
  id: totrans-4346
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[blosc:blosclz](https://www.blosc.org/) This is the default compressor for
    `blosc`'
  id: totrans-4347
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html): A compact, very
    popular and fast compressor.'
  id: totrans-4348
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html): A tweaked version
    of LZ4, produces better compression ratios at the expense of speed.'
  id: totrans-4349
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:snappy](https://google.github.io/snappy/): A popular compressor used
    in many places.'
  id: totrans-4350
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zlib](https://zlib.net/): A classic; somewhat slower than the previous
    ones, but achieving better compression ratios.'
  id: totrans-4351
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zstd](https://facebook.github.io/zstd/): An extremely well balanced
    codec; it provides the best compression ratios among the others above, and at
    reasonably fast speed.'
  id: totrans-4352
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `complib` is defined as something other than the listed libraries a `ValueError`
    exception is issued.
  id: totrans-4353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-4354
  prefs: []
  type: TYPE_NORMAL
- en: If the library specified with the `complib` option is missing on your platform,
    compression defaults to `zlib` without further ado.
  id: totrans-4355
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable compression for all objects within the file:'
  id: totrans-4356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE901]'
  id: totrans-4357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE901]'
- en: 'Or on-the-fly compression (this only applies to tables) in stores where compression
    is not enabled:'
  id: totrans-4358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE902]'
  id: totrans-4359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE902]'
- en: '#### ptrepack'
  id: totrans-4360
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` offers better write performance when tables are compressed after
    they are written, as opposed to turning on compression at the very beginning.
    You can use the supplied `PyTables` utility `ptrepack`. In addition, `ptrepack`
    can change compression levels after the fact.'
  id: totrans-4361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE903]'
  id: totrans-4362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE903]'
- en: Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow you to reuse
    previously deleted space. Alternatively, one can simply remove the file and write
    again, or use the `copy` method.
  id: totrans-4363
  prefs: []
  type: TYPE_NORMAL
- en: '#### Caveats'
  id: totrans-4364
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4365
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is **not-threadsafe for writing**. The underlying `PyTables` only
    supports concurrent reads (via threading or processes). If you need reading and
    writing *at the same time*, you need to serialize these operations in a single
    thread in a single process. You will corrupt your data otherwise. See the ([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397)) for more information.'
  id: totrans-4366
  prefs: []
  type: TYPE_NORMAL
- en: If you use locks to manage write access between multiple processes, you may
    want to use [`fsync()`](https://docs.python.org/3/library/os.html#os.fsync "(in
    Python v3.12)") before releasing write locks. For convenience you can use `store.flush(fsync=True)`
    to do this for you.
  id: totrans-4367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a `table` is created columns (DataFrame) are fixed; only exactly the same
    columns can be appended
  id: totrans-4368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not necessarily
    equal across timezone versions. So if data is localized to a specific timezone
    in the HDFStore using one version of a timezone library and that data is updated
    with another version, the data will be converted to UTC since these timezones
    are not considered equal. Either use the same version of timezone library or use
    `tz_convert` with the updated timezone definition.
  id: totrans-4369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4370
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` will show a `NaturalNameWarning` if a column name cannot be used
    as an attribute selector. *Natural* identifiers contain only letters, numbers,
    and underscores, and may not begin with a number. Other identifiers cannot be
    used in a `where` clause and are generally a bad idea.'
  id: totrans-4371
  prefs: []
  type: TYPE_NORMAL
- en: '### DataTypes'
  id: totrans-4372
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` will map an object dtype to the `PyTables` underlying dtype. This
    means the following types are known to work:'
  id: totrans-4373
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Represents missing values |'
  id: totrans-4374
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-4375
  prefs: []
  type: TYPE_TB
- en: '| floating : `float64, float32, float16` | `np.nan` |'
  id: totrans-4376
  prefs: []
  type: TYPE_TB
- en: '| integer : `int64, int32, int8, uint64,uint32, uint8` |  |'
  id: totrans-4377
  prefs: []
  type: TYPE_TB
- en: '| boolean |  |'
  id: totrans-4378
  prefs: []
  type: TYPE_TB
- en: '| `datetime64[ns]` | `NaT` |'
  id: totrans-4379
  prefs: []
  type: TYPE_TB
- en: '| `timedelta64[ns]` | `NaT` |'
  id: totrans-4380
  prefs: []
  type: TYPE_TB
- en: '| categorical : see the section below |  |'
  id: totrans-4381
  prefs: []
  type: TYPE_TB
- en: '| object : `strings` | `np.nan` |'
  id: totrans-4382
  prefs: []
  type: TYPE_TB
- en: '`unicode` columns are not supported, and **WILL FAIL**.'
  id: totrans-4383
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  id: totrans-4384
  prefs: []
  type: TYPE_NORMAL
- en: You can write data that contains `category` dtypes to a `HDFStore`. Queries
    work the same as if it was an object array. However, the `category` dtyped data
    is stored in a more efficient manner.
  id: totrans-4385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE904]'
  id: totrans-4386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE904]'
- en: String columns
  id: totrans-4387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**min_itemsize**'
  id: totrans-4388
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation of `HDFStore` uses a fixed column width (itemsize)
    for string columns. A string column itemsize is calculated as the maximum of the
    length of data (for that column) that is passed to the `HDFStore`, **in the first
    append**. Subsequent appends, may introduce a string for a column **larger** than
    the column can hold, an Exception will be raised (otherwise you could have a silent
    truncation of these columns, leading to loss of information). In the future we
    may relax this and allow a user-specified truncation to occur.
  id: totrans-4389
  prefs: []
  type: TYPE_NORMAL
- en: Pass `min_itemsize` on the first table creation to a-priori specify the minimum
    length of a particular string column. `min_itemsize` can be an integer, or a dict
    mapping a column name to an integer. You can pass `values` as a key to allow all
    *indexables* or *data_columns* to have this min_itemsize.
  id: totrans-4390
  prefs: []
  type: TYPE_NORMAL
- en: Passing a `min_itemsize` dict will cause all passed columns to be created as
    *data_columns* automatically.
  id: totrans-4391
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4392
  prefs: []
  type: TYPE_NORMAL
- en: If you are not passing any `data_columns`, then the `min_itemsize` will be the
    maximum of the length of any string passed
  id: totrans-4393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE905]'
  id: totrans-4394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE905]'
- en: '**nan_rep**'
  id: totrans-4395
  prefs: []
  type: TYPE_NORMAL
- en: String columns will serialize a `np.nan` (a missing value) with the `nan_rep`
    string representation. This defaults to the string value `nan`. You could inadvertently
    turn an actual `nan` value into a missing value.
  id: totrans-4396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE906]'
  id: totrans-4397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE906]'
- en: '#### Categorical data'
  id: totrans-4398
  prefs: []
  type: TYPE_NORMAL
- en: You can write data that contains `category` dtypes to a `HDFStore`. Queries
    work the same as if it was an object array. However, the `category` dtyped data
    is stored in a more efficient manner.
  id: totrans-4399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE907]'
  id: totrans-4400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE907]'
- en: String columns
  id: totrans-4401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**min_itemsize**'
  id: totrans-4402
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation of `HDFStore` uses a fixed column width (itemsize)
    for string columns. A string column itemsize is calculated as the maximum of the
    length of data (for that column) that is passed to the `HDFStore`, **in the first
    append**. Subsequent appends, may introduce a string for a column **larger** than
    the column can hold, an Exception will be raised (otherwise you could have a silent
    truncation of these columns, leading to loss of information). In the future we
    may relax this and allow a user-specified truncation to occur.
  id: totrans-4403
  prefs: []
  type: TYPE_NORMAL
- en: Pass `min_itemsize` on the first table creation to a-priori specify the minimum
    length of a particular string column. `min_itemsize` can be an integer, or a dict
    mapping a column name to an integer. You can pass `values` as a key to allow all
    *indexables* or *data_columns* to have this min_itemsize.
  id: totrans-4404
  prefs: []
  type: TYPE_NORMAL
- en: Passing a `min_itemsize` dict will cause all passed columns to be created as
    *data_columns* automatically.
  id: totrans-4405
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4406
  prefs: []
  type: TYPE_NORMAL
- en: If you are not passing any `data_columns`, then the `min_itemsize` will be the
    maximum of the length of any string passed
  id: totrans-4407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE908]'
  id: totrans-4408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE908]'
- en: '**nan_rep**'
  id: totrans-4409
  prefs: []
  type: TYPE_NORMAL
- en: String columns will serialize a `np.nan` (a missing value) with the `nan_rep`
    string representation. This defaults to the string value `nan`. You could inadvertently
    turn an actual `nan` value into a missing value.
  id: totrans-4410
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE909]'
  id: totrans-4411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE909]'
- en: Performance
  id: totrans-4412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`tables` format come with a writing performance penalty as compared to `fixed`
    stores. The benefit is the ability to append/delete and query (potentially very
    large amounts of data). Write times are generally longer as compared with regular
    stores. Query times can be quite fast, especially on an indexed axis.'
  id: totrans-4413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `chunksize=<int>` to `append`, specifying the write chunksize (default
    is 50000). This will significantly lower your memory usage on writing.
  id: totrans-4414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `expectedrows=<int>` to the first `append`, to set the TOTAL number
    of rows that `PyTables` will expect. This will optimize read/write performance.
  id: totrans-4415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate rows can be written to tables, but are filtered out in selection (with
    the last items being selected; thus a table is unique on major, minor pairs)
  id: totrans-4416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `PerformanceWarning` will be raised if you are attempting to store types that
    will be pickled by PyTables (rather than stored as endemic types). See [Here](https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190)
    for more information and some solutions.
  id: totrans-4417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '## Feather'
  id: totrans-4418
  prefs: []
  type: TYPE_NORMAL
- en: Feather provides binary columnar serialization for data frames. It is designed
    to make reading and writing data frames efficient, and to make sharing data across
    data analysis languages easy.
  id: totrans-4419
  prefs: []
  type: TYPE_NORMAL
- en: Feather is designed to faithfully serialize and de-serialize DataFrames, supporting
    all of the pandas dtypes, including extension dtypes such as categorical and datetime
    with tz.
  id: totrans-4420
  prefs: []
  type: TYPE_NORMAL
- en: 'Several caveats:'
  id: totrans-4421
  prefs: []
  type: TYPE_NORMAL
- en: The format will NOT write an `Index`, or `MultiIndex` for the `DataFrame` and
    will raise an error if a non-default one is provided. You can `.reset_index()`
    to store the index or `.reset_index(drop=True)` to ignore it.
  id: totrans-4422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate column names and non-string columns names are not supported
  id: totrans-4423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actual Python objects in object dtype columns are not supported. These will
    raise a helpful error message on an attempt at serialization.
  id: totrans-4424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See the [Full Documentation](https://github.com/wesm/feather).
  id: totrans-4425
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE910]'
  id: totrans-4426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE910]'
- en: Write to a feather file.
  id: totrans-4427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE911]'
  id: totrans-4428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE911]'
- en: Read from a feather file.
  id: totrans-4429
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE912]'
  id: totrans-4430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE912]'
- en: '## Parquet'
  id: totrans-4431
  prefs: []
  type: TYPE_NORMAL
- en: '[Apache Parquet](https://parquet.apache.org/) provides a partitioned binary
    columnar serialization for data frames. It is designed to make reading and writing
    data frames efficient, and to make sharing data across data analysis languages
    easy. Parquet can use a variety of compression techniques to shrink the file size
    as much as possible while still maintaining good read performance.'
  id: totrans-4432
  prefs: []
  type: TYPE_NORMAL
- en: Parquet is designed to faithfully serialize and de-serialize `DataFrame` s,
    supporting all of the pandas dtypes, including extension dtypes such as datetime
    with tz.
  id: totrans-4433
  prefs: []
  type: TYPE_NORMAL
- en: Several caveats.
  id: totrans-4434
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate column names and non-string columns names are not supported.
  id: totrans-4435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine always writes the index to the output, but `fastparquet`
    only writes non-default indexes. This extra column can cause problems for non-pandas
    consumers that are not expecting it. You can force including or omitting indexes
    with the `index` argument, regardless of the underlying engine.
  id: totrans-4436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index level names, if specified, must be strings.
  id: totrans-4437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `pyarrow` engine, categorical dtypes for non-string types can be serialized
    to parquet, but will de-serialize as their primitive dtype.
  id: totrans-4438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine preserves the `ordered` flag of categorical dtypes with
    string types. `fastparquet` does not preserve the `ordered` flag.
  id: totrans-4439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non supported types include `Interval` and actual Python object types. These
    will raise a helpful error message on an attempt at serialization. `Period` type
    is supported with pyarrow >= 0.16.0.
  id: totrans-4440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine preserves extension data types such as the nullable integer
    and string data type (requiring pyarrow >= 0.16.0, and requiring the extension
    type to implement the needed protocols, see the [extension types documentation](../development/extending.html#extending-extension-arrow)).
  id: totrans-4441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can specify an `engine` to direct the serialization. This can be one of
    `pyarrow`, or `fastparquet`, or `auto`. If the engine is NOT specified, then the
    `pd.options.io.parquet.engine` option is checked; if this is also `auto`, then
    `pyarrow` is tried, and falling back to `fastparquet`.
  id: totrans-4442
  prefs: []
  type: TYPE_NORMAL
- en: See the documentation for [pyarrow](https://arrow.apache.org/docs/python/) and
    [fastparquet](https://fastparquet.readthedocs.io/en/latest/).
  id: totrans-4443
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4444
  prefs: []
  type: TYPE_NORMAL
- en: These engines are very similar and should read/write nearly identical parquet
    format files. `pyarrow>=8.0.0` supports timedelta data, `fastparquet>=0.1.4` supports
    timezone aware datetimes. These libraries differ by having different underlying
    dependencies (`fastparquet` by using `numba`, while `pyarrow` uses a c-library).
  id: totrans-4445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE913]'
  id: totrans-4446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE913]'
- en: Write to a parquet file.
  id: totrans-4447
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE914]'
  id: totrans-4448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE914]'
- en: Read from a parquet file.
  id: totrans-4449
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE915]'
  id: totrans-4450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE915]'
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  id: totrans-4451
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE916]'
  id: totrans-4452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE916]'
- en: Note
  id: totrans-4453
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is not supported for `fastparquet`.
  id: totrans-4454
  prefs: []
  type: TYPE_NORMAL
- en: Read only certain columns of a parquet file.
  id: totrans-4455
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE917]'
  id: totrans-4456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE917]'
- en: Handling indexes
  id: totrans-4457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Serializing a `DataFrame` to parquet may include the implicit index as one
    or more columns in the output file. Thus, this code:'
  id: totrans-4458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE918]'
  id: totrans-4459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE918]'
- en: 'creates a parquet file with *three* columns if you use `pyarrow` for serialization:
    `a`, `b`, and `__index_level_0__`. If you’re using `fastparquet`, the index [may
    or may not](https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write)
    be written to the file.'
  id: totrans-4460
  prefs: []
  type: TYPE_NORMAL
- en: This unexpected extra column causes some databases like Amazon Redshift to reject
    the file, because that column doesn’t exist in the target table.
  id: totrans-4461
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to omit a dataframe’s indexes when writing, pass `index=False`
    to [`to_parquet()`](../reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet
    "pandas.DataFrame.to_parquet"):'
  id: totrans-4462
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE919]'
  id: totrans-4463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE919]'
- en: This creates a parquet file with just the two expected columns, `a` and `b`.
    If your `DataFrame` has a custom index, you won’t get it back when you load this
    file into a `DataFrame`.
  id: totrans-4464
  prefs: []
  type: TYPE_NORMAL
- en: Passing `index=True` will *always* write the index, even if that’s not the underlying
    engine’s default behavior.
  id: totrans-4465
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning Parquet files
  id: totrans-4466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parquet supports partitioning of data based on the values of one or more columns.
  id: totrans-4467
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE920]'
  id: totrans-4468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE920]'
- en: 'The `path` specifies the parent directory to which data will be saved. The
    `partition_cols` are the column names by which the dataset will be partitioned.
    Columns are partitioned in the order they are given. The partition splits are
    determined by the unique values in the partition columns. The above example creates
    a partitioned dataset that may look like:'
  id: totrans-4469
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE921]'
  id: totrans-4470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE921]'
- en: Handling indexes
  id: totrans-4471
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Serializing a `DataFrame` to parquet may include the implicit index as one
    or more columns in the output file. Thus, this code:'
  id: totrans-4472
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE922]'
  id: totrans-4473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE922]'
- en: 'creates a parquet file with *three* columns if you use `pyarrow` for serialization:
    `a`, `b`, and `__index_level_0__`. If you’re using `fastparquet`, the index [may
    or may not](https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write)
    be written to the file.'
  id: totrans-4474
  prefs: []
  type: TYPE_NORMAL
- en: This unexpected extra column causes some databases like Amazon Redshift to reject
    the file, because that column doesn’t exist in the target table.
  id: totrans-4475
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to omit a dataframe’s indexes when writing, pass `index=False`
    to [`to_parquet()`](../reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet
    "pandas.DataFrame.to_parquet"):'
  id: totrans-4476
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE923]'
  id: totrans-4477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE923]'
- en: This creates a parquet file with just the two expected columns, `a` and `b`.
    If your `DataFrame` has a custom index, you won’t get it back when you load this
    file into a `DataFrame`.
  id: totrans-4478
  prefs: []
  type: TYPE_NORMAL
- en: Passing `index=True` will *always* write the index, even if that’s not the underlying
    engine’s default behavior.
  id: totrans-4479
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning Parquet files
  id: totrans-4480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parquet supports partitioning of data based on the values of one or more columns.
  id: totrans-4481
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE924]'
  id: totrans-4482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE924]'
- en: 'The `path` specifies the parent directory to which data will be saved. The
    `partition_cols` are the column names by which the dataset will be partitioned.
    Columns are partitioned in the order they are given. The partition splits are
    determined by the unique values in the partition columns. The above example creates
    a partitioned dataset that may look like:'
  id: totrans-4483
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE925]'
  id: totrans-4484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE925]'
- en: '## ORC'
  id: totrans-4485
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the [parquet](#io-parquet) format, the [ORC Format](https://orc.apache.org/)
    is a binary columnar serialization for data frames. It is designed to make reading
    data frames efficient. pandas provides both the reader and the writer for the
    ORC format, [`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc
    "pandas.read_orc") and [`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc"). This requires the [pyarrow](https://arrow.apache.org/docs/python/)
    library.
  id: totrans-4486
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4487
  prefs: []
  type: TYPE_NORMAL
- en: It is *highly recommended* to install pyarrow using conda due to some issues
    occurred by pyarrow.
  id: totrans-4488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc") requires pyarrow>=7.0.0.'
  id: totrans-4489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc "pandas.read_orc")
    and [`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc") are not supported on Windows yet, you can find valid
    environments on [install optional dependencies](../getting_started/install.html#install-warn-orc).'
  id: totrans-4490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For supported dtypes please refer to [supported ORC features in Arrow](https://arrow.apache.org/docs/cpp/orc.html#data-types).
  id: totrans-4491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently timezones in datetime columns are not preserved when a dataframe is
    converted into ORC files.
  id: totrans-4492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE926]'
  id: totrans-4493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE926]'
- en: Write to an orc file.
  id: totrans-4494
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE927]'
  id: totrans-4495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE927]'
- en: Read from an orc file.
  id: totrans-4496
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE928]'
  id: totrans-4497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE928]'
- en: Read only certain columns of an orc file.
  id: totrans-4498
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE929]'
  id: totrans-4499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE929]'
- en: '## SQL queries'
  id: totrans-4500
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas.io.sql` module provides a collection of query wrappers to both facilitate
    data retrieval and to reduce dependency on DB-specific API.
  id: totrans-4501
  prefs: []
  type: TYPE_NORMAL
- en: Where available, users may first want to opt for [Apache Arrow ADBC](https://arrow.apache.org/adbc/current/index.html)
    drivers. These drivers should provide the best performance, null handling, and
    type detection.
  id: totrans-4502
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 2.2.0: Added native support for ADBC drivers'
  id: totrans-4503
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For a full list of ADBC drivers and their development status, see the [ADBC
    Driver Implementation Status](https://arrow.apache.org/adbc/current/driver/status.html)
    documentation.
  id: totrans-4504
  prefs: []
  type: TYPE_NORMAL
- en: Where an ADBC driver is not available or may be missing functionality, users
    should opt for installing SQLAlchemy alongside their database driver library.
    Examples of such drivers are [psycopg2](https://www.psycopg.org/) for PostgreSQL
    or [pymysql](https://github.com/PyMySQL/PyMySQL) for MySQL. For [SQLite](https://docs.python.org/3/library/sqlite3.html)
    this is included in Python’s standard library by default. You can find an overview
    of supported drivers for each SQL dialect in the [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/dialects/index.html).
  id: totrans-4505
  prefs: []
  type: TYPE_NORMAL
- en: If SQLAlchemy is not installed, you can use a [`sqlite3.Connection`](https://docs.python.org/3/library/sqlite3.html#sqlite3.Connection
    "(in Python v3.12)") in place of a SQLAlchemy engine, connection, or URI string.
  id: totrans-4506
  prefs: []
  type: TYPE_NORMAL
- en: See also some [cookbook examples](cookbook.html#cookbook-sql) for some advanced
    strategies.
  id: totrans-4507
  prefs: []
  type: TYPE_NORMAL
- en: 'The key functions are:'
  id: totrans-4508
  prefs: []
  type: TYPE_NORMAL
- en: '| [`read_sql_table`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table")(table_name, con[, schema, ...]) | Read SQL database table
    into a DataFrame. |'
  id: totrans-4509
  prefs: []
  type: TYPE_TB
- en: '| [`read_sql_query`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query")(sql, con[, index_col, ...]) | Read SQL query into a DataFrame.
    |'
  id: totrans-4510
  prefs: []
  type: TYPE_TB
- en: '| [`read_sql`](../reference/api/pandas.read_sql.html#pandas.read_sql "pandas.read_sql")(sql, con[, index_col, ...])
    | Read SQL query or database table into a DataFrame. |'
  id: totrans-4511
  prefs: []
  type: TYPE_TB
- en: '| [`DataFrame.to_sql`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql")(name, con, *[, schema, ...]) | Write records stored
    in a DataFrame to a SQL database. |'
  id: totrans-4512
  prefs: []
  type: TYPE_TB
- en: Note
  id: totrans-4513
  prefs: []
  type: TYPE_NORMAL
- en: The function [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") is a convenience wrapper around [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") (and for backward compatibility) and will delegate to
    specific function depending on the provided input (database table name or sql
    query). Table names do not need to be quoted if they have special characters.
  id: totrans-4514
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we use the [SQlite](https://www.sqlite.org/index.html)
    SQL database engine. You can use a temporary SQLite database where data are stored
    in “memory”.
  id: totrans-4515
  prefs: []
  type: TYPE_NORMAL
- en: To connect using an ADBC driver you will want to install the `adbc_driver_sqlite`
    using your package manager. Once installed, you can use the DBAPI interface provided
    by the ADBC driver to connect to your database.
  id: totrans-4516
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE930]'
  id: totrans-4517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE930]'
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to. For more information on `create_engine()` and
    the URI formatting, see the examples below and the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  id: totrans-4518
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE931]'
  id: totrans-4519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE931]'
- en: If you want to manage your own connections you can pass one of those instead.
    The example below opens a connection to the database using a Python context manager
    that automatically closes the connection after the block has completed. See the
    [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/core/connections.html#basic-usage)
    for an explanation of how the database connection is handled.
  id: totrans-4520
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE932]'
  id: totrans-4521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE932]'
- en: Warning
  id: totrans-4522
  prefs: []
  type: TYPE_NORMAL
- en: When you open a connection to a database you are also responsible for closing
    it. Side effects of leaving a connection open may include locking the database
    or other breaking behaviour.
  id: totrans-4523
  prefs: []
  type: TYPE_NORMAL
- en: Writing DataFrames
  id: totrans-4524
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assuming the following data is in a `DataFrame` `data`, we can insert it into
    the database using [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql").
  id: totrans-4525
  prefs: []
  type: TYPE_NORMAL
- en: '| id | Date | Col_1 | Col_2 | Col_3 |'
  id: totrans-4526
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-4527
  prefs: []
  type: TYPE_TB
- en: '| 26 | 2012-10-18 | X | 25.7 | True |'
  id: totrans-4528
  prefs: []
  type: TYPE_TB
- en: '| 42 | 2012-10-19 | Y | -12.4 | False |'
  id: totrans-4529
  prefs: []
  type: TYPE_TB
- en: '| 63 | 2012-10-20 | Z | 5.73 | True |'
  id: totrans-4530
  prefs: []
  type: TYPE_TB
- en: '[PRE933]'
  id: totrans-4531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE933]'
- en: 'With some databases, writing large DataFrames can result in errors due to packet
    size limitations being exceeded. This can be avoided by setting the `chunksize`
    parameter when calling `to_sql`. For example, the following writes `data` to the
    database in batches of 1000 rows at a time:'
  id: totrans-4532
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE934]'
  id: totrans-4533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE934]'
- en: SQL data types
  id: totrans-4534
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensuring consistent data type management across SQL databases is challenging.
    Not every SQL database offers the same types, and even when they do the implementation
    of a given type can vary in ways that have subtle effects on how types can be
    preserved.
  id: totrans-4535
  prefs: []
  type: TYPE_NORMAL
- en: 'For the best odds at preserving database types users are advised to use ADBC
    drivers when available. The Arrow type system offers a wider array of types that
    more closely match database types than the historical pandas/NumPy type system.
    To illustrate, note this (non-exhaustive) listing of types available in different
    databases and pandas backends:'
  id: totrans-4536
  prefs: []
  type: TYPE_NORMAL
- en: '| numpy/pandas | arrow | postgres | sqlite |'
  id: totrans-4537
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-4538
  prefs: []
  type: TYPE_TB
- en: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
  id: totrans-4539
  prefs: []
  type: TYPE_TB
- en: '| int32/Int32 | int32 | INTEGER | INTEGER |'
  id: totrans-4540
  prefs: []
  type: TYPE_TB
- en: '| int64/Int64 | int64 | BIGINT | INTEGER |'
  id: totrans-4541
  prefs: []
  type: TYPE_TB
- en: '| float32 | float32 | REAL | REAL |'
  id: totrans-4542
  prefs: []
  type: TYPE_TB
- en: '| float64 | float64 | DOUBLE PRECISION | REAL |'
  id: totrans-4543
  prefs: []
  type: TYPE_TB
- en: '| object | string | TEXT | TEXT |'
  id: totrans-4544
  prefs: []
  type: TYPE_TB
- en: '| bool | `bool_` | BOOLEAN |  |'
  id: totrans-4545
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
  id: totrans-4546
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
  id: totrans-4547
  prefs: []
  type: TYPE_TB
- en: '|  | date32 | DATE |  |'
  id: totrans-4548
  prefs: []
  type: TYPE_TB
- en: '|  | month_day_nano_interval | INTERVAL |  |'
  id: totrans-4549
  prefs: []
  type: TYPE_TB
- en: '|  | binary | BINARY | BLOB |'
  id: totrans-4550
  prefs: []
  type: TYPE_TB
- en: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
  id: totrans-4551
  prefs: []
  type: TYPE_TB
- en: '|  | list | ARRAY [[1]](#f1) |  |'
  id: totrans-4552
  prefs: []
  type: TYPE_TB
- en: '|  | struct |'
  id: totrans-4553
  prefs: []
  type: TYPE_TB
- en: COMPOSITE TYPE
  id: totrans-4554
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]](#f1)'
  id: totrans-4555
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-4556
  prefs: []
  type: TYPE_TB
- en: Footnotes
  id: totrans-4557
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in preserving database types as best as possible throughout
    the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"`
    argument of [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql")
  id: totrans-4558
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE935]'
  id: totrans-4559
  prefs: []
  type: TYPE_PRE
  zh: '[PRE935]'
- en: This will prevent your data from being converted to the traditional pandas/NumPy
    type system, which often converts SQL types in ways that make them impossible
    to round-trip.
  id: totrans-4560
  prefs: []
  type: TYPE_NORMAL
- en: In case an ADBC driver is not available, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") will try to map your data to an appropriate SQL data
    type based on the dtype of the data. When you have columns of dtype `object`,
    pandas will try to infer the data type.
  id: totrans-4561
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always override the default type by specifying the desired SQL type
    of any of the columns by using the `dtype` argument. This argument needs a dictionary
    mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback
    mode). For example, specifying to use the sqlalchemy `String` type instead of
    the default `Text` type for string columns:'
  id: totrans-4562
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE936]'
  id: totrans-4563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE936]'
- en: Note
  id: totrans-4564
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limited support for timedelta’s in the different database flavors,
    columns with type `timedelta64` will be written as integer values as nanoseconds
    to the database and a warning will be raised. The only exception to this is when
    using the ADBC PostgreSQL driver in which case a timedelta will be written to
    the database as an `INTERVAL`
  id: totrans-4565
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4566
  prefs: []
  type: TYPE_NORMAL
- en: Columns of `category` dtype will be converted to the dense representation as
    you would get with `np.asarray(categorical)` (e.g. for string categories this
    gives an array of strings). Because of this, reading the database table back in
    does **not** generate a categorical.
  id: totrans-4567
  prefs: []
  type: TYPE_NORMAL
- en: '### Datetime data types'
  id: totrans-4568
  prefs: []
  type: TYPE_NORMAL
- en: Using ADBC or SQLAlchemy, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") is capable of writing datetime data that is timezone
    naive or timezone aware. However, the resulting data stored in the database ultimately
    depends on the supported data type for datetime data of the database system being
    used.
  id: totrans-4569
  prefs: []
  type: TYPE_NORMAL
- en: The following table lists supported data types for datetime data for some common
    databases. Other database dialects may have different data types for datetime
    data.
  id: totrans-4570
  prefs: []
  type: TYPE_NORMAL
- en: '| Database | SQL Datetime Types | Timezone Support |'
  id: totrans-4571
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-4572
  prefs: []
  type: TYPE_TB
- en: '| SQLite | `TEXT` | No |'
  id: totrans-4573
  prefs: []
  type: TYPE_TB
- en: '| MySQL | `TIMESTAMP` or `DATETIME` | No |'
  id: totrans-4574
  prefs: []
  type: TYPE_TB
- en: '| PostgreSQL | `TIMESTAMP` or `TIMESTAMP WITH TIME ZONE` | Yes |'
  id: totrans-4575
  prefs: []
  type: TYPE_TB
- en: When writing timezone aware data to databases that do not support timezones,
    the data will be written as timezone naive timestamps that are in local time with
    respect to the timezone.
  id: totrans-4576
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") is also capable of reading datetime data that is timezone
    aware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will convert
    the data to UTC.'
  id: totrans-4577
  prefs: []
  type: TYPE_NORMAL
- en: '#### Insertion method'
  id: totrans-4578
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `method` controls the SQL insertion clause used. Possible values
    are:'
  id: totrans-4579
  prefs: []
  type: TYPE_NORMAL
- en: '`None`: Uses standard SQL `INSERT` clause (one per row).'
  id: totrans-4580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''multi''`: Pass multiple values in a single `INSERT` clause. It uses a *special*
    SQL syntax not supported by all backends. This usually provides better performance
    for analytic databases like *Presto* and *Redshift*, but has worse performance
    for traditional SQL backend if the table contains many columns. For more information
    check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).'
  id: totrans-4581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'callable with signature `(pd_table, conn, keys, data_iter)`: This can be used
    to implement a more performant insertion method based on specific backend dialect
    features.'
  id: totrans-4582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):'
  id: totrans-4583
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE937]'
  id: totrans-4584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE937]'
- en: Reading tables
  id: totrans-4585
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") will read a database table given the table name and optionally
    a subset of columns to read.'
  id: totrans-4586
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4587
  prefs: []
  type: TYPE_NORMAL
- en: In order to use [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table"), you **must** have the ADBC driver or SQLAlchemy optional
    dependency installed.
  id: totrans-4588
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE938]'
  id: totrans-4589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE938]'
- en: Note
  id: totrans-4590
  prefs: []
  type: TYPE_NORMAL
- en: ADBC drivers will map database types directly back to arrow types. For other
    drivers note that pandas infers column dtypes from query outputs, and not by looking
    up data types in the physical database schema. For example, assume `userid` is
    an integer column in a table. Then, intuitively, `select userid ...` will return
    integer-valued series, while `select cast(userid as text) ...` will return object-valued
    (str) series. Accordingly, if the query output is empty, then all resulting columns
    will be returned as object-valued (since they are most general). If you foresee
    that your query will sometimes generate an empty result, you may want to explicitly
    typecast afterwards to ensure dtype integrity.
  id: totrans-4591
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify the name of the column as the `DataFrame` index, and specify
    a subset of columns to be read.
  id: totrans-4592
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE939]'
  id: totrans-4593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE939]'
- en: 'And you can explicitly force columns to be parsed as dates:'
  id: totrans-4594
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE940]'
  id: totrans-4595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE940]'
- en: 'If needed you can explicitly specify a format string, or a dict of arguments
    to pass to [`pandas.to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime"):'
  id: totrans-4596
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE941]'
  id: totrans-4597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE941]'
- en: You can check if a table exists using `has_table()`
  id: totrans-4598
  prefs: []
  type: TYPE_NORMAL
- en: Schema support
  id: totrans-4599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reading from and writing to different schema’s is supported through the `schema`
    keyword in the [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") functions. Note however that this depends on the database
    flavor (sqlite does not have schema’s). For example:'
  id: totrans-4600
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE942]'
  id: totrans-4601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE942]'
- en: Querying
  id: totrans-4602
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can query using raw SQL in the [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function. In this case you must use the SQL variant appropriate
    for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression
    language constructs, which are database-agnostic.
  id: totrans-4603
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE943]'
  id: totrans-4604
  prefs: []
  type: TYPE_PRE
  zh: '[PRE943]'
- en: Of course, you can specify a more “complex” query.
  id: totrans-4605
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE944]'
  id: totrans-4606
  prefs: []
  type: TYPE_PRE
  zh: '[PRE944]'
- en: 'The [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function supports a `chunksize` argument. Specifying
    this will return an iterator through chunks of the query result:'
  id: totrans-4607
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE945]'
  id: totrans-4608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE945]'
- en: '[PRE946]'
  id: totrans-4609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE946]'
- en: Engine connection examples
  id: totrans-4610
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to.
  id: totrans-4611
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE947]'
  id: totrans-4612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE947]'
- en: For more information see the examples the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  id: totrans-4613
  prefs: []
  type: TYPE_NORMAL
- en: Advanced SQLAlchemy queries
  id: totrans-4614
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use SQLAlchemy constructs to describe your query.
  id: totrans-4615
  prefs: []
  type: TYPE_NORMAL
- en: Use `sqlalchemy.text()` to specify query parameters in a backend-neutral way
  id: totrans-4616
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE948]'
  id: totrans-4617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE948]'
- en: If you have an SQLAlchemy description of your database you can express where
    conditions using SQLAlchemy expressions
  id: totrans-4618
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE949]'
  id: totrans-4619
  prefs: []
  type: TYPE_PRE
  zh: '[PRE949]'
- en: You can combine SQLAlchemy expressions with parameters passed to [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") using `sqlalchemy.bindparam()`
  id: totrans-4620
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE950]'
  id: totrans-4621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE950]'
- en: Sqlite fallback
  id: totrans-4622
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of sqlite is supported without using SQLAlchemy. This mode requires
    a Python database adapter which respect the [Python DB-API](https://www.python.org/dev/peps/pep-0249/).
  id: totrans-4623
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create connections like so:'
  id: totrans-4624
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE951]'
  id: totrans-4625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE951]'
- en: 'And then issue the following queries:'
  id: totrans-4626
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE952]'
  id: totrans-4627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE952]'
- en: Writing DataFrames
  id: totrans-4628
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assuming the following data is in a `DataFrame` `data`, we can insert it into
    the database using [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql").
  id: totrans-4629
  prefs: []
  type: TYPE_NORMAL
- en: '| id | Date | Col_1 | Col_2 | Col_3 |'
  id: totrans-4630
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-4631
  prefs: []
  type: TYPE_TB
- en: '| 26 | 2012-10-18 | X | 25.7 | True |'
  id: totrans-4632
  prefs: []
  type: TYPE_TB
- en: '| 42 | 2012-10-19 | Y | -12.4 | False |'
  id: totrans-4633
  prefs: []
  type: TYPE_TB
- en: '| 63 | 2012-10-20 | Z | 5.73 | True |'
  id: totrans-4634
  prefs: []
  type: TYPE_TB
- en: '[PRE953]'
  id: totrans-4635
  prefs: []
  type: TYPE_PRE
  zh: '[PRE953]'
- en: 'With some databases, writing large DataFrames can result in errors due to packet
    size limitations being exceeded. This can be avoided by setting the `chunksize`
    parameter when calling `to_sql`. For example, the following writes `data` to the
    database in batches of 1000 rows at a time:'
  id: totrans-4636
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE954]'
  id: totrans-4637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE954]'
- en: SQL data types
  id: totrans-4638
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensuring consistent data type management across SQL databases is challenging.
    Not every SQL database offers the same types, and even when they do the implementation
    of a given type can vary in ways that have subtle effects on how types can be
    preserved.
  id: totrans-4639
  prefs: []
  type: TYPE_NORMAL
- en: 'For the best odds at preserving database types users are advised to use ADBC
    drivers when available. The Arrow type system offers a wider array of types that
    more closely match database types than the historical pandas/NumPy type system.
    To illustrate, note this (non-exhaustive) listing of types available in different
    databases and pandas backends:'
  id: totrans-4640
  prefs: []
  type: TYPE_NORMAL
- en: '| numpy/pandas | arrow | postgres | sqlite |'
  id: totrans-4641
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-4642
  prefs: []
  type: TYPE_TB
- en: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
  id: totrans-4643
  prefs: []
  type: TYPE_TB
- en: '| int32/Int32 | int32 | INTEGER | INTEGER |'
  id: totrans-4644
  prefs: []
  type: TYPE_TB
- en: '| int64/Int64 | int64 | BIGINT | INTEGER |'
  id: totrans-4645
  prefs: []
  type: TYPE_TB
- en: '| float32 | float32 | REAL | REAL |'
  id: totrans-4646
  prefs: []
  type: TYPE_TB
- en: '| float64 | float64 | DOUBLE PRECISION | REAL |'
  id: totrans-4647
  prefs: []
  type: TYPE_TB
- en: '| object | string | TEXT | TEXT |'
  id: totrans-4648
  prefs: []
  type: TYPE_TB
- en: '| bool | `bool_` | BOOLEAN |  |'
  id: totrans-4649
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
  id: totrans-4650
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
  id: totrans-4651
  prefs: []
  type: TYPE_TB
- en: '|  | date32 | DATE |  |'
  id: totrans-4652
  prefs: []
  type: TYPE_TB
- en: '|  | month_day_nano_interval | INTERVAL |  |'
  id: totrans-4653
  prefs: []
  type: TYPE_TB
- en: '|  | binary | BINARY | BLOB |'
  id: totrans-4654
  prefs: []
  type: TYPE_TB
- en: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
  id: totrans-4655
  prefs: []
  type: TYPE_TB
- en: '|  | list | ARRAY [[1]](#f1) |  |'
  id: totrans-4656
  prefs: []
  type: TYPE_TB
- en: '|  | struct |'
  id: totrans-4657
  prefs: []
  type: TYPE_TB
- en: COMPOSITE TYPE
  id: totrans-4658
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]](#f1)'
  id: totrans-4659
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-4660
  prefs: []
  type: TYPE_TB
- en: Footnotes
  id: totrans-4661
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in preserving database types as best as possible throughout
    the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"`
    argument of [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql")
  id: totrans-4662
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE955]'
  id: totrans-4663
  prefs: []
  type: TYPE_PRE
  zh: '[PRE955]'
- en: This will prevent your data from being converted to the traditional pandas/NumPy
    type system, which often converts SQL types in ways that make them impossible
    to round-trip.
  id: totrans-4664
  prefs: []
  type: TYPE_NORMAL
- en: In case an ADBC driver is not available, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") will try to map your data to an appropriate SQL data
    type based on the dtype of the data. When you have columns of dtype `object`,
    pandas will try to infer the data type.
  id: totrans-4665
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always override the default type by specifying the desired SQL type
    of any of the columns by using the `dtype` argument. This argument needs a dictionary
    mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback
    mode). For example, specifying to use the sqlalchemy `String` type instead of
    the default `Text` type for string columns:'
  id: totrans-4666
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE956]'
  id: totrans-4667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE956]'
- en: Note
  id: totrans-4668
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limited support for timedelta’s in the different database flavors,
    columns with type `timedelta64` will be written as integer values as nanoseconds
    to the database and a warning will be raised. The only exception to this is when
    using the ADBC PostgreSQL driver in which case a timedelta will be written to
    the database as an `INTERVAL`
  id: totrans-4669
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4670
  prefs: []
  type: TYPE_NORMAL
- en: Columns of `category` dtype will be converted to the dense representation as
    you would get with `np.asarray(categorical)` (e.g. for string categories this
    gives an array of strings). Because of this, reading the database table back in
    does **not** generate a categorical.
  id: totrans-4671
  prefs: []
  type: TYPE_NORMAL
- en: SQL data types
  id: totrans-4672
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensuring consistent data type management across SQL databases is challenging.
    Not every SQL database offers the same types, and even when they do the implementation
    of a given type can vary in ways that have subtle effects on how types can be
    preserved.
  id: totrans-4673
  prefs: []
  type: TYPE_NORMAL
- en: 'For the best odds at preserving database types users are advised to use ADBC
    drivers when available. The Arrow type system offers a wider array of types that
    more closely match database types than the historical pandas/NumPy type system.
    To illustrate, note this (non-exhaustive) listing of types available in different
    databases and pandas backends:'
  id: totrans-4674
  prefs: []
  type: TYPE_NORMAL
- en: '| numpy/pandas | arrow | postgres | sqlite |'
  id: totrans-4675
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-4676
  prefs: []
  type: TYPE_TB
- en: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
  id: totrans-4677
  prefs: []
  type: TYPE_TB
- en: '| int32/Int32 | int32 | INTEGER | INTEGER |'
  id: totrans-4678
  prefs: []
  type: TYPE_TB
- en: '| int64/Int64 | int64 | BIGINT | INTEGER |'
  id: totrans-4679
  prefs: []
  type: TYPE_TB
- en: '| float32 | float32 | REAL | REAL |'
  id: totrans-4680
  prefs: []
  type: TYPE_TB
- en: '| float64 | float64 | DOUBLE PRECISION | REAL |'
  id: totrans-4681
  prefs: []
  type: TYPE_TB
- en: '| object | string | TEXT | TEXT |'
  id: totrans-4682
  prefs: []
  type: TYPE_TB
- en: '| bool | `bool_` | BOOLEAN |  |'
  id: totrans-4683
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
  id: totrans-4684
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
  id: totrans-4685
  prefs: []
  type: TYPE_TB
- en: '|  | date32 | DATE |  |'
  id: totrans-4686
  prefs: []
  type: TYPE_TB
- en: '|  | month_day_nano_interval | INTERVAL |  |'
  id: totrans-4687
  prefs: []
  type: TYPE_TB
- en: '|  | binary | BINARY | BLOB |'
  id: totrans-4688
  prefs: []
  type: TYPE_TB
- en: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
  id: totrans-4689
  prefs: []
  type: TYPE_TB
- en: '|  | list | ARRAY [[1]](#f1) |  |'
  id: totrans-4690
  prefs: []
  type: TYPE_TB
- en: '|  | struct |'
  id: totrans-4691
  prefs: []
  type: TYPE_TB
- en: COMPOSITE TYPE
  id: totrans-4692
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]](#f1)'
  id: totrans-4693
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-4694
  prefs: []
  type: TYPE_TB
- en: Footnotes
  id: totrans-4695
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in preserving database types as best as possible throughout
    the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"`
    argument of [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql")
  id: totrans-4696
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE957]'
  id: totrans-4697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE957]'
- en: This will prevent your data from being converted to the traditional pandas/NumPy
    type system, which often converts SQL types in ways that make them impossible
    to round-trip.
  id: totrans-4698
  prefs: []
  type: TYPE_NORMAL
- en: In case an ADBC driver is not available, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") will try to map your data to an appropriate SQL data
    type based on the dtype of the data. When you have columns of dtype `object`,
    pandas will try to infer the data type.
  id: totrans-4699
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always override the default type by specifying the desired SQL type
    of any of the columns by using the `dtype` argument. This argument needs a dictionary
    mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback
    mode). For example, specifying to use the sqlalchemy `String` type instead of
    the default `Text` type for string columns:'
  id: totrans-4700
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE958]'
  id: totrans-4701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE958]'
- en: Note
  id: totrans-4702
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limited support for timedelta’s in the different database flavors,
    columns with type `timedelta64` will be written as integer values as nanoseconds
    to the database and a warning will be raised. The only exception to this is when
    using the ADBC PostgreSQL driver in which case a timedelta will be written to
    the database as an `INTERVAL`
  id: totrans-4703
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4704
  prefs: []
  type: TYPE_NORMAL
- en: Columns of `category` dtype will be converted to the dense representation as
    you would get with `np.asarray(categorical)` (e.g. for string categories this
    gives an array of strings). Because of this, reading the database table back in
    does **not** generate a categorical.
  id: totrans-4705
  prefs: []
  type: TYPE_NORMAL
- en: '### Datetime data types'
  id: totrans-4706
  prefs: []
  type: TYPE_NORMAL
- en: Using ADBC or SQLAlchemy, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") is capable of writing datetime data that is timezone
    naive or timezone aware. However, the resulting data stored in the database ultimately
    depends on the supported data type for datetime data of the database system being
    used.
  id: totrans-4707
  prefs: []
  type: TYPE_NORMAL
- en: The following table lists supported data types for datetime data for some common
    databases. Other database dialects may have different data types for datetime
    data.
  id: totrans-4708
  prefs: []
  type: TYPE_NORMAL
- en: '| Database | SQL Datetime Types | Timezone Support |'
  id: totrans-4709
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-4710
  prefs: []
  type: TYPE_TB
- en: '| SQLite | `TEXT` | No |'
  id: totrans-4711
  prefs: []
  type: TYPE_TB
- en: '| MySQL | `TIMESTAMP` or `DATETIME` | No |'
  id: totrans-4712
  prefs: []
  type: TYPE_TB
- en: '| PostgreSQL | `TIMESTAMP` or `TIMESTAMP WITH TIME ZONE` | Yes |'
  id: totrans-4713
  prefs: []
  type: TYPE_TB
- en: When writing timezone aware data to databases that do not support timezones,
    the data will be written as timezone naive timestamps that are in local time with
    respect to the timezone.
  id: totrans-4714
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") is also capable of reading datetime data that is timezone
    aware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will convert
    the data to UTC.'
  id: totrans-4715
  prefs: []
  type: TYPE_NORMAL
- en: '#### Insertion method'
  id: totrans-4716
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `method` controls the SQL insertion clause used. Possible values
    are:'
  id: totrans-4717
  prefs: []
  type: TYPE_NORMAL
- en: '`None`: Uses standard SQL `INSERT` clause (one per row).'
  id: totrans-4718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''multi''`: Pass multiple values in a single `INSERT` clause. It uses a *special*
    SQL syntax not supported by all backends. This usually provides better performance
    for analytic databases like *Presto* and *Redshift*, but has worse performance
    for traditional SQL backend if the table contains many columns. For more information
    check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).'
  id: totrans-4719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'callable with signature `(pd_table, conn, keys, data_iter)`: This can be used
    to implement a more performant insertion method based on specific backend dialect
    features.'
  id: totrans-4720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):'
  id: totrans-4721
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE959]  #### Insertion method'
  id: totrans-4722
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `method` controls the SQL insertion clause used. Possible values
    are:'
  id: totrans-4723
  prefs: []
  type: TYPE_NORMAL
- en: '`None`: Uses standard SQL `INSERT` clause (one per row).'
  id: totrans-4724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''multi''`: Pass multiple values in a single `INSERT` clause. It uses a *special*
    SQL syntax not supported by all backends. This usually provides better performance
    for analytic databases like *Presto* and *Redshift*, but has worse performance
    for traditional SQL backend if the table contains many columns. For more information
    check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).'
  id: totrans-4725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'callable with signature `(pd_table, conn, keys, data_iter)`: This can be used
    to implement a more performant insertion method based on specific backend dialect
    features.'
  id: totrans-4726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):'
  id: totrans-4727
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE960]'
  id: totrans-4728
  prefs: []
  type: TYPE_PRE
  zh: '[PRE960]'
- en: Reading tables
  id: totrans-4729
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") will read a database table given the table name and optionally
    a subset of columns to read.'
  id: totrans-4730
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4731
  prefs: []
  type: TYPE_NORMAL
- en: In order to use [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table"), you **must** have the ADBC driver or SQLAlchemy optional
    dependency installed.
  id: totrans-4732
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE961]'
  id: totrans-4733
  prefs: []
  type: TYPE_PRE
  zh: '[PRE961]'
- en: Note
  id: totrans-4734
  prefs: []
  type: TYPE_NORMAL
- en: ADBC drivers will map database types directly back to arrow types. For other
    drivers note that pandas infers column dtypes from query outputs, and not by looking
    up data types in the physical database schema. For example, assume `userid` is
    an integer column in a table. Then, intuitively, `select userid ...` will return
    integer-valued series, while `select cast(userid as text) ...` will return object-valued
    (str) series. Accordingly, if the query output is empty, then all resulting columns
    will be returned as object-valued (since they are most general). If you foresee
    that your query will sometimes generate an empty result, you may want to explicitly
    typecast afterwards to ensure dtype integrity.
  id: totrans-4735
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify the name of the column as the `DataFrame` index, and specify
    a subset of columns to be read.
  id: totrans-4736
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE962]'
  id: totrans-4737
  prefs: []
  type: TYPE_PRE
  zh: '[PRE962]'
- en: 'And you can explicitly force columns to be parsed as dates:'
  id: totrans-4738
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE963]'
  id: totrans-4739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE963]'
- en: 'If needed you can explicitly specify a format string, or a dict of arguments
    to pass to [`pandas.to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime"):'
  id: totrans-4740
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE964]'
  id: totrans-4741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE964]'
- en: You can check if a table exists using `has_table()`
  id: totrans-4742
  prefs: []
  type: TYPE_NORMAL
- en: Schema support
  id: totrans-4743
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reading from and writing to different schema’s is supported through the `schema`
    keyword in the [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") functions. Note however that this depends on the database
    flavor (sqlite does not have schema’s). For example:'
  id: totrans-4744
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE965]'
  id: totrans-4745
  prefs: []
  type: TYPE_PRE
  zh: '[PRE965]'
- en: Querying
  id: totrans-4746
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can query using raw SQL in the [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function. In this case you must use the SQL variant appropriate
    for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression
    language constructs, which are database-agnostic.
  id: totrans-4747
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE966]'
  id: totrans-4748
  prefs: []
  type: TYPE_PRE
  zh: '[PRE966]'
- en: Of course, you can specify a more “complex” query.
  id: totrans-4749
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE967]'
  id: totrans-4750
  prefs: []
  type: TYPE_PRE
  zh: '[PRE967]'
- en: 'The [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function supports a `chunksize` argument. Specifying
    this will return an iterator through chunks of the query result:'
  id: totrans-4751
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE968]'
  id: totrans-4752
  prefs: []
  type: TYPE_PRE
  zh: '[PRE968]'
- en: '[PRE969]'
  id: totrans-4753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE969]'
- en: Engine connection examples
  id: totrans-4754
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to.
  id: totrans-4755
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE970]'
  id: totrans-4756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE970]'
- en: For more information see the examples the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  id: totrans-4757
  prefs: []
  type: TYPE_NORMAL
- en: Advanced SQLAlchemy queries
  id: totrans-4758
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use SQLAlchemy constructs to describe your query.
  id: totrans-4759
  prefs: []
  type: TYPE_NORMAL
- en: Use `sqlalchemy.text()` to specify query parameters in a backend-neutral way
  id: totrans-4760
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE971]'
  id: totrans-4761
  prefs: []
  type: TYPE_PRE
  zh: '[PRE971]'
- en: If you have an SQLAlchemy description of your database you can express where
    conditions using SQLAlchemy expressions
  id: totrans-4762
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE972]'
  id: totrans-4763
  prefs: []
  type: TYPE_PRE
  zh: '[PRE972]'
- en: You can combine SQLAlchemy expressions with parameters passed to [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") using `sqlalchemy.bindparam()`
  id: totrans-4764
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE973]'
  id: totrans-4765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE973]'
- en: Sqlite fallback
  id: totrans-4766
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of sqlite is supported without using SQLAlchemy. This mode requires
    a Python database adapter which respect the [Python DB-API](https://www.python.org/dev/peps/pep-0249/).
  id: totrans-4767
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create connections like so:'
  id: totrans-4768
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE974]'
  id: totrans-4769
  prefs: []
  type: TYPE_PRE
  zh: '[PRE974]'
- en: 'And then issue the following queries:'
  id: totrans-4770
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE975]'
  id: totrans-4771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE975]'
- en: '## Google BigQuery'
  id: totrans-4772
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas-gbq` package provides functionality to read/write from Google BigQuery.
  id: totrans-4773
  prefs: []
  type: TYPE_NORMAL
- en: pandas integrates with this external package. if `pandas-gbq` is installed,
    you can use the pandas methods `pd.read_gbq` and `DataFrame.to_gbq`, which will
    call the respective functions from `pandas-gbq`.
  id: totrans-4774
  prefs: []
  type: TYPE_NORMAL
- en: Full documentation can be found [here](https://pandas-gbq.readthedocs.io/en/latest/).
  id: totrans-4775
  prefs: []
  type: TYPE_NORMAL
- en: '## Stata format'
  id: totrans-4776
  prefs: []
  type: TYPE_NORMAL
- en: '### Writing to stata format'
  id: totrans-4777
  prefs: []
  type: TYPE_NORMAL
- en: The method `DataFrame.to_stata()` will write a DataFrame into a .dta file. The
    format version of this file is always 115 (Stata 12).
  id: totrans-4778
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE976]'
  id: totrans-4779
  prefs: []
  type: TYPE_PRE
  zh: '[PRE976]'
- en: '*Stata* data files have limited data type support; only strings with 244 or
    fewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be stored
    in `.dta` files. Additionally, *Stata* reserves certain values to represent missing
    data. Exporting a non-missing value that is outside of the permitted range in
    Stata for a particular data type will retype the variable to the next larger size.
    For example, `int8` values are restricted to lie between -127 and 100 in Stata,
    and so variables with values above 100 will trigger a conversion to `int16`. `nan`
    values in floating points data types are stored as the basic missing data type
    (`.` in *Stata*).'
  id: totrans-4780
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4781
  prefs: []
  type: TYPE_NORMAL
- en: It is not possible to export missing data values for integer data types.
  id: totrans-4782
  prefs: []
  type: TYPE_NORMAL
- en: The *Stata* writer gracefully handles other data types including `int64`, `bool`,
    `uint8`, `uint16`, `uint32` by casting to the smallest supported type that can
    represent the data. For example, data with a type of `uint8` will be cast to `int8`
    if all values are less than 100 (the upper bound for non-missing `int8` data in
    *Stata*), or, if values are outside of this range, the variable is cast to `int16`.
  id: totrans-4783
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4784
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from `int64` to `float64` may result in a loss of precision if `int64`
    values are larger than 2**53.
  id: totrans-4785
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4786
  prefs: []
  type: TYPE_NORMAL
- en: '`StataWriter` and `DataFrame.to_stata()` only support fixed width strings containing
    up to 244 characters, a limitation imposed by the version 115 dta file format.
    Attempting to write *Stata* dta files with strings longer than 244 characters
    raises a `ValueError`.  ### Reading from Stata format'
  id: totrans-4787
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function `read_stata` will read a dta file and return either a
    `DataFrame` or a `pandas.api.typing.StataReader` that can be used to read the
    file incrementally.
  id: totrans-4788
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE977]'
  id: totrans-4789
  prefs: []
  type: TYPE_PRE
  zh: '[PRE977]'
- en: Specifying a `chunksize` yields a `pandas.api.typing.StataReader` instance that
    can be used to read `chunksize` lines from the file at a time. The `StataReader`
    object can be used as an iterator.
  id: totrans-4790
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE978]'
  id: totrans-4791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE978]'
- en: For more fine-grained control, use `iterator=True` and specify `chunksize` with
    each call to `read()`.
  id: totrans-4792
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE979]'
  id: totrans-4793
  prefs: []
  type: TYPE_PRE
  zh: '[PRE979]'
- en: Currently the `index` is retrieved as a column.
  id: totrans-4794
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_categoricals` indicates whether value labels should be
    read and used to create a `Categorical` variable from them. Value labels can also
    be retrieved by the function `value_labels`, which requires `read()` to be called
    before use.
  id: totrans-4795
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_missing` indicates whether missing value representations
    in Stata should be preserved. If `False` (the default), missing values are represented
    as `np.nan`. If `True`, missing values are represented using `StataMissingValue`
    objects, and columns containing missing values will have `object` data type.
  id: totrans-4796
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4797
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") and `StataReader` support .dta formats 113-115 (Stata 10-12),
    117 (Stata 13), and 118 (Stata 14).'
  id: totrans-4798
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4799
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `preserve_dtypes=False` will upcast to the standard pandas data types:
    `int64` for all integer types and `float64` for floating point data. By default,
    the Stata data types are preserved when importing.'
  id: totrans-4800
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4801
  prefs: []
  type: TYPE_NORMAL
- en: All `StataReader` objects, whether created by [`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") (when using `iterator=True` or `chunksize`) or instantiated
    by hand, must be used as context managers (e.g. the `with` statement). While the
    `close()` method is available, its use is unsupported. It is not part of the public
    API and will be removed in with future without warning.
  id: totrans-4802
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  id: totrans-4803
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` data can be exported to *Stata* data files as value labeled data.
    The exported data consists of the underlying category codes as integer data values
    and the categories as value labels. *Stata* does not have an explicit equivalent
    to a `Categorical` and information about *whether* the variable is ordered is
    lost when exporting.'
  id: totrans-4804
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4805
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* only supports string value labels, and so `str` is called on the categories
    when exporting data. Exporting `Categorical` variables with non-string categories
    produces a warning, and can result a loss of information if the `str` representations
    of the categories are not unique.'
  id: totrans-4806
  prefs: []
  type: TYPE_NORMAL
- en: Labeled data can similarly be imported from *Stata* data files as `Categorical`
    variables using the keyword argument `convert_categoricals` (`True` by default).
    The keyword argument `order_categoricals` (`True` by default) determines whether
    imported `Categorical` variables are ordered.
  id: totrans-4807
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4808
  prefs: []
  type: TYPE_NORMAL
- en: 'When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since `Categorical` variables always use integer data
    types between `-1` and `n-1` where `n` is the number of categories. If the original
    values in the *Stata* data file are required, these can be imported by setting
    `convert_categoricals=False`, which will import original data (but not the variable
    labels). The original values can be matched to the imported categorical data since
    there is a simple mapping between the original *Stata* data values and the category
    codes of imported Categorical variables: missing values are assigned code `-1`,
    and the smallest original value is assigned `0`, the second smallest is assigned
    `1` and so on until the largest original value is assigned the code `n-1`.'
  id: totrans-4809
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4810
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a `Categorical` with string categories for the values that are labeled and numeric
    categories for values with no label.  ### Writing to stata format'
  id: totrans-4811
  prefs: []
  type: TYPE_NORMAL
- en: The method `DataFrame.to_stata()` will write a DataFrame into a .dta file. The
    format version of this file is always 115 (Stata 12).
  id: totrans-4812
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE980]'
  id: totrans-4813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE980]'
- en: '*Stata* data files have limited data type support; only strings with 244 or
    fewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be stored
    in `.dta` files. Additionally, *Stata* reserves certain values to represent missing
    data. Exporting a non-missing value that is outside of the permitted range in
    Stata for a particular data type will retype the variable to the next larger size.
    For example, `int8` values are restricted to lie between -127 and 100 in Stata,
    and so variables with values above 100 will trigger a conversion to `int16`. `nan`
    values in floating points data types are stored as the basic missing data type
    (`.` in *Stata*).'
  id: totrans-4814
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4815
  prefs: []
  type: TYPE_NORMAL
- en: It is not possible to export missing data values for integer data types.
  id: totrans-4816
  prefs: []
  type: TYPE_NORMAL
- en: The *Stata* writer gracefully handles other data types including `int64`, `bool`,
    `uint8`, `uint16`, `uint32` by casting to the smallest supported type that can
    represent the data. For example, data with a type of `uint8` will be cast to `int8`
    if all values are less than 100 (the upper bound for non-missing `int8` data in
    *Stata*), or, if values are outside of this range, the variable is cast to `int16`.
  id: totrans-4817
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4818
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from `int64` to `float64` may result in a loss of precision if `int64`
    values are larger than 2**53.
  id: totrans-4819
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4820
  prefs: []
  type: TYPE_NORMAL
- en: '`StataWriter` and `DataFrame.to_stata()` only support fixed width strings containing
    up to 244 characters, a limitation imposed by the version 115 dta file format.
    Attempting to write *Stata* dta files with strings longer than 244 characters
    raises a `ValueError`.'
  id: totrans-4821
  prefs: []
  type: TYPE_NORMAL
- en: '### Reading from Stata format'
  id: totrans-4822
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function `read_stata` will read a dta file and return either a
    `DataFrame` or a `pandas.api.typing.StataReader` that can be used to read the
    file incrementally.
  id: totrans-4823
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE981]'
  id: totrans-4824
  prefs: []
  type: TYPE_PRE
  zh: '[PRE981]'
- en: Specifying a `chunksize` yields a `pandas.api.typing.StataReader` instance that
    can be used to read `chunksize` lines from the file at a time. The `StataReader`
    object can be used as an iterator.
  id: totrans-4825
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE982]'
  id: totrans-4826
  prefs: []
  type: TYPE_PRE
  zh: '[PRE982]'
- en: For more fine-grained control, use `iterator=True` and specify `chunksize` with
    each call to `read()`.
  id: totrans-4827
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE983]'
  id: totrans-4828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE983]'
- en: Currently the `index` is retrieved as a column.
  id: totrans-4829
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_categoricals` indicates whether value labels should be
    read and used to create a `Categorical` variable from them. Value labels can also
    be retrieved by the function `value_labels`, which requires `read()` to be called
    before use.
  id: totrans-4830
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_missing` indicates whether missing value representations
    in Stata should be preserved. If `False` (the default), missing values are represented
    as `np.nan`. If `True`, missing values are represented using `StataMissingValue`
    objects, and columns containing missing values will have `object` data type.
  id: totrans-4831
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4832
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") and `StataReader` support .dta formats 113-115 (Stata 10-12),
    117 (Stata 13), and 118 (Stata 14).'
  id: totrans-4833
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4834
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `preserve_dtypes=False` will upcast to the standard pandas data types:
    `int64` for all integer types and `float64` for floating point data. By default,
    the Stata data types are preserved when importing.'
  id: totrans-4835
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4836
  prefs: []
  type: TYPE_NORMAL
- en: All `StataReader` objects, whether created by [`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") (when using `iterator=True` or `chunksize`) or instantiated
    by hand, must be used as context managers (e.g. the `with` statement). While the
    `close()` method is available, its use is unsupported. It is not part of the public
    API and will be removed in with future without warning.
  id: totrans-4837
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  id: totrans-4838
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` data can be exported to *Stata* data files as value labeled data.
    The exported data consists of the underlying category codes as integer data values
    and the categories as value labels. *Stata* does not have an explicit equivalent
    to a `Categorical` and information about *whether* the variable is ordered is
    lost when exporting.'
  id: totrans-4839
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4840
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* only supports string value labels, and so `str` is called on the categories
    when exporting data. Exporting `Categorical` variables with non-string categories
    produces a warning, and can result a loss of information if the `str` representations
    of the categories are not unique.'
  id: totrans-4841
  prefs: []
  type: TYPE_NORMAL
- en: Labeled data can similarly be imported from *Stata* data files as `Categorical`
    variables using the keyword argument `convert_categoricals` (`True` by default).
    The keyword argument `order_categoricals` (`True` by default) determines whether
    imported `Categorical` variables are ordered.
  id: totrans-4842
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4843
  prefs: []
  type: TYPE_NORMAL
- en: 'When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since `Categorical` variables always use integer data
    types between `-1` and `n-1` where `n` is the number of categories. If the original
    values in the *Stata* data file are required, these can be imported by setting
    `convert_categoricals=False`, which will import original data (but not the variable
    labels). The original values can be matched to the imported categorical data since
    there is a simple mapping between the original *Stata* data values and the category
    codes of imported Categorical variables: missing values are assigned code `-1`,
    and the smallest original value is assigned `0`, the second smallest is assigned
    `1` and so on until the largest original value is assigned the code `n-1`.'
  id: totrans-4844
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4845
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a `Categorical` with string categories for the values that are labeled and numeric
    categories for values with no label.  #### Categorical data'
  id: totrans-4846
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` data can be exported to *Stata* data files as value labeled data.
    The exported data consists of the underlying category codes as integer data values
    and the categories as value labels. *Stata* does not have an explicit equivalent
    to a `Categorical` and information about *whether* the variable is ordered is
    lost when exporting.'
  id: totrans-4847
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-4848
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* only supports string value labels, and so `str` is called on the categories
    when exporting data. Exporting `Categorical` variables with non-string categories
    produces a warning, and can result a loss of information if the `str` representations
    of the categories are not unique.'
  id: totrans-4849
  prefs: []
  type: TYPE_NORMAL
- en: Labeled data can similarly be imported from *Stata* data files as `Categorical`
    variables using the keyword argument `convert_categoricals` (`True` by default).
    The keyword argument `order_categoricals` (`True` by default) determines whether
    imported `Categorical` variables are ordered.
  id: totrans-4850
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4851
  prefs: []
  type: TYPE_NORMAL
- en: 'When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since `Categorical` variables always use integer data
    types between `-1` and `n-1` where `n` is the number of categories. If the original
    values in the *Stata* data file are required, these can be imported by setting
    `convert_categoricals=False`, which will import original data (but not the variable
    labels). The original values can be matched to the imported categorical data since
    there is a simple mapping between the original *Stata* data values and the category
    codes of imported Categorical variables: missing values are assigned code `-1`,
    and the smallest original value is assigned `0`, the second smallest is assigned
    `1` and so on until the largest original value is assigned the code `n-1`.'
  id: totrans-4852
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-4853
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a `Categorical` with string categories for the values that are labeled and numeric
    categories for values with no label.'
  id: totrans-4854
  prefs: []
  type: TYPE_NORMAL
- en: '## SAS formats'
  id: totrans-4855
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function [`read_sas()`](../reference/api/pandas.read_sas.html#pandas.read_sas
    "pandas.read_sas") can read (but not write) SAS XPORT (.xpt) and SAS7BDAT (.sas7bdat)
    format files.
  id: totrans-4856
  prefs: []
  type: TYPE_NORMAL
- en: 'SAS files only contain two value types: ASCII text and floating point values
    (usually 8 bytes but sometimes truncated). For xport files, there is no automatic
    type conversion to integers, dates, or categoricals. For SAS7BDAT files, the format
    codes may allow date variables to be automatically converted to dates. By default
    the whole file is read and returned as a `DataFrame`.'
  id: totrans-4857
  prefs: []
  type: TYPE_NORMAL
- en: Specify a `chunksize` or use `iterator=True` to obtain reader objects (`XportReader`
    or `SAS7BDATReader`) for incrementally reading the file. The reader objects also
    have attributes that contain additional information about the file and its variables.
  id: totrans-4858
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a SAS7BDAT file:'
  id: totrans-4859
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE984]'
  id: totrans-4860
  prefs: []
  type: TYPE_PRE
  zh: '[PRE984]'
- en: 'Obtain an iterator and read an XPORT file 100,000 lines at a time:'
  id: totrans-4861
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE985]'
  id: totrans-4862
  prefs: []
  type: TYPE_PRE
  zh: '[PRE985]'
- en: The [specification](https://support.sas.com/content/dam/SAS/support/en/technical-papers/record-layout-of-a-sas-version-5-or-6-data-set-in-sas-transport-xport-format.pdf)
    for the xport file format is available from the SAS web site.
  id: totrans-4863
  prefs: []
  type: TYPE_NORMAL
- en: No official documentation is available for the SAS7BDAT format.
  id: totrans-4864
  prefs: []
  type: TYPE_NORMAL
- en: '## SPSS formats'
  id: totrans-4865
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function [`read_spss()`](../reference/api/pandas.read_spss.html#pandas.read_spss
    "pandas.read_spss") can read (but not write) SPSS SAV (.sav) and ZSAV (.zsav)
    format files.
  id: totrans-4866
  prefs: []
  type: TYPE_NORMAL
- en: SPSS files contain column names. By default the whole file is read, categorical
    columns are converted into `pd.Categorical`, and a `DataFrame` with all columns
    is returned.
  id: totrans-4867
  prefs: []
  type: TYPE_NORMAL
- en: Specify the `usecols` parameter to obtain a subset of columns. Specify `convert_categoricals=False`
    to avoid converting categorical columns into `pd.Categorical`.
  id: totrans-4868
  prefs: []
  type: TYPE_NORMAL
- en: 'Read an SPSS file:'
  id: totrans-4869
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE986]'
  id: totrans-4870
  prefs: []
  type: TYPE_PRE
  zh: '[PRE986]'
- en: 'Extract a subset of columns contained in `usecols` from an SPSS file and avoid
    converting categorical columns into `pd.Categorical`:'
  id: totrans-4871
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE987]'
  id: totrans-4872
  prefs: []
  type: TYPE_PRE
  zh: '[PRE987]'
- en: More information about the SAV and ZSAV file formats is available [here](https://www.ibm.com/docs/en/spss-statistics/22.0.0).
  id: totrans-4873
  prefs: []
  type: TYPE_NORMAL
- en: '## Other file formats'
  id: totrans-4874
  prefs: []
  type: TYPE_NORMAL
- en: pandas itself only supports IO with a limited set of file formats that map cleanly
    to its tabular data model. For reading and writing other file formats into and
    from pandas, we recommend these packages from the broader community.
  id: totrans-4875
  prefs: []
  type: TYPE_NORMAL
- en: netCDF
  id: totrans-4876
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[xarray](https://xarray.pydata.org/en/stable/) provides data structures inspired
    by the pandas `DataFrame` for working with multi-dimensional datasets, with a
    focus on the netCDF file format and easy conversion to and from pandas.'
  id: totrans-4877
  prefs: []
  type: TYPE_NORMAL
- en: netCDF
  id: totrans-4878
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[xarray](https://xarray.pydata.org/en/stable/) provides data structures inspired
    by the pandas `DataFrame` for working with multi-dimensional datasets, with a
    focus on the netCDF file format and easy conversion to and from pandas.'
  id: totrans-4879
  prefs: []
  type: TYPE_NORMAL
- en: '## Performance considerations'
  id: totrans-4880
  prefs: []
  type: TYPE_NORMAL
- en: This is an informal comparison of various IO methods, using pandas 0.24.2\.
    Timings are machine dependent and small differences should be ignored.
  id: totrans-4881
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE988]'
  id: totrans-4882
  prefs: []
  type: TYPE_PRE
  zh: '[PRE988]'
- en: 'The following test functions will be used below to compare the performance
    of several IO methods:'
  id: totrans-4883
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE989]'
  id: totrans-4884
  prefs: []
  type: TYPE_PRE
  zh: '[PRE989]'
- en: When writing, the top three functions in terms of speed are `test_feather_write`,
    `test_hdf_fixed_write` and `test_hdf_fixed_write_compress`.
  id: totrans-4885
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE990]'
  id: totrans-4886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE990]'
- en: When reading, the top three functions in terms of speed are `test_feather_read`,
    `test_pickle_read` and `test_hdf_fixed_read`.
  id: totrans-4887
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE991]'
  id: totrans-4888
  prefs: []
  type: TYPE_PRE
  zh: '[PRE991]'
- en: The files `test.pkl.compress`, `test.parquet` and `test.feather` took the least
    space on disk (in bytes).
  id: totrans-4889
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE992]'
  id: totrans-4890
  prefs: []
  type: TYPE_PRE
  zh: '[PRE992]'
