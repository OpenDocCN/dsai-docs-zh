- en: Triton Inference Server with Ultralytics YOLOv8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Triton 推理服务器与 Ultralytics YOLOv8。
- en: 原文：[`docs.ultralytics.com/guides/triton-inference-server/`](https://docs.ultralytics.com/guides/triton-inference-server/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`docs.ultralytics.com/guides/triton-inference-server/`](https://docs.ultralytics.com/guides/triton-inference-server/)
- en: The [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)
    (formerly known as TensorRT Inference Server) is an open-source software solution
    developed by NVIDIA. It provides a cloud inference solution optimized for NVIDIA
    GPUs. Triton simplifies the deployment of AI models at scale in production. Integrating
    Ultralytics YOLOv8 with Triton Inference Server allows you to deploy scalable,
    high-performance deep learning inference workloads. This guide provides steps
    to set up and test the integration.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[Triton 推理服务器](https://developer.nvidia.com/nvidia-triton-inference-server)（以前称为
    TensorRT 推理服务器）是 NVIDIA 开发的开源软件解决方案。它提供了一个针对 NVIDIA GPU 优化的云推理解决方案。Triton 简化了生产环境中大规模部署
    AI 模型的过程。将 Ultralytics YOLOv8 与 Triton 推理服务器集成，可以部署可扩展、高性能的深度学习推理工作负载。本指南提供了设置和测试集成的步骤。'
- en: '[`www.youtube.com/embed/NQDtfSi5QF4`](https://www.youtube.com/embed/NQDtfSi5QF4)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[`www.youtube.com/embed/NQDtfSi5QF4`](https://www.youtube.com/embed/NQDtfSi5QF4)'
- en: '**Watch:** Getting Started with NVIDIA Triton Inference Server.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**观看：**开始使用 NVIDIA Triton 推理服务器。'
- en: What is Triton Inference Server?
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Triton 推理服务器？
- en: 'Triton Inference Server is designed to deploy a variety of AI models in production.
    It supports a wide range of deep learning and machine learning frameworks, including
    TensorFlow, PyTorch, ONNX Runtime, and many others. Its primary use cases are:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Triton 推理服务器旨在生产部署各种 AI 模型，支持 TensorFlow、PyTorch、ONNX Runtime 等广泛的深度学习和机器学习框架。其主要用例包括：
- en: Serving multiple models from a single server instance.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从单个服务器实例中服务多个模型。
- en: Dynamic model loading and unloading without server restart.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态模型加载和卸载，无需服务器重启。
- en: Ensemble inference, allowing multiple models to be used together to achieve
    results.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成推理，允许多个模型一起使用以实现结果。
- en: Model versioning for A/B testing and rolling updates.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 A/B 测试和滚动更新进行模型版本控制。
- en: Prerequisites
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件
- en: 'Ensure you have the following prerequisites before proceeding:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保具备以下先决条件：
- en: Docker installed on your machine.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的机器上安装了 Docker。
- en: 'Install `tritonclient`:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装`tritonclient`：
- en: '[PRE0]'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Exporting YOLOv8 to ONNX Format
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 YOLOv8 导出为 ONNX 格式
- en: 'Before deploying the model on Triton, it must be exported to the ONNX format.
    ONNX (Open Neural Network Exchange) is a format that allows models to be transferred
    between different deep learning frameworks. Use the `export` function from the
    `YOLO` class:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型部署到 Triton 之前，必须将其导出为 ONNX 格式。ONNX（开放神经网络交换）是一种允许在不同深度学习框架之间转移模型的格式。使用`YOLO`类的`export`功能：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Setting Up Triton Model Repository
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Triton 模型仓库
- en: The Triton Model Repository is a storage location where Triton can access and
    load models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Triton 模型仓库是 Triton 可以访问和加载模型的存储位置。
- en: 'Create the necessary directory structure:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建必要的目录结构：
- en: '[PRE2]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Move the exported ONNX model to the Triton repository:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将导出的 ONNX 模型移至 Triton 仓库：
- en: '[PRE3]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Running Triton Inference Server
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 Triton 推理服务器
- en: 'Run the Triton Inference Server using Docker:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker 运行 Triton 推理服务器：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then run inference using the Triton Server model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 Triton 服务器模型进行推理：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Cleanup the container:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 清理容器：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '* * *'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: By following the above steps, you can deploy and run Ultralytics YOLOv8 models
    efficiently on Triton Inference Server, providing a scalable and high-performance
    solution for deep learning inference tasks. If you face any issues or have further
    queries, refer to the [official Triton documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)
    or reach out to the Ultralytics community for support.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循以上步骤，您可以在 Triton 推理服务器上高效部署和运行 Ultralytics YOLOv8 模型，为深度学习推理任务提供可扩展和高性能的解决方案。如果遇到任何问题或有进一步的疑问，请参阅[官方
    Triton 文档](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)或联系
    Ultralytics 社区获取支持。
- en: FAQ
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见问题
- en: How do I set up Ultralytics YOLOv8 with NVIDIA Triton Inference Server?
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何设置 Ultralytics YOLOv8 与 NVIDIA Triton 推理服务器？
- en: 'Setting up [Ultralytics YOLOv8](https://docs.ultralytics.com/models/yolov8)
    with [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)
    involves a few key steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[NVIDIA Triton 推理服务器](https://developer.nvidia.com/nvidia-triton-inference-server)设置[Ultralytics
    YOLOv8](https://docs.ultralytics.com/models/yolov8)涉及几个关键步骤：
- en: '**Export YOLOv8 to ONNX format**:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将 YOLOv8 导出为 ONNX 格式**：'
- en: '[PRE7]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Set up Triton Model Repository**:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置 Triton 模型仓库**：'
- en: '[PRE8]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Run the Triton Server**:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**运行Triton服务器**：'
- en: '[PRE9]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This setup can help you efficiently deploy YOLOv8 models at scale on Triton
    Inference Server for high-performance AI model inference.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置可帮助您高效地在Triton推断服务器上部署YOLOv8模型，用于高性能AI模型推断。
- en: What benefits does using Ultralytics YOLOv8 with NVIDIA Triton Inference Server
    offer?
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Ultralytics YOLOv8与NVIDIA Triton推断服务器有什么好处？
- en: 'Integrating Ultralytics YOLOv8 with [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)
    provides several advantages:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将[Ultralytics YOLOv8](https://docs.ultralytics.com/models/yolov8)与[NVIDIA Triton
    Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)集成，具有多个优势：
- en: '**Scalable AI Inference**: Triton allows serving multiple models from a single
    server instance, supporting dynamic model loading and unloading, making it highly
    scalable for diverse AI workloads.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展的AI推断**：Triton允许从单个服务器实例中服务多个模型，支持动态模型的加载和卸载，因此对各种AI工作负载具有高度可扩展性。'
- en: '**High Performance**: Optimized for NVIDIA GPUs, Triton Inference Server ensures
    high-speed inference operations, perfect for real-time applications such as object
    detection.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高性能**：针对NVIDIA GPU进行优化，Triton推断服务器确保高速推断操作，非常适合实时目标检测等实时应用。'
- en: '**Ensemble and Model Versioning**: Triton''s ensemble mode enables combining
    multiple models to improve results, and its model versioning supports A/B testing
    and rolling updates.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成和模型版本控制**：Triton的集成模式允许组合多个模型以提高结果，其模型版本控制支持A/B测试和滚动更新。'
- en: For detailed instructions on setting up and running YOLOv8 with Triton, you
    can refer to the setup guide.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有关设置和运行YOLOv8与Triton的详细说明，请参考设置指南。
- en: Why should I export my YOLOv8 model to ONNX format before using Triton Inference
    Server?
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么在使用Triton推断服务器之前需要将YOLOv8模型导出为ONNX格式？
- en: 'Using ONNX (Open Neural Network Exchange) format for your Ultralytics YOLOv8
    model before deploying it on [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)
    offers several key benefits:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署在[NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)上之前，为您的Ultralytics
    YOLOv8模型使用ONNX（开放神经网络交换格式）提供了几个关键的好处：
- en: '**Interoperability**: ONNX format supports transfer between different deep
    learning frameworks (such as PyTorch, TensorFlow), ensuring broader compatibility.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互操作性**：ONNX格式支持不同深度学习框架（如PyTorch、TensorFlow）之间的转换，确保更广泛的兼容性。'
- en: '**Optimization**: Many deployment environments, including Triton, optimize
    for ONNX, enabling faster inference and better performance.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：包括Triton在内的许多部署环境都为ONNX进行了优化，实现更快的推断和更好的性能。'
- en: '**Ease of Deployment**: ONNX is widely supported across frameworks and platforms,
    simplifying the deployment process in various operating systems and hardware configurations.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署简便性**：ONNX在各种操作系统和硬件配置中广泛支持，简化了部署过程。'
- en: 'To export your model, use:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要导出您的模型，请使用：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can follow the steps in the exporting guide to complete the process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照导出指南中的步骤完成该过程。
- en: Can I run inference using the Ultralytics YOLOv8 model on Triton Inference Server?
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我可以在Triton推断服务器上使用Ultralytics YOLOv8模型进行推断吗？
- en: 'Yes, you can run inference using the Ultralytics YOLOv8 model on [NVIDIA Triton
    Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server).
    Once your model is set up in the Triton Model Repository and the server is running,
    you can load and run inference on your model as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，您可以在[NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)上运行Ultralytics
    YOLOv8模型进行推断。一旦您的模型设置在Triton模型存储库中并且服务器正在运行，您可以加载并运行推断模型如下：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For an in-depth guide on setting up and running Triton Server with YOLOv8, refer
    to the running triton inference server section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有关设置和运行Triton服务器与YOLOv8的深入指南，请参考运行Triton推断服务器部分。
- en: How does Ultralytics YOLOv8 compare to TensorFlow and PyTorch models for deployment?
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ultralytics YOLOv8在部署时与TensorFlow和PyTorch模型有何区别？
- en: '[Ultralytics YOLOv8](https://docs.ultralytics.com/models/yolov8) offers several
    unique advantages compared to TensorFlow and PyTorch models for deployment:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ultralytics YOLOv8](https://docs.ultralytics.com/models/yolov8)相比于TensorFlow和PyTorch模型，在部署时提供了几个独特的优势：'
- en: '**Real-time Performance**: Optimized for real-time object detection tasks,
    YOLOv8 provides state-of-the-art accuracy and speed, making it ideal for applications
    requiring live video analytics.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时性能**：优化用于实时目标检测任务，YOLOv8提供了最先进的精度和速度，非常适合需要实时视频分析的应用。'
- en: '**Ease of Use**: YOLOv8 integrates seamlessly with Triton Inference Server
    and supports diverse export formats (ONNX, TensorRT, CoreML), making it flexible
    for various deployment scenarios.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性**：YOLOv8 与 Triton 推理服务器无缝集成，并支持多种导出格式（ONNX、TensorRT、CoreML），使其在各种部署场景下具备灵活性。'
- en: '**Advanced Features**: YOLOv8 includes features like dynamic model loading,
    model versioning, and ensemble inference, which are crucial for scalable and reliable
    AI deployments.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级功能**：YOLOv8 包括动态模型加载、模型版本管理和集成推理等功能，对于可扩展和可靠的AI部署至关重要。'
- en: For more details, compare the deployment options in the model deployment guide.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请比较模型部署指南中的部署选项。
