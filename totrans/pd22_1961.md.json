["```py\n# df is a DataFrame\n# f, g, and h are functions that take and return DataFrames\nf(g(h(df), arg1=1), arg2=2, arg3=3)  # noqa F821 \n```", "```py\n(\n    df.pipe(h)  # noqa F821\n    .pipe(g, arg1=1)  # noqa F821\n    .pipe(f, arg2=2, arg3=3)  # noqa F821\n) \n```", "```py\nIn [1]: import statsmodels.formula.api as sm\n\nIn [2]: bb = pd.read_csv(\"data/baseball.csv\", index_col=\"id\")\n\n# sm.ols takes (formula, data)\nIn [3]: (\n...:     bb.query(\"h > 0\")\n...:     .assign(ln_h=lambda df: np.log(df.h))\n...:     .pipe((sm.ols, \"data\"), \"hr ~ ln_h + year + g + C(lg)\")\n...:     .fit()\n...:     .summary()\n...: )\n...:\nOut[3]:\n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n OLS Regression Results\n==============================================================================\nDep. Variable:                     hr   R-squared:                       0.685\nModel:                            OLS   Adj. R-squared:                  0.665\nMethod:                 Least Squares   F-statistic:                     34.28\nDate:                Tue, 22 Nov 2022   Prob (F-statistic):           3.48e-15\nTime:                        05:35:23   Log-Likelihood:                -205.92\nNo. Observations:                  68   AIC:                             421.8\nDf Residuals:                      63   BIC:                             432.9\nDf Model:                           4\nCovariance Type:            nonrobust\n===============================================================================\n coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -8484.7720   4664.146     -1.819      0.074   -1.78e+04     835.780\nC(lg)[T.NL]    -2.2736      1.325     -1.716      0.091      -4.922       0.375\nln_h           -1.3542      0.875     -1.547      0.127      -3.103       0.395\nyear            4.2277      2.324      1.819      0.074      -0.417       8.872\ng               0.1841      0.029      6.258      0.000       0.125       0.243\n==============================================================================\nOmnibus:                       10.875   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.004   Jarque-Bera (JB):               17.298\nSkew:                           0.537   Prob(JB):                     0.000175\nKurtosis:                       5.225   Cond. No.                     1.49e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\" \n```", "```py\n# df is a DataFrame\n# f, g, and h are functions that take and return DataFrames\nf(g(h(df), arg1=1), arg2=2, arg3=3)  # noqa F821 \n```", "```py\n(\n    df.pipe(h)  # noqa F821\n    .pipe(g, arg1=1)  # noqa F821\n    .pipe(f, arg2=2, arg3=3)  # noqa F821\n) \n```", "```py\nIn [1]: import statsmodels.formula.api as sm\n\nIn [2]: bb = pd.read_csv(\"data/baseball.csv\", index_col=\"id\")\n\n# sm.ols takes (formula, data)\nIn [3]: (\n...:     bb.query(\"h > 0\")\n...:     .assign(ln_h=lambda df: np.log(df.h))\n...:     .pipe((sm.ols, \"data\"), \"hr ~ ln_h + year + g + C(lg)\")\n...:     .fit()\n...:     .summary()\n...: )\n...:\nOut[3]:\n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n OLS Regression Results\n==============================================================================\nDep. Variable:                     hr   R-squared:                       0.685\nModel:                            OLS   Adj. R-squared:                  0.665\nMethod:                 Least Squares   F-statistic:                     34.28\nDate:                Tue, 22 Nov 2022   Prob (F-statistic):           3.48e-15\nTime:                        05:35:23   Log-Likelihood:                -205.92\nNo. Observations:                  68   AIC:                             421.8\nDf Residuals:                      63   BIC:                             432.9\nDf Model:                           4\nCovariance Type:            nonrobust\n===============================================================================\n coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -8484.7720   4664.146     -1.819      0.074   -1.78e+04     835.780\nC(lg)[T.NL]    -2.2736      1.325     -1.716      0.091      -4.922       0.375\nln_h           -1.3542      0.875     -1.547      0.127      -3.103       0.395\nyear            4.2277      2.324      1.819      0.074      -0.417       8.872\ng               0.1841      0.029      6.258      0.000       0.125       0.243\n==============================================================================\nOmnibus:                       10.875   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.004   Jarque-Bera (JB):               17.298\nSkew:                           0.537   Prob(JB):                     0.000175\nKurtosis:                       5.225   Cond. No.                     1.49e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\" \n```", "```py\n# df is a DataFrame\n# f, g, and h are functions that take and return DataFrames\nf(g(h(df), arg1=1), arg2=2, arg3=3)  # noqa F821 \n```", "```py\n(\n    df.pipe(h)  # noqa F821\n    .pipe(g, arg1=1)  # noqa F821\n    .pipe(f, arg2=2, arg3=3)  # noqa F821\n) \n```", "```py\nIn [1]: import statsmodels.formula.api as sm\n\nIn [2]: bb = pd.read_csv(\"data/baseball.csv\", index_col=\"id\")\n\n# sm.ols takes (formula, data)\nIn [3]: (\n...:     bb.query(\"h > 0\")\n...:     .assign(ln_h=lambda df: np.log(df.h))\n...:     .pipe((sm.ols, \"data\"), \"hr ~ ln_h + year + g + C(lg)\")\n...:     .fit()\n...:     .summary()\n...: )\n...:\nOut[3]:\n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n OLS Regression Results\n==============================================================================\nDep. Variable:                     hr   R-squared:                       0.685\nModel:                            OLS   Adj. R-squared:                  0.665\nMethod:                 Least Squares   F-statistic:                     34.28\nDate:                Tue, 22 Nov 2022   Prob (F-statistic):           3.48e-15\nTime:                        05:35:23   Log-Likelihood:                -205.92\nNo. Observations:                  68   AIC:                             421.8\nDf Residuals:                      63   BIC:                             432.9\nDf Model:                           4\nCovariance Type:            nonrobust\n===============================================================================\n coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -8484.7720   4664.146     -1.819      0.074   -1.78e+04     835.780\nC(lg)[T.NL]    -2.2736      1.325     -1.716      0.091      -4.922       0.375\nln_h           -1.3542      0.875     -1.547      0.127      -3.103       0.395\nyear            4.2277      2.324      1.819      0.074      -0.417       8.872\ng               0.1841      0.029      6.258      0.000       0.125       0.243\n==============================================================================\nOmnibus:                       10.875   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.004   Jarque-Bera (JB):               17.298\nSkew:                           0.537   Prob(JB):                     0.000175\nKurtosis:                       5.225   Cond. No.                     1.49e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\" \n```"]