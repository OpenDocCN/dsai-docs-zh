# `scipy.optimize.fmin_bfgs`

> 原文：[`docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.fmin_bfgs.html#scipy.optimize.fmin_bfgs`](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.fmin_bfgs.html#scipy.optimize.fmin_bfgs)

```py
scipy.optimize.fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None, xrtol=0, c1=0.0001, c2=0.9, hess_inv0=None)
```

使用 BFGS 算法最小化函数。

参数：

**f**可调用函数 `f(x,*args)`

要最小化的目标函数。

**x0**ndarray

初始猜测，形状为 (n,)

**fprime**可调用函数 `f'(x,*args)`，可选

f 的梯度。

**args**元组，可选

传递给 f 和 fprime 的额外参数。

**gtol**浮点数，可选

如果梯度范数小于 *gtol*，则成功终止。

**norm**浮点数，可选

范数的顺序（Inf 为最大，-Inf 为最小）

**epsilon**整数或 ndarray，可选

如果 *fprime* 是近似的，则使用此值作为步长。

**callback**可调用对象，可选

每次迭代后调用的可选用户提供的函数。调用方式为 `callback(xk)`，其中 `xk` 是当前的参数向量。

**maxiter**整数，可选

要执行的最大迭代次数。

**full_output**布尔值，可选

如果为 True，则除了 *xopt* 外还返回 `fopt`、`func_calls`、`grad_calls` 和 `warnflag`。

**disp**布尔值，可选

如果为 True，则打印收敛消息。

**retall**布尔值，可选

如果为 True，则返回每次迭代的结果列表。

**xrtol**浮点数，默认值：0

*x* 的相对容差。如果步长小于 `xk * xrtol`，其中 `xk` 是当前的参数向量，则成功终止。

**c1**浮点数，默认值：1e-4

用于 Armijo 条件规则的参数。

**c2**浮点数，默认值：0.9

曲率条件规则的参数。

**hess_inv0**None 或 ndarray，可选``

初始逆海森估计，形状为 (n, n)。如果为 None（默认），则使用单位矩阵。

返回：

**xopt**ndarray

最小化函数 f 的参数，即 `f(xopt) == fopt`。

**fopt**浮点数

最小值。

**gopt**ndarray

最小值处的梯度值，即 f’(xopt)，应接近 0。

**Bopt**ndarray

f’’(xopt) 的倒数值，即逆海森矩阵。

**func_calls**整数

函数调用数。

**grad_calls**整数

进行的梯度调用数。

**warnflag**整数

1：超过最大迭代次数。2：梯度和/或函数调用未更改。3：遇到 NaN 结果。

**allvecs**列表

*xopt* 在每次迭代时的值。仅在 *retall* 为 True 时返回。

另请参见

[`最小化`](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize "scipy.optimize.minimize")

多变量函数最小化算法的接口。特别是看 `method='BFGS'`。

注意

优化函数 *f*，其梯度由 *fprime* 给出，使用 Broyden、Fletcher、Goldfarb 和 Shanno（BFGS）的拟牛顿方法。

参数 *c1* 和 *c2* 必须满足 `0 < c1 < c2 < 1`。

参考

Wright 和 Nocedal，《数值优化》，1999 年，第 198 页。

示例

```py
>>> import numpy as np
>>> from scipy.optimize import fmin_bfgs
>>> def quadratic_cost(x, Q):
...     return x @ Q @ x
...
>>> x0 = np.array([-3, -4])
>>> cost_weight =  np.diag([1., 10.])
>>> # Note that a trailing comma is necessary for a tuple with single element
>>> fmin_bfgs(quadratic_cost, x0, args=(cost_weight,))
Optimization terminated successfully.
 Current function value: 0.000000
 Iterations: 7                   # may vary
 Function evaluations: 24        # may vary
 Gradient evaluations: 8         # may vary
array([ 2.85169950e-06, -4.61820139e-07]) 
```

```py
>>> def quadratic_cost_grad(x, Q):
...     return 2 * Q @ x
...
>>> fmin_bfgs(quadratic_cost, x0, quadratic_cost_grad, args=(cost_weight,))
Optimization terminated successfully.
 Current function value: 0.000000
 Iterations: 7
 Function evaluations: 8
 Gradient evaluations: 8
array([ 2.85916637e-06, -4.54371951e-07]) 
```
