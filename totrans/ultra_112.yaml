- en: ROS (Robot Operating System) quickstart guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/guides/ros-quickstart/`](https://docs.ultralytics.com/guides/ros-quickstart/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[`player.vimeo.com/video/639236696?h=740f412ce5`](https://player.vimeo.com/video/639236696?h=740f412ce5)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ROS Introduction (captioned)](https://vimeo.com/639236696) from [Open Robotics](https://vimeo.com/osrfoundation)
    on [Vimeo](https://vimeo.com).'
  prefs: []
  type: TYPE_NORMAL
- en: What is ROS?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [Robot Operating System (ROS)](https://www.ros.org/) is an open-source framework
    widely used in robotics research and industry. ROS provides a collection of [libraries
    and tools](https://www.ros.org/blog/ecosystem/) to help developers create robot
    applications. ROS is designed to work with various [robotic platforms](https://robots.ros.org/),
    making it a flexible and powerful tool for roboticists.
  prefs: []
  type: TYPE_NORMAL
- en: Key Features of ROS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Modular Architecture**: ROS has a modular architecture, allowing developers
    to build complex systems by combining smaller, reusable components called [nodes](https://wiki.ros.org/ROS/Tutorials/UnderstandingNodes).
    Each node typically performs a specific function, and nodes communicate with each
    other using messages over [topics](https://wiki.ros.org/ROS/Tutorials/UnderstandingTopics)
    or [services](https://wiki.ros.org/ROS/Tutorials/UnderstandingServicesParams).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Communication Middleware**: ROS offers a robust communication infrastructure
    that supports inter-process communication and distributed computing. This is achieved
    through a publish-subscribe model for data streams (topics) and a request-reply
    model for service calls.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hardware Abstraction**: ROS provides a layer of abstraction over the hardware,
    enabling developers to write device-agnostic code. This allows the same code to
    be used with different hardware setups, facilitating easier integration and experimentation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tools and Utilities**: ROS comes with a rich set of tools and utilities for
    visualization, debugging, and simulation. For instance, RViz is used for visualizing
    sensor data and robot state information, while Gazebo provides a powerful simulation
    environment for testing algorithms and robot designs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Extensive Ecosystem**: The ROS ecosystem is vast and continually growing,
    with numerous packages available for different robotic applications, including
    navigation, manipulation, perception, and more. The community actively contributes
    to the development and maintenance of these packages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <details class="note" open="open"><summary>Evolution of ROS Versions</summary>
  prefs: []
  type: TYPE_NORMAL
- en: 'Since its development in 2007, ROS has evolved through [multiple versions](https://wiki.ros.org/Distributions),
    each introducing new features and improvements to meet the growing needs of the
    robotics community. The development of ROS can be categorized into two main series:
    ROS 1 and ROS 2\. This guide focuses on the Long Term Support (LTS) version of
    ROS 1, known as ROS Noetic Ninjemys, the code should also work with earlier versions.'
  prefs: []
  type: TYPE_NORMAL
- en: ROS 1 vs. ROS 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While ROS 1 provided a solid foundation for robotic development, ROS 2 addresses
    its shortcomings by offering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time Performance**: Improved support for real-time systems and deterministic
    behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Enhanced security features for safe and reliable operation in
    various environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Better support for multi-robot systems and large-scale deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-platform Support**: Expanded compatibility with various operating systems
    beyond Linux, including Windows and macOS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible Communication**: Use of DDS for more flexible and efficient inter-process
    communication.</details>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROS Messages and Topics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In ROS, communication between nodes is facilitated through [messages](https://wiki.ros.org/Messages)
    and [topics](https://wiki.ros.org/Topics). A message is a data structure that
    defines the information exchanged between nodes, while a topic is a named channel
    over which messages are sent and received. Nodes can publish messages to a topic
    or subscribe to messages from a topic, enabling them to communicate with each
    other. This publish-subscribe model allows for asynchronous communication and
    decoupling between nodes. Each sensor or actuator in a robotic system typically
    publishes data to a topic, which can then be consumed by other nodes for processing
    or control. For the purpose of this guide, we will focus on Image, Depth and PointCloud
    messages and camera topics.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Ultralytics YOLO with ROS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This guide has been tested using [this ROS environment](https://github.com/ambitious-octopus/rosbot_ros/tree/noetic),
    which is a fork of the [ROSbot ROS repository](https://github.com/husarion/rosbot_ros).
    This environment includes the Ultralytics YOLO package, a Docker container for
    easy setup, comprehensive ROS packages, and Gazebo worlds for rapid testing. It
    is designed to work with the [Husarion ROSbot 2 PRO](https://husarion.com/manuals/rosbot/).
    The code examples provided will work in any ROS Noetic/Melodic environment, including
    both simulation and real-world.
  prefs: []
  type: TYPE_NORMAL
- en: '![Husarion ROSbot 2 PRO](img/6c8e2c01a43d9da9a2eeff4f5afc66f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Dependencies Installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apart from the ROS environment, you will need to install the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[ROS Numpy package](https://github.com/eric-wieser/ros_numpy)**: This is
    required for fast conversion between ROS Image messages and numpy arrays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Ultralytics package**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use Ultralytics with ROS `sensor_msgs/Image`
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `sensor_msgs/Image` [message type](https://docs.ros.org/en/api/sensor_msgs/html/msg/Image.html)
    is commonly used in ROS for representing image data. It contains fields for encoding,
    height, width, and pixel data, making it suitable for transmitting images captured
    by cameras or other sensors. Image messages are widely used in robotic applications
    for tasks such as visual perception, object detection, and navigation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Detection and Segmentation in ROS Gazebo](img/62c6684e47f97d6c0debdbced67d9c27.png)'
  prefs: []
  type: TYPE_IMG
- en: Image Step-by-Step Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following code snippet demonstrates how to use the Ultralytics YOLO package
    with ROS. In this example, we subscribe to a camera topic, process the incoming
    image using YOLO, and publish the detected objects to new topics for detection
    and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the necessary libraries and instantiate two models: one for segmentation
    and one for detection. Initialize a ROS node (with the name `ultralytics`) to
    enable communication with the ROS master. To ensure a stable connection, we include
    a brief pause, giving the node sufficient time to establish the connection before
    proceeding.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize two ROS topics: one for detection and one for segmentation. These
    topics will be used to publish the annotated images, making them accessible for
    further processing. The communication between nodes is facilitated using `sensor_msgs/Image`
    messages.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, create a subscriber that listens to messages on the `/camera/color/image_raw`
    topic and calls a callback function for each new message. This callback function
    receives messages of type `sensor_msgs/Image`, converts them into a numpy array
    using `ros_numpy`, processes the images with the previously instantiated YOLO
    models, annotates the images, and then publishes them back to the respective topics:
    `/ultralytics/detection/image` for detection and `/ultralytics/segmentation/image`
    for segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <details class="example"><summary>Complete code</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]</details> <details class="tip" open="open"><summary>Debugging</summary>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Debugging ROS (Robot Operating System) nodes can be challenging due to the
    system''s distributed nature. Several tools can assist with this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rostopic echo <TOPIC-NAME>` : This command allows you to view messages published
    on a specific topic, helping you inspect the data flow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`rostopic list`: Use this command to list all available topics in the ROS system,
    giving you an overview of the active data streams.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`rqt_graph`: This visualization tool displays the communication graph between
    nodes, providing insights into how nodes are interconnected and how they interact.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For more complex visualizations, such as 3D representations, you can use [RViz](https://wiki.ros.org/rviz).
    RViz (ROS Visualization) is a powerful 3D visualization tool for ROS. It allows
    you to visualize the state of your robot and its environment in real-time. With
    RViz, you can view sensor data (e.g. `sensors_msgs/Image`), robot model states,
    and various other types of information, making it easier to debug and understand
    the behavior of your robotic system.</details>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Publish Detected Classes with `std_msgs/String`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standard ROS messages also include `std_msgs/String` messages. In many applications,
    it is not necessary to republish the entire annotated image; instead, only the
    classes present in the robot's view are needed. The following example demonstrates
    how to use `std_msgs/String` [messages](https://docs.ros.org/en/noetic/api/std_msgs/html/msg/String.html)
    to republish the detected classes on the `/ultralytics/detection/classes` topic.
    These messages are more lightweight and provide essential information, making
    them valuable for various applications.
  prefs: []
  type: TYPE_NORMAL
- en: Example Use Case
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider a warehouse robot equipped with a camera and object detection model.
    Instead of sending large annotated images over the network, the robot can publish
    a list of detected classes as `std_msgs/String` messages. For instance, when the
    robot detects objects like "box", "pallet" and "forklift" it publishes these classes
    to the `/ultralytics/detection/classes` topic. This information can then be used
    by a central monitoring system to track the inventory in real-time, optimize the
    robot's path planning to avoid obstacles, or trigger specific actions such as
    picking up a detected box. This approach reduces the bandwidth required for communication
    and focuses on transmitting critical data.
  prefs: []
  type: TYPE_NORMAL
- en: String Step-by-Step Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This example demonstrates how to use the Ultralytics YOLO package with ROS.
    In this example, we subscribe to a camera topic, process the incoming image using
    YOLO, and publish the detected objects to new topic `/ultralytics/detection/classes`
    using `std_msgs/String` messages. The `ros_numpy` package is used to convert the
    ROS Image message to a numpy array for processing with YOLO.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Use Ultralytics with ROS Depth Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to RGB images, ROS supports [depth images](https://en.wikipedia.org/wiki/Depth_map),
    which provide information about the distance of objects from the camera. Depth
    images are crucial for robotic applications such as obstacle avoidance, 3D mapping,
    and localization.
  prefs: []
  type: TYPE_NORMAL
- en: A depth image is an image where each pixel represents the distance from the
    camera to an object. Unlike RGB images that capture color, depth images capture
    spatial information, enabling robots to perceive the 3D structure of their environment.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining Depth Images
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth images can be obtained using various sensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Stereo Cameras](https://en.wikipedia.org/wiki/Stereo_camera): Use two cameras
    to calculate depth based on image disparity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Time-of-Flight (ToF) Cameras](https://en.wikipedia.org/wiki/Time-of-flight_camera):
    Measure the time light takes to return from an object.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Structured Light Sensors](https://en.wikipedia.org/wiki/Structured-light_3D_scanner):
    Project a pattern and measure its deformation on surfaces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using YOLO with Depth Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In ROS, depth images are represented by the `sensor_msgs/Image` message type,
    which includes fields for encoding, height, width, and pixel data. The encoding
    field for depth images often uses a format like "16UC1", indicating a 16-bit unsigned
    integer per pixel, where each value represents the distance to the object. Depth
    images are commonly used in conjunction with RGB images to provide a more comprehensive
    view of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Using YOLO, it is possible to extract and combine information from both RGB
    and depth images. For instance, YOLO can detect objects within an RGB image, and
    this detection can be used to pinpoint corresponding regions in the depth image.
    This allows for the extraction of precise depth information for detected objects,
    enhancing the robot's ability to understand its environment in three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: RGB-D Cameras
  prefs: []
  type: TYPE_NORMAL
- en: When working with depth images, it is essential to ensure that the RGB and depth
    images are correctly aligned. RGB-D cameras, such as the [Intel RealSense](https://www.intelrealsense.com/)
    series, provide synchronized RGB and depth images, making it easier to combine
    information from both sources. If using separate RGB and depth cameras, it is
    crucial to calibrate them to ensure accurate alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Depth Step-by-Step Usage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this example, we use YOLO to segment an image and apply the extracted mask
    to segment the object in the depth image. This allows us to determine the distance
    of each pixel of the object of interest from the camera's focal center. By obtaining
    this distance information, we can calculate the distance between the camera and
    the specific object in the scene. Begin by importing the necessary libraries,
    creating a ROS node, and instantiating a segmentation model and a ROS topic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, define a callback function that processes the incoming depth image message.
    The function waits for the depth image and RGB image messages, converts them into
    numpy arrays, and applies the segmentation model to the RGB image. It then extracts
    the segmentation mask for each detected object and calculates the average distance
    of the object from the camera using the depth image. Most sensors have a maximum
    distance, known as the clip distance, beyond which values are represented as inf
    (`np.inf`). Before processing, it is important to filter out these null values
    and assign them a value of `0`. Finally, it publishes the detected objects along
    with their average distances to the `/ultralytics/detection/distance` topic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: <details class="example"><summary>Complete code</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: Use Ultralytics with ROS `sensor_msgs/PointCloud2`
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Detection and Segmentation in ROS Gazebo](img/034919731770fe377697d6eddc2c6aa4.png)'
  prefs: []
  type: TYPE_IMG
- en: The `sensor_msgs/PointCloud2` [message type](https://docs.ros.org/en/api/sensor_msgs/html/msg/PointCloud2.html)
    is a data structure used in ROS to represent 3D point cloud data. This message
    type is integral to robotic applications, enabling tasks such as 3D mapping, object
    recognition, and localization.
  prefs: []
  type: TYPE_NORMAL
- en: A point cloud is a collection of data points defined within a three-dimensional
    coordinate system. These data points represent the external surface of an object
    or a scene, captured via 3D scanning technologies. Each point in the cloud has
    `X`, `Y`, and `Z` coordinates, which correspond to its position in space, and
    may also include additional information such as color and intensity.
  prefs: []
  type: TYPE_NORMAL
- en: Reference frame
  prefs: []
  type: TYPE_NORMAL
- en: When working with `sensor_msgs/PointCloud2`, it's essential to consider the
    reference frame of the sensor from which the point cloud data was acquired. The
    point cloud is initially captured in the sensor's reference frame. You can determine
    this reference frame by listening to the `/tf_static` topic. However, depending
    on your specific application requirements, you might need to convert the point
    cloud into another reference frame. This transformation can be achieved using
    the `tf2_ros` package, which provides tools for managing coordinate frames and
    transforming data between them.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining Point clouds
  prefs: []
  type: TYPE_NORMAL
- en: 'Point Clouds can be obtained using various sensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LIDAR (Light Detection and Ranging)**: Uses laser pulses to measure distances
    to objects and create high-precision 3D maps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Depth Cameras**: Capture depth information for each pixel, allowing for 3D
    reconstruction of the scene.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Stereo Cameras**: Utilize two or more cameras to obtain depth information
    through triangulation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Structured Light Scanners**: Project a known pattern onto a surface and measure
    the deformation to calculate depth.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using YOLO with Point Clouds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To integrate YOLO with `sensor_msgs/PointCloud2` type messages, we can employ
    a method similar to the one used for depth maps. By leveraging the color information
    embedded in the point cloud, we can extract a 2D image, perform segmentation on
    this image using YOLO, and then apply the resulting mask to the three-dimensional
    points to isolate the 3D object of interest.
  prefs: []
  type: TYPE_NORMAL
- en: For handling point clouds, we recommend using Open3D (`pip install open3d`),
    a user-friendly Python library. Open3D provides robust tools for managing point
    cloud data structures, visualizing them, and executing complex operations seamlessly.
    This library can significantly simplify the process and enhance our ability to
    manipulate and analyze point clouds in conjunction with YOLO-based segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Point Clouds Step-by-Step Usage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Import the necessary libraries and instantiate the YOLO model for segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Create a function `pointcloud2_to_array`, which transforms a `sensor_msgs/PointCloud2`
    message into two numpy arrays. The `sensor_msgs/PointCloud2` messages contain
    `n` points based on the `width` and `height` of the acquired image. For instance,
    a `480 x 640` image will have `307,200` points. Each point includes three spatial
    coordinates (`xyz`) and the corresponding color in `RGB` format. These can be
    considered as two separate channels of information.
  prefs: []
  type: TYPE_NORMAL
- en: The function returns the `xyz` coordinates and `RGB` values in the format of
    the original camera resolution (`width x height`). Most sensors have a maximum
    distance, known as the clip distance, beyond which values are represented as inf
    (`np.inf`). Before processing, it is important to filter out these null values
    and assign them a value of `0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Next, subscribe to the `/camera/depth/points` topic to receive the point cloud
    message and convert the `sensor_msgs/PointCloud2` message into numpy arrays containing
    the XYZ coordinates and RGB values (using the `pointcloud2_to_array` function).
    Process the RGB image using the YOLO model to extract segmented objects. For each
    detected object, extract the segmentation mask and apply it to both the RGB image
    and the XYZ coordinates to isolate the object in 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the mask is straightforward since it consists of binary values, with
    `1` indicating the presence of the object and `0` indicating the absence. To apply
    the mask, simply multiply the original channels by the mask. This operation effectively
    isolates the object of interest within the image. Finally, create an Open3D point
    cloud object and visualize the segmented object in 3D space with associated colors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <details class="example"><summary>Complete code</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: '![Point Cloud Segmentation with Ultralytics ](img/53ab87c81395c1cae864d340d0d0fd07.png)'
  prefs: []
  type: TYPE_IMG
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the Robot Operating System (ROS)?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [Robot Operating System (ROS)](https://www.ros.org/) is an open-source framework
    commonly used in robotics to help developers create robust robot applications.
    It provides a collection of [libraries and tools](https://www.ros.org/blog/ecosystem/)
    for building and interfacing with robotic systems, enabling easier development
    of complex applications. ROS supports communication between nodes using messages
    over [topics](https://wiki.ros.org/ROS/Tutorials/UnderstandingTopics) or [services](https://wiki.ros.org/ROS/Tutorials/UnderstandingServicesParams).
  prefs: []
  type: TYPE_NORMAL
- en: How do I integrate Ultralytics YOLO with ROS for real-time object detection?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Integrating Ultralytics YOLO with ROS involves setting up a ROS environment
    and using YOLO for processing sensor data. Begin by installing the required dependencies
    like `ros_numpy` and Ultralytics YOLO:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a ROS node and subscribe to an image topic to process the incoming
    data. Here is a minimal example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: What are ROS topics and how are they used in Ultralytics YOLO?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ROS topics facilitate communication between nodes in a ROS network by using
    a publish-subscribe model. A topic is a named channel that nodes use to send and
    receive messages asynchronously. In the context of Ultralytics YOLO, you can make
    a node subscribe to an image topic, process the images using YOLO for tasks like
    detection or segmentation, and publish outcomes to new topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, subscribe to a camera topic and process the incoming image for
    detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Why use depth images with Ultralytics YOLO in ROS?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depth images in ROS, represented by `sensor_msgs/Image`, provide the distance
    of objects from the camera, crucial for tasks like obstacle avoidance, 3D mapping,
    and localization. By [using depth information](https://en.wikipedia.org/wiki/Depth_map)
    along with RGB images, robots can better understand their 3D environment.
  prefs: []
  type: TYPE_NORMAL
- en: With YOLO, you can extract segmentation masks from RGB images and apply these
    masks to depth images to obtain precise 3D object information, improving the robot's
    ability to navigate and interact with its surroundings.
  prefs: []
  type: TYPE_NORMAL
- en: How can I visualize 3D point clouds with YOLO in ROS?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To visualize 3D point clouds in ROS with YOLO:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert `sensor_msgs/PointCloud2` messages to numpy arrays.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use YOLO to segment RGB images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the segmentation mask to the point cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s an example using Open3D for visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This approach provides a 3D visualization of segmented objects, useful for tasks
    like navigation and manipulation.
  prefs: []
  type: TYPE_NORMAL
