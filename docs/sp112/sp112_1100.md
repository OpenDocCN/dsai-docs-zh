# scipy.special.xlogy

> 原文：[https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.special.xlogy.html#scipy.special.xlogy](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.special.xlogy.html#scipy.special.xlogy)

```py
scipy.special.xlogy(x, y, out=None) = <ufunc 'xlogy'>
```

计算 `x*log(y)`，如果 `x = 0` 则结果为 0。

参数：

**x**array_like

乘法器

**y**array_like

参数

**out**数组，可选

函数结果的可选输出数组

返回：

**z**标量或数组

计算 x*log(y)

注意

计算中使用的对数函数是自然对数。

从版本 0.13.0 开始新增。

示例

我们可以使用此函数来计算二元逻辑损失，也称为二元交叉熵。这种损失函数用于二元分类问题，定义如下：

\[L = 1/n * \sum_{i=0}^n -(y_i*log(y\_pred_i) + (1-y_i)*log(1-y\_pred_i))\]

我们可以将参数 *x* 和 *y* 定义为 y 和 y_pred，y 是实际标签的数组，这里可以是 0 或 1。y_pred 是相对于正类（1）的预测概率数组。

```py
>>> import numpy as np
>>> from scipy.special import xlogy
>>> y = np.array([0, 1, 0, 1, 1, 0])
>>> y_pred = np.array([0.3, 0.8, 0.4, 0.7, 0.9, 0.2])
>>> n = len(y)
>>> loss = -(xlogy(y, y_pred) + xlogy(1 - y, 1 - y_pred)).sum()
>>> loss /= n
>>> loss
0.29597052165495025 
```

较低的损失通常更好，因为它表明预测与实际标签相似。在这个例子中，由于我们的预测概率接近实际标签，我们得到的总体损失是合理低且适当的。
