- en: NumPy benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://numpy.org/doc/1.26/benchmarking.html](https://numpy.org/doc/1.26/benchmarking.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Benchmarking NumPy with Airspeed Velocity.
  prefs: []
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Airspeed Velocity manages building and Python virtualenvs by itself, unless
    told otherwise. To run the benchmarks, you do not need to install a development
    version of NumPy to your current Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before beginning, ensure that *airspeed velocity* is installed. By default,
    *asv* ships with support for anaconda and virtualenv:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After contributing new benchmarks, you should test them locally before submitting
    a pull request.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run all benchmarks, navigate to the root NumPy directory at the command
    line and execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This builds NumPy and runs all available benchmarks defined in `benchmarks/`.
    (Note: this could take a while. Each benchmark is run multiple times to measure
    the distribution in execution times.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For **testing** benchmarks locally, it may be better to run these without replications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Where the regular expression used to match benchmarks is stored in `$REGEXP`,
    and *–quick* is used to avoid repetitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run benchmarks from a particular benchmark module, such as `bench_core.py`,
    simply append the filename without the extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To run a benchmark defined in a class, such as `MeshGrid` from `bench_creation.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare changes in benchmark results to another version/commit/branch, use
    the `--compare` option (or the equivalent `-c`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'All of the commands above display the results in plain text in the console,
    and the results are not saved for comparison with future commits. For greater
    control, a graphical view, and to have results saved for future comparison you
    can run ASV commands (record results and generate HTML):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: More on how to use `asv` can be found in [ASV documentation](https://asv.readthedocs.io/)
    Command-line help is available as usual via `asv --help` and `asv run --help`.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To benchmark or visualize only releases on different machines locally, the
    tags with their commits can be generated, before being run with `asv`, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For details on contributing these, see the [benchmark results repository](https://github.com/HaoZeke/asv-numpy).
  prefs: []
  type: TYPE_NORMAL
- en: Writing benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See [ASV documentation](https://asv.readthedocs.io/) for basics on how to write
    benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some things to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The benchmark suite should be importable with any NumPy version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benchmark parameters etc. should not depend on which NumPy version is installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to keep the runtime of the benchmark reasonable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefer ASV’s `time_` methods for benchmarking times rather than cooking up time
    measurements via `time.clock`, even if it requires some juggling when writing
    the benchmark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing arrays etc. should generally be put in the `setup` method rather than
    the `time_` methods, to avoid counting preparation time together with the time
    of the benchmarked operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be mindful that large arrays created with `np.empty` or `np.zeros` might not
    be allocated in physical memory until the memory is accessed. If this is desired
    behaviour, make sure to comment it in your setup function. If you are benchmarking
    an algorithm, it is unlikely that a user will be executing said algorithm on a
    newly created empty/zero array. One can force pagefaults to occur in the setup
    phase either by calling `np.ones` or `arr.fill(value)` after creating the array,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Airspeed Velocity manages building and Python virtualenvs by itself, unless
    told otherwise. To run the benchmarks, you do not need to install a development
    version of NumPy to your current Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before beginning, ensure that *airspeed velocity* is installed. By default,
    *asv* ships with support for anaconda and virtualenv:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After contributing new benchmarks, you should test them locally before submitting
    a pull request.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run all benchmarks, navigate to the root NumPy directory at the command
    line and execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This builds NumPy and runs all available benchmarks defined in `benchmarks/`.
    (Note: this could take a while. Each benchmark is run multiple times to measure
    the distribution in execution times.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For **testing** benchmarks locally, it may be better to run these without replications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Where the regular expression used to match benchmarks is stored in `$REGEXP`,
    and *–quick* is used to avoid repetitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run benchmarks from a particular benchmark module, such as `bench_core.py`,
    simply append the filename without the extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To run a benchmark defined in a class, such as `MeshGrid` from `bench_creation.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare changes in benchmark results to another version/commit/branch, use
    the `--compare` option (or the equivalent `-c`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'All of the commands above display the results in plain text in the console,
    and the results are not saved for comparison with future commits. For greater
    control, a graphical view, and to have results saved for future comparison you
    can run ASV commands (record results and generate HTML):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: More on how to use `asv` can be found in [ASV documentation](https://asv.readthedocs.io/)
    Command-line help is available as usual via `asv --help` and `asv run --help`.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To benchmark or visualize only releases on different machines locally, the
    tags with their commits can be generated, before being run with `asv`, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For details on contributing these, see the [benchmark results repository](https://github.com/HaoZeke/asv-numpy).
  prefs: []
  type: TYPE_NORMAL
- en: Writing benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See [ASV documentation](https://asv.readthedocs.io/) for basics on how to write
    benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some things to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The benchmark suite should be importable with any NumPy version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benchmark parameters etc. should not depend on which NumPy version is installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to keep the runtime of the benchmark reasonable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prefer ASV’s `time_` methods for benchmarking times rather than cooking up time
    measurements via `time.clock`, even if it requires some juggling when writing
    the benchmark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing arrays etc. should generally be put in the `setup` method rather than
    the `time_` methods, to avoid counting preparation time together with the time
    of the benchmarked operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be mindful that large arrays created with `np.empty` or `np.zeros` might not
    be allocated in physical memory until the memory is accessed. If this is desired
    behaviour, make sure to comment it in your setup function. If you are benchmarking
    an algorithm, it is unlikely that a user will be executing said algorithm on a
    newly created empty/zero array. One can force pagefaults to occur in the setup
    phase either by calling `np.ones` or `arr.fill(value)` after creating the array,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
