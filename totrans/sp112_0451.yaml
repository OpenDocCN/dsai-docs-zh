- en: scipy.optimize.isotonic_regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.isotonic_regression.html#scipy.optimize.isotonic_regression](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.isotonic_regression.html#scipy.optimize.isotonic_regression)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Nonparametric isotonic regression.
  prefs: []
  type: TYPE_NORMAL
- en: A (not strictly) monotonically increasing array *x* with the same length as
    *y* is calculated by the pool adjacent violators algorithm (PAVA), see [[1]](#rddcb72c1ad4d-1).
    See the Notes section for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**y**(N,) array_like'
  prefs: []
  type: TYPE_NORMAL
- en: Response variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**weights**(N,) array_like or None'
  prefs: []
  type: TYPE_NORMAL
- en: Case weights.
  prefs: []
  type: TYPE_NORMAL
- en: '**increasing**bool'
  prefs: []
  type: TYPE_NORMAL
- en: If True, fit monotonic increasing, i.e. isotonic, regression. If False, fit
    a monotonic decreasing, i.e. antitonic, regression. Default is True.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**res**OptimizeResult'
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization result represented as a `OptimizeResult` object. Important
    attributes are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: The isotonic regression solution, i.e. an increasing (or decreasing) array
    of the same length than y, with elements in the range from min(y) to max(y).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weights` : Array with the sum of case weights for each block (or pool) B.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blocks`: Array of length B+1 with the indices of the start positions of each
    block (or pool) B. The j-th block is given by `x[blocks[j]:blocks[j+1]]` for which
    all values are the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notes
  prefs: []
  type: TYPE_NORMAL
- en: 'Given data \(y\) and case weights \(w\), the isotonic regression solves the
    following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\operatorname{argmin}_{x_i} \sum_i w_i (y_i - x_i)^2 \quad \text{subject to
    } x_i \leq x_j \text{ whenever } i \leq j \,.\]
  prefs: []
  type: TYPE_NORMAL
- en: For every input value \(y_i\), it generates a value \(x_i\) such that \(x\)
    is increasing (but not strictly), i.e. \(x_i \leq x_{i+1}\). This is accomplished
    by the PAVA. The solution consists of pools or blocks, i.e. neighboring elements
    of \(x\), e.g. \(x_i\) and \(x_{i+1}\), that all have the same value.
  prefs: []
  type: TYPE_NORMAL
- en: Most interestingly, the solution stays the same if the squared loss is replaced
    by the wide class of Bregman functions which are the unique class of strictly
    consistent scoring functions for the mean, see [[2]](#rddcb72c1ad4d-2) and references
    therein.
  prefs: []
  type: TYPE_NORMAL
- en: The implemented version of PAVA according to [[1]](#rddcb72c1ad4d-1) has a computational
    complexity of O(N) with input size N.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs: []
  type: TYPE_NORMAL
- en: '[1] ([1](#id1),[2](#id3))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Busing, F. M. T. A. (2022). Monotone Regression: A Simple and Fast O(n) PAVA
    Implementation. Journal of Statistical Software, Code Snippets, 102(1), 1-25.
    [DOI:10.18637/jss.v102.c01](https://doi.org/10.18637/jss.v102.c01)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2](#id2)]'
  prefs: []
  type: TYPE_NORMAL
- en: Jordan, A.I., Mühlemann, A. & Ziegel, J.F. Characterizing the optimal solutions
    to the isotonic regression problem for identifiable functionals. Ann Inst Stat
    Math 74, 489-514 (2022). [DOI:10.1007/s10463-021-00808-0](https://doi.org/10.1007/s10463-021-00808-0)
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates that `isotonic_regression` really solves a constrained
    optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The big advantage of `isotonic_regression` compared to calling `minimize` is
    that it is more user friendly, i.e. one does not need to define objective and
    constraint functions, and that it is orders of magnitudes faster. On commodity
    hardware (in 2023), for normal distributed input y of length 1000, the minimizer
    takes about 4 seconds, while `isotonic_regression` takes about 200 microseconds.
  prefs: []
  type: TYPE_NORMAL
