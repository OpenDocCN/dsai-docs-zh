["```py\nscipy.optimize.least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})\n```", "```py\nminimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\nsubject to lb <= x <= ub \n```", "```py\n>>> import numpy as np\n>>> def fun_rosenbrock(x):\n...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])]) \n```", "```py\n>>> from scipy.optimize import least_squares\n>>> x0_rosenbrock = np.array([2, 2])\n>>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\n>>> res_1.x\narray([ 1.,  1.])\n>>> res_1.cost\n9.8669242910846867e-30\n>>> res_1.optimality\n8.8928864934219529e-14 \n```", "```py\n>>> def jac_rosenbrock(x):\n...     return np.array([\n...         [-20 * x[0], 10],\n...         [-1, 0]]) \n```", "```py\n>>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\n...                       bounds=([-np.inf, 1.5], np.inf))\n>>> res_2.x\narray([ 1.22437075,  1.5       ])\n>>> res_2.cost\n0.025213093946805685\n>>> res_2.optimality\n1.5885401433157753e-07 \n```", "```py\n>>> def fun_broyden(x):\n...     f = (3 - x) * x + 1\n...     f[1:] -= x[:-1]\n...     f[:-1] -= 2 * x[1:]\n...     return f \n```", "```py\n>>> from scipy.sparse import lil_matrix\n>>> def sparsity_broyden(n):\n...     sparsity = lil_matrix((n, n), dtype=int)\n...     i = np.arange(n)\n...     sparsity[i, i] = 1\n...     i = np.arange(1, n)\n...     sparsity[i, i - 1] = 1\n...     i = np.arange(n - 1)\n...     sparsity[i, i + 1] = 1\n...     return sparsity\n...\n>>> n = 100000\n>>> x0_broyden = -np.ones(n)\n...\n>>> res_3 = least_squares(fun_broyden, x0_broyden,\n...                       jac_sparsity=sparsity_broyden(n))\n>>> res_3.cost\n4.5687069299604613e-23\n>>> res_3.optimality\n1.1650454296851518e-11 \n```", "```py\n>>> from numpy.random import default_rng\n>>> rng = default_rng()\n>>> def gen_data(t, a, b, c, noise=0., n_outliers=0, seed=None):\n...     rng = default_rng(seed)\n...\n...     y = a + b * np.exp(t * c)\n...\n...     error = noise * rng.standard_normal(t.size)\n...     outliers = rng.integers(0, t.size, n_outliers)\n...     error[outliers] *= 10\n...\n...     return y + error\n...\n>>> a = 0.5\n>>> b = 2.0\n>>> c = -1\n>>> t_min = 0\n>>> t_max = 10\n>>> n_points = 15\n...\n>>> t_train = np.linspace(t_min, t_max, n_points)\n>>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3) \n```", "```py\n>>> def fun(x, t, y):\n...     return x[0] + x[1] * np.exp(x[2] * t) - y\n...\n>>> x0 = np.array([1.0, 1.0, 0.0]) \n```", "```py\n>>> res_lsq = least_squares(fun, x0, args=(t_train, y_train)) \n```", "```py\n>>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n...                             args=(t_train, y_train))\n>>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n...                         args=(t_train, y_train)) \n```", "```py\n>>> t_test = np.linspace(t_min, t_max, n_points * 10)\n>>> y_true = gen_data(t_test, a, b, c)\n>>> y_lsq = gen_data(t_test, *res_lsq.x)\n>>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n>>> y_log = gen_data(t_test, *res_log.x)\n...\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(t_train, y_train, 'o')\n>>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n>>> plt.plot(t_test, y_lsq, label='linear loss')\n>>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n>>> plt.plot(t_test, y_log, label='cauchy loss')\n>>> plt.xlabel(\"t\")\n>>> plt.ylabel(\"y\")\n>>> plt.legend()\n>>> plt.show() \n```", "```py\n>>> def f(z):\n...     return z - (0.5 + 0.5j) \n```", "```py\n>>> def f_wrap(x):\n...     fx = f(x[0] + 1j*x[1])\n...     return np.array([fx.real, fx.imag]) \n```", "```py\n>>> from scipy.optimize import least_squares\n>>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\n>>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\n>>> z\n(0.49999999999925893+0.49999999999925893j) \n```"]