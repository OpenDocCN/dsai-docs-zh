# 理解 YOLOv8 的部署选项

> 原文：[`docs.ultralytics.com/guides/model-deployment-options/`](https://docs.ultralytics.com/guides/model-deployment-options/)

## 介绍

在您的 YOLOv8 之旅中走过了很长的一段路。您勤奋地收集数据，细致地标注它，并花费时间训练和严格评估您的定制 YOLOv8 模型。现在，是时候将您的模型应用于您特定的应用、用例或项目了。但在您面前有一个关键决策：如何有效地导出和部署您的模型。

本指南将带您了解 YOLOv8 的部署选项以及选择适合您项目的正确选项所需考虑的关键因素。

## 如何为您的 YOLOv8 模型选择合适的部署选项

当需要部署您的 YOLOv8 模型时，选择合适的导出格式非常重要。如 Ultralytics YOLOv8 Modes 文档中所述，model.export() 函数可将训练好的模型转换为多种格式，以满足不同环境和性能要求。

理想的格式取决于您模型的预期操作环境，平衡速度、硬件限制和集成的便利性。在接下来的部分中，我们将详细查看每个导出选项，了解何时选择每个选项。

### YOLOv8 的部署选项

让我们详细了解不同的 YOLOv8 部署选项。有关导出过程的详细步骤，请访问 Ultralytics 的导出文档页面。

#### PyTorch

PyTorch 是一个广泛用于深度学习和人工智能应用的开源机器学习库。它提供了高度的灵活性和速度，使其成为研究人员和开发人员喜爱的选择。

+   **性能基准**：PyTorch 因其易用性和灵活性而闻名，与其他更专业和优化的框架相比，可能会稍微牺牲一些原始性能。

+   **兼容性和集成**：与 Python 中各种数据科学和机器学习库具有良好的兼容性。

+   **社区支持和生态系统**：拥有最活跃的社区之一，提供丰富的学习和故障排除资源。

+   **案例研究**：在研究原型中常用，许多学术论文引用在 PyTorch 中部署的模型。

+   **维护和更新**：定期更新，积极开发并支持新功能。

+   **安全性考虑**：定期修补安全问题，但安全性在很大程度上取决于部署环境的整体情况。

+   **硬件加速**：支持 CUDA 进行 GPU 加速，对于加速模型训练和推断至关重要。

#### TorchScript

TorchScript 通过允许将模型导出到 C++ 运行时环境来扩展 PyTorch 的能力。这使其适用于 Python 不可用的生产环境。

+   **性能基准测试**：在生产环境中，可以比原生 PyTorch 提供更好的性能。

+   **兼容性和集成**：设计用于从 PyTorch 无缝过渡到 C++生产环境，尽管一些高级功能可能无法完美转换。

+   **社区支持和生态系统**：受益于 PyTorch 庞大的社区，但专业开发者范围较窄。

+   **案例研究**：广泛用于产业设置中，Python 的性能开销是一个瓶颈。

+   **维护和更新**：与 PyTorch 并行维护，并保持持续更新。

+   **安全考虑**：通过允许在没有完整 Python 安装的环境中运行模型，提供了改进的安全性。

+   **硬件加速**：继承了 PyTorch 的 CUDA 支持，确保了有效的 GPU 利用率。

#### ONNX

开放神经网络交换（ONNX）是一种允许模型在不同框架之间互操作的格式，在部署到各种平台时尤为关键。

+   **性能基准测试**：ONNX 模型在特定运行时上的性能可能有所不同。

+   **兼容性和集成**：由于其与框架无关的特性，高度支持多平台和硬件的互操作性。

+   **社区支持和生态系统**：得到许多组织的支持，导致一个广泛的生态系统，并提供多种优化工具。

+   **案例研究**：经常用于在不同机器学习框架之间转移模型，展示了其灵活性。

+   **维护和更新**：作为一个开放标准，ONNX 定期更新以支持新的操作和模型。

+   **安全考虑**：与任何跨平台工具一样，确保在转换和部署流程中采用安全实践至关重要。

+   **硬件加速**：使用 ONNX Runtime，模型可以利用各种硬件优化。

#### OpenVINO

OpenVINO 是 Intel 工具包，旨在促进在 Intel 硬件上部署深度学习模型，提升性能和速度。

+   **性能基准测试**：专为 Intel CPU、GPU 和 VPU 优化，可在兼容硬件上显著提升性能。

+   **兼容性和集成**：在 Intel 生态系统内表现最佳，但也支持一系列其他平台。

+   **社区支持和生态系统**：由 Intel 支持，尤其在计算机视觉领域拥有坚实的用户群体。

+   **案例研究**：通常在物联网和边缘计算场景中使用，其中 Intel 硬件占据主导地位。

+   **维护和更新**：Intel 定期更新 OpenVINO，以支持最新的深度学习模型和 Intel 硬件。

+   **安全考虑**：提供强大的安全功能，适合在敏感应用中部署。

+   **硬件加速**：专为在 Intel 硬件上加速而设计，利用专门的指令集和硬件功能。

若要了解更多关于使用 OpenVINO 进行部署的详细信息，请参阅 Ultralytics Integration documentation: Intel OpenVINO Export。

#### TensorRT

TensorRT 是 NVIDIA 提供的高性能深度学习推理优化器和运行时，非常适合需要速度和效率的应用。

+   **性能基准测试**：在 NVIDIA GPU 上提供顶级性能，支持高速推理。

+   **兼容性和集成**：最适合 NVIDIA 硬件，对于这个环境之外的支持有限。

+   **社区支持和生态系统**：通过 NVIDIA 的开发者论坛和文档提供强大的支持网络。

+   **案例研究**：在需要对视频和图像数据进行实时推理的行业中广泛采用。

+   **维护和更新**：NVIDIA 定期维护 TensorRT，以增强性能并支持新的 GPU 架构。

+   **安全考虑**：像许多 NVIDIA 产品一样，它非常重视安全性，但具体情况取决于部署环境。

+   **硬件加速**：专为 NVIDIA GPU 设计，提供深度优化和加速。

#### CoreML

CoreML 是苹果的机器学习框架，专为 iOS、macOS、watchOS 和 tvOS 等 Apple 生态系统的设备性能优化。

+   **性能基准测试**：在 Apple 硬件上优化设备性能，电池使用率最小化。

+   **兼容性和集成**：专为 Apple 生态系统设计，为 iOS 和 macOS 应用提供流畅的工作流程。

+   **社区支持和生态系统**：得到 Apple 和专门开发者社区的大力支持，具备广泛的文档和工具。

+   **案例研究**：在需要 Apple 产品上设备内机器学习能力的应用中广泛使用。

+   **维护和更新**：由 Apple 定期更新，以支持最新的机器学习进展和 Apple 硬件。

+   **安全考虑**：受益于 Apple 对用户隐私和数据安全的关注。

+   **硬件加速**：充分利用 Apple 的神经引擎和 GPU，加速机器学习任务。

#### TF SavedModel

TF SavedModel 是 TensorFlow 用于保存和提供机器学习模型的格式，特别适用于可扩展的服务器环境。

+   **性能基准测试**：在服务器环境中提供可扩展的性能，特别是与 TensorFlow Serving 一起使用时。

+   **兼容性和集成**：在 TensorFlow 生态系统内具有广泛的兼容性，包括云和企业服务器部署。

+   **社区支持和生态系统**：由于 TensorFlow 的流行，拥有庞大的社区支持，提供大量用于部署和优化的工具。

+   **案例研究**：在生产环境中广泛应用，用于大规模提供深度学习模型。

+   **维护和更新**：由 Google 和 TensorFlow 社区支持，确保定期更新和新功能。

+   **安全考虑**：使用 TensorFlow Serving 部署时，包含了面向企业级应用的强大安全功能。

+   **硬件加速**：通过 TensorFlow 后端支持各种硬件加速。

#### TF GraphDef

TF GraphDef 是一种表示模型为图的 TensorFlow 格式，对于需要静态计算图的环境非常有益。

+   **性能基准测试**：提供静态计算图的稳定性能，侧重于一致性和可靠性。

+   **兼容性和集成**：在 TensorFlow 基础架构内易于集成，但与 SavedModel 相比不够灵活。

+   **社区支持和生态系统**：TensorFlow 生态系统的良好支持，提供许多用于优化静态图的资源。

+   **案例研究**：在需要静态图的场景中非常有用，例如某些嵌入式系统。

+   **维护和更新**：与 TensorFlow 核心更新一起定期更新。

+   **安全考虑**：确保使用 TensorFlow 已建立的安全实践进行安全部署。

+   **硬件加速**：可以利用 TensorFlow 的硬件加速选项，尽管不如 SavedModel 灵活。

#### TF Lite

TF Lite 是 TensorFlow 针对移动和嵌入式设备的解决方案，提供轻量级库进行设备端推理。

+   **性能基准测试**：专为移动和嵌入式设备的速度和效率设计。

+   **兼容性和集成**：由于其轻量化特性，可在广泛设备上使用。

+   **社区支持和生态系统**：由 Google 支持，拥有强大的社区和越来越多的开发者资源。

+   **案例研究**：在需要在设备上推理并保持最小占用空间的移动应用中流行。

+   **维护和更新**：定期更新，包括最新功能和优化，适用于移动设备。

+   **安全考虑**：为在终端用户设备上运行模型提供安全环境。

+   **硬件加速**：支持包括 GPU 和 DSP 在内的多种硬件加速选项。

#### TF Edge TPU

TF Edge TPU 专为在 Google Edge TPU 硬件上进行高速、高效计算设计，非常适合需要实时处理的物联网设备。

+   **性能基准测试**：专门针对 Google Edge TPU 硬件的高速、高效计算进行优化。

+   **兼容性和集成**：仅在 Edge TPU 设备上与 TensorFlow Lite 模型配合使用。

+   **社区支持和生态系统**：得到 Google 和第三方开发者提供的资源支持的增长。

+   **案例研究**：用于物联网设备和需要低延迟实时处理的应用。

+   **维护和更新**：持续改进以利用新的 Edge TPU 硬件发布的能力。

+   **安全考虑**：与 Google 为物联网和边缘设备提供的强大安全集成。

+   **硬件加速**：定制设计以充分利用 Google Coral 设备的性能。

#### TF.js

TensorFlow.js（TF.js）是一个库，将机器学习能力直接带到浏览器中，为 web 开发者和用户提供了新的可能性。它允许在 web 应用中集成机器学习模型，无需后端基础设施支持。

+   **性能基准**：可以依据客户端设备的性能，在浏览器中直接实现机器学习。

+   **兼容性和集成**：与 web 技术高度兼容，易于集成到 web 应用中。

+   **社区支持和生态系统**：得到 web 和 Node.js 开发者社区的支持，提供多种部署 ML 模型的工具。

+   **案例研究**：适合需要客户端机器学习支持的交互式 web 应用，无需服务器端处理。

+   **维护和更新**：由 TensorFlow 团队维护，并得到开源社区的贡献。

+   **安全考虑**：在浏览器的安全环境中运行，利用 web 平台的安全模型。

+   **硬件加速**：通过访问像 WebGL 这样的 web API，可增强性能。

#### PaddlePaddle

PaddlePaddle 是百度开发的开源深度学习框架。它旨在为研究人员提供高效的同时，也为开发者提供易用性。在中国特别受欢迎，并且提供专门支持中文语言处理。

+   **性能基准**：提供竞争性能，注重易用性和可扩展性。

+   **兼容性和集成**：在百度生态系统内良好集成，并支持广泛的应用场景。

+   **社区支持和生态系统**：虽然全球社区规模较小，但在中国特别是快速增长。

+   **案例研究**：在中国市场普遍使用，并受开发者青睐，作为其他主要框架的替代选择。

+   **维护和更新**：定期更新，专注于服务于中文语言的 AI 应用和服务。

+   **安全考虑**：注重数据隐私和安全，符合中国的数据治理标准。

+   **硬件加速**：支持包括百度鲲鹏芯片在内的各种硬件加速。

#### NCNN

NCNN 是一个专为移动平台优化的高性能神经网络推断框架。它因其轻量和高效而脱颖而出，特别适用于资源有限的移动和嵌入式设备。

+   **性能基准**：针对移动平台高度优化，能在基于 ARM 的设备上提供高效的推断。

+   **兼容性和集成**：适用于基于 ARM 架构的手机和嵌入式系统应用。

+   **社区支持和生态系统**：得到专注于移动和嵌入式 ML 应用的小众但活跃的社区支持。

+   **案例研究**：适用于移动应用，特别是在 Android 和其他基于 ARM 的系统上，效率和速度至关重要。

+   **维护和更新**：持续改进，以保持在各种 ARM 设备上的高性能。

+   **安全考虑**：侧重于在设备上本地运行，利用设备处理的固有安全性。

+   **硬件加速**：专为 ARM CPU 和 GPU 定制，针对这些架构进行了特定的优化。

## YOLOv8 部署选项的比较分析

以下表格展示了 YOLOv8 模型的各种部署选项，帮助您根据几个关键标准评估哪种最适合您的项目需求。要深入了解每种部署选项的格式，请查看 Ultralytics 的文档页面上的导出格式。

| 部署选项 | 性能基准 | 兼容性和集成 | 社区支持和生态系统 | 案例研究 | 维护和更新 | 安全考虑 | 硬件加速 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| PyTorch | 灵活性强；可能会在原始性能上做出一些让步 | 与 Python 库兼容性优秀 | 广泛的资源和社区支持 | 研究和原型开发 | 持续、活跃的开发 | 取决于部署环境 | 支持 CUDA 进行 GPU 加速 |
| TorchScript | 用于生产比 PyTorch 更好 | 从 PyTorch 迁移到 C++ 更加顺畅 | 比 PyTorch 更专业但范围较窄 | 在 Python 是瓶颈的行业 | 与 PyTorch 一致的更新 | 在没有完整 Python 的情况下提高安全性 | 继承自 PyTorch 的 CUDA 支持 |
| ONNX | 取决于运行时变量 | 在不同框架中表现良好 | 广泛的生态系统，得到许多组织的支持 | 在 ML 框架中具有灵活性 | 定期更新以支持新操作 | 确保安全的转换和部署实践 | 各种硬件优化 |
| OpenVINO | 优化于 Intel 硬件 | 在 Intel 生态系统内表现最佳 | 在计算机视觉领域具有坚实基础 | IoT 和边缘计算中的 Intel 硬件 | 针对 Intel 硬件的定期更新 | 针对敏感应用提供强大的功能 | 专为 Intel 硬件定制 |
| TensorRT | 在 NVIDIA GPU 上处于顶尖水平 | 最适合 NVIDIA 硬件 | 通过 NVIDIA 形成强大的网络 | 实时视频和图像推理 | 针对新 GPU 的频繁更新 | 重视安全性 | 设计用于 NVIDIA GPU |
| CoreML | 优化用于设备上的 Apple 硬件 | 专属于 Apple 生态系统 | 强大的 Apple 和开发者支持 | 在 Apple 产品上的设备 ML | Apple 定期更新 | 专注于隐私和安全 | Apple 神经引擎和 GPU |
| TF SavedModel | 在服务器环境中可扩展 | TensorFlow 生态系统中广泛兼容 | 由于 TensorFlow 的流行性得到大量支持 | 大规模服务模型 | 谷歌和社区的定期更新 | 企业级强大功能 | 各种硬件加速选项 |
| TF GraphDef | 适用于静态计算图的稳定性 | 与 TensorFlow 基础设施良好集成 | 用于优化静态图的资源 | 需要静态图的场景 | 与 TensorFlow 核心同时更新 | 已建立的 TensorFlow 安全实践 | TensorFlow 加速选项 |
| TF Lite | 在移动/嵌入式设备上的速度和效率 | 广泛的设备支持 | 强大的社区，由 Google 支持 | 在端用户设备上的安全环境 | 移动应用程序的最新功能 | GPU 和 DSP 等多种硬件加速 |
| TF Edge TPU | 针对谷歌 Edge TPU 硬件优化 | 专为 Edge TPU 设备独家设计 | 与谷歌及第三方资源一起增长 | 需要实时处理的 IoT 设备 | 为新的 Edge TPU 硬件进行改进 | 谷歌强大的 IoT 安全性 | 专为谷歌 Coral 定制设计 |
| TF.js | 在浏览器中表现合理 | 与 Web 技术高度集成 | 支持 Web 和 Node.js 开发者 | 交互式 Web 应用程序 | TensorFlow 团队和社区的贡献 | Web 平台安全模型 | 通过 WebGL 和其他 API 增强 |
| PaddlePaddle | 具有竞争力、易于使用和可扩展性 | 百度生态系统，广泛的应用支持 | 快速增长，特别是在中国 | 中文市场和语言处理 | 专注于中国人工智能应用 | 强调数据隐私和安全性 | 包括百度的昆仑芯片 |
| NCNN | 针对移动基于 ARM 设备进行优化 | 移动和嵌入式 ARM 系统 | 小众但活跃的移动/嵌入式 ML 社区 | Android 和 ARM 系统的效率 | 在 ARM 上的高性能维护 | 设备上的安全优势 | ARM CPU 和 GPU 的优化 |

这份比较分析为您提供了高层次的概述。在部署时，重要的是考虑您项目的具体要求和限制，并参考每个选项的详细文档和资源。

## 社区与支持

当您开始使用 YOLOv8 时，拥有一个乐于助人的社区和支持可以产生重要影响。以下是如何与分享您兴趣的其他人联系并获取所需帮助的方法。

### 与更广泛的社区互动

+   **GitHub 讨论：** GitHub 上的 YOLOv8 仓库有一个 "讨论" 部分，您可以在此提问、报告问题和建议改进。

+   **Ultralytics Discord 服务器：** Ultralytics 拥有一个 [Discord 服务器](https://ultralytics.com/discord/)，您可以在此与其他用户和开发者交流。

### 官方文档和资源

+   **Ultralytics YOLOv8 文档：** 官方文档提供了 YOLOv8 的全面概述，以及有关安装、使用和故障排除的指南。

这些资源将帮助您解决挑战，并保持对 YOLOv8 社区最新趋势和最佳实践的更新。

## 结论

在本指南中，我们探讨了 YOLOv8 的不同部署选项。我们还讨论了在做出选择时需要考虑的重要因素。这些选项允许您根据不同的环境和性能要求定制您的模型，使其适用于实际应用。

不要忘记，YOLOv8 和 Ultralytics 社区是帮助的宝贵来源。与其他开发人员和专家联系，学习您在常规文档中找不到的独特技巧和解决方案。继续追求知识，探索新思路，并分享您的经验。

部署愉快！

## 常见问题解答

### YOLOv8 在不同硬件平台上的部署选项有哪些？

Ultralytics YOLOv8 支持各种部署格式，每种都针对特定的环境和硬件平台设计。关键格式包括：

+   **PyTorch** 用于研究和原型设计，具有优秀的 Python 集成。

+   **TorchScript** 用于在 Python 不可用的生产环境中。

+   **ONNX** 用于跨平台兼容性和硬件加速。

+   **OpenVINO** 用于在 Intel 硬件上优化性能。

+   **TensorRT** 用于在 NVIDIA GPU 上进行高速推断。

每种格式都有独特的优势。详细步骤请参阅我们的导出过程文档。

### 如何提高在 Intel CPU 上的 YOLOv8 模型推断速度？

要提高在 Intel CPU 上的推断速度，可以使用 Intel 的 OpenVINO 工具包部署 YOLOv8 模型。OpenVINO 通过优化模型以高效利用 Intel 硬件，显著提升性能。

1.  使用`model.export()`函数将您的 YOLOv8 模型转换为 OpenVINO 格式。

1.  在 Intel OpenVINO 导出文档中，按照详细的设置指南进行设置。

获得更多见解，请查看我们的[博客文章](https://www.ultralytics.com/blog/achieve-faster-inference-speeds-ultralytics-yolov8-openvino)。

### 我能在移动设备上部署 YOLOv8 模型吗？

是的，YOLOv8 模型可以使用 TensorFlow Lite（TF Lite）在 Android 和 iOS 平台上的移动设备上进行部署。TF Lite 专为移动和嵌入式设备设计，提供高效的设备端推断能力。

示例

```py
`# Export command for TFLite format model.export(format="tflite")` 
```

```py
`# CLI command for TFLite export yolo  export  --format  tflite` 
```

有关在移动设备上部署模型的详细信息，请参阅我们的 TF Lite 集成指南。

### 选择 YOLOv8 模型部署格式时应考虑哪些因素？

在选择 YOLOv8 部署格式时，需要考虑以下因素：

+   **性能**：像 TensorRT 这样的格式在 NVIDIA GPU 上提供卓越的速度，而 OpenVINO 则针对 Intel 硬件进行了优化。

+   **兼容性**：ONNX 在不同平台上具有广泛的兼容性。

+   **集成便捷性**：像 CoreML 或 TF Lite 这样的格式专为 iOS 和 Android 等特定生态系统量身定制。

+   **社区支持**：像 PyTorch 和 TensorFlow 这样的格式拥有丰富的社区资源和支持。

要进行比较分析，请参阅我们的导出格式文档。

### 如何在 Web 应用程序中部署 YOLOv8 模型？

要在 Web 应用程序中部署 YOLOv8 模型，您可以使用 TensorFlow.js（TF.js），它允许在浏览器中直接运行机器学习模型。这种方法消除了后端基础设施的需求，并提供实时性能。

1.  将 YOLOv8 模型导出到 TF.js 格式。

1.  将导出的模型集成到您的 Web 应用程序中。

对于逐步说明，请参阅我们关于 TensorFlow.js 集成的指南。
