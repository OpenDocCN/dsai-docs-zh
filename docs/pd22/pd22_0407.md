# `pandas.DataFrame.to_parquet`

> 原文：[`pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)

```py
DataFrame.to_parquet(path=None, *, engine='auto', compression='snappy', index=None, partition_cols=None, storage_options=None, **kwargs)
```

将 DataFrame 写入二进制 parquet 格式。

此函数将数据框写入[parquet 文件](https://parquet.apache.org/)。您可以选择不同的 parquet 后端，并选择压缩选项。有关更多详细信息，请参阅用户指南。

参数：

**path**str，路径对象，文件对象或 None，默认为 None

字符串、路径对象（实现`os.PathLike[str]`）或实现二进制`write()`函数的文件对象。如果为 None，则结果以字节形式返回。如果是字符串或路径，则在写入分区数据集时将用作根目录路径。

**engine**{‘auto’, ‘pyarrow’, ‘fastparquet’}，默认为‘auto’

要使用的 parquet 库。如果为‘auto’，则使用选项`io.parquet.engine`。默认`io.parquet.engine`行为是尝试‘pyarrow’，如果‘pyarrow’不可用，则退回到‘fastparquet’。

**compression**str 或 None，默认为‘snappy’

使用的压缩名称。使用`None`表示不压缩。支持的选项：‘snappy’、‘gzip’、‘brotli’、‘lz4’、‘zstd’。

**index**bool，默认为 None

如果为`True`，则将数据框的索引包含在文件输出中。如果为`False`，则不会写入文件。如果为`None`，类似于`True`，数据框的索引将被保存。但是，RangeIndex 将作为元数据中的范围存储，因此不需要太多空间并且更快。其他索引将包含在文件输出中作为列。

**partition_cols**list，可选，默认为 None

用于分区数据集的列名。按给定顺序对列进行分区。如果路径不是字符串，则必须为 None。

**storage_options**dict，可选

适用于特定存储连接的额外选项，例如主机、端口、用户名、密码等。对于 HTTP(S) URL，键值对将作为标头选项转发给`urllib.request.Request`。对于其他 URL（例如以“s3://”和“gcs://”开头的 URL），键值对将转发给`fsspec.open`。请参阅`fsspec`和`urllib`以获取更多详细信息，并且有关存储选项的更多示例，请参考[这里](https://pandas.pydata.org/docs/user_guide/io.html?highlight=storage_options#reading-writing-remote-files)。

****kwargs**

传递给 parquet 库的其他参数。有关更多详细信息，请参阅 pandas io。

返回：

如果未提供路径参数，则为字节，否则为 None

另请参阅

`read_parquet`

读取 parquet 文件。

`DataFrame.to_orc`

写入 orc 文件。

`DataFrame.to_csv`

写入 csv 文件。

`DataFrame.to_sql`

写入到一个 SQL 表。

`DataFrame.to_hdf`

写入到 HDF。

注意事项

这个函数需要使用[fastparquet](https://pypi.org/project/fastparquet)或[pyarrow](https://arrow.apache.org/docs/python/)库。

示例

```py
>>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})
>>> df.to_parquet('df.parquet.gzip',
...               compression='gzip')  
>>> pd.read_parquet('df.parquet.gzip')  
 col1  col2
0     1     3
1     2     4 
```

如果你想要获取到 parquet 内容的缓冲区，你可以使用一个 io.BytesIO 对象，只要你不使用 partition_cols，这会创建多个文件。

```py
>>> import io
>>> f = io.BytesIO()
>>> df.to_parquet(f)
>>> f.seek(0)
0
>>> content = f.read() 
```
