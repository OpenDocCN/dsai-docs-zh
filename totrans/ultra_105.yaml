- en: 'Quick Start Guide: NVIDIA Jetson with Ultralytics YOLOv8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/guides/nvidia-jetson/`](https://docs.ultralytics.com/guides/nvidia-jetson/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This comprehensive guide provides a detailed walkthrough for deploying Ultralytics
    YOLOv8 on [NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)
    devices. Additionally, it showcases performance benchmarks to demonstrate the
    capabilities of YOLOv8 on these small and powerful devices.
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/mUybgOlSxxA`](https://www.youtube.com/embed/mUybgOlSxxA)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** How to Setup NVIDIA Jetson with Ultralytics YOLOv8'
  prefs: []
  type: TYPE_NORMAL
- en: '![NVIDIA Jetson Ecosystem](img/7e331b3dafcec8f50a767f1c09c15a5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This guide has been tested with both [Seeed Studio reComputer J4012](https://www.seeedstudio.com/reComputer-J4012-p-5586.html)
    which is based on NVIDIA Jetson Orin NX 16GB running the latest stable JetPack
    release of [JP6.0](https://developer.nvidia.com/embedded/jetpack-sdk-60), JetPack
    release of [JP5.1.3](https://developer.nvidia.com/embedded/jetpack-sdk-513) and
    [Seeed Studio reComputer J1020 v2](https://www.seeedstudio.com/reComputer-J1020-v2-p-5498.html)
    which is based on NVIDIA Jetson Nano 4GB running JetPack release of [JP4.6.1](https://developer.nvidia.com/embedded/jetpack-sdk-461).
    It is expected to work across all the NVIDIA Jetson hardware lineup including
    latest and legacy.
  prefs: []
  type: TYPE_NORMAL
- en: What is NVIDIA Jetson?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NVIDIA Jetson is a series of embedded computing boards designed to bring accelerated
    AI (artificial intelligence) computing to edge devices. These compact and powerful
    devices are built around NVIDIA's GPU architecture and are capable of running
    complex AI algorithms and deep learning models directly on the device, without
    needing to rely on cloud computing resources. Jetson boards are often used in
    robotics, autonomous vehicles, industrial automation, and other applications where
    AI inference needs to be performed locally with low latency and high efficiency.
    Additionally, these boards are based on the ARM64 architecture and runs on lower
    power compared to traditional GPU computing devices.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA Jetson Series Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Jetson Orin](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)
    is the latest iteration of the NVIDIA Jetson family based on NVIDIA Ampere architecture
    which brings drastically improved AI performance when compared to the previous
    generations. Below table compared few of the Jetson devices in the ecosystem.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Jetson AGX Orin 64GB | Jetson Orin NX 16GB | Jetson Orin Nano 8GB | Jetson
    AGX Xavier | Jetson Xavier NX | Jetson Nano |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AI Performance | 275 TOPS | 100 TOPS | 40 TOPs | 32 TOPS | 21 TOPS | 472
    GFLOPS |'
  prefs: []
  type: TYPE_TB
- en: '| GPU | 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores | 1024-core
    NVIDIA Ampere architecture GPU with 32 Tensor Cores | 1024-core NVIDIA Ampere
    architecture GPU with 32 Tensor Cores | 512-core NVIDIA Volta architecture GPU
    with 64 Tensor Cores | 384-core NVIDIA Volta™ architecture GPU with 48 Tensor
    Cores | 128-core NVIDIA Maxwell™ architecture GPU |'
  prefs: []
  type: TYPE_TB
- en: '| GPU Max Frequency | 1.3 GHz | 918 MHz | 625 MHz | 1377 MHz | 1100 MHz | 921MHz
    |'
  prefs: []
  type: TYPE_TB
- en: '| CPU | 12-core NVIDIA Arm® Cortex A78AE v8.2 64-bit CPU 3MB L2 + 6MB L3 |
    8-core NVIDIA Arm® Cortex A78AE v8.2 64-bit CPU 2MB L2 + 4MB L3 | 6-core Arm®
    Cortex®-A78AE v8.2 64-bit CPU 1.5MB L2 + 4MB L3 | 8-core NVIDIA Carmel Arm®v8.2
    64-bit CPU 8MB L2 + 4MB L3 | 6-core NVIDIA Carmel Arm®v8.2 64-bit CPU 6MB L2 +
    4MB L3 | Quad-Core Arm® Cortex®-A57 MPCore processor |'
  prefs: []
  type: TYPE_TB
- en: '| CPU Max Frequency | 2.2 GHz | 2.0 GHz | 1.5 GHz | 2.2 GHz | 1.9 GHz | 1.43GHz
    |'
  prefs: []
  type: TYPE_TB
- en: '| Memory | 64GB 256-bit LPDDR5 204.8GB/s | 16GB 128-bit LPDDR5 102.4GB/s |
    8GB 128-bit LPDDR5 68 GB/s | 32GB 256-bit LPDDR4x 136.5GB/s | 8GB 128-bit LPDDR4x
    59.7GB/s | 4GB 64-bit LPDDR4 25.6GB/s" |'
  prefs: []
  type: TYPE_TB
- en: For a more detailed comparison table, please visit the **Technical Specifications**
    section of [official NVIDIA Jetson page](https://developer.nvidia.com/embedded/jetson-modules).
  prefs: []
  type: TYPE_NORMAL
- en: What is NVIDIA JetPack?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[NVIDIA JetPack SDK](https://developer.nvidia.com/embedded/jetpack) powering
    the Jetson modules is the most comprehensive solution and provides full development
    environment for building end-to-end accelerated AI applications and shortens time
    to market. JetPack includes Jetson Linux with bootloader, Linux kernel, Ubuntu
    desktop environment, and a complete set of libraries for acceleration of GPU computing,
    multimedia, graphics, and computer vision. It also includes samples, documentation,
    and developer tools for both host computer and developer kit, and supports higher
    level SDKs such as DeepStream for streaming video analytics, Isaac for robotics,
    and Riva for conversational AI.'
  prefs: []
  type: TYPE_NORMAL
- en: Flash JetPack to NVIDIA Jetson
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step after getting your hands on an NVIDIA Jetson device is to flash
    NVIDIA JetPack to the device. There are several different way of flashing NVIDIA
    Jetson devices.
  prefs: []
  type: TYPE_NORMAL
- en: If you own an official NVIDIA Development Kit such as the Jetson Orin Nano Developer
    Kit, you can [download an image and prepare an SD card with JetPack for booting
    the device](https://developer.nvidia.com/embedded/learn/get-started-jetson-orin-nano-devkit).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you own any other NVIDIA Development Kit, you can [flash JetPack to the device
    using SDK Manager](https://docs.nvidia.com/sdk-manager/install-with-sdkm-jetson/index.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you own a Seeed Studio reComputer J4012 device, you can [flash JetPack to
    the included SSD](https://wiki.seeedstudio.com/reComputer_J4012_Flash_Jetpack)
    and if you own a Seeed Studio reComputer J1020 v2 device, you can [flash JetPack
    to the eMMC/ SSD](https://wiki.seeedstudio.com/reComputer_J2021_J202_Flash_Jetpack).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you own any other third party device powered by the NVIDIA Jetson module,
    it is recommended to follow [command-line flashing](https://docs.nvidia.com/jetson/archives/r35.5.0/DeveloperGuide/IN/QuickStart.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For methods 3 and 4 above, after flashing the system and booting the device,
    please enter "sudo apt update && sudo apt install nvidia-jetpack -y" on the device
    terminal to install all the remaining JetPack components needed.
  prefs: []
  type: TYPE_NORMAL
- en: JetPack Support Based on Jetson Device
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The below table highlights NVIDIA JetPack versions supported by different NVIDIA
    Jetson devices.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | JetPack 4 | JetPack 5 | JetPack 6 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Jetson Nano | ✅ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| Jetson TX2 | ✅ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| Jetson Xavier NX | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| Jetson AGX Xavier | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| Jetson AGX Orin | ❌ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| Jetson Orin NX | ❌ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| Jetson Orin Nano | ❌ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: Quick Start with Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fastest way to get started with Ultralytics YOLOv8 on NVIDIA Jetson is to
    run with pre-built docker images for Jetson. Refer to the table above and choose
    the JetPack version according to the Jetson device you own.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After this is done, skip to Use TensorRT on NVIDIA Jetson section.
  prefs: []
  type: TYPE_NORMAL
- en: Start with Native Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a native installation without Docker, please refer to the steps below.
  prefs: []
  type: TYPE_NORMAL
- en: Run on JetPack 6.x
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Install Ultralytics Package
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we will install Ultralytics package on the Jetson with optional dependencies
    so that we can export the PyTorch models to other different formats. We will mainly
    focus on NVIDIA TensorRT exports because TensorRT will make sure we can get the
    maximum performance out of the Jetson devices.
  prefs: []
  type: TYPE_NORMAL
- en: Update packages list, install pip and upgrade to latest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install `ultralytics` pip package with optional dependencies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Reboot the device
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install PyTorch and Torchvision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The above ultralytics installation will install Torch and Torchvision. However,
    these 2 packages installed via pip are not compatible to run on Jetson platform
    which is based on ARM64 architecture. Therefore, we need to manually install pre-built
    PyTorch pip wheel and compile/ install Torchvision from source.
  prefs: []
  type: TYPE_NORMAL
- en: Install `torch 2.3.0` and `torchvision 0.18` according to JP6.0
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Visit the [PyTorch for Jetson page](https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048)
    to access all different versions of PyTorch for different JetPack versions. For
    a more detailed list on the PyTorch, Torchvision compatibility, visit the [PyTorch
    and Torchvision compatibility page](https://github.com/pytorch/vision).
  prefs: []
  type: TYPE_NORMAL
- en: Install `onnxruntime-gpu`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The [onnxruntime-gpu](https://pypi.org/project/onnxruntime-gpu/) package hosted
    in PyPI does not have `aarch64` binaries for the Jetson. So we need to manually
    install this package. This package is needed for some of the exports.
  prefs: []
  type: TYPE_NORMAL
- en: All different `onnxruntime-gpu` packages corresponding to different JetPack
    and Python versions are listed [here](https://elinux.org/Jetson_Zoo#ONNX_Runtime).
    However, here we will download and install `onnxruntime-gpu 1.18.0` with `Python3.10`
    support.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`onnxruntime-gpu` will automatically revert back the numpy version to latest.
    So we need to reinstall numpy to `1.23.5` to fix an issue by executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install numpy==1.23.5`'
  prefs: []
  type: TYPE_NORMAL
- en: Run on JetPack 5.x
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Install Ultralytics Package
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we will install Ultralytics package on the Jetson with optional dependencies
    so that we can export the PyTorch models to other different formats. We will mainly
    focus on NVIDIA TensorRT exports because TensorRT will make sure we can get the
    maximum performance out of the Jetson devices.
  prefs: []
  type: TYPE_NORMAL
- en: Update packages list, install pip and upgrade to latest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install `ultralytics` pip package with optional dependencies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Reboot the device
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install PyTorch and Torchvision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The above ultralytics installation will install Torch and Torchvision. However,
    these 2 packages installed via pip are not compatible to run on Jetson platform
    which is based on ARM64 architecture. Therefore, we need to manually install pre-built
    PyTorch pip wheel and compile/ install Torchvision from source.
  prefs: []
  type: TYPE_NORMAL
- en: Uninstall currently installed PyTorch and Torchvision
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install PyTorch 2.1.0 according to JP5.1.3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install Torchvision v0.16.2 according to PyTorch v2.1.0
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Visit the [PyTorch for Jetson page](https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048)
    to access all different versions of PyTorch for different JetPack versions. For
    a more detailed list on the PyTorch, Torchvision compatibility, visit the [PyTorch
    and Torchvision compatibility page](https://github.com/pytorch/vision).
  prefs: []
  type: TYPE_NORMAL
- en: Install `onnxruntime-gpu`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The [onnxruntime-gpu](https://pypi.org/project/onnxruntime-gpu/) package hosted
    in PyPI does not have `aarch64` binaries for the Jetson. So we need to manually
    install this package. This package is needed for some of the exports.
  prefs: []
  type: TYPE_NORMAL
- en: All different `onnxruntime-gpu` packages corresponding to different JetPack
    and Python versions are listed [here](https://elinux.org/Jetson_Zoo#ONNX_Runtime).
    However, here we will download and install `onnxruntime-gpu 1.17.0` with `Python3.8`
    support.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`onnxruntime-gpu` will automatically revert back the numpy version to latest.
    So we need to reinstall numpy to `1.23.5` to fix an issue by executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip install numpy==1.23.5`'
  prefs: []
  type: TYPE_NORMAL
- en: Use TensorRT on NVIDIA Jetson
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Out of all the model export formats supported by Ultralytics, TensorRT delivers
    the best inference performance when working with NVIDIA Jetson devices and our
    recommendation is to use TensorRT with Jetson. We also have a detailed document
    on TensorRT here.
  prefs: []
  type: TYPE_NORMAL
- en: Convert Model to TensorRT and Run Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The YOLOv8n model in PyTorch format is converted to TensorRT to run inference
    with the exported model.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Visit the Export page to access additional arguments when exporting models to
    different model formats
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA Jetson Orin YOLOv8 Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YOLOv8 benchmarks were run by the Ultralytics team on 10 different model formats
    measuring speed and accuracy: PyTorch, TorchScript, ONNX, OpenVINO, TensorRT,
    TF SavedModel, TF GraphDef, TF Lite, PaddlePaddle, NCNN. Benchmarks were run on
    Seeed Studio reComputer J4012 powered by Jetson Orin NX 16GB device at FP32 precision
    with default input image size of 640.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison Chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though all model exports are working with NVIDIA Jetson, we have only included
    **PyTorch, TorchScript, TensorRT** for the comparison chart below because, they
    make use of the GPU on the Jetson and are guaranteed to produce the best results.
    All the other exports only utilize the CPU and the performance is not as good
    as the above three. You can find benchmarks for all exports in the section after
    this chart.
  prefs: []
  type: TYPE_NORMAL
- en: '![NVIDIA Jetson Ecosystem](img/de74d093f481acb6f94a952545a46535.png)'
  prefs: []
  type: TYPE_IMG
- en: Detailed Comparison Table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The below table represents the benchmark results for five different models (YOLOv8n,
    YOLOv8s, YOLOv8m, YOLOv8l, YOLOv8x) across ten different formats (PyTorch, TorchScript,
    ONNX, OpenVINO, TensorRT, TF SavedModel, TF GraphDef, TF Lite, PaddlePaddle, NCNN),
    giving us the status, size, mAP50-95(B) metric, and inference time for each combination.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 6.2 | 0.6381 | 14.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | ✅ | 12.4 | 0.6117 | 13.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | ✅ | 12.2 | 0.6092 | 70.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | ✅ | 12.3 | 0.6092 | 104.2 |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | ✅ | 13.6 | 0.6117 | 8.9 |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | ✅ | 30.6 | 0.6092 | 141.74 |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | ✅ | 12.3 | 0.6092 | 199.93 |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | ✅ | 12.3 | 0.6092 | 349.18 |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | ✅ | 24.4 | 0.6030 | 555 |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | ✅ | 12.2 | 0.6092 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 21.5 | 0.6967 | 18 |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | ✅ | 43.0 | 0.7136 | 23.81 |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | ✅ | 42.8 | 0.7136 | 185.55 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | ✅ | 42.9 | 0.7136 | 243.97 |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | ✅ | 44.0 | 0.7136 | 14.82 |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | ✅ | 107 | 0.7136 | 260.03 |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | ✅ | 42.8 | 0.7136 | 423.4 |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | ✅ | 42.8 | 0.7136 | 1046.64 |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | ✅ | 85.5 | 0.7140 | 1464 |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | ✅ | 42.7 | 0.7200 | 63 |'
  prefs: []
  type: TYPE_TB
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 49.7 | 0.7370 | 36.4 |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | ✅ | 99.2 | 0.7285 | 53.58 |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | ✅ | 99 | 0.7280 | 452.09 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | ✅ | 99.1 | 0.7280 | 544.36 |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | ✅ | 100.3 | 0.7285 | 33.21 |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | ✅ | 247.5 | 0.7280 | 543.65 |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | ✅ | 99 | 0.7280 | 906.63 |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | ✅ | 99 | 0.7280 | 2758.08 |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | ✅ | 197.9 | 0.7280 | 3678 |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | ✅ | 98.9 | 0.7260 | 135 |'
  prefs: []
  type: TYPE_TB
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 83.7 | 0.7768 | 61.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | ✅ | 167.2 | 0.7554 | 87.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | ✅ | 166.8 | 0.7551 | 852.29 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | ✅ | 167 | 0.7551 | 1012.6 |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | ✅ | 168.4 | 0.7554 | 51.23 |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | ✅ | 417.2 | 0.7551 | 990.45 |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | ✅ | 166.9 | 0.7551 | 1649.86 |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | ✅ | 166.9 | 0.7551 | 5652.37 |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | ✅ | 333.6 | 0.7551 | 7114.67 |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | ✅ | 166.8 | 0.7685 | 231.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 130.5 | 0.7759 | 93 |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | ✅ | 260.7 | 0.7472 | 135.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | ✅ | 260.4 | 0.7479 | 1296.13 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | ✅ | 260.6 | 0.7479 | 1502.15 |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | ✅ | 261.8 | 0.7469 | 84.53 |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | ✅ | 651.1 | 0.7479 | 1451.76 |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | ✅ | 260.5 | 0.7479 | 4029.36 |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | ✅ | 260.4 | 0.7479 | 8772.86 |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | ✅ | 520.8 | 0.7479 | 10619.53 |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | ✅ | 260.4 | 0.7646 | 376.38 |'
  prefs: []
  type: TYPE_TB
- en: '[Explore more benchmarking efforts by Seeed Studio](https://www.seeedstudio.com/blog/2023/03/30/yolov8-performance-benchmarks-on-nvidia-jetson-devices)
    running on different versions of NVIDIA Jetson hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: Reproduce Our Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To reproduce the above Ultralytics benchmarks on all export formats run this
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that benchmarking results might vary based on the exact hardware and software
    configuration of a system, as well as the current workload of the system at the
    time the benchmarks are run. For the most reliable results use a dataset with
    a large number of images, i.e. `data='coco8.yaml' (4 val images), or`data='coco.yaml'`
    (5000 val images).
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices when using NVIDIA Jetson
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using NVIDIA Jetson, there are a couple of best practices to follow in
    order to enable maximum performance on the NVIDIA Jetson running YOLOv8.
  prefs: []
  type: TYPE_NORMAL
- en: Enable MAX Power Mode
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enabling MAX Power Mode on the Jetson will make sure all CPU, GPU cores are
    turned on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Enable Jetson Clocks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enabling Jetson Clocks will make sure all CPU, GPU cores are clocked at their
    maximum frequency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install Jetson Stats Application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use jetson stats application to monitor the temperatures of the system
    components and check other system details such as view CPU, GPU, RAM utilization,
    change power modes, set to max clocks, check JetPack information
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Jetson Stats](img/eca9687add440601967d5a07e832db7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Next Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Congratulations on successfully setting up YOLOv8 on your NVIDIA Jetson! For
    further learning and support, visit more guide at Ultralytics YOLOv8 Docs!
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do I deploy Ultralytics YOLOv8 on NVIDIA Jetson devices?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deploying Ultralytics YOLOv8 on NVIDIA Jetson devices is a straightforward process.
    First, flash your Jetson device with the NVIDIA JetPack SDK. Then, either use
    a pre-built Docker image for quick setup or manually install the required packages.
    Detailed steps for each approach can be found in sections Quick Start with Docker
    and Start with Native Installation.
  prefs: []
  type: TYPE_NORMAL
- en: What performance benchmarks can I expect from YOLOv8 models on NVIDIA Jetson
    devices?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: YOLOv8 models have been benchmarked on various NVIDIA Jetson devices showing
    significant performance improvements. For example, the TensorRT format delivers
    the best inference performance. The table in the Detailed Comparison Table section
    provides a comprehensive view of performance metrics like mAP50-95 and inference
    time across different model formats.
  prefs: []
  type: TYPE_NORMAL
- en: Why should I use TensorRT for deploying YOLOv8 on NVIDIA Jetson?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorRT is highly recommended for deploying YOLOv8 models on NVIDIA Jetson
    due to its optimal performance. It accelerates inference by leveraging the Jetson's
    GPU capabilities, ensuring maximum efficiency and speed. Learn more about how
    to convert to TensorRT and run inference in the Use TensorRT on NVIDIA Jetson
    section.
  prefs: []
  type: TYPE_NORMAL
- en: How can I install PyTorch and Torchvision on NVIDIA Jetson?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To install PyTorch and Torchvision on NVIDIA Jetson, first uninstall any existing
    versions that may have been installed via pip. Then, manually install the compatible
    PyTorch and Torchvision versions for the Jetson's ARM64 architecture. Detailed
    instructions for this process are provided in the Install PyTorch and Torchvision
    section.
  prefs: []
  type: TYPE_NORMAL
- en: What are the best practices for maximizing performance on NVIDIA Jetson when
    using YOLOv8?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To maximize performance on NVIDIA Jetson with YOLOv8, follow these best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Enable MAX Power Mode to utilize all CPU and GPU cores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable Jetson Clocks to run all cores at their maximum frequency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the Jetson Stats application for monitoring system metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For commands and additional details, refer to the Best Practices when using
    NVIDIA Jetson section.
  prefs: []
  type: TYPE_NORMAL
