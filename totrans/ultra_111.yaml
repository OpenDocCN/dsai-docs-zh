- en: 'Optimizing OpenVINO Inference for Ultralytics YOLO Models: A Comprehensive
    Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/guides/optimizing-openvino-latency-vs-throughput-modes/`](https://docs.ultralytics.com/guides/optimizing-openvino-latency-vs-throughput-modes/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![OpenVINO Ecosystem](img/055ab2bfede0cf7c3fa779f33d3a6ea3.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When deploying deep learning models, particularly those for object detection
    such as Ultralytics YOLO models, achieving optimal performance is crucial. This
    guide delves into leveraging Intel's OpenVINO toolkit to optimize inference, focusing
    on latency and throughput. Whether you're working on consumer-grade applications
    or large-scale deployments, understanding and applying these optimization strategies
    will ensure your models run efficiently on various devices.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing for Latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Latency optimization is vital for applications requiring immediate response
    from a single model given a single input, typical in consumer scenarios. The goal
    is to minimize the delay between input and inference result. However, achieving
    low latency involves careful consideration, especially when running concurrent
    inferences or managing multiple models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key Strategies for Latency Optimization:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Single Inference per Device:** The simplest way to achieve low latency is
    by limiting to one inference at a time per device. Additional concurrency often
    leads to increased latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leveraging Sub-Devices:** Devices like multi-socket CPUs or multi-tile GPUs
    can execute multiple requests with minimal latency increase by utilizing their
    internal sub-devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenVINO Performance Hints:** Utilizing OpenVINO''s `ov::hint::PerformanceMode::LATENCY`
    for the `ov::hint::performance_mode` property during model compilation simplifies
    performance tuning, offering a device-agnostic and future-proof approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Managing First-Inference Latency:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Model Caching:** To mitigate model load and compile times impacting latency,
    use model caching where possible. For scenarios where caching isn''t viable, CPUs
    generally offer the fastest model load times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Mapping vs. Reading:** To reduce load times, OpenVINO replaced model
    reading with mapping. However, if the model is on a removable or network drive,
    consider using `ov::enable_mmap(false)` to switch back to reading.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUTO Device Selection:** This mode begins inference on the CPU, shifting
    to an accelerator once ready, seamlessly reducing first-inference latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing for Throughput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughput optimization is crucial for scenarios serving numerous inference
    requests simultaneously, maximizing resource utilization without significantly
    sacrificing individual request performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approaches to Throughput Optimization:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**OpenVINO Performance Hints:** A high-level, future-proof method to enhance
    throughput across devices using performance hints.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Explicit Batching and Streams:** A more granular approach involving explicit
    batching and the use of streams for advanced performance tuning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Designing Throughput-Oriented Applications:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To maximize throughput, applications should:'
  prefs: []
  type: TYPE_NORMAL
- en: Process inputs in parallel, making full use of the device's capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompose data flow into concurrent inference requests, scheduled for parallel
    execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize the Async API with callbacks to maintain efficiency and avoid device
    starvation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-Device Execution:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenVINO's multi-device mode simplifies scaling throughput by automatically
    balancing inference requests across devices without requiring application-level
    device management.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimizing Ultralytics YOLO models for latency and throughput with OpenVINO
    can significantly enhance your application's performance. By carefully applying
    the strategies outlined in this guide, developers can ensure their models run
    efficiently, meeting the demands of various deployment scenarios. Remember, the
    choice between optimizing for latency or throughput depends on your specific application
    needs and the characteristics of the deployment environment.
  prefs: []
  type: TYPE_NORMAL
- en: For more detailed technical information and the latest updates, refer to the
    [OpenVINO documentation](https://docs.openvino.ai/latest/index.html) and [Ultralytics
    YOLO repository](https://github.com/ultralytics/ultralytics). These resources
    provide in-depth guides, tutorials, and community support to help you get the
    most out of your deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring your models achieve optimal performance is not just about tweaking
    configurations; it's about understanding your application's needs and making informed
    decisions. Whether you're optimizing for real-time responses or maximizing throughput
    for large-scale processing, the combination of Ultralytics YOLO models and OpenVINO
    offers a powerful toolkit for developers to deploy high-performance AI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do I optimize Ultralytics YOLO models for low latency using OpenVINO?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Optimizing Ultralytics YOLO models for low latency involves several key strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single Inference per Device:** Limit inferences to one at a time per device
    to minimize delays.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Leveraging Sub-Devices:** Utilize devices like multi-socket CPUs or multi-tile
    GPUs which can handle multiple requests with minimal latency increase.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**OpenVINO Performance Hints:** Use OpenVINO''s `ov::hint::PerformanceMode::LATENCY`
    during model compilation for simplified, device-agnostic tuning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For more practical tips on optimizing latency, check out the Latency Optimization
    section of our guide.
  prefs: []
  type: TYPE_NORMAL
- en: Why should I use OpenVINO for optimizing Ultralytics YOLO throughput?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenVINO enhances Ultralytics YOLO model throughput by maximizing device resource
    utilization without sacrificing performance. Key benefits include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance Hints:** Simple, high-level performance tuning across devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explicit Batching and Streams:** Fine-tuning for advanced performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-Device Execution:** Automated inference load balancing, easing application-level
    management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about throughput optimization in the Throughput Optimization section
    of our detailed guide.
  prefs: []
  type: TYPE_NORMAL
- en: What is the best practice for reducing first-inference latency in OpenVINO?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To reduce first-inference latency, consider these practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Caching:** Use model caching to decrease load and compile times.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Mapping vs. Reading:** Use mapping (`ov::enable_mmap(true)`) by default
    but switch to reading (`ov::enable_mmap(false)`) if the model is on a removable
    or network drive.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AUTO Device Selection:** Utilize AUTO mode to start with CPU inference and
    transition to an accelerator seamlessly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For detailed strategies on managing first-inference latency, refer to the Managing
    First-Inference Latency section.
  prefs: []
  type: TYPE_NORMAL
- en: How do I balance optimizing for latency and throughput with Ultralytics YOLO
    and OpenVINO?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Balancing latency and throughput optimization requires understanding your application
    needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency Optimization:** Ideal for real-time applications requiring immediate
    responses (e.g., consumer-grade apps).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Throughput Optimization:** Best for scenarios with many concurrent inferences,
    maximizing resource use (e.g., large-scale deployments).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using OpenVINO's high-level performance hints and multi-device modes can help
    strike the right balance. Choose the appropriate [OpenVINO Performance hints](https://docs.ultralytics.com/integrations/openvino#openvino-performance-hints)
    based on your specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Can I use Ultralytics YOLO models with other AI frameworks besides OpenVINO?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Yes, Ultralytics YOLO models are highly versatile and can be integrated with
    various AI frameworks. Options include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorRT:** For NVIDIA GPU optimization, follow the [TensorRT integration
    guide](https://docs.ultralytics.com/integrations/tensorrt).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoreML:** For Apple devices, refer to our [CoreML export instructions](https://docs.ultralytics.com/integrations/coreml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow.js:** For web and Node.js apps, see the [TF.js conversion guide](https://docs.ultralytics.com/integrations/tfjs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore more integrations on the [Ultralytics Integrations page](https://docs.ultralytics.com/integrations).
  prefs: []
  type: TYPE_NORMAL
