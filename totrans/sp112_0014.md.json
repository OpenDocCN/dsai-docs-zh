["```py\n>>> from scipy import stats \n```", "```py\n>>> from scipy.stats import norm \n```", "```py\n>>> print('bounds of distribution lower: %s, upper: %s' % norm.support())\nbounds of distribution lower: -inf, upper: inf \n```", "```py\n>>> rv = norm()\n>>> dir(rv)  # reformatted\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__',\n '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__',\n '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__',\n '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__',\n '__str__', '__subclasshook__', '__weakref__', 'a', 'args', 'b', 'cdf',\n 'dist', 'entropy', 'expect', 'interval', 'isf', 'kwds', 'logcdf',\n 'logpdf', 'logpmf', 'logsf', 'mean', 'median', 'moment', 'pdf', 'pmf',\n 'ppf', 'random_state', 'rvs', 'sf', 'stats', 'std', 'var'] \n```", "```py\n>>> dist_continu = [d for d in dir(stats) if\n...                 isinstance(getattr(stats, d), stats.rv_continuous)]\n>>> dist_discrete = [d for d in dir(stats) if\n...                  isinstance(getattr(stats, d), stats.rv_discrete)]\n>>> print('number of continuous distributions: %d' % len(dist_continu))\nnumber of continuous distributions: 108\n>>> print('number of discrete distributions: %d' % len(dist_discrete))\nnumber of discrete distributions:   20 \n```", "```py\n>>> norm.cdf(0)\n0.5 \n```", "```py\n>>> norm.cdf([-1., 0, 1])\narray([ 0.15865525,  0.5,  0.84134475])\n>>> import numpy as np\n>>> norm.cdf(np.array([-1., 0, 1]))\narray([ 0.15865525,  0.5,  0.84134475]) \n```", "```py\n>>> norm.mean(), norm.std(), norm.var()\n(0.0, 1.0, 1.0)\n>>> norm.stats(moments=\"mv\")\n(array(0.0), array(1.0)) \n```", "```py\n>>> norm.ppf(0.5)\n0.0 \n```", "```py\n>>> norm.rvs(size=3)\narray([-0.35687759,  1.34347647, -0.11710531])   # random \n```", "```py\n>>> norm.rvs(5)\n5.471435163732493  # random \n```", "```py\n>>> from numpy.random import default_rng\n>>> rng = default_rng() \n```", "```py\n>>> # do NOT copy this value\n>>> rng = default_rng(301439351238479871608357552876690613766) \n```", "```py\n>>> from numpy.random import SeedSequence\n>>> print(SeedSequence().entropy)\n301439351238479871608357552876690613766  # random \n```", "```py\n>>> norm.rvs(size=5, random_state=rng)\narray([ 0.47143516, -1.19097569,  1.43270697, -0.3126519 , -0.72058873])  # random \n```", "```py\n>>> norm.stats(loc=3, scale=4, moments=\"mv\")\n(array(3.0), array(16.0)) \n```", "```py\n>>> from scipy.stats import expon\n>>> expon.mean(scale=3.)\n3.0 \n```", "```py\n>>> from scipy.stats import uniform\n>>> uniform.cdf([0, 1, 2, 3, 4, 5], loc=1, scale=4)\narray([ 0\\.  ,  0\\.  ,  0.25,  0.5 ,  0.75,  1\\.  ]) \n```", "```py\n>>> np.mean(norm.rvs(5, size=500))\n5.0098355106969992  # random \n```", "```py\n>>> from scipy.stats import gamma\n>>> gamma.numargs\n1\n>>> gamma.shapes\n'a' \n```", "```py\n>>> gamma(1, scale=2.).stats(moments=\"mv\")\n(array(2.0), array(4.0)) \n```", "```py\n>>> gamma(a=1, scale=2.).stats(moments=\"mv\")\n(array(2.0), array(4.0)) \n```", "```py\n>>> rv = gamma(1, scale=2.) \n```", "```py\n>>> rv.mean(), rv.std()\n(2.0, 2.0) \n```", "```py\n>>> stats.t.isf([0.1, 0.05, 0.01], [[10], [11]])\narray([[ 1.37218364,  1.81246112,  2.76376946],\n [ 1.36343032,  1.79588482,  2.71807918]]) \n```", "```py\n>>> stats.t.isf([0.1, 0.05, 0.01], 10)\narray([ 1.37218364,  1.81246112,  2.76376946])\n>>> stats.t.isf([0.1, 0.05, 0.01], 11)\narray([ 1.36343032,  1.79588482,  2.71807918]) \n```", "```py\n>>> stats.t.isf([0.1, 0.05, 0.01], [10, 11, 12])\narray([ 1.37218364,  1.79588482,  2.68099799]) \n```", "```py\nppf(q) = min{x : cdf(x) >= q, x integer} \n```", "```py\n>>> from scipy.stats import hypergeom\n>>> [M, n, N] = [20, 7, 12] \n```", "```py\n>>> x = np.arange(4) * 2\n>>> x\narray([0, 2, 4, 6])\n>>> prb = hypergeom.cdf(x, M, n, N)\n>>> prb\narray([  1.03199174e-04,   5.21155831e-02,   6.08359133e-01,\n 9.89783282e-01])\n>>> hypergeom.ppf(prb, M, n, N)\narray([ 0.,  2.,  4.,  6.]) \n```", "```py\n>>> hypergeom.ppf(prb + 1e-8, M, n, N)\narray([ 1.,  3.,  5.,  7.])\n>>> hypergeom.ppf(prb - 1e-8, M, n, N)\narray([ 0.,  2.,  4.,  6.]) \n```", "```py\n>>> from scipy import stats\n>>> class deterministic_gen(stats.rv_continuous):\n...     def _cdf(self, x):\n...         return np.where(x < 0, 0., 1.)\n...     def _stats(self):\n...         return 0., 0., 0., 0. \n```", "```py\n>>> deterministic = deterministic_gen(name=\"deterministic\")\n>>> deterministic.cdf(np.arange(-3, 3, 0.5))\narray([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.]) \n```", "```py\n>>> deterministic.pdf(np.arange(-3, 3, 0.5))\narray([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n 0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n 5.83333333e+04,   4.16333634e-12,   4.16333634e-12,\n 4.16333634e-12,   4.16333634e-12,   4.16333634e-12]) \n```", "```py\n>>> from scipy.integrate import quad\n>>> quad(deterministic.pdf, -1e-1, 1e-1)\n(4.163336342344337e-13, 0.0) \n```", "```py\n>>> quad(deterministic.pdf, -1e-3, 1e-3)  # warning removed\n(1.000076872229173, 0.0010625571718182458) \n```", "```py\n>>> npoints = 20   # number of integer support points of the distribution minus 1\n>>> npointsh = npoints // 2\n>>> npointsf = float(npoints)\n>>> nbound = 4   # bounds for the truncated normal\n>>> normbound = (1+1/npointsf) * nbound   # actual bounds of truncated normal\n>>> grid = np.arange(-npointsh, npointsh+2, 1)   # integer grid\n>>> gridlimitsnorm = (grid-0.5) / npointsh * nbound   # bin limits for the truncnorm\n>>> gridlimits = grid - 0.5   # used later in the analysis\n>>> grid = grid[:-1]\n>>> probs = np.diff(stats.truncnorm.cdf(gridlimitsnorm, -normbound, normbound))\n>>> gridint = grid \n```", "```py\n>>> normdiscrete = stats.rv_discrete(values=(gridint,\n...              np.round(probs, decimals=7)), name='normdiscrete') \n```", "```py\n>>> print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076 \n```", "```py\n>>> nd_std = np.sqrt(normdiscrete.stats(moments='v')) \n```", "```py\n>>> n_sample = 500\n>>> rvs = normdiscrete.rvs(size=n_sample)\n>>> f, l = np.histogram(rvs, bins=gridlimits)\n>>> sfreq = np.vstack([gridint, f, probs*n_sample]).T\n>>> print(sfreq)\n[[-1.00000000e+01  0.00000000e+00  2.95019349e-02]  # random\n [-9.00000000e+00  0.00000000e+00  1.32294142e-01]\n [-8.00000000e+00  0.00000000e+00  5.06497902e-01]\n [-7.00000000e+00  2.00000000e+00  1.65568919e+00]\n [-6.00000000e+00  1.00000000e+00  4.62125309e+00]\n [-5.00000000e+00  9.00000000e+00  1.10137298e+01]\n [-4.00000000e+00  2.60000000e+01  2.24137683e+01]\n [-3.00000000e+00  3.70000000e+01  3.89503370e+01]\n [-2.00000000e+00  5.10000000e+01  5.78004747e+01]\n [-1.00000000e+00  7.10000000e+01  7.32455414e+01]\n [ 0.00000000e+00  7.40000000e+01  7.92618251e+01]\n [ 1.00000000e+00  8.90000000e+01  7.32455414e+01]\n [ 2.00000000e+00  5.50000000e+01  5.78004747e+01]\n [ 3.00000000e+00  5.00000000e+01  3.89503370e+01]\n [ 4.00000000e+00  1.70000000e+01  2.24137683e+01]\n [ 5.00000000e+00  1.10000000e+01  1.10137298e+01]\n [ 6.00000000e+00  4.00000000e+00  4.62125309e+00]\n [ 7.00000000e+00  3.00000000e+00  1.65568919e+00]\n [ 8.00000000e+00  0.00000000e+00  5.06497902e-01]\n [ 9.00000000e+00  0.00000000e+00  1.32294142e-01]\n [ 1.00000000e+01  0.00000000e+00  2.95019349e-02]] \n```", "```py\n>>> f2 = np.hstack([f[:5].sum(), f[5:-5], f[-5:].sum()])\n>>> p2 = np.hstack([probs[:5].sum(), probs[5:-5], probs[-5:].sum()])\n>>> ch2, pval = stats.chisquare(f2, p2*n_sample) \n```", "```py\n>>> print('chisquare for normdiscrete: chi2 = %6.3f pvalue = %6.4f' % (ch2, pval))\nchisquare for normdiscrete: chi2 = 12.466 pvalue = 0.4090  # random \n```", "```py\n>>> x = stats.t.rvs(10, size=1000) \n```", "```py\n>>> print(x.min())   # equivalent to np.min(x)\n-3.78975572422  # random\n>>> print(x.max())   # equivalent to np.max(x)\n5.26327732981  # random\n>>> print(x.mean())  # equivalent to np.mean(x)\n0.0140610663985  # random\n>>> print(x.var())   # equivalent to np.var(x))\n1.28899386208  # random \n```", "```py\n>>> m, v, s, k = stats.t.stats(10, moments='mvsk')\n>>> n, (smin, smax), sm, sv, ss, sk = stats.describe(x) \n```", "```py\n>>> sstr = '%-14s mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f'\n>>> print(sstr % ('distribution:', m, v, s ,k))\ndistribution:  mean = 0.0000, variance = 1.2500, skew = 0.0000, kurtosis = 1.0000  # random\n>>> print(sstr % ('sample:', sm, sv, ss, sk))\nsample:        mean = 0.0141, variance = 1.2903, skew = 0.2165, kurtosis = 1.0556  # random \n```", "```py\n>>> print('t-statistic = %6.3f pvalue = %6.4f' %  stats.ttest_1samp(x, m))\nt-statistic =  0.391 pvalue = 0.6955  # random \n```", "```py\n>>> tt = (sm-m)/np.sqrt(sv/float(n))  # t-statistic for mean\n>>> pval = stats.t.sf(np.abs(tt), n-1)*2  # two-sided pvalue = Prob(abs(t)>tt)\n>>> print('t-statistic = %6.3f pvalue = %6.4f' % (tt, pval))\nt-statistic =  0.391 pvalue = 0.6955  # random \n```", "```py\n>>> print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(x, 't', (10,)))\nKS-statistic D =  0.016 pvalue = 0.9571  # random \n```", "```py\n>>> print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(x, 'norm'))\nKS-statistic D =  0.028 pvalue = 0.3918  # random \n```", "```py\n>>> d, pval = stats.kstest((x-x.mean())/x.std(), 'norm')\n>>> print('KS-statistic D = %6.3f pvalue = %6.4f' % (d, pval))\nKS-statistic D =  0.032 pvalue = 0.2397  # random \n```", "```py\n>>> crit01, crit05, crit10 = stats.t.ppf([1-0.01, 1-0.05, 1-0.10], 10)\n>>> print('critical values from ppf at 1%%, 5%% and 10%%  %8.4f  %8.4f  %8.4f' % (crit01, crit05, crit10))\ncritical values from ppf at 1%, 5% and 10%   2.7638   1.8125   1.3722\n>>> print('critical values from isf at 1%%, 5%% and 10%%  %8.4f  %8.4f  %8.4f' % tuple(stats.t.isf([0.01,0.05,0.10],10)))\ncritical values from isf at 1%, 5% and 10%   2.7638   1.8125   1.3722 \n```", "```py\n>>> freq01 = np.sum(x>crit01) / float(n) * 100\n>>> freq05 = np.sum(x>crit05) / float(n) * 100\n>>> freq10 = np.sum(x>crit10) / float(n) * 100\n>>> print('sample %%-frequency at 1%%, 5%% and 10%% tail %8.4f  %8.4f  %8.4f' % (freq01, freq05, freq10))\nsample %-frequency at 1%, 5% and 10% tail   1.4000   5.8000  10.5000  # random \n```", "```py\n>>> freq05l = np.sum(stats.t.rvs(10, size=10000) > crit05) / 10000.0 * 100\n>>> print('larger sample %%-frequency at 5%% tail %8.4f' % freq05l)\nlarger sample %-frequency at 5% tail   4.8000  # random \n```", "```py\n>>> print('tail prob. of normal at 1%%, 5%% and 10%%  %8.4f  %8.4f  %8.4f' %\n...       tuple(stats.norm.sf([crit01, crit05, crit10])*100))\ntail prob. of normal at 1%, 5% and 10%   0.2857   3.4957   8.5003 \n```", "```py\n>>> quantiles = [0.0, 0.01, 0.05, 0.1, 1-0.10, 1-0.05, 1-0.01, 1.0]\n>>> crit = stats.t.ppf(quantiles, 10)\n>>> crit\narray([       -inf, -2.76376946, -1.81246112, -1.37218364,  1.37218364,\n 1.81246112,  2.76376946,         inf])\n>>> n_sample = x.size\n>>> freqcount = np.histogram(x, bins=crit)[0]\n>>> tprob = np.diff(quantiles)\n>>> nprob = np.diff(stats.norm.cdf(crit))\n>>> tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n>>> nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n>>> print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  2.30 pvalue = 0.8901  # random\n>>> print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 64.60 pvalue = 0.0000  # random \n```", "```py\n>>> tdof, tloc, tscale = stats.t.fit(x)\n>>> nloc, nscale = stats.norm.fit(x)\n>>> tprob = np.diff(stats.t.cdf(crit, tdof, loc=tloc, scale=tscale))\n>>> nprob = np.diff(stats.norm.cdf(crit, loc=nloc, scale=nscale))\n>>> tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n>>> nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n>>> print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  1.58 pvalue = 0.9542  # random\n>>> print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 11.08 pvalue = 0.0858  # random \n```", "```py\n>>> print('normal skewtest teststat = %6.3f pvalue = %6.4f' % stats.skewtest(x))\nnormal skewtest teststat =  2.785 pvalue = 0.0054  # random\n>>> print('normal kurtosistest teststat = %6.3f pvalue = %6.4f' % stats.kurtosistest(x))\nnormal kurtosistest teststat =  4.757 pvalue = 0.0000  # random \n```", "```py\n>>> print('normaltest teststat = %6.3f pvalue = %6.4f' % stats.normaltest(x))\nnormaltest teststat = 30.379 pvalue = 0.0000  # random \n```", "```py\n>>> print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest((x-x.mean())/x.std()))\nnormaltest teststat = 30.379 pvalue = 0.0000  # random \n```", "```py\n>>> print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest(stats.t.rvs(10, size=100)))\nnormaltest teststat =  4.698 pvalue = 0.0955  # random\n>>> print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...              stats.normaltest(stats.norm.rvs(size=1000)))\nnormaltest teststat =  0.613 pvalue = 0.7361  # random \n```", "```py\n>>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500)\n>>> rvs2 = stats.norm.rvs(loc=5, scale=10, size=500)\n>>> stats.ttest_ind(rvs1, rvs2)\nTtest_indResult(statistic=-0.5489036175088705, pvalue=0.5831943748663959)  # random \n```", "```py\n>>> rvs3 = stats.norm.rvs(loc=8, scale=10, size=500)\n>>> stats.ttest_ind(rvs1, rvs3)\nTtest_indResult(statistic=-4.533414290175026, pvalue=6.507128186389019e-06)  # random \n```", "```py\n>>> stats.ks_2samp(rvs1, rvs2)\nKstestResult(statistic=0.026, pvalue=0.9959527565364388)  # random \n```", "```py\n>>> stats.ks_2samp(rvs1, rvs3)\nKstestResult(statistic=0.114, pvalue=0.00299005061044668)  # random \n```", "```py\n>>> from scipy import stats\n>>> import matplotlib.pyplot as plt \n```", "```py\n>>> x1 = np.array([-7, -5, 1, 4, 5], dtype=np.float64)\n>>> kde1 = stats.gaussian_kde(x1)\n>>> kde2 = stats.gaussian_kde(x1, bw_method='silverman') \n```", "```py\n>>> fig = plt.figure()\n>>> ax = fig.add_subplot(111) \n```", "```py\n>>> ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n>>> x_eval = np.linspace(-10, 10, num=200)\n>>> ax.plot(x_eval, kde1(x_eval), 'k-', label=\"Scott's Rule\")\n>>> ax.plot(x_eval, kde2(x_eval), 'r-', label=\"Silverman's Rule\") \n```", "```py\n>>> plt.show() \n```", "```py\n>>> def my_kde_bandwidth(obj, fac=1./5):\n...  \"\"\"We use Scott's Rule, multiplied by a constant factor.\"\"\"\n...     return np.power(obj.n, -1./(obj.d+4)) * fac \n```", "```py\n>>> fig = plt.figure()\n>>> ax = fig.add_subplot(111) \n```", "```py\n>>> ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n>>> kde3 = stats.gaussian_kde(x1, bw_method=my_kde_bandwidth)\n>>> ax.plot(x_eval, kde3(x_eval), 'g-', label=\"With smaller BW\") \n```", "```py\n>>> plt.show() \n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nrng = np.random.default_rng()\nx1 = rng.normal(size=200)  # random data, normal distribution\nxs = np.linspace(x1.min()-1, x1.max()+1, 200)\n\nkde1 = stats.gaussian_kde(x1)\nkde2 = stats.gaussian_kde(x1, bw_method='silverman')\n\nfig = plt.figure(figsize=(8, 6))\n\nax1 = fig.add_subplot(211)\nax1.plot(x1, np.zeros(x1.shape), 'b+', ms=12)  # rug plot\nax1.plot(xs, kde1(xs), 'k-', label=\"Scott's Rule\")\nax1.plot(xs, kde2(xs), 'b-', label=\"Silverman's Rule\")\nax1.plot(xs, stats.norm.pdf(xs), 'r--', label=\"True PDF\")\n\nax1.set_xlabel('x')\nax1.set_ylabel('Density')\nax1.set_title(\"Normal (top) and Student's T$_{df=5}$ (bottom) distributions\")\nax1.legend(loc=1)\n\nx2 = stats.t.rvs(5, size=200, random_state=rng)  # random data, T distribution\nxs = np.linspace(x2.min() - 1, x2.max() + 1, 200)\n\nkde3 = stats.gaussian_kde(x2)\nkde4 = stats.gaussian_kde(x2, bw_method='silverman')\n\nax2 = fig.add_subplot(212)\nax2.plot(x2, np.zeros(x2.shape), 'b+', ms=12)  # rug plot\nax2.plot(xs, kde3(xs), 'k-', label=\"Scott's Rule\")\nax2.plot(xs, kde4(xs), 'b-', label=\"Silverman's Rule\")\nax2.plot(xs, stats.t.pdf(xs, 5), 'r--', label=\"True PDF\")\n\nax2.set_xlabel('x')\nax2.set_ylabel('Density')\n\nplt.show() \n```", "```py\n>>> from functools import partial \n```", "```py\n>>> loc1, scale1, size1 = (-2, 1, 175)\n>>> loc2, scale2, size2 = (2, 0.2, 50)\n>>> x2 = np.concatenate([np.random.normal(loc=loc1, scale=scale1, size=size1),\n...                      np.random.normal(loc=loc2, scale=scale2, size=size2)]) \n```", "```py\n>>> x_eval = np.linspace(x2.min() - 1, x2.max() + 1, 500) \n```", "```py\n>>> kde = stats.gaussian_kde(x2)\n>>> kde2 = stats.gaussian_kde(x2, bw_method='silverman')\n>>> kde3 = stats.gaussian_kde(x2, bw_method=partial(my_kde_bandwidth, fac=0.2))\n>>> kde4 = stats.gaussian_kde(x2, bw_method=partial(my_kde_bandwidth, fac=0.5)) \n```", "```py\n>>> pdf = stats.norm.pdf\n>>> bimodal_pdf = pdf(x_eval, loc=loc1, scale=scale1) * float(size1) / x2.size + \\\n...               pdf(x_eval, loc=loc2, scale=scale2) * float(size2) / x2.size \n```", "```py\n>>> fig = plt.figure(figsize=(8, 6))\n>>> ax = fig.add_subplot(111) \n```", "```py\n>>> ax.plot(x2, np.zeros(x2.shape), 'b+', ms=12)\n>>> ax.plot(x_eval, kde(x_eval), 'k-', label=\"Scott's Rule\")\n>>> ax.plot(x_eval, kde2(x_eval), 'b-', label=\"Silverman's Rule\")\n>>> ax.plot(x_eval, kde3(x_eval), 'g-', label=\"Scott * 0.2\")\n>>> ax.plot(x_eval, kde4(x_eval), 'c-', label=\"Scott * 0.5\")\n>>> ax.plot(x_eval, bimodal_pdf, 'r--', label=\"Actual PDF\") \n```", "```py\n>>> ax.set_xlim([x_eval.min(), x_eval.max()])\n>>> ax.legend(loc=2)\n>>> ax.set_xlabel('x')\n>>> ax.set_ylabel('Density')\n>>> plt.show() \n```", "```py\n>>> def measure(n):\n...  \"\"\"Measurement model, return two coupled measurements.\"\"\"\n...     m1 = np.random.normal(size=n)\n...     m2 = np.random.normal(scale=0.5, size=n)\n...     return m1+m2, m1-m2 \n```", "```py\n>>> m1, m2 = measure(2000)\n>>> xmin = m1.min()\n>>> xmax = m1.max()\n>>> ymin = m2.min()\n>>> ymax = m2.max() \n```", "```py\n>>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n>>> positions = np.vstack([X.ravel(), Y.ravel()])\n>>> values = np.vstack([m1, m2])\n>>> kernel = stats.gaussian_kde(values)\n>>> Z = np.reshape(kernel.evaluate(positions).T, X.shape) \n```", "```py\n>>> fig = plt.figure(figsize=(8, 6))\n>>> ax = fig.add_subplot(111) \n```", "```py\n>>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n...           extent=[xmin, xmax, ymin, ymax])\n>>> ax.plot(m1, m2, 'k.', markersize=2) \n```", "```py\n>>> ax.set_xlim([xmin, xmax])\n>>> ax.set_ylim([ymin, ymax]) \n```", "```py\n>>> plt.show() \n```", "```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt; plt.style.use('classic')\n>>> from scipy.stats import multiscale_graphcorr \n```", "```py\n>>> def mgc_plot(x, y, sim_name, mgc_dict=None, only_viz=False,\n...              only_mgc=False):\n...  \"\"\"Plot sim and MGC-plot\"\"\"\n...     if not only_mgc:\n...         # simulation\n...         plt.figure(figsize=(8, 8))\n...         ax = plt.gca()\n...         ax.set_title(sim_name + \" Simulation\", fontsize=20)\n...         ax.scatter(x, y)\n...         ax.set_xlabel('X', fontsize=15)\n...         ax.set_ylabel('Y', fontsize=15)\n...         ax.axis('equal')\n...         ax.tick_params(axis=\"x\", labelsize=15)\n...         ax.tick_params(axis=\"y\", labelsize=15)\n...         plt.show()\n...     if not only_viz:\n...         # local correlation map\n...         plt.figure(figsize=(8,8))\n...         ax = plt.gca()\n...         mgc_map = mgc_dict[\"mgc_map\"]\n...         # draw heatmap\n...         ax.set_title(\"Local Correlation Map\", fontsize=20)\n...         im = ax.imshow(mgc_map, cmap='YlGnBu')\n...         # colorbar\n...         cbar = ax.figure.colorbar(im, ax=ax)\n...         cbar.ax.set_ylabel(\"\", rotation=-90, va=\"bottom\")\n...         ax.invert_yaxis()\n...         # Turn spines off and create white grid.\n...         for edge, spine in ax.spines.items():\n...             spine.set_visible(False)\n...         # optimal scale\n...         opt_scale = mgc_dict[\"opt_scale\"]\n...         ax.scatter(opt_scale[0], opt_scale[1],\n...                    marker='X', s=200, color='red')\n...         # other formatting\n...         ax.tick_params(bottom=\"off\", left=\"off\")\n...         ax.set_xlabel('#Neighbors for X', fontsize=15)\n...         ax.set_ylabel('#Neighbors for Y', fontsize=15)\n...         ax.tick_params(axis=\"x\", labelsize=15)\n...         ax.tick_params(axis=\"y\", labelsize=15)\n...         ax.set_xlim(0, 100)\n...         ax.set_ylim(0, 100)\n...         plt.show() \n```", "```py\n>>> rng = np.random.default_rng()\n>>> x = np.linspace(-1, 1, num=100)\n>>> y = x + 0.3 * rng.random(x.size) \n```", "```py\n>>> mgc_plot(x, y, \"Linear\", only_viz=True) \n```", "```py\n>>> stat, pvalue, mgc_dict = multiscale_graphcorr(x, y)\n>>> print(\"MGC test statistic: \", round(stat, 1))\nMGC test statistic:  1.0\n>>> print(\"P-value: \", round(pvalue, 1))\nP-value:  0.0\n>>> mgc_plot(x, y, \"Linear\", mgc_dict, only_mgc=True) \n```", "```py\n>>> unif = np.array(rng.uniform(0, 5, size=100))\n>>> x = unif * np.cos(np.pi * unif)\n>>> y = unif * np.sin(np.pi * unif) + 0.4 * rng.random(x.size) \n```", "```py\n>>> mgc_plot(x, y, \"Spiral\", only_viz=True) \n```", "```py\n>>> stat, pvalue, mgc_dict = multiscale_graphcorr(x, y)\n>>> print(\"MGC test statistic: \", round(stat, 1))\nMGC test statistic:  0.2  # random\n>>> print(\"P-value: \", round(pvalue, 1))\nP-value:  0.0\n>>> mgc_plot(x, y, \"Spiral\", mgc_dict, only_mgc=True) \n```", "```py\n>>> import numpy as np\n>>> disc = 10\n>>> x1 = np.linspace(0, 1, disc)\n>>> x2 = np.linspace(0, 1, disc)\n>>> x3 = np.linspace(0, 1, disc)\n>>> x1, x2, x3 = np.meshgrid(x1, x2, x3) \n```", "```py\n>>> import numpy as np\n>>> from scipy.stats import qmc\n>>> space_1 = np.array([[1, 3], [2, 6], [3, 2], [4, 5], [5, 1], [6, 4]])\n>>> space_2 = np.array([[1, 5], [2, 4], [3, 3], [4, 2], [5, 1], [6, 6]])\n>>> l_bounds = [0.5, 0.5]\n>>> u_bounds = [6.5, 6.5]\n>>> space_1 = qmc.scale(space_1, l_bounds, u_bounds, reverse=True)\n>>> space_2 = qmc.scale(space_2, l_bounds, u_bounds, reverse=True)\n>>> qmc.discrepancy(space_1)\n0.008142039609053464\n>>> qmc.discrepancy(space_2)\n0.010456854423869011 \n```", "```py\n\"\"\"Sobol' and Halton sequences.\"\"\"\nfrom scipy.stats import qmc\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng()\n\nn_sample = 256\ndim = 2\n\nsample = {}\n\n# Sobol'\nengine = qmc.Sobol(d=dim, seed=rng)\nsample[\"Sobol'\"] = engine.random(n_sample)\n\n# Halton\nengine = qmc.Halton(d=dim, seed=rng)\nsample[\"Halton\"] = engine.random(n_sample)\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i, kind in enumerate(sample):\n    axs[i].scatter(sample[kind][:, 0], sample[kind][:, 1])\n\n    axs[i].set_aspect('equal')\n    axs[i].set_xlabel(r'$x_1$')\n    axs[i].set_ylabel(r'$x_2$')\n    axs[i].set_title(f'{kind}—$C^2 = ${qmc.discrepancy(sample[kind]):.2}')\n\nplt.tight_layout()\nplt.show() \n```", "```py\n>>> from scipy.stats import qmc\n>>> engine = qmc.Halton(d=2)\n>>> engine.random(5)\narray([[0.22166437, 0.07980522],  # random\n [0.72166437, 0.93165708],\n [0.47166437, 0.41313856],\n [0.97166437, 0.19091633],\n [0.01853937, 0.74647189]])\n>>> engine.random(5)\narray([[0.51853937, 0.52424967],  # random\n [0.26853937, 0.30202745],\n [0.76853937, 0.857583  ],\n [0.14353937, 0.63536078],\n [0.64353937, 0.01807683]]) \n```", "```py\n>>> engine.reset()\n>>> engine.random(5)\narray([[0.22166437, 0.07980522],  # random\n [0.72166437, 0.93165708],\n [0.47166437, 0.41313856],\n [0.97166437, 0.19091633],\n [0.01853937, 0.74647189]]) \n```", "```py\n>>> engine.reset()\n>>> engine.fast_forward(5)\n>>> engine.random(5)\narray([[0.51853937, 0.52424967],  # random\n [0.26853937, 0.30202745],\n [0.76853937, 0.857583  ],\n [0.14353937, 0.63536078],\n [0.64353937, 0.01807683]]) \n```", "```py\n>>> import numpy as np\n>>> from scipy.stats import qmc\n>>> class RandomEngine(qmc.QMCEngine):\n...     def __init__(self, d, seed=None):\n...         super().__init__(d=d, seed=seed)\n...         self.rng = np.random.default_rng(self.rng_seed)\n...\n...\n...     def _random(self, n=1, *, workers=1):\n...         return self.rng.random((n, self.d))\n...\n...\n...     def reset(self):\n...         self.rng = np.random.default_rng(self.rng_seed)\n...         self.num_generated = 0\n...         return self\n...\n...\n...     def fast_forward(self, n):\n...         self.random(n)\n...         return self \n```", "```py\n>>> engine = RandomEngine(2)\n>>> engine.random(5)\narray([[0.22733602, 0.31675834],  # random\n [0.79736546, 0.67625467],\n [0.39110955, 0.33281393],\n [0.59830875, 0.18673419],\n [0.67275604, 0.94180287]])\n>>> engine.reset()\n>>> engine.random(5)\narray([[0.22733602, 0.31675834],  # random\n [0.79736546, 0.67625467],\n [0.39110955, 0.33281393],\n [0.59830875, 0.18673419],\n [0.67275604, 0.94180287]]) \n```"]