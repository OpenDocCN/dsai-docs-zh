- en: Machine Learning Best Practices and Tips for Model Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/guides/model-training-tips/`](https://docs.ultralytics.com/guides/model-training-tips/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important steps when working on a computer vision project is
    model training. Before reaching this step, you need to define your goals and collect
    and annotate your data. After preprocessing the data to make sure it is clean
    and consistent, you can move on to training your model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: So, what is model training? Model training is the process of teaching your model
    to recognize visual patterns and make predictions based on your data. It directly
    impacts the performance and accuracy of your application. In this guide, we'll
    cover best practices, optimization techniques, and troubleshooting tips to help
    you train your computer vision models effectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: How to Train a Machine Learning Model
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A computer vision model is trained by adjusting its internal parameters to minimize
    errors. Initially, the model is fed a large set of labeled images. It makes predictions
    about what is in these images, and the predictions are compared to the actual
    labels or contents to calculate errors. These errors show how far off the model's
    predictions are from the true values.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model iteratively makes predictions, calculates errors,
    and updates its parameters through a process called backpropagation. In this process,
    the model adjusts its internal parameters (weights and biases) to reduce the errors.
    By repeating this cycle many times, the model gradually improves its accuracy.
    Over time, it learns to recognize complex patterns such as shapes, colors, and
    textures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Backpropagation?](img/420c5cde68b3bbab55e0e3fc50ff336a.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: This learning process makes it possible for the computer vision model to perform
    various tasks, including object detection, instance segmentation, and image classification.
    The ultimate goal is to create a model that can generalize its learning to new,
    unseen images so that it can accurately understand visual data in real-world applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what is happening behind the scenes when we train a model,
    let's look at points to consider when training a model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Training on Large Datasets
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few different aspects to think about when you are planning on using
    a large dataset to train a model. For example, you can adjust the batch size,
    control the GPU utilization, choose to use multiscale training, etc. Let's walk
    through each of these options in detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Batch Size and GPU Utilization
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When training models on large datasets, efficiently utilizing your GPU is key.
    Batch size is an important factor. It is the number of data samples that a machine
    learning model processes in a single training iteration. Using the maximum batch
    size supported by your GPU, you can fully take advantage of its capabilities and
    reduce the time model training takes. However, you want to avoid running out of
    GPU memory. If you encounter memory errors, reduce the batch size incrementally
    until the model trains smoothly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: With respect to YOLOv8, you can set the `batch_size` parameter in the training
    configuration to match your GPU capacity. Also, setting `batch=-1` in your training
    script will automatically determine the batch size that can be efficiently processed
    based on your device's capabilities. By fine-tuning the batch size, you can make
    the most of your GPU resources and improve the overall training process.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Subset Training
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Subset training is a smart strategy that involves training your model on a smaller
    set of data that represents the larger dataset. It can save time and resources,
    especially during initial model development and testing. If you are running short
    on time or experimenting with different model configurations, subset training
    is a good option.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to YOLOv8, you can easily implement subset training by using the
    `fraction` parameter. This parameter lets you specify what fraction of your dataset
    to use for training. For example, setting `fraction=0.1` will train your model
    on 10% of the data. You can use this technique for quick iterations and tuning
    your model before committing to training a model using a full dataset. Subset
    training helps you make rapid progress and identify potential issues early on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Multi-scale Training
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiscale training is a technique that improves your model's ability to generalize
    by training it on images of varying sizes. Your model can learn to detect objects
    at different scales and distances and become more robust.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: For example, when you train YOLOv8, you can enable multiscale training by setting
    the `scale` parameter. This parameter adjusts the size of training images by a
    specified factor, simulating objects at different distances. For example, setting
    `scale=0.5` will reduce the image size by half, while `scale=2.0` will double
    it. Configuring this parameter allows your model to experience a variety of image
    scales and improve its detection capabilities across different object sizes and
    scenarios.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Caching is an important technique to improve the efficiency of training machine
    learning models. By storing preprocessed images in memory, caching reduces the
    time the GPU spends waiting for data to be loaded from the disk. The model can
    continuously receive data without delays caused by disk I/O operations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Caching can be controlled when training YOLOv8 using the `cache` parameter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '*`cache=True`*: Stores dataset images in RAM, providing the fastest access
    speed but at the cost of increased memory usage.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`cache=''disk''`*: Stores the images on disk, slower than RAM but faster than
    loading fresh data each time.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`cache=False`*: Disables caching, relying entirely on disk I/O, which is the
    slowest option.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixed Precision Training
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mixed precision training uses both 16-bit (FP16) and 32-bit (FP32) floating-point
    types. The strengths of both FP16 and FP32 are leveraged by using FP16 for faster
    computation and FP32 to maintain precision where needed. Most of the neural network's
    operations are done in FP16 to benefit from faster computation and lower memory
    usage. However, a master copy of the model's weights is kept in FP32 to ensure
    accuracy during the weight update steps. You can handle larger models or larger
    batch sizes within the same hardware constraints.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Mixed Precision Training Overview](img/6af178f2fe756fe4041f92405d26630e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: To implement mixed precision training, you'll need to modify your training scripts
    and ensure your hardware (like GPUs) supports it. Many modern deep learning frameworks,
    such as Tensorflow, offer built-in support for mixed precision.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Mixed precision training is straightforward when working with YOLOv8\. You can
    use the `amp` flag in your training configuration. Setting `amp=True` enables
    Automatic Mixed Precision (AMP) training. Mixed precision training is a simple
    yet effective way to optimize your model training process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained Weights
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using pretrained weights is a smart way to speed up your model's training process.
    Pretrained weights come from models already trained on large datasets, giving
    your model a head start. Transfer learning adapts pretrained models to new, related
    tasks. Fine-tuning a pre-trained model involves starting with these weights and
    then continuing training on your specific dataset. This method of training results
    in faster training times and often better performance because the model starts
    with a solid understanding of basic features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The `pretrained` parameter makes transfer learning easy with YOLOv8\. Setting
    `pretrained=True` will use default pre-trained weights, or you can specify a path
    to a custom pre-trained model. Using pre-trained weights and transfer learning
    effectively boosts your model's capabilities and reduces training costs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Other Techniques to Consider When Handling a Large Dataset
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a couple of other techniques to consider when handling a large dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate Schedulers**: Implementing learning rate schedulers dynamically
    adjusts the learning rate during training. A well-tuned learning rate can prevent
    the model from overshooting minima and improve stability. When training YOLOv8,
    the `lrf` parameter helps manage learning rate scheduling by setting the final
    learning rate as a fraction of the initial rate.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed Training**: For handling large datasets, distributed training
    can be a game-changer. You can reduce the training time by spreading the training
    workload across multiple GPUs or machines.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Number of Epochs To Train For
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training a model, an epoch refers to one complete pass through the entire
    training dataset. During an epoch, the model processes each example in the training
    set once and updates its parameters based on the learning algorithm. Multiple
    epochs are usually needed to allow the model to learn and refine its parameters
    over time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: A common question that comes up is how to determine the number of epochs to
    train the model for. A good starting point is 300 epochs. If the model overfits
    early, you can reduce the number of epochs. If overfitting does not occur after
    300 epochs, you can extend the training to 600, 1200, or more epochs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: However, the ideal number of epochs can vary based on your dataset's size and
    project goals. Larger datasets might require more epochs for the model to learn
    effectively, while smaller datasets might need fewer epochs to avoid overfitting.
    With respect to YOLOv8, you can set the `epochs` parameter in your training script.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Early Stopping
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Early stopping is a valuable technique for optimizing model training. By monitoring
    validation performance, you can halt training once the model stops improving.
    You can save computational resources and prevent overfitting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The process involves setting a patience parameter that determines how many epochs
    to wait for an improvement in validation metrics before stopping training. If
    the model's performance does not improve within these epochs, training is stopped
    to avoid wasting time and resources.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Early Stopping Overview](img/ecfd06cb65546cf9964874a8a983c402.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: For YOLOv8, you can enable early stopping by setting the patience parameter
    in your training configuration. For example, `patience=5` means training will
    stop if there's no improvement in validation metrics for 5 consecutive epochs.
    Using this method ensures the training process remains efficient and achieves
    optimal performance without excessive computation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Choosing Between Cloud and Local Training
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two options for training your model: cloud training and local training.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Cloud training offers scalability and powerful hardware and is ideal for handling
    large datasets and complex models. Platforms like Google Cloud, AWS, and Azure
    provide on-demand access to high-performance GPUs and TPUs, speeding up training
    times and enabling experiments with larger models. However, cloud training can
    be expensive, especially for long periods, and data transfer can add to costs
    and latency.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Local training provides greater control and customization, letting you tailor
    your environment to specific needs and avoid ongoing cloud costs. It can be more
    economical for long-term projects, and since your data stays on-premises, it's
    more secure. However, local hardware may have resource limitations and require
    maintenance, which can lead to longer training times for large models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Selecting an Optimizer
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An optimizer is an algorithm that adjusts the weights of your neural network
    to minimize the loss function, which measures how well the model is performing.
    In simpler terms, the optimizer helps the model learn by tweaking its parameters
    to reduce errors. Choosing the right optimizer directly affects how quickly and
    accurately the model learns.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: You can also fine-tune optimizer parameters to improve model performance. Adjusting
    the learning rate sets the size of the steps when updating parameters. For stability,
    you might start with a moderate learning rate and gradually decrease it over time
    to improve long-term learning. Additionally, setting the momentum determines how
    much influence past updates have on current updates. A common value for momentum
    is around 0.9\. It generally provides a good balance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Common Optimizers
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different optimizers have various strengths and weaknesses. Let's take a glimpse
    at a few common optimizers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '**SGD (Stochastic Gradient Descent)**:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates model parameters using the gradient of the loss function with respect
    to the parameters.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple and efficient but can be slow to converge and might get stuck in local
    minima.
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adam (Adaptive Moment Estimation)**:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combines the benefits of both SGD with momentum and RMSProp.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusts the learning rate for each parameter based on estimates of the first
    and second moments of the gradients.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Well-suited for noisy data and sparse gradients.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient and generally requires less tuning, making it a recommended optimizer
    for YOLOv8.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSProp (Root Mean Square Propagation)**:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusts the learning rate for each parameter by dividing the gradient by a running
    average of the magnitudes of recent gradients.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps in handling the vanishing gradient problem and is effective for recurrent
    neural networks.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For YOLOv8, the `optimizer` parameter lets you choose from various optimizers,
    including SGD, Adam, AdamW, NAdam, RAdam, and RMSProp, or you can set it to `auto`
    for automatic selection based on model configuration.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Connecting with the Community
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being part of a community of computer vision enthusiasts can help you solve
    problems and learn faster. Here are some ways to connect, get help, and share
    ideas.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Community Resources
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**GitHub Issues:** Visit the [YOLOv8 GitHub repository](https://github.com/ultralytics/ultralytics/issues)
    and use the Issues tab to ask questions, report bugs, and suggest new features.
    The community and maintainers are very active and ready to help.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ultralytics Discord Server:** Join the [Ultralytics Discord server](https://ultralytics.com/discord/)
    to chat with other users and developers, get support, and share your experiences.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Official Documentation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Ultralytics YOLOv8 Documentation:** Check out the official YOLOv8 documentation
    for detailed guides and helpful tips on various computer vision projects.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these resources will help you solve challenges and stay up-to-date with
    the latest trends and practices in the computer vision community.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training computer vision models involves following good practices, optimizing
    your strategies, and solving problems as they arise. Techniques like adjusting
    batch sizes, mixed precision training, and starting with pre-trained weights can
    make your models work better and train faster. Methods like subset training and
    early stopping help you save time and resources. Staying connected with the community
    and keeping up with new trends will help you keep improving your model training
    skills.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can I improve GPU utilization when training a large dataset with Ultralytics
    YOLO?
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To improve GPU utilization, set the `batch_size` parameter in your training
    configuration to the maximum size supported by your GPU. This ensures that you
    make full use of the GPU's capabilities, reducing training time. If you encounter
    memory errors, incrementally reduce the batch size until training runs smoothly.
    For YOLOv8, setting `batch=-1` in your training script will automatically determine
    the optimal batch size for efficient processing. For further information, refer
    to the training configuration.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: What is mixed precision training, and how do I enable it in YOLOv8?
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mixed precision training utilizes both 16-bit (FP16) and 32-bit (FP32) floating-point
    types to balance computational speed and precision. This approach speeds up training
    and reduces memory usage without sacrificing model accuracy. To enable mixed precision
    training in YOLOv8, set the `amp` parameter to `True` in your training configuration.
    This activates Automatic Mixed Precision (AMP) training. For more details on this
    optimization technique, see the training configuration.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: How does multiscale training enhance YOLOv8 model performance?
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiscale training enhances model performance by training on images of varying
    sizes, allowing the model to better generalize across different scales and distances.
    In YOLOv8, you can enable multiscale training by setting the `scale` parameter
    in the training configuration. For example, `scale=0.5` reduces the image size
    by half, while `scale=2.0` doubles it. This technique simulates objects at different
    distances, making the model more robust across various scenarios. For settings
    and more details, check out the training configuration.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: How can I use pre-trained weights to speed up training in YOLOv8?
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using pre-trained weights can significantly reduce training times and improve
    model performance by starting from a model that already understands basic features.
    In YOLOv8, you can set the `pretrained` parameter to `True` or specify a path
    to custom pre-trained weights in your training configuration. This approach, known
    as transfer learning, leverages knowledge from large datasets to adapt to your
    specific task. Learn more about pre-trained weights and their advantages here.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练权重可以显著减少训练时间，并通过从已经理解基本特征的模型开始来提高模型性能。在YOLOv8中，您可以将`pretrained`参数设置为`True`，或在训练配置中指定自定义预训练权重的路径。这种称为迁移学习的方法利用大型数据集的知识来适应您的特定任务。在这里了解更多关于预训练权重及其优势的信息。
- en: What is the recommended number of epochs for training a model, and how do I
    set this in YOLOv8?
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型的推荐周期数是多少，我如何在YOLOv8中设置它？
- en: The number of epochs refers to the complete passes through the training dataset
    during model training. A typical starting point is 300 epochs. If your model overfits
    early, you can reduce the number. Alternatively, if overfitting isn't observed,
    you might extend training to 600, 1200, or more epochs. To set this in YOLOv8,
    use the `epochs` parameter in your training script. For additional advice on determining
    the ideal number of epochs, refer to this section on number of epochs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 周期数指的是模型训练过程中完全通过训练数据集的次数。一个典型的起始点是300个周期。如果您的模型早期出现过拟合，可以减少周期数。或者，如果没有观察到过拟合，可以将训练延长至600、1200或更多个周期。要在YOLOv8中设置这一参数，使用您的训练脚本中的`epochs`参数。有关确定理想周期数的额外建议，请参考关于周期数的这一部分。
