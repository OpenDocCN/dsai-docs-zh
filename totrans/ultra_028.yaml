- en: Fast Segment Anything Model (FastSAM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/models/fast-sam/`](https://docs.ultralytics.com/models/fast-sam/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Fast Segment Anything Model (FastSAM) is a novel, real-time CNN-based solution
    for the Segment Anything task. This task is designed to segment any object within
    an image based on various possible user interaction prompts. FastSAM significantly
    reduces computational demands while maintaining competitive performance, making
    it a practical choice for a variety of vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/F7db-EHhxss`](https://www.youtube.com/embed/F7db-EHhxss)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** Object Tracking using FastSAM with Ultralytics'
  prefs: []
  type: TYPE_NORMAL
- en: Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Fast Segment Anything Model (FastSAM) architecture overview](img/d6a00a050d052df768636ead7f971185.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FastSAM is designed to address the limitations of the Segment Anything Model
    (SAM), a heavy Transformer model with substantial computational resource requirements.
    The FastSAM decouples the segment anything task into two sequential stages: all-instance
    segmentation and prompt-guided selection. The first stage uses YOLOv8-seg to produce
    the segmentation masks of all instances in the image. In the second stage, it
    outputs the region-of-interest corresponding to the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Key Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Real-time Solution:** By leveraging the computational efficiency of CNNs,
    FastSAM provides a real-time solution for the segment anything task, making it
    valuable for industrial applications that require quick results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficiency and Performance:** FastSAM offers a significant reduction in computational
    and resource demands without compromising on performance quality. It achieves
    comparable performance to SAM but with drastically reduced computational resources,
    enabling real-time application.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prompt-guided Segmentation:** FastSAM can segment any object within an image
    guided by various possible user interaction prompts, providing flexibility and
    adaptability in different scenarios.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Based on YOLOv8-seg:** FastSAM is based on YOLOv8-seg, an object detector
    equipped with an instance segmentation branch. This allows it to effectively produce
    the segmentation masks of all instances in an image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Competitive Results on Benchmarks:** On the object proposal task on MS COCO,
    FastSAM achieves high scores at a significantly faster speed than SAM on a single
    NVIDIA RTX 3090, demonstrating its efficiency and capability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Practical Applications:** The proposed approach provides a new, practical
    solution for a large number of vision tasks at a really high speed, tens or hundreds
    of times faster than current methods.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Compression Feasibility:** FastSAM demonstrates the feasibility of
    a path that can significantly reduce the computational effort by introducing an
    artificial prior to the structure, thus opening new possibilities for large model
    architecture for general vision tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Available Models, Supported Tasks, and Operating Modes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This table presents the available models with their specific pre-trained weights,
    the tasks they support, and their compatibility with different operating modes
    like Inference, Validation, Training, and Export, indicated by ✅ emojis for supported
    modes and ❌ emojis for unsupported modes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FastSAM-s | [FastSAM-s.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/FastSAM-s.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| FastSAM-x | [FastSAM-x.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/FastSAM-x.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: Usage Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The FastSAM models are easy to integrate into your Python applications. Ultralytics
    provides user-friendly Python API and CLI commands to streamline development.
  prefs: []
  type: TYPE_NORMAL
- en: Predict Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To perform object detection on an image, use the `predict` method as shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This snippet demonstrates the simplicity of loading a pre-trained model and
    running a prediction on an image.
  prefs: []
  type: TYPE_NORMAL
- en: FastSAMPredictor example
  prefs: []
  type: TYPE_NORMAL
- en: This way you can run inference on image and get all the segment `results` once
    and run prompts inference multiple times without running inference multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All the returned `results` in above examples are Results object which allows
    access predicted masks and source image easily.
  prefs: []
  type: TYPE_NORMAL
- en: Val Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Validation of the model on a dataset can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Please note that FastSAM only supports detection and segmentation of a single
    class of object. This means it will recognize and segment all objects as the same
    class. Therefore, when preparing the dataset, you need to convert all object category
    IDs to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Track Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To perform object tracking on an image, use the `track` method as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: FastSAM official Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FastSAM is also available directly from the [`github.com/CASIA-IVA-Lab/FastSAM`](https://github.com/CASIA-IVA-Lab/FastSAM)
    repository. Here is a brief overview of the typical steps you might take to use
    FastSAM:'
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Clone the FastSAM repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create and activate a Conda environment with Python 3.9:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Navigate to the cloned repository and install the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the CLIP model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Example Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Download a [model checkpoint](https://drive.google.com/file/d/1m1sjY4ihXBU1fZXdQ-Xdj-mDltW-2Rqv/view?usp=sharing).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use FastSAM for inference. Example commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Segment everything in an image:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Segment specific objects using text prompt:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Segment objects within a bounding box (provide box coordinates in xywh format):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Segment objects near specific points:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Additionally, you can try FastSAM through a [Colab demo](https://colab.research.google.com/drive/1oX14f6IneGGw612WgVlAiy91UHwFAvr9?usp=sharing)
    or on the [HuggingFace web demo](https://huggingface.co/spaces/An-619/FastSAM)
    for a visual experience.
  prefs: []
  type: TYPE_NORMAL
- en: Citations and Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like to acknowledge the FastSAM authors for their significant contributions
    in the field of real-time instance segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The original FastSAM paper can be found on [arXiv](https://arxiv.org/abs/2306.12156).
    The authors have made their work publicly available, and the codebase can be accessed
    on [GitHub](https://github.com/CASIA-IVA-Lab/FastSAM). We appreciate their efforts
    in advancing the field and making their work accessible to the broader community.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is FastSAM and how does it differ from SAM?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FastSAM, short for Fast Segment Anything Model, is a real-time convolutional
    neural network (CNN)-based solution designed to reduce computational demands while
    maintaining high performance in object segmentation tasks. Unlike the Segment
    Anything Model (SAM), which uses a heavier Transformer-based architecture, FastSAM
    leverages Ultralytics YOLOv8-seg for efficient instance segmentation in two stages:
    all-instance segmentation followed by prompt-guided selection.'
  prefs: []
  type: TYPE_NORMAL
- en: How does FastSAM achieve real-time segmentation performance?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FastSAM achieves real-time segmentation by decoupling the segmentation task
    into all-instance segmentation with YOLOv8-seg and prompt-guided selection stages.
    By utilizing the computational efficiency of CNNs, FastSAM offers significant
    reductions in computational and resource demands while maintaining competitive
    performance. This dual-stage approach enables FastSAM to deliver fast and efficient
    segmentation suitable for applications requiring quick results.
  prefs: []
  type: TYPE_NORMAL
- en: What are the practical applications of FastSAM?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FastSAM is practical for a variety of computer vision tasks that require real-time
    segmentation performance. Applications include:'
  prefs: []
  type: TYPE_NORMAL
- en: Industrial automation for quality control and assurance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time video analysis for security and surveillance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autonomous vehicles for object detection and segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical imaging for precise and quick segmentation tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its ability to handle various user interaction prompts makes FastSAM adaptable
    and flexible for diverse scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: How do I use the FastSAM model for inference in Python?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To use FastSAM for inference in Python, you can follow the example below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For more details on inference methods, check the Predict Usage section of the
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: What types of prompts does FastSAM support for segmentation tasks?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FastSAM supports multiple prompt types for guiding the segmentation tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Everything Prompt**: Generates segmentation for all visible objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bounding Box (BBox) Prompt**: Segments objects within a specified bounding
    box.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text Prompt**: Uses a descriptive text to segment objects matching the description.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Point Prompt**: Segments objects near specific user-defined points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This flexibility allows FastSAM to adapt to a wide range of user interaction
    scenarios, enhancing its utility across different applications. For more information
    on using these prompts, refer to the Key Features section.
  prefs: []
  type: TYPE_NORMAL
