# `scipy.optimize.fmin_cg`

> 原文链接：[`docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.fmin_cg.html#scipy.optimize.fmin_cg`](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.fmin_cg.html#scipy.optimize.fmin_cg)

```py
scipy.optimize.fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None, c1=0.0001, c2=0.4)
```

使用非线性共轭梯度算法最小化函数。

参数：

**f**callable，`f(x, *args)`

要最小化的目标函数。这里*x*必须是要在搜索最小值时更改的变量的 1-D 数组，*args*是*f*的其他（固定）参数。

**x0**ndarray

*xopt*的用户提供的初始估计值，即*x*的最优值。必须是值的 1-D 数组。

**fprime**callable，`fprime(x, *args)`，可选

返回*f*在*x*处的梯度的函数。这里*x*和*args*如上所述为*f*。返回的值必须是 1-D 数组。默认为 None，此时数值上近似梯度（见下面的*epsilon*）。

**args**tuple，可选

传递给*f*和*fprime*的参数值。每当需要额外的固定参数完全指定*f*和*fprime*函数时，必须提供。

**gtol**float，可选

当梯度的范数小于*gtol*时停止。

**norm**float，可选

用于梯度范数的顺序（`-np.inf`是最小值，`np.inf`是最大值）。

**epsilon**float 或 ndarray，可选

当*fprime*被数值近似时使用的步长。可以是标量或 1-D 数组。默认为`sqrt(eps)`，其中 eps 是浮点数机器精度。通常`sqrt(eps)`约为 1.5e-8。

**maxiter**int，可选

要执行的最大迭代次数。默认为`200 * len(x0)`。

**full_output**bool，可选

如果为 True，则除了*xopt*之外，还返回*fopt*、*func_calls*、*grad_calls*和*warnflag*。有关可选返回值的详细信息，请参见下面的 Returns 部分。

**disp**bool，可选

如果为 True，则返回一个收敛消息，然后是*xopt*。

**retall**bool，可选

如果为 True，则将每次迭代的结果添加到返回值中。

**callback**callable，可选

一个可选的用户提供的函数，在每次迭代后调用。以`callback(xk)`的形式调用，其中`xk`是*x0*的当前值。

**c1**float，默认值：1e-4

Armijo 条件规则的参数。

**c2**float，默认值：0.4

曲率条件规则的参数。

返回：

**xopt**ndarray

最小化 f 的参数，即`f(xopt) == fopt`。

**fopt**float，可选

找到的最小值，f(xopt)。仅当*full_output*为 True 时返回。

**func_calls**int，可选

进行的函数调用次数。仅当*full_output*为 True 时返回。

**grad_calls**int，可选

进行的梯度调用次数。仅当*full_output*为 True 时返回。

**warnflag**int，可选

警告状态的整数值，仅当*full_output*为 True 时返回。

0：成功。

1：超过了最大迭代次数。

2：梯度和/或函数调用未更改。可能表示

即精度丢失，即例程未收敛。

3：遇到 NaN 结果。

**allvecs** 是一个 ndarray 的列表，可选

数组列表，包含每次迭代的结果。仅在 *retall* 为 True 时返回。

另请参见

`minimize`

`scipy.optimize` 的所有算法，无论是多变量函数的无约束还是有约束最小化，都有一个共同的接口。它提供了一种通过指定 `method='CG'` 来调用 `fmin_cg` 的替代方式。

注意事项

此共轭梯度算法基于 Polak 和 Ribiere 的算法 [[1]](#r675e71ddb23e-1)。

共轭梯度方法在以下情况下表现更好：

1.  *f* 有一个唯一的全局最小点，并且没有局部最小值或其他静止点，

1.  *f* 至少在局部范围内可以被变量的二次函数合理逼近，

1.  *f* 是连续的，并且具有连续的梯度，

1.  *fprime* 不应过大，例如其范数应小于 1000，

1.  初始猜测 *x0* 应该足够接近 *f* 的全局最小点 *xopt*。

参数 *c1* 和 *c2* 必须满足 `0 < c1 < c2 < 1`。

参考文献

[1]

Wright & Nocedal，《数值优化》，1999 年，第 120-122 页。

示例

示例 1：寻找给定参数值和初始猜测 `(u, v) = (0, 0)` 下表达式 `a*u**2 + b*u*v + c*v**2 + d*u + e*v + f` 的最小值。

```py
>>> import numpy as np
>>> args = (2, 3, 7, 8, 9, 10)  # parameter values
>>> def f(x, *args):
...     u, v = x
...     a, b, c, d, e, f = args
...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f
>>> def gradf(x, *args):
...     u, v = x
...     a, b, c, d, e, f = args
...     gu = 2*a*u + b*v + d     # u-component of the gradient
...     gv = b*u + 2*c*v + e     # v-component of the gradient
...     return np.asarray((gu, gv))
>>> x0 = np.asarray((0, 0))  # Initial guess.
>>> from scipy import optimize
>>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)
Optimization terminated successfully.
 Current function value: 1.617021
 Iterations: 4
 Function evaluations: 8
 Gradient evaluations: 8
>>> res1
array([-1.80851064, -0.25531915]) 
```

示例 2：使用 `minimize` 函数解决相同问题。（*myopts* 字典显示所有可用选项，实际应用中通常只需要非默认值。返回值将是一个字典。）

```py
>>> opts = {'maxiter' : None,    # default value.
...         'disp' : True,    # non-default value.
...         'gtol' : 1e-5,    # default value.
...         'norm' : np.inf,  # default value.
...         'eps' : 1.4901161193847656e-08}  # default value.
>>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,
...                          method='CG', options=opts)
Optimization terminated successfully.
 Current function value: 1.617021
 Iterations: 4
 Function evaluations: 8
 Gradient evaluations: 8
>>> res2.x  # minimum found
array([-1.80851064, -0.25531915]) 
```
