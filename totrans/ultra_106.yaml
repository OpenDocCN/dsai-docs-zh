- en: Ultralytics YOLOv8 on NVIDIA Jetson using DeepStream SDK and TensorRT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/guides/deepstream-nvidia-jetson/`](https://docs.ultralytics.com/guides/deepstream-nvidia-jetson/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This comprehensive guide provides a detailed walkthrough for deploying Ultralytics
    YOLOv8 on [NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)
    devices using DeepStream SDK and TensorRT. Here we use TensorRT to maximize the
    inference performance on the Jetson platform.
  prefs: []
  type: TYPE_NORMAL
- en: '![DeepStream on NVIDIA Jetson](img/f66f632ba33d1fe10163a54c15db50ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This guide has been tested with both [Seeed Studio reComputer J4012](https://www.seeedstudio.com/reComputer-J4012-p-5586.html)
    which is based on NVIDIA Jetson Orin NX 16GB running JetPack release of [JP5.1.3](https://developer.nvidia.com/embedded/jetpack-sdk-513)
    and [Seeed Studio reComputer J1020 v2](https://www.seeedstudio.com/reComputer-J1020-v2-p-5498.html)
    which is based on NVIDIA Jetson Nano 4GB running JetPack release of [JP4.6.4](https://developer.nvidia.com/jetpack-sdk-464).
    It is expected to work across all the NVIDIA Jetson hardware lineup including
    latest and legacy.
  prefs: []
  type: TYPE_NORMAL
- en: What is NVIDIA DeepStream?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[NVIDIA''s DeepStream SDK](https://developer.nvidia.com/deepstream-sdk) is
    a complete streaming analytics toolkit based on GStreamer for AI-based multi-sensor
    processing, video, audio, and image understanding. It''s ideal for vision AI developers,
    software partners, startups, and OEMs building IVA (Intelligent Video Analytics)
    apps and services. You can now create stream-processing pipelines that incorporate
    neural networks and other complex processing tasks like tracking, video encoding/decoding,
    and video rendering. These pipelines enable real-time analytics on video, image,
    and sensor data. DeepStream''s multi-platform support gives you a faster, easier
    way to develop vision AI applications and services on-premise, at the edge, and
    in the cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you start to follow this guide:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit our documentation, Quick Start Guide: NVIDIA Jetson with Ultralytics
    YOLOv8 to set up your NVIDIA Jetson device with Ultralytics YOLOv8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install [DeepStream SDK](https://developer.nvidia.com/deepstream-getting-started)
    according to the JetPack version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For JetPack 4.6.4, install [DeepStream 6.0.1](https://docs.nvidia.com/metropolis/deepstream/6.0.1/dev-guide/text/DS_Quickstart.html)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For JetPack 5.1.3, install [DeepStream 6.3](https://docs.nvidia.com/metropolis/deepstream/6.3/dev-guide/text/DS_Quickstart.html)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In this guide we have used the Debian package method of installing DeepStream
    SDK to the Jetson device. You can also visit the [DeepStream SDK on Jetson (Archived)](https://developer.nvidia.com/embedded/deepstream-on-jetson-downloads-archived)
    to access legacy versions of DeepStream.
  prefs: []
  type: TYPE_NORMAL
- en: DeepStream Configuration for YOLOv8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we are using [marcoslucianops/DeepStream-Yolo](https://github.com/marcoslucianops/DeepStream-Yolo)
    GitHub repository which includes NVIDIA DeepStream SDK support for YOLO models.
    We appreciate the efforts of marcoslucianops for his contributions!
  prefs: []
  type: TYPE_NORMAL
- en: Install dependencies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Clone the following repository
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Download Ultralytics YOLOv8 detection model (.pt) of your choice from [YOLOv8
    releases](https://github.com/ultralytics/assets/releases). Here we use [yolov8s.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s.pt).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also use a [custom trained YOLOv8 model](https://docs.ultralytics.com/modes/train/).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Convert model to ONNX
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Pass the below arguments to the above command
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For DeepStream 6.0.1, use opset 12 or lower. The default opset is 16.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To change the inference size (default: 640)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Example for 1280:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To simplify the ONNX model (DeepStream >= 6.0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To use dynamic batch-size (DeepStream >= 6.1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To use static batch-size (example for batch-size = 4)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set the CUDA version according to the JetPack version installed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For JetPack 4.6.4:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For JetPack 5.1.3:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Compile the library
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Edit the `config_infer_primary_yoloV8.txt` file according to your model (for
    YOLOv8s with 80 classes)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Edit the `deepstream_app_config` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can also change the video source in `deepstream_app_config` file. Here a
    default video file is loaded
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It will take a long time to generate the TensorRT engine file before starting
    the inference. So please be patient.
  prefs: []
  type: TYPE_NORMAL
- en: '![YOLOv8 with deepstream](img/9f70e5b78ecc1d90ec84e7ede95712e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If you want to convert the model to FP16 precision, simply set `model-engine-file=model_b1_gpu0_fp16.engine`
    and `network-mode=2` inside `config_infer_primary_yoloV8.txt`
  prefs: []
  type: TYPE_NORMAL
- en: INT8 Calibration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to use INT8 precision for inference, you need to follow the steps
    below
  prefs: []
  type: TYPE_NORMAL
- en: Set `OPENCV` environment variable
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Compile the library
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For COCO dataset, download the [val2017](http://images.cocodataset.org/zips/val2017.zip),
    extract, and move to `DeepStream-Yolo` folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a new directory for calibration images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the following to select 1000 random images from COCO dataset to run calibration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NVIDIA recommends at least 500 images to get a good accuracy. On this example,
    1000 images are chosen to get better accuracy (more images = more accuracy). You
    can set it from **head -1000**. For example, for 2000 images, **head -2000**.
    This process can take a long time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create the `calibration.txt` file with all selected images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set environment variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Higher INT8_CALIB_BATCH_SIZE values will result in more accuracy and faster
    calibration speed. Set it according to you GPU memory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Update the `config_infer_primary_yoloV8.txt` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: MultiStream Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To set up multiple streams under a single deepstream application, you can do
    the following changes to the `deepstream_app_config.txt` file
  prefs: []
  type: TYPE_NORMAL
- en: Change the rows and columns to build a grid display according to the number
    of streams you want to have. For example, for 4 streams, we can add 2 rows and
    2 columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set `num-sources=4` and add `uri` of all the 4 streams
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Multistream setup](img/0392649c7f1d05b4d620d143951d4cf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Benchmark Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following table summarizes how YOLOv8s models perform at different TensorRT
    precision levels with an input size of 640x640 on NVIDIA Jetson Orin NX 16GB.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Name | Precision | Inference Time (ms/im) | FPS |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8s | FP32 | 15.63 | 64 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FP16 | 7.94 | 126 |'
  prefs: []
  type: TYPE_TB
- en: '|  | INT8 | 5.53 | 181 |'
  prefs: []
  type: TYPE_TB
- en: Acknowledgements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This guide was initially created by our friends at Seeed Studio, Lakshantha
    and Elaine.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do I set up Ultralytics YOLOv8 on an NVIDIA Jetson device?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To set up Ultralytics YOLOv8 on an [NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)
    device, you first need to install the [DeepStream SDK](https://developer.nvidia.com/deepstream-getting-started)
    compatible with your JetPack version. Follow the step-by-step guide in our Quick
    Start Guide to configure your NVIDIA Jetson for YOLOv8 deployment.
  prefs: []
  type: TYPE_NORMAL
- en: What is the benefit of using TensorRT with YOLOv8 on NVIDIA Jetson?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using TensorRT with YOLOv8 optimizes the model for inference, significantly
    reducing latency and improving throughput on NVIDIA Jetson devices. TensorRT provides
    high-performance, low-latency deep learning inference through layer fusion, precision
    calibration, and kernel auto-tuning. This leads to faster and more efficient execution,
    particularly useful for real-time applications like video analytics and autonomous
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: Can I run Ultralytics YOLOv8 with DeepStream SDK across different NVIDIA Jetson
    hardware?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Yes, the guide for deploying Ultralytics YOLOv8 with the DeepStream SDK and
    TensorRT is compatible across the entire NVIDIA Jetson lineup. This includes devices
    like the Jetson Orin NX 16GB with [JetPack 5.1.3](https://developer.nvidia.com/embedded/jetpack-sdk-513)
    and the Jetson Nano 4GB with [JetPack 4.6.4](https://developer.nvidia.com/jetpack-sdk-464).
    Refer to the section DeepStream Configuration for YOLOv8 for detailed steps.
  prefs: []
  type: TYPE_NORMAL
- en: How can I convert a YOLOv8 model to ONNX for DeepStream?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To convert a YOLOv8 model to ONNX format for deployment with DeepStream, use
    the `utils/export_yoloV8.py` script from the [DeepStream-Yolo](https://github.com/marcoslucianops/DeepStream-Yolo)
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: For more details on model conversion, check out our model export section.
  prefs: []
  type: TYPE_NORMAL
- en: What are the performance benchmarks for YOLOv8 on NVIDIA Jetson Orin NX?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The performance of YOLOv8 models on NVIDIA Jetson Orin NX 16GB varies based
    on TensorRT precision levels. For example, YOLOv8s models achieve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FP32 Precision**: 15.63 ms/im, 64 FPS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP16 Precision**: 7.94 ms/im, 126 FPS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**INT8 Precision**: 5.53 ms/im, 181 FPS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These benchmarks underscore the efficiency and capability of using TensorRT-optimized
    YOLOv8 models on NVIDIA Jetson hardware. For further details, see our Benchmark
    Results section.
  prefs: []
  type: TYPE_NORMAL
