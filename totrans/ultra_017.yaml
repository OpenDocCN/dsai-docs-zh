- en: YOLOv3, YOLOv3-Ultralytics, and YOLOv3u
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/models/yolov3/`](https://docs.ultralytics.com/models/yolov3/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This document presents an overview of three closely related object detection
    models, namely [YOLOv3](https://pjreddie.com/darknet/yolo/), [YOLOv3-Ultralytics](https://github.com/ultralytics/yolov3),
    and [YOLOv3u](https://github.com/ultralytics/ultralytics).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '**YOLOv3:** This is the third version of the You Only Look Once (YOLO) object
    detection algorithm. Originally developed by Joseph Redmon, YOLOv3 improved on
    its predecessors by introducing features such as multiscale predictions and three
    different sizes of detection kernels.'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**YOLOv3-Ultralytics:** This is Ultralytics'' implementation of the YOLOv3
    model. It reproduces the original YOLOv3 architecture and offers additional functionalities,
    such as support for more pre-trained models and easier customization options.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**YOLOv3u:** This is an updated version of YOLOv3-Ultralytics that incorporates
    the anchor-free, objectness-free split head used in YOLOv8 models. YOLOv3u maintains
    the same backbone and neck architecture as YOLOv3 but with the updated detection
    head from YOLOv8.'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Ultralytics YOLOv3](img/f50df2eb05ecc42b4900c27d1abb4812.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Key Features
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**YOLOv3:** Introduced the use of three different scales for detection, leveraging
    three different sizes of detection kernels: 13x13, 26x26, and 52x52\. This significantly
    improved detection accuracy for objects of different sizes. Additionally, YOLOv3
    added features such as multi-label predictions for each bounding box and a better
    feature extractor network.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YOLOv3-Ultralytics:** Ultralytics'' implementation of YOLOv3 provides the
    same performance as the original model but comes with added support for more pre-trained
    models, additional training methods, and easier customization options. This makes
    it more versatile and user-friendly for practical applications.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YOLOv3u:** This updated model incorporates the anchor-free, objectness-free
    split head from YOLOv8\. By eliminating the need for pre-defined anchor boxes
    and objectness scores, this detection head design can improve the model''s ability
    to detect objects of varying sizes and shapes. This makes YOLOv3u more robust
    and accurate for object detection tasks.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported Tasks and Modes
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The YOLOv3 series, including YOLOv3, YOLOv3-Ultralytics, and YOLOv3u, are designed
    specifically for object detection tasks. These models are renowned for their effectiveness
    in various real-world scenarios, balancing accuracy and speed. Each variant offers
    unique features and optimizations, making them suitable for a range of applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: All three models support a comprehensive set of modes, ensuring versatility
    in various stages of model deployment and development. These modes include Inference,
    Validation, Training, and Export, providing users with a complete toolkit for
    effective object detection.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Type | Tasks Supported | Inference | Validation | Training | Export
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
- en: '| YOLOv3 | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
- en: '| YOLOv3-Ultralytics | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| YOLOv3u | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: This table provides an at-a-glance view of the capabilities of each YOLOv3 variant,
    highlighting their versatility and suitability for various tasks and operational
    modes in object detection workflows.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Usage Examples
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This example provides simple YOLOv3 training and inference examples. For full
    documentation on these and other modes see the Predict, Train, Val and Export
    docs pages.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Example
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch pretrained `*.pt` models as well as configuration `*.yaml` files can
    be passed to the `YOLO()` class to create a model instance in python:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'CLI commands are available to directly run the models:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Citations and Acknowledgements
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you use YOLOv3 in your research, please cite the original YOLO papers and
    the Ultralytics YOLOv3 repository:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Thank you to Joseph Redmon and Ali Farhadi for developing the original YOLOv3.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What are the differences between YOLOv3, YOLOv3-Ultralytics, and YOLOv3u?
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: YOLOv3 is the third iteration of the YOLO (You Only Look Once) object detection
    algorithm developed by Joseph Redmon, known for its balance of accuracy and speed,
    utilizing three different scales (13x13, 26x26, and 52x52) for detections. YOLOv3-Ultralytics
    is Ultralytics' adaptation of YOLOv3 that adds support for more pre-trained models
    and facilitates easier model customization. YOLOv3u is an upgraded variant of
    YOLOv3-Ultralytics, integrating the anchor-free, objectness-free split head from
    YOLOv8, improving detection robustness and accuracy for various object sizes.
    For more details on the variants, refer to the [YOLOv3 series](https://github.com/ultralytics/yolov3).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: How can I train a YOLOv3 model using Ultralytics?
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training a YOLOv3 model with Ultralytics is straightforward. You can train
    the model using either Python or CLI:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Example
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For more comprehensive training options and guidelines, visit our Train mode
    documentation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: What makes YOLOv3u more accurate for object detection tasks?
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: YOLOv3u improves upon YOLOv3 and YOLOv3-Ultralytics by incorporating the anchor-free,
    objectness-free split head used in YOLOv8 models. This upgrade eliminates the
    need for pre-defined anchor boxes and objectness scores, enhancing its capability
    to detect objects of varying sizes and shapes more precisely. This makes YOLOv3u
    a better choice for complex and diverse object detection tasks. For more information,
    refer to the Why YOLOv3u section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: How can I use YOLOv3 models for inference?
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can perform inference using YOLOv3 models by either Python scripts or CLI
    commands:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Example
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Refer to the Inference mode documentation for more details on running YOLO models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: What tasks are supported by YOLOv3 and its variants?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: YOLOv3, YOLOv3-Ultralytics, and YOLOv3u primarily support object detection tasks.
    These models can be used for various stages of model deployment and development,
    such as Inference, Validation, Training, and Export. For a comprehensive set of
    tasks supported and more in-depth details, visit our Object Detection tasks documentation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv3、YOLOv3-Ultralytics和YOLOv3u主要支持目标检测任务。这些模型可用于模型部署和开发的各个阶段，例如推断、验证、训练和导出。有关支持的全面任务集合和更深入的详细信息，请访问我们的目标检测任务文档。
- en: Where can I find resources to cite YOLOv3 in my research?
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我在哪里可以找到引用YOLOv3在我的研究中所需的资源？
- en: 'If you use YOLOv3 in your research, please cite the original YOLO papers and
    the Ultralytics YOLOv3 repository. Example BibTeX citation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在研究中使用了YOLOv3，请引用原始的YOLO论文和Ultralytics YOLOv3代码库。示例BibTeX引用：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For more citation details, refer to the Citations and Acknowledgements section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多引用详细信息，请参阅引文和致谢部分。
