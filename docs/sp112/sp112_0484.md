# `scipy.optimize.fmin_tnc`

> 原文链接：[`docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.fmin_tnc.html#scipy.optimize.fmin_tnc`](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.fmin_tnc.html#scipy.optimize.fmin_tnc)

```py
scipy.optimize.fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)
```

使用截断牛顿算法中的梯度信息最小化受限变量的函数。此方法包装了算法的 C 实现。

参数：

**func**callable `func(x, *args)`

要最小化的函数。必须执行以下操作之一：

1.  返回 f 和 g，其中 f 是函数的值，g 是其梯度（一个浮点数列表）。

1.  返回函数值，但单独提供梯度函数作为 *fprime*。

1.  返回函数值并设置 `approx_grad=True`。

如果函数返回 None，则最小化过程中止。

**x0**array_like

最小值的初始估计。

**fprime**callable `fprime(x, *args)`，可选

*func* 的梯度。如果为 None，则 *func* 必须返回函数值和梯度 (`f,g = func(x, *args)`)，或者 *approx_grad* 必须为 True。

**args**元组，可选

传递给函数的参数。

**approx_grad**布尔值，可选

如果为真，则通过数值方法近似梯度。

**bounds**列表，可选

x0 中每个元素的（最小值，最大值）对，定义该参数的边界。当某个方向没有边界时，使用 None 或 +/-inf。

**epsilon**浮点数，可选

如果 approx_grad 为 True，则使用有限差分逼近中的步长。

**scale**array_like，可选

应用于每个变量的缩放因子。如果为 None，则对于区间边界变量，因子是上限-下限，对于其他变量，因子是 1+|x|。默认为 None。

**offset**array_like，可选

从每个变量中减去的值。如果为 None，则对于区间边界变量，偏移量为 (上限+下限)/2，对于其他变量，偏移量为 x。

**messages**整数，可选

位掩码用于选择在最小化过程中显示的消息，值在 MSGS 字典中定义。默认为 MGS_ALL。

**disp**整数，可选

消息的整数界面。0 = 无消息，5 = 所有消息

**maxCGit**整数，可选

每次主迭代中的 Hessian*vector 评估的最大次数。如果 maxCGit == 0，则选择的方向为 -gradient；如果 maxCGit < 0，则 maxCGit 被设置为 max(1,min(50,n/2))。默认为 -1。

**maxfun**整数，可选

最大函数评估次数。如果为 None，则 maxfun 被设置为 max(100, 10*len(x0))。默认为 None。请注意，由于通过数值微分评估梯度，此函数可能会违反限制。

**eta**浮点数，可选

线搜索的严重性。如果 < 0 或 > 1，则设置为 0.25。默认为 -1。

**stepmx**浮点数，可选

线搜索的最大步长。可能在调用过程中增加。如果太小，则设置为 10.0。默认为 0。

**accuracy**浮点数，可选

有限差分计算的相对精度。如果 <= 机器精度，则设置为 sqrt(机器精度)。默认为 0。

**fmin**浮点数，可选

最小函数值估计。默认为 0。

**ftol**浮点数，可选

在停止标准中的 f 值的精度目标。如果 ftol < 0.0，则将 ftol 设置为 0.0，默认为-1。

**xtol**float，可选

在停止标准中的 x 值的精度目标（应用 x 缩放因子后）。如果 xtol < 0.0，则将 xtol 设置为 sqrt(machine_precision)。默认为-1。

**pgtol**float，可选

在停止标准中的投影梯度值的精度目标（应用 x 缩放因子后）。如果 pgtol < 0.0，则将 pgtol 设置为 1e-2 * sqrt(accuracy)。不建议将其设置为 0.0。默认为-1。

**rescale**float，可选

触发 f 值重新缩放的使用的缩放因子（以 log10 为单位）。如果为 0，则在每次迭代时重新缩放。如果为大值，则永不重新缩放。如果< 0，则将 rescale 设置为 1.3。

**callback**callable，可选

在每次迭代后调用，作为 callback(xk)，其中 xk 为当前参数向量。

返回：

**x**ndarray

解决方案。

**nfeval**int

函数评估次数。

**rc**int

返回代码，请参见下文

亦可参见

`minimize`

多元函数最小化算法的接口。特别是请参见‘TNC’ *方法*。

注释

底层算法为截断牛顿法，也称为牛顿共轭梯度。该方法与 scipy.optimize.fmin_ncg 不同之处在于

1.  它包装了该算法的 C 实现

1.  它允许为每个变量设定上下界。

该算法通过确定下降方向来整合约束条件，就像在无约束的截断牛顿法中一样，但从不采取足以离开可行 x 空间的步长。该算法跟踪一组当前活动约束，并在计算最小允许步长时忽略它们。（与活动约束相关联的 x 被保持不变。）如果最大允许步长为零，则添加新约束。在每次迭代结束时，可能会被认为不再活动并删除一个约束。如果当前活动但变量梯度向内从约束点，那么约束被认为不再活动。具体删除的约束是与不再活动约束的最大索引变量相关联的约束。

返回代码如下定义：

```py
-1 : Infeasible (lower bound > upper bound)
 0 : Local minimum reached (|pg| ~= 0)
 1 : Converged (|f_n-f_(n-1)| ~= 0)
 2 : Converged (|x_n-x_(n-1)| ~= 0)
 3 : Max. number of function evaluations reached
 4 : Linear search failed
 5 : All lower bounds are equal to the upper bounds
 6 : Unable to progress
 7 : User requested end of minimization 
```

参考文献

Wright S., Nocedal J.（2006 年），‘Numerical Optimization’

Nash S.G.（1984 年），“通过 Lanczos 方法的牛顿型最小化”，SIAM 数值分析期刊 21，pp. 770-778
