# `scipy.optimize.line_search`

> 原文：[`docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.line_search.html#scipy.optimize.line_search`](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.line_search.html#scipy.optimize.line_search)

```py
scipy.optimize.line_search(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=None, extra_condition=None, maxiter=10)
```

找到满足强 Wolfe 条件的 alpha。

参数：

**f**callable f(x,*args)

目标函数。

**myfprime**callable f’(x,*args)

目标函数梯度。

**xk**ndarray

起始点。

**pk**ndarray

搜索方向。搜索方向必须是算法收敛的下降方向。

**gfk**ndarray，可选

x=xk 处的梯度值（xk 为当前参数估计）。如果省略，则将重新计算。

**old_fval**float，可选

x=xk 处的函数值。如果省略，则将重新计算。

**old_old_fval**float，可选

x=xk 之前点的函数值。

**args**tuple，可选

传递给目标函数的额外参数。

**c1**float，可选

Armijo 条件规则的参数。

**c2**float，可选

曲率条件规则的参数。

**amax**float，可选

最大步长

**extra_condition**callable，可选

形如 `extra_condition(alpha, x, f, g)` 的可调用对象，返回布尔值。参数是建议的步长 `alpha` 及其相应的 `x`、`f` 和 `g` 值。只有在满足强 Wolfe 条件的迭代中才接受 `alpha` 的值。如果步长的可调用对象返回假，则算法将继续进行新的迭代。只有在满足强 Wolfe 条件的迭代中才会调用该可调用对象。

**maxiter**int，可选

执行的最大迭代次数。

返回：

**alpha**float 或 None

Alpha 使得 `x_new = x0 + alpha * pk`，如果线搜索算法未收敛，则为 None。

**fc**int

执行的函数评估次数。

**gc**int

执行的梯度评估次数。

**new_fval**float 或 None

新函数值 `f(x_new)=f(x0+alpha*pk)`，如果线搜索算法未收敛，则为 None。

**old_fval**float

旧函数值 `f(x0)`。

**new_slope**float 或 None

在新值处沿搜索方向的局部斜率 `<myfprime(x_new), pk>`，如果线搜索算法未收敛，则为 None。

注意事项

使用线搜索算法来强制实施强 Wolfe 条件。参见 Wright 和 Nocedal，《Numerical Optimization》，1999 年，第 59-61 页。

搜索方向 *pk* 必须是下降方向（例如 `-myfprime(xk)`）以找到满足强 Wolfe 条件的步长。如果搜索方向不是下降方向（例如 `myfprime(xk)`），则 *alpha*、*new_fval* 和 *new_slope* 将为 None。

示例

```py
>>> import numpy as np
>>> from scipy.optimize import line_search 
```

定义了一个目标函数及其梯度。

```py
>>> def obj_func(x):
...     return (x[0])**2+(x[1])**2
>>> def obj_grad(x):
...     return [2*x[0], 2*x[1]] 
```

我们可以找到满足强 Wolfe 条件的 alpha。

```py
>>> start_point = np.array([1.8, 1.7])
>>> search_gradient = np.array([-1.0, -1.0])
>>> line_search(obj_func, obj_grad, start_point, search_gradient)
(1.0, 2, 1, 1.1300000000000001, 6.13, [1.6, 1.4]) 
```
