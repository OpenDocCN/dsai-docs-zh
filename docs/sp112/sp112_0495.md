# scipy.optimize.anderson

> 原文链接：[https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.anderson.html#scipy.optimize.anderson](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.anderson.html#scipy.optimize.anderson)

```py
scipy.optimize.anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)
```

使用（扩展）安德森混合查找函数的根。

雅可比矩阵由在最后*M*向量所张成的空间中为“最佳”解形成。因此，只需要进行MxM矩阵反演和MxN乘法。[[Ey]](#r00c808e12704-ey)

参数：

**F**function(x) -> f

要找到其根的函数；应接受并返回一个类似数组的对象。

**xin**array_like

解决方案的初始猜测

**alpha**float, 可选

雅可比矩阵的初始猜测为（-1/alpha）。

**M**float, 可选

要保留的先前向量数。默认为5。

**w0**float, 可选

用于数值稳定性的正则化参数。与单位相比，阶数为0.01的良好值。

**iter**int, 可选

要进行的迭代次数。如果省略（默认），则进行所需数量的迭代以满足容差。

**verbose**bool, 可选

在每次迭代时向标准输出打印状态。

**maxiter**int, 可选

最大迭代次数。如果需要更多迭代以达到收敛，将引发*NoConvergence*。

**f_tol**float, 可选

绝对残差（在最大范数中）。如果省略，默认值为6e-6。

**f_rtol**float, 可选

相对残差的容差。如果省略，则不使用。

**x_tol**float, 可选

绝对最小步长，由雅可比近似确定。如果步长小于此值，则优化被视为成功终止。如果省略，则不使用。

**x_rtol**float, 可选

相对最小步长。如果省略，则不使用。

**tol_norm**函数（向量）->标量，可选

在收敛检查中使用的范数。默认为最大范数。

**line_search**{None, ‘armijo’ (默认), ‘wolfe’}, 可选

用于确定由雅可比近似给定方向上的步长大小的线搜索类型。默认为“armijo”。

**callback**函数，可选

可选回调函数。每次迭代时调用`callback(x, f)`，其中*x*为当前解决方案，*f*为相应残差。

返回：

**sol**ndarray

包含最终解的类似*x0*的数组（相似的数组类型）。

引发：

未收敛

未找到解决方案时。

另见

[`root`](scipy.optimize.root.html#scipy.optimize.root "scipy.optimize.root")

多变量函数根查找算法的接口。特别查看`method='anderson'`。

参考文献

[[Ey](#id1)]

1.  Eyert，J. Comp. Phys.，124，271（1996）。

示例

以下函数定义了一个非线性方程组

```py
>>> def fun(x):
...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,
...             0.5 * (x[1] - x[0])**3 + x[1]] 
```

可以通过以下方法获得解决方案。

```py
>>> from scipy import optimize
>>> sol = optimize.anderson(fun, [0, 0])
>>> sol
array([0.84116588, 0.15883789]) 
```
