- en: YOLO-World Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/models/yolo-world/`](https://docs.ultralytics.com/models/yolo-world/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The YOLO-World Model introduces an advanced, real-time [Ultralytics](https://ultralytics.com)
    YOLOv8-based approach for Open-Vocabulary Detection tasks. This innovation enables
    the detection of any object within an image based on descriptive texts. By significantly
    lowering computational demands while preserving competitive performance, YOLO-World
    emerges as a versatile tool for numerous vision-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/cfTKj96TjSE`](https://www.youtube.com/embed/cfTKj96TjSE)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** YOLO World training workflow on custom dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '![YOLO-World Model architecture overview](img/87b4fbb7a6b841ad8f9e14443e030b03.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YOLO-World tackles the challenges faced by traditional Open-Vocabulary detection
    models, which often rely on cumbersome Transformer models requiring extensive
    computational resources. These models' dependence on pre-defined object categories
    also restricts their utility in dynamic scenarios. YOLO-World revitalizes the
    YOLOv8 framework with open-vocabulary detection capabilities, employing vision-language
    modeling and pre-training on expansive datasets to excel at identifying a broad
    array of objects in zero-shot scenarios with unmatched efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Key Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Real-time Solution:** Harnessing the computational speed of CNNs, YOLO-World
    delivers a swift open-vocabulary detection solution, catering to industries in
    need of immediate results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficiency and Performance:** YOLO-World slashes computational and resource
    requirements without sacrificing performance, offering a robust alternative to
    models like SAM but at a fraction of the computational cost, enabling real-time
    applications.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inference with Offline Vocabulary:** YOLO-World introduces a "prompt-then-detect"
    strategy, employing an offline vocabulary to enhance efficiency further. This
    approach enables the use of custom prompts computed apriori, including captions
    or categories, to be encoded and stored as offline vocabulary embeddings, streamlining
    the detection process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Powered by YOLOv8:** Built upon Ultralytics YOLOv8, YOLO-World leverages
    the latest advancements in real-time object detection to facilitate open-vocabulary
    detection with unparalleled accuracy and speed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Benchmark Excellence:** YOLO-World outperforms existing open-vocabulary detectors,
    including MDETR and GLIP series, in terms of speed and efficiency on standard
    benchmarks, showcasing YOLOv8''s superior capability on a single NVIDIA V100 GPU.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Versatile Applications:** YOLO-World''s innovative approach unlocks new possibilities
    for a multitude of vision tasks, delivering speed improvements by orders of magnitude
    over existing methods.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Available Models, Supported Tasks, and Operating Modes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section details the models available with their specific pre-trained weights,
    the tasks they support, and their compatibility with various operating modes such
    as Inference, Validation, Training, and Export, denoted by ✅ for supported modes
    and ❌ for unsupported modes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All the YOLOv8-World weights have been directly migrated from the official [YOLO-World](https://github.com/AILab-CVC/YOLO-World)
    repository, highlighting their excellent contributions.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8s-world | [yolov8s-world.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-world.pt)
    | Object Detection | ✅ | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8s-worldv2 | [yolov8s-worldv2.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-worldv2.pt)
    | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8m-world | [yolov8m-world.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-world.pt)
    | Object Detection | ✅ | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8m-worldv2 | [yolov8m-worldv2.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-worldv2.pt)
    | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8l-world | [yolov8l-world.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-world.pt)
    | Object Detection | ✅ | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8l-worldv2 | [yolov8l-worldv2.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-worldv2.pt)
    | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8x-world | [yolov8x-world.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-world.pt)
    | Object Detection | ✅ | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8x-worldv2 | [yolov8x-worldv2.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-worldv2.pt)
    | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: Zero-shot Transfer on COCO Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Model Type | mAP | mAP50 | mAP75 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| yolov8s-world | 37.4 | 52.0 | 40.6 |'
  prefs: []
  type: TYPE_TB
- en: '| yolov8s-worldv2 | 37.7 | 52.2 | 41.0 |'
  prefs: []
  type: TYPE_TB
- en: '| yolov8m-world | 42.0 | 57.0 | 45.6 |'
  prefs: []
  type: TYPE_TB
- en: '| yolov8m-worldv2 | 43.0 | 58.4 | 46.8 |'
  prefs: []
  type: TYPE_TB
- en: '| yolov8l-world | 45.7 | 61.3 | 49.8 |'
  prefs: []
  type: TYPE_TB
- en: '| yolov8l-worldv2 | 45.8 | 61.3 | 49.8 |'
  prefs: []
  type: TYPE_TB
- en: '| yolov8x-world | 47.0 | 63.0 | 51.2 |'
  prefs: []
  type: TYPE_TB
- en: '| yolov8x-worldv2 | 47.1 | 62.8 | 51.4 |'
  prefs: []
  type: TYPE_TB
- en: Usage Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The YOLO-World models are easy to integrate into your Python applications. Ultralytics
    provides user-friendly Python API and CLI commands to streamline development.
  prefs: []
  type: TYPE_NORMAL
- en: Train Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We strongly recommend to use `yolov8-worldv2` model for custom training, because
    it supports deterministic training and also easy to export other formats i.e onnx/tensorrt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Object detection is straightforward with the `train` method, as illustrated
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch pretrained `*.pt` models as well as configuration `*.yaml` files can
    be passed to the `YOLOWorld()` class to create a model instance in python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Predict Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Object detection is straightforward with the `predict` method, as illustrated
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This snippet demonstrates the simplicity of loading a pre-trained model and
    running a prediction on an image.
  prefs: []
  type: TYPE_NORMAL
- en: Val Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Model validation on a dataset is streamlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Track Usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Object tracking with YOLO-World model on a video/images is streamlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The YOLO-World models provided by Ultralytics come pre-configured with COCO
    dataset categories as part of their offline vocabulary, enhancing efficiency for
    immediate application. This integration allows the YOLOv8-World models to directly
    recognize and predict the 80 standard categories defined in the COCO dataset without
    requiring additional setup or customization.
  prefs: []
  type: TYPE_NORMAL
- en: Set prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![YOLO-World prompt class names overview](img/37d7ad91fe35a588f5424187c10a95ed.png)'
  prefs: []
  type: TYPE_IMG
- en: The YOLO-World framework allows for the dynamic specification of classes through
    custom prompts, empowering users to tailor the model to their specific needs **without
    retraining**. This feature is particularly useful for adapting the model to new
    domains or specific tasks that were not originally part of the training data.
    By setting custom prompts, users can essentially guide the model's focus towards
    objects of interest, enhancing the relevance and accuracy of the detection results.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if your application only requires detecting ''person'' and ''bus''
    objects, you can specify these classes directly:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also save a model after setting custom classes. By doing this you create
    a version of the YOLO-World model that is specialized for your specific use case.
    This process embeds your custom class definitions directly into the model file,
    making the model ready to use with your specified classes without further adjustments.
    Follow these steps to save and load your custom YOLOv8 model:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: 'First load a YOLO-World model, set custom classes for it and save it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After saving, the custom_yolov8s.pt model behaves like any other pre-trained
    YOLOv8 model but with a key difference: it is now optimized to detect only the
    classes you have defined. This customization can significantly improve detection
    performance and efficiency for your specific application scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Benefits of Saving with Custom Vocabulary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Efficiency**: Streamlines the detection process by focusing on relevant objects,
    reducing computational overhead and speeding up inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Allows for easy adaptation of the model to new or niche detection
    tasks without the need for extensive retraining or data collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplicity**: Simplifies deployment by eliminating the need to repeatedly
    specify custom classes at runtime, making the model directly usable with its embedded
    vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Enhances detection accuracy for specified classes by focusing
    the model''s attention and resources on recognizing the defined objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach provides a powerful means of customizing state-of-the-art object
    detection models for specific tasks, making advanced AI more accessible and applicable
    to a broader range of practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Reproduce official results from scratch(Experimental)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prepare datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Train data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Dataset | Type | Samples | Boxes | Annotation Files |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [Objects365v1](https://opendatalab.com/OpenDataLab/Objects365_v1) | Detection
    | 609k | 9621k | [objects365_train.json](https://opendatalab.com/OpenDataLab/Objects365_v1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [GQA](https://nlp.stanford.edu/data/gqa/images.zip) | Grounding | 621k |
    3681k | [final_mixed_train_no_coco.json](https://huggingface.co/GLIPModel/GLIP/blob/main/mdetr_annotations/final_mixed_train_no_coco.json)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/) | Grounding
    | 149k | 641k | [final_flickr_separateGT_train.json](https://huggingface.co/GLIPModel/GLIP/blob/main/mdetr_annotations/final_flickr_separateGT_train.json)
    |'
  prefs: []
  type: TYPE_TB
- en: Val data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Dataset | Type | Annotation Files |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [LVIS minival](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/lvis.yaml)
    | Detection | [minival.txt](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/lvis.yaml)
    |'
  prefs: []
  type: TYPE_TB
- en: Launch training from scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`WorldTrainerFromScratch` is highly customized to allow training yolo-world
    models on both detection datasets and grounding datasets simultaneously. More
    details please checkout [ultralytics.model.yolo.world.train_world.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/yolo/world/train_world.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Citations and Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We extend our gratitude to the [Tencent AILab Computer Vision Center](https://ai.tencent.com/)
    for their pioneering work in real-time open-vocabulary object detection with YOLO-World:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: For further reading, the original YOLO-World paper is available on [arXiv](https://arxiv.org/pdf/2401.17270v2.pdf).
    The project's source code and additional resources can be accessed via their [GitHub
    repository](https://github.com/AILab-CVC/YOLO-World). We appreciate their commitment
    to advancing the field and sharing their valuable insights with the community.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the YOLO-World model and how does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The YOLO-World model is an advanced, real-time object detection approach based
    on the Ultralytics YOLOv8 framework. It excels in Open-Vocabulary Detection tasks
    by identifying objects within an image based on descriptive texts. Using vision-language
    modeling and pre-training on large datasets, YOLO-World achieves high efficiency
    and performance with significantly reduced computational demands, making it ideal
    for real-time applications across various industries.
  prefs: []
  type: TYPE_NORMAL
- en: How does YOLO-World handle inference with custom prompts?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'YOLO-World supports a "prompt-then-detect" strategy, which utilizes an offline
    vocabulary to enhance efficiency. Custom prompts like captions or specific object
    categories are pre-encoded and stored as offline vocabulary embeddings. This approach
    streamlines the detection process without the need for retraining. You can dynamically
    set these prompts within the model to tailor it to specific detection tasks, as
    shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Why should I choose YOLO-World over traditional Open-Vocabulary detection models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'YOLO-World provides several advantages over traditional Open-Vocabulary detection
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-Time Performance:** It leverages the computational speed of CNNs to
    offer quick, efficient detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency and Low Resource Requirement:** YOLO-World maintains high performance
    while significantly reducing computational and resource demands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customizable Prompts:** The model supports dynamic prompt setting, allowing
    users to specify custom detection classes without retraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Benchmark Excellence:** It outperforms other open-vocabulary detectors like
    MDETR and GLIP in both speed and efficiency on standard benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I train a YOLO-World model on my dataset?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training a YOLO-World model on your dataset is straightforward through the
    provided Python API or CLI commands. Here''s how to start training using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Or using CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: What are the available pre-trained YOLO-World models and their supported tasks?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ultralytics offers multiple pre-trained YOLO-World models supporting various
    tasks and operating modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8s-world | [yolov8s-world.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-world.pt)
    | Object Detection | ✅ | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8s-worldv2 | [yolov8s-worldv2.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-worldv2.pt)
    | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8m-world | [yolov8m-world.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-world.pt)
    | Object Detection | ✅ | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8m-worldv2 | [yolov8m-worldv2.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-worldv2.pt)
    | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8l-world | [yolov8l-world.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-world.pt)
    | Object Detection | ✅ | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8l-worldv2 | [yolov8l-worldv2.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-worldv2.pt)
    | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8x-world | [yolov8x-world.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-world.pt)
    | Object Detection | ✅ | ✅ | ✅ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv8x-worldv2 | [yolov8x-worldv2.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-worldv2.pt)
    | Object Detection | ✅ | ✅ | ✅ | ✅ |'
  prefs: []
  type: TYPE_TB
- en: How do I reproduce the official results of YOLO-World from scratch?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To reproduce the official results from scratch, you need to prepare the datasets
    and launch the training using the provided code. The training procedure involves
    creating a data dictionary and running the `train` method with a custom trainer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
