["```py\n    `pip  install  tritonclient[all]` \n    ```", "```py\n`from ultralytics import YOLO  # Load a model model = YOLO(\"yolov8n.pt\")  # load an official model  # Export the model onnx_file = model.export(format=\"onnx\", dynamic=True)` \n```", "```py\n    `from pathlib import Path  # Define paths model_name = \"yolo\" triton_repo_path = Path(\"tmp\") / \"triton_repo\" triton_model_path = triton_repo_path / model_name  # Create directories (triton_model_path / \"1\").mkdir(parents=True, exist_ok=True)` \n    ```", "```py\n    `from pathlib import Path  # Move ONNX model to Triton Model path Path(onnx_file).rename(triton_model_path / \"1\" / \"model.onnx\")  # Create config file (triton_model_path / \"config.pbtxt\").touch()` \n    ```", "```py\n`import contextlib import subprocess import time  from tritonclient.http import InferenceServerClient  # Define image https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver tag = \"nvcr.io/nvidia/tritonserver:23.09-py3\"  # 6.4 GB  # Pull the image subprocess.call(f\"docker pull {tag}\", shell=True)  # Run the Triton server and capture the container ID container_id = (     subprocess.check_output(         f\"docker run -d --rm -v {triton_repo_path}:/models -p 8000:8000 {tag} tritonserver --model-repository=/models\",         shell=True,     )     .decode(\"utf-8\")     .strip() )  # Wait for the Triton server to start triton_client = InferenceServerClient(url=\"localhost:8000\", verbose=False, ssl=False)  # Wait until model is ready for _ in range(10):     with contextlib.suppress(Exception):         assert triton_client.is_model_ready(model_name)         break     time.sleep(1)` \n```", "```py\n`from ultralytics import YOLO  # Load the Triton Server model model = YOLO(\"http://localhost:8000/yolo\", task=\"detect\")  # Run inference on the server results = model(\"path/to/image.jpg\")` \n```", "```py\n`# Kill and remove the container at the end of the test subprocess.call(f\"docker kill {container_id}\", shell=True)` \n```", "```py\n    `from ultralytics import YOLO  # Load a model model = YOLO(\"yolov8n.pt\")  # load an official model  # Export the model to ONNX format onnx_file = model.export(format=\"onnx\", dynamic=True)` \n    ```", "```py\n    `from pathlib import Path  # Define paths model_name = \"yolo\" triton_repo_path = Path(\"tmp\") / \"triton_repo\" triton_model_path = triton_repo_path / model_name  # Create directories (triton_model_path / \"1\").mkdir(parents=True, exist_ok=True) Path(onnx_file).rename(triton_model_path / \"1\" / \"model.onnx\") (triton_model_path / \"config.pbtxt\").touch()` \n    ```", "```py\n    `import contextlib import subprocess import time  from tritonclient.http import InferenceServerClient  # Define image https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver tag = \"nvcr.io/nvidia/tritonserver:23.09-py3\"  subprocess.call(f\"docker pull {tag}\", shell=True)  container_id = (     subprocess.check_output(         f\"docker run -d --rm -v {triton_repo_path}/models -p 8000:8000 {tag} tritonserver --model-repository=/models\",         shell=True,     )     .decode(\"utf-8\")     .strip() )  triton_client = InferenceServerClient(url=\"localhost:8000\", verbose=False, ssl=False)  for _ in range(10):     with contextlib.suppress(Exception):         assert triton_client.is_model_ready(model_name)         break     time.sleep(1)` \n    ```", "```py\n`from ultralytics import YOLO  model = YOLO(\"yolov8n.pt\") onnx_file = model.export(format=\"onnx\", dynamic=True)` \n```", "```py\n`from ultralytics import YOLO  # Load the Triton Server model model = YOLO(\"http://localhost:8000/yolo\", task=\"detect\")  # Run inference on the server results = model(\"path/to/image.jpg\")` \n```"]