["```py\n`from ultralytics import FastSAM  # Define an inference source source = \"path/to/bus.jpg\"  # Create a FastSAM model model = FastSAM(\"FastSAM-s.pt\")  # or FastSAM-x.pt  # Run inference on an image everything_results = model(source, device=\"cpu\", retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)  # Run inference with bboxes prompt results = model(source, bboxes=[439, 437, 524, 709])  # Run inference with points prompt results = model(source, points=[[200, 200]], labels=[1])  # Run inference with texts prompt results = model(source, texts=\"a photo of a dog\")  # Run inference with bboxes and points and texts prompt at the same time results = model(source, bboxes=[439, 437, 524, 709], points=[[200, 200]], labels=[1], texts=\"a photo of a dog\")` \n```", "```py\n`# Load a FastSAM model and segment everything with it yolo  segment  predict  model=FastSAM-s.pt  source=path/to/bus.jpg  imgsz=640` \n```", "```py\n`from ultralytics.models.fastsam import FastSAMPredictor  # Create FastSAMPredictor overrides = dict(conf=0.25, task=\"segment\", mode=\"predict\", model=\"FastSAM-s.pt\", save=False, imgsz=1024) predictor = FastSAMPredictor(overrides=overrides)  # Segment everything everything_results = predictor(\"ultralytics/assets/bus.jpg\")  # Prompt inference bbox_results = predictor.prompt(everything_results, bboxes=[[200, 200, 300, 300]]) point_results = predictor.prompt(everything_results, points=[200, 200]) text_results = predictor.prompt(everything_results, texts=\"a photo of a dog\")` \n```", "```py\n`from ultralytics import FastSAM  # Create a FastSAM model model = FastSAM(\"FastSAM-s.pt\")  # or FastSAM-x.pt  # Validate the model results = model.val(data=\"coco8-seg.yaml\")` \n```", "```py\n`# Load a FastSAM model and validate it on the COCO8 example dataset at image size 640 yolo  segment  val  model=FastSAM-s.pt  data=coco8.yaml  imgsz=640` \n```", "```py\n`from ultralytics import FastSAM  # Create a FastSAM model model = FastSAM(\"FastSAM-s.pt\")  # or FastSAM-x.pt  # Track with a FastSAM model on a video results = model.track(source=\"path/to/video.mp4\", imgsz=640)` \n```", "```py\n`yolo  segment  track  model=FastSAM-s.pt  source=\"path/to/video/file.mp4\"  imgsz=640` \n```", "```py\n    `git  clone  https://github.com/CASIA-IVA-Lab/FastSAM.git` \n    ```", "```py\n    `conda  create  -n  FastSAM  python=3.9 conda  activate  FastSAM` \n    ```", "```py\n    `cd  FastSAM pip  install  -r  requirements.txt` \n    ```", "```py\n    `pip  install  git+https://github.com/ultralytics/CLIP.git` \n    ```", "```py\n        `python  Inference.py  --model_path  ./weights/FastSAM.pt  --img_path  ./images/dogs.jpg` \n        ```", "```py\n        `python  Inference.py  --model_path  ./weights/FastSAM.pt  --img_path  ./images/dogs.jpg  --text_prompt  \"the yellow dog\"` \n        ```", "```py\n        `python  Inference.py  --model_path  ./weights/FastSAM.pt  --img_path  ./images/dogs.jpg  --box_prompt  \"[570,200,230,400]\"` \n        ```", "```py\n        `python  Inference.py  --model_path  ./weights/FastSAM.pt  --img_path  ./images/dogs.jpg  --point_prompt  \"[[520,360],[620,300]]\"  --point_label  \"[1,0]\"` \n        ```", "```py\n`@misc{zhao2023fast,   title={Fast Segment Anything},   author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},   year={2023},   eprint={2306.12156},   archivePrefix={arXiv},   primaryClass={cs.CV} }` \n```", "```py\n`from ultralytics import FastSAM  # Define an inference source source = \"path/to/bus.jpg\"  # Create a FastSAM model model = FastSAM(\"FastSAM-s.pt\")  # or FastSAM-x.pt  # Run inference on an image everything_results = model(source, device=\"cpu\", retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)  # Run inference with bboxes prompt results = model(source, bboxes=[439, 437, 524, 709])  # Run inference with points prompt results = model(source, points=[[200, 200]], labels=[1])  # Run inference with texts prompt results = model(source, texts=\"a photo of a dog\")  # Run inference with bboxes and points and texts prompt at the same time results = model(source, bboxes=[439, 437, 524, 709], points=[[200, 200]], labels=[1], texts=\"a photo of a dog\")` \n```"]