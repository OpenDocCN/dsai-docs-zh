- en: pandas.DataFrame.to_parquet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Write a DataFrame to the binary parquet format.
  prefs: []
  type: TYPE_NORMAL
- en: This function writes the dataframe as a [parquet file](https://parquet.apache.org/).
    You can choose different parquet backends, and have the option of compression.
    See [the user guide](../../user_guide/io.html#io-parquet) for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**path**str, path object, file-like object, or None, default None'
  prefs: []
  type: TYPE_NORMAL
- en: String, path object (implementing `os.PathLike[str]`), or file-like object implementing
    a binary `write()` function. If None, the result is returned as bytes. If a string
    or path, it will be used as Root Directory path when writing a partitioned dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**engine**{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’'
  prefs: []
  type: TYPE_NORMAL
- en: Parquet library to use. If ‘auto’, then the option `io.parquet.engine` is used.
    The default `io.parquet.engine` behavior is to try ‘pyarrow’, falling back to
    ‘fastparquet’ if ‘pyarrow’ is unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: '**compression**str or None, default ‘snappy’'
  prefs: []
  type: TYPE_NORMAL
- en: 'Name of the compression to use. Use `None` for no compression. Supported options:
    ‘snappy’, ‘gzip’, ‘brotli’, ‘lz4’, ‘zstd’.'
  prefs: []
  type: TYPE_NORMAL
- en: '**index**bool, default None'
  prefs: []
  type: TYPE_NORMAL
- en: If `True`, include the dataframe’s index(es) in the file output. If `False`,
    they will not be written to the file. If `None`, similar to `True` the dataframe’s
    index(es) will be saved. However, instead of being saved as values, the RangeIndex
    will be stored as a range in the metadata so it doesn’t require much space and
    is faster. Other indexes will be included as columns in the file output.
  prefs: []
  type: TYPE_NORMAL
- en: '**partition_cols**list, optional, default None'
  prefs: []
  type: TYPE_NORMAL
- en: Column names by which to partition the dataset. Columns are partitioned in the
    order they are given. Must be None if path is not a string.
  prefs: []
  type: TYPE_NORMAL
- en: '**storage_options**dict, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Extra options that make sense for a particular storage connection, e.g. host,
    port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded
    to `urllib.request.Request` as header options. For other URLs (e.g. starting with
    “s3://”, and “gcs://”) the key-value pairs are forwarded to `fsspec.open`. Please
    see `fsspec` and `urllib` for more details, and for more examples on storage options
    refer [here](https://pandas.pydata.org/docs/user_guide/io.html?highlight=storage_options#reading-writing-remote-files).
  prefs: []
  type: TYPE_NORMAL
- en: '****kwargs**'
  prefs: []
  type: TYPE_NORMAL
- en: Additional arguments passed to the parquet library. See [pandas io](../../user_guide/io.html#io-parquet)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: bytes if no path argument is provided else None
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_parquet`](pandas.read_parquet.html#pandas.read_parquet "pandas.read_parquet")'
  prefs: []
  type: TYPE_NORMAL
- en: Read a parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[`DataFrame.to_orc`](pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc "pandas.DataFrame.to_orc")'
  prefs: []
  type: TYPE_NORMAL
- en: Write an orc file.
  prefs: []
  type: TYPE_NORMAL
- en: '[`DataFrame.to_csv`](pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv "pandas.DataFrame.to_csv")'
  prefs: []
  type: TYPE_NORMAL
- en: Write a csv file.
  prefs: []
  type: TYPE_NORMAL
- en: '[`DataFrame.to_sql`](pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql "pandas.DataFrame.to_sql")'
  prefs: []
  type: TYPE_NORMAL
- en: Write to a sql table.
  prefs: []
  type: TYPE_NORMAL
- en: '[`DataFrame.to_hdf`](pandas.DataFrame.to_hdf.html#pandas.DataFrame.to_hdf "pandas.DataFrame.to_hdf")'
  prefs: []
  type: TYPE_NORMAL
- en: Write to hdf.
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs: []
  type: TYPE_NORMAL
- en: This function requires either the [fastparquet](https://pypi.org/project/fastparquet)
    or [pyarrow](https://arrow.apache.org/docs/python/) library.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you want to get a buffer to the parquet content you can use a io.BytesIO
    object, as long as you don’t use partition_cols, which creates multiple files.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
