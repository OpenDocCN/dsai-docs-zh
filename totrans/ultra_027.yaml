- en: Mobile Segment Anything (MobileSAM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/models/mobile-sam/`](https://docs.ultralytics.com/models/mobile-sam/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![MobileSAM Logo](img/453c729475acdee37d7f0db7d4748c60.png)'
  prefs: []
  type: TYPE_IMG
- en: The MobileSAM paper is now available on [arXiv](https://arxiv.org/pdf/2306.14289.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: A demonstration of MobileSAM running on a CPU can be accessed at this [demo
    link](https://huggingface.co/spaces/dhkim2810/MobileSAM). The performance on a
    Mac i5 CPU takes approximately 3 seconds. On the Hugging Face demo, the interface
    and lower-performance CPUs contribute to a slower response, but it continues to
    function effectively.
  prefs: []
  type: TYPE_NORMAL
- en: MobileSAM is implemented in various projects including [Grounding-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything),
    [AnyLabeling](https://github.com/vietanhdev/anylabeling), and [Segment Anything
    in 3D](https://github.com/Jumpat/SegmentAnythingin3D).
  prefs: []
  type: TYPE_NORMAL
- en: MobileSAM is trained on a single GPU with a 100k dataset (1% of the original
    images) in less than a day. The code for this training will be made available
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Available Models, Supported Tasks, and Operating Modes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This table presents the available models with their specific pre-trained weights,
    the tasks they support, and their compatibility with different operating modes
    like Inference, Validation, Training, and Export, indicated by ✅ emojis for supported
    modes and ❌ emojis for unsupported modes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MobileSAM | [mobile_sam.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/mobile_sam.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: Adapting from SAM to MobileSAM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since MobileSAM retains the same pipeline as the original SAM, we have incorporated
    the original's pre-processing, post-processing, and all other interfaces. Consequently,
    those currently using the original SAM can transition to MobileSAM with minimal
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'MobileSAM performs comparably to the original SAM and retains the same pipeline
    except for a change in the image encoder. Specifically, we replace the original
    heavyweight ViT-H encoder (632M) with a smaller Tiny-ViT (5M). On a single GPU,
    MobileSAM operates at about 12ms per image: 8ms on the image encoder and 4ms on
    the mask decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table provides a comparison of ViT-based image encoders:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Image Encoder | Original SAM | MobileSAM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Parameters | 611M | 5M |'
  prefs: []
  type: TYPE_TB
- en: '| Speed | 452ms | 8ms |'
  prefs: []
  type: TYPE_TB
- en: 'Both the original SAM and MobileSAM utilize the same prompt-guided mask decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Mask Decoder | Original SAM | MobileSAM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Parameters | 3.876M | 3.876M |'
  prefs: []
  type: TYPE_TB
- en: '| Speed | 4ms | 4ms |'
  prefs: []
  type: TYPE_TB
- en: 'Here is the comparison of the whole pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Whole Pipeline (Enc+Dec) | Original SAM | MobileSAM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Parameters | 615M | 9.66M |'
  prefs: []
  type: TYPE_TB
- en: '| Speed | 456ms | 12ms |'
  prefs: []
  type: TYPE_TB
- en: The performance of MobileSAM and the original SAM are demonstrated using both
    a point and a box as prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image with Point as Prompt](img/f2294f007c6c2f0e63d2508720eae89f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Image with Box as Prompt](img/f2294f007c6c2f0e63d2508720eae89f.png)'
  prefs: []
  type: TYPE_IMG
- en: With its superior performance, MobileSAM is approximately 5 times smaller and
    7 times faster than the current FastSAM. More details are available at the [MobileSAM
    project page](https://github.com/ChaoningZhang/MobileSAM).
  prefs: []
  type: TYPE_NORMAL
- en: Testing MobileSAM in Ultralytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like the original SAM, we offer a straightforward testing method in Ultralytics,
    including modes for both Point and Box prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Model Download
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can download the model [here](https://github.com/ChaoningZhang/MobileSAM/blob/master/weights/mobile_sam.pt).
  prefs: []
  type: TYPE_NORMAL
- en: Point Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Box Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We have implemented `MobileSAM` and `SAM` using the same API. For more usage
    information, please see the SAM page.
  prefs: []
  type: TYPE_NORMAL
- en: Citations and Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you find MobileSAM useful in your research or development work, please consider
    citing our paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is MobileSAM and how does it differ from the original SAM model?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MobileSAM is a lightweight, fast image segmentation model designed for mobile
    applications. It retains the same pipeline as the original SAM but replaces the
    heavyweight ViT-H encoder (632M parameters) with a smaller Tiny-ViT encoder (5M
    parameters). This change results in MobileSAM being approximately 5 times smaller
    and 7 times faster than the original SAM. For instance, MobileSAM operates at
    about 12ms per image, compared to the original SAM's 456ms. You can learn more
    about the MobileSAM implementation in various projects [here](https://github.com/ChaoningZhang/MobileSAM).
  prefs: []
  type: TYPE_NORMAL
- en: How can I test MobileSAM using Ultralytics?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Testing MobileSAM in Ultralytics can be accomplished through straightforward
    methods. You can use Point and Box prompts to predict segments. Here''s an example
    using a Point prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can also refer to the Testing MobileSAM section for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Why should I use MobileSAM for my mobile application?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MobileSAM is ideal for mobile applications due to its lightweight architecture
    and fast inference speed. Compared to the original SAM, MobileSAM is approximately
    5 times smaller and 7 times faster, making it suitable for environments where
    computational resources are limited. This efficiency ensures that mobile devices
    can perform real-time image segmentation without significant latency. Additionally,
    MobileSAM's models, such as Inference, are optimized for mobile performance.
  prefs: []
  type: TYPE_NORMAL
- en: How was MobileSAM trained, and is the training code available?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MobileSAM was trained on a single GPU with a 100k dataset, which is 1% of the
    original images, in less than a day. While the training code will be made available
    in the future, you can currently explore other aspects of MobileSAM in the [MobileSAM
    GitHub repository](https://github.com/ultralytics/assets/releases/download/v8.2.0/mobile_sam.pt).
    This repository includes pre-trained weights and implementation details for various
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: What are the primary use cases for MobileSAM?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MobileSAM is designed for fast and efficient image segmentation in mobile environments.
    Primary use cases include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time object detection and segmentation** for mobile applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low-latency image processing** in devices with limited computational resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration in AI-driven mobile apps** for tasks such as augmented reality
    (AR) and real-time analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more detailed use cases and performance comparisons, see the section on
    Adapting from SAM to MobileSAM.
  prefs: []
  type: TYPE_NORMAL
