["```py\nIn [1]: left = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\n\nIn [2]: right = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n\nIn [3]: left\nOut[3]: \n a left_val\n0   1        a\n1   5        b\n2  10        c\n\n[3 rows x 2 columns]\n\nIn [4]: right\nOut[4]: \n a  right_val\n0  1          1\n1  2          2\n2  3          3\n3  6          6\n4  7          7\n\n[5 rows x 2 columns] \n```", "```py\nIn [5]: pd.merge_asof(left, right, on=\"a\")\nOut[5]: \n a left_val  right_val\n0   1        a          1\n1   5        b          3\n2  10        c          7\n\n[3 rows x 3 columns] \n```", "```py\nIn [6]: pd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\nOut[6]: \n a left_val  right_val\n0   1        a        NaN\n1   5        b        3.0\n2  10        c        7.0\n\n[3 rows x 3 columns] \n```", "```py\nIn [7]: trades = pd.DataFrame(\n ...:    {\n ...:        \"time\": pd.to_datetime(\n ...:            [\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.038\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.048\",\n ...:            ]\n ...:        ),\n ...:        \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n ...:        \"price\": [51.95, 51.95, 720.77, 720.92, 98.00],\n ...:        \"quantity\": [75, 155, 100, 100, 100],\n ...:    },\n ...:    columns=[\"time\", \"ticker\", \"price\", \"quantity\"],\n ...: )\n ...: \n\nIn [8]: quotes = pd.DataFrame(\n ...:    {\n ...:        \"time\": pd.to_datetime(\n ...:            [\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.030\",\n ...:                \"20160525 13:30:00.041\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.049\",\n ...:                \"20160525 13:30:00.072\",\n ...:                \"20160525 13:30:00.075\",\n ...:            ]\n ...:        ),\n ...:        \"ticker\": [\"GOOG\", \"MSFT\", \"MSFT\", \"MSFT\", \"GOOG\", \"AAPL\", \"GOOG\", \"MSFT\"],\n ...:        \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n ...:        \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],\n ...:    },\n ...:    columns=[\"time\", \"ticker\", \"bid\", \"ask\"],\n ...: )\n ...: \n```", "```py\nIn [9]: trades\nOut[9]: \n time ticker   price  quantity\n0 2016-05-25 13:30:00.023   MSFT   51.95        75\n1 2016-05-25 13:30:00.038   MSFT   51.95       155\n2 2016-05-25 13:30:00.048   GOOG  720.77       100\n3 2016-05-25 13:30:00.048   GOOG  720.92       100\n4 2016-05-25 13:30:00.048   AAPL   98.00       100\n\n[5 rows x 4 columns]\n\nIn [10]: quotes\nOut[10]: \n time ticker     bid     ask\n0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n\n[8 rows x 4 columns] \n```", "```py\nIn [11]: pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\nOut[11]: \n time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n\n[5 rows x 6 columns] \n```", "```py\nIn [12]: dft = pd.DataFrame(\n ....:    {\"B\": [0, 1, 2, np.nan, 4]},\n ....:    index=pd.date_range(\"20130101 09:00:00\", periods=5, freq=\"s\"),\n ....: )\n ....: \n\nIn [13]: dft\nOut[13]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  2.0\n2013-01-01 09:00:03  NaN\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [14]: dft.rolling(2).sum()\nOut[14]: \n B\n2013-01-01 09:00:00  NaN\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  NaN\n2013-01-01 09:00:04  NaN\n\n[5 rows x 1 columns]\n\nIn [15]: dft.rolling(2, min_periods=1).sum()\nOut[15]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [16]: dft.rolling(\"2s\").sum()\nOut[16]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [17]: dft = pd.DataFrame(\n ....:    {\"B\": [0, 1, 2, np.nan, 4]},\n ....:    index=pd.Index(\n ....:        [\n ....:            pd.Timestamp(\"20130101 09:00:00\"),\n ....:            pd.Timestamp(\"20130101 09:00:02\"),\n ....:            pd.Timestamp(\"20130101 09:00:03\"),\n ....:            pd.Timestamp(\"20130101 09:00:05\"),\n ....:            pd.Timestamp(\"20130101 09:00:06\"),\n ....:        ],\n ....:        name=\"foo\",\n ....:    ),\n ....: )\n ....: \n\nIn [18]: dft\nOut[18]: \n B\nfoo \n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0\n\n[5 rows x 1 columns]\n\nIn [19]: dft.rolling(2).sum()\nOut[19]: \n B\nfoo \n2013-01-01 09:00:00  NaN\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  NaN\n\n[5 rows x 1 columns] \n```", "```py\nIn [20]: dft.rolling(\"2s\").sum()\nOut[20]: \n B\nfoo \n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [21]: dft = dft.reset_index()\n\nIn [22]: dft\nOut[22]: \n foo    B\n0 2013-01-01 09:00:00  0.0\n1 2013-01-01 09:00:02  1.0\n2 2013-01-01 09:00:03  2.0\n3 2013-01-01 09:00:05  NaN\n4 2013-01-01 09:00:06  4.0\n\n[5 rows x 2 columns]\n\nIn [23]: dft.rolling(\"2s\", on=\"foo\").sum()\nOut[23]: \n foo    B\n0 2013-01-01 09:00:00  0.0\n1 2013-01-01 09:00:02  1.0\n2 2013-01-01 09:00:03  3.0\n3 2013-01-01 09:00:05  NaN\n4 2013-01-01 09:00:06  4.0\n\n[5 rows x 2 columns] \n```", "```py\nIn [24]: data = \"0,1,2\\n3,4,5\"\n\nIn [25]: names = [\"a\", \"b\", \"a\"] \n```", "```py\nIn [2]: pd.read_csv(StringIO(data), names=names)\nOut[2]:\n a  b  a\n0  2  1  2\n1  5  4  5 \n```", "```py\nIn [26]: pd.read_csv(StringIO(data), names=names)\n---------------------------------------------------------------------------\nValueError  Traceback (most recent call last)\nCell In[26], line 1\n----> 1 pd.read_csv(StringIO(data), names=names)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:617, in _read(filepath_or_buffer, kwds)\n  614 nrows = kwds.get(\"nrows\", None)\n  616 # Check for duplicates in names.\n--> 617 _validate_names(kwds.get(\"names\", None))\n  619 # Create the parser.\n  620 parser = TextFileReader(filepath_or_buffer, **kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:576, in _validate_names(names)\n  574 if names is not None:\n  575     if len(names) != len(set(names)):\n--> 576         raise ValueError(\"Duplicate names are not allowed.\")\n  577     if not (\n  578         is_list_like(names, allow_sets=False) or isinstance(names, abc.KeysView)\n  579     ):\n  580         raise ValueError(\"Names should be an ordered collection.\")\n\nValueError: Duplicate names are not allowed. \n```", "```py\nIn [27]: data = \"\"\"\n ....: col1,col2,col3\n ....: a,b,1\n ....: a,b,2\n ....: c,d,3\n ....: \"\"\"\n ....: \n\nIn [28]: pd.read_csv(StringIO(data))\nOut[28]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\n[3 rows x 3 columns]\n\nIn [29]: pd.read_csv(StringIO(data)).dtypes\nOut[29]: \ncol1    object\ncol2    object\ncol3     int64\nLength: 3, dtype: object\n\nIn [30]: pd.read_csv(StringIO(data), dtype=\"category\").dtypes\nOut[30]: \ncol1    category\ncol2    category\ncol3    category\nLength: 3, dtype: object \n```", "```py\nIn [31]: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\nOut[31]: \ncol1    category\ncol2      object\ncol3       int64\nLength: 3, dtype: object \n```", "```py\nIn [32]: df = pd.read_csv(StringIO(data), dtype=\"category\")\n\nIn [33]: df.dtypes\nOut[33]: \ncol1    category\ncol2    category\ncol3    category\nLength: 3, dtype: object\n\nIn [34]: df[\"col3\"]\nOut[34]: \n0    1\n1    2\n2    3\nName: col3, Length: 3, dtype: category\nCategories (3, object): ['1', '2', '3']\n\nIn [35]: new_categories = pd.to_numeric(df[\"col3\"].cat.categories)\n\nIn [36]: df[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\n\nIn [37]: df[\"col3\"]\nOut[37]: \n0    1\n1    2\n2    3\nName: col3, Length: 3, dtype: category\nCategories (3, int64): [1, 2, 3] \n```", "```py\n    In [38]: from pandas.api.types import union_categoricals\n\n    In [39]: a = pd.Categorical([\"b\", \"c\"])\n\n    In [40]: b = pd.Categorical([\"a\", \"b\"])\n\n    In [41]: union_categoricals([a, b])\n    Out[41]: \n    ['b', 'c', 'a', 'b']\n    Categories (3, object): ['b', 'c', 'a'] \n    ```", "```py\n    In [42]: s1 = pd.Series([\"a\", \"b\"], dtype=\"category\")\n\n    In [43]: s2 = pd.Series([\"b\", \"c\"], dtype=\"category\") \n    ```", "```py\nIn [1]: pd.concat([s1, s2])\nValueError: incompatible categories in categorical concat \n```", "```py\nIn [44]: pd.concat([s1, s2])\nOut[44]: \n0    a\n1    b\n0    b\n1    c\nLength: 4, dtype: object \n```", "```py\nIn [45]: from pandas.tseries.offsets import SemiMonthEnd, SemiMonthBegin \n```", "```py\nIn [46]: pd.Timestamp(\"2016-01-01\") + SemiMonthEnd()\nOut[46]: Timestamp('2016-01-15 00:00:00')\n\nIn [47]: pd.date_range(\"2015-01-01\", freq=\"SM\", periods=4)\nOut[47]: DatetimeIndex(['2015-01-15', '2015-01-31', '2015-02-15', '2015-02-28'], dtype='datetime64[ns]', freq='SM-15') \n```", "```py\nIn [46]: pd.Timestamp(\"2016-01-01\") + SemiMonthBegin()\nOut[46]: Timestamp('2016-01-15 00:00:00')\n\nIn [47]: pd.date_range(\"2015-01-01\", freq=\"SMS\", periods=4)\nOut[47]: DatetimeIndex(['2015-01-01', '2015-01-15', '2015-02-01', '2015-02-15'], dtype='datetime64[ns]', freq='SMS-15') \n```", "```py\nIn [50]: pd.date_range(\"2015-01-01\", freq=\"SMS-16\", periods=4)\nOut[50]: DatetimeIndex(['2015-01-01', '2015-01-16', '2015-02-01', '2015-02-16'], dtype='datetime64[ns]', freq='SMS-16')\n\nIn [51]: pd.date_range(\"2015-01-01\", freq=\"SM-14\", periods=4)\nOut[51]: DatetimeIndex(['2015-01-14', '2015-01-31', '2015-02-14', '2015-02-28'], dtype='datetime64[ns]', freq='SM-14') \n```", "```py\nIn [48]: idx = pd.Index([\"a\", \"b\", \"c\"])\n\nIn [49]: idx.where([True, False, True])\nOut[49]: Index(['a', None, 'c'], dtype='object') \n```", "```py\nIn [50]: idx = pd.Index([1, 2, np.nan, 4])\n\nIn [51]: idx.dropna()\nOut[51]: Index([1.0, 2.0, 4.0], dtype='float64') \n```", "```py\nIn [52]: midx = pd.MultiIndex.from_arrays([[1, 2, np.nan, 4], [1, 2, np.nan, np.nan]])\n\nIn [53]: midx\nOut[53]: \nMultiIndex([(1.0, 1.0),\n (2.0, 2.0),\n (nan, nan),\n (4.0, nan)],\n )\n\nIn [54]: midx.dropna()\nOut[54]: \nMultiIndex([(1, 1),\n (2, 2)],\n )\n\nIn [55]: midx.dropna(how=\"all\")\nOut[55]: \nMultiIndex([(1, 1.0),\n (2, 2.0),\n (4, nan)],\n ) \n```", "```py\nIn [56]: idx = pd.Index([\"a1a2\", \"b1\", \"c1\"])\n\nIn [57]: idx.str.extractall(r\"[ab](?P<digit>\\d)\")\nOut[57]: \n digit\n match \n0 0         1\n 1         2\n1 0         1\n\n[3 rows x 1 columns] \n```", "```py\nIn [1]: pd.get_dummies(['a', 'b', 'a', 'c']).dtypes\n\nOut[1]:\na    float64\nb    float64\nc    float64\ndtype: object \n```", "```py\nIn [58]: pd.get_dummies([\"a\", \"b\", \"a\", \"c\"]).dtypes\nOut[58]: \na    bool\nb    bool\nc    bool\nLength: 3, dtype: object \n```", "```py\nIn [59]: s = [\"1\", 2, 3]\n\nIn [60]: pd.to_numeric(s, downcast=\"unsigned\")\nOut[60]: array([1, 2, 3], dtype=uint8)\n\nIn [61]: pd.to_numeric(s, downcast=\"integer\")\nOut[61]: array([1, 2, 3], dtype=int8) \n```", "```py\nIn [62]: import pprint\n\nIn [63]: from pandas.api import types\n\nIn [64]: funcs = [f for f in dir(types) if not f.startswith(\"_\")]\n\nIn [65]: pprint.pprint(funcs)\n['CategoricalDtype',\n 'DatetimeTZDtype',\n 'IntervalDtype',\n 'PeriodDtype',\n 'infer_dtype',\n 'is_any_real_numeric_dtype',\n 'is_array_like',\n 'is_bool',\n 'is_bool_dtype',\n 'is_categorical_dtype',\n 'is_complex',\n 'is_complex_dtype',\n 'is_datetime64_any_dtype',\n 'is_datetime64_dtype',\n 'is_datetime64_ns_dtype',\n 'is_datetime64tz_dtype',\n 'is_dict_like',\n 'is_dtype_equal',\n 'is_extension_array_dtype',\n 'is_file_like',\n 'is_float',\n 'is_float_dtype',\n 'is_hashable',\n 'is_int64_dtype',\n 'is_integer',\n 'is_integer_dtype',\n 'is_interval',\n 'is_interval_dtype',\n 'is_iterator',\n 'is_list_like',\n 'is_named_tuple',\n 'is_number',\n 'is_numeric_dtype',\n 'is_object_dtype',\n 'is_period_dtype',\n 'is_re',\n 'is_re_compilable',\n 'is_scalar',\n 'is_signed_integer_dtype',\n 'is_sparse',\n 'is_string_dtype',\n 'is_timedelta64_dtype',\n 'is_timedelta64_ns_dtype',\n 'is_unsigned_integer_dtype',\n 'pandas_dtype',\n 'union_categoricals'] \n```", "```py\n    In [66]: pd.Timestamp(2012, 1, 1)\n    Out[66]: Timestamp('2012-01-01 00:00:00')\n\n    In [67]: pd.Timestamp(year=2012, month=1, day=1, hour=8, minute=30)\n    Out[67]: Timestamp('2012-01-01 08:30:00') \n    ```", "```py\n    In [68]: df = pd.DataFrame(\n     ....:    {\"date\": pd.date_range(\"2015-01-01\", freq=\"W\", periods=5), \"a\": np.arange(5)},\n     ....:    index=pd.MultiIndex.from_arrays(\n     ....:        [[1, 2, 3, 4, 5], pd.date_range(\"2015-01-01\", freq=\"W\", periods=5)],\n     ....:        names=[\"v\", \"d\"],\n     ....:    ),\n     ....: )\n     ....: \n\n    In [69]: df\n    Out[69]: \n     date  a\n    v d \n    1 2015-01-04 2015-01-04  0\n    2 2015-01-11 2015-01-11  1\n    3 2015-01-18 2015-01-18  2\n    4 2015-01-25 2015-01-25  3\n    5 2015-02-01 2015-02-01  4\n\n    [5 rows x 2 columns] \n    ```", "```py\n    In [74]: df.resample(\"M\", on=\"date\")[[\"a\"]].sum()\n    Out[74]:\n     a\n    date\n    2015-01-31  6\n    2015-02-28  4\n\n    [2 rows x 1 columns]\n\n    In [75]: df.resample(\"M\", level=\"d\")[[\"a\"]].sum()\n    Out[75]:\n     a\n    d\n    2015-01-31  6\n    2015-02-28  4\n\n    [2 rows x 1 columns] \n    ```", "```py\n    In [70]: df = pd.DataFrame({\"A\": [2, 7], \"B\": [3, 5], \"C\": [4, 8]}, index=[\"row1\", \"row2\"])\n\n    In [71]: df\n    Out[71]: \n     A  B  C\n    row1  2  3  4\n    row2  7  5  8\n\n    [2 rows x 3 columns]\n\n    In [72]: df.sort_values(by=\"row2\", axis=1)\n    Out[72]: \n     B  A  C\n    row1  3  2  4\n    row2  5  7  8\n\n    [2 rows x 3 columns] \n    ```", "```py\nIn [73]: s = pd.Series([1, 2, 3]) \n```", "```py\nIn [7]: type(s.tolist()[0])\nOut[7]:\n <class 'numpy.int64'> \n```", "```py\nIn [74]: type(s.tolist()[0])\nOut[74]: int \n```", "```py\nIn [75]: s1 = pd.Series([1, 2, 3], index=list(\"ABC\"))\n\nIn [76]: s2 = pd.Series([2, 2, 2], index=list(\"ABD\"))\n\nIn [77]: s1 + s2\nOut[77]: \nA    3.0\nB    4.0\nC    NaN\nD    NaN\nLength: 4, dtype: float64\n\nIn [78]: df1 = pd.DataFrame([1, 2, 3], index=list(\"ABC\"))\n\nIn [79]: df2 = pd.DataFrame([2, 2, 2], index=list(\"ABD\"))\n\nIn [80]: df1 + df2\nOut[80]: \n 0\nA  3.0\nB  4.0\nC  NaN\nD  NaN\n\n[4 rows x 1 columns] \n```", "```py\nIn [1]: s1 == s2\nOut[1]:\nA    False\nB     True\nC    False\ndtype: bool \n```", "```py\nIn [2]: s1 == s2\nOut[2]:\nValueError: Can only compare identically-labeled Series objects \n```", "```py\nIn [81]: s1.values == s2.values\nOut[81]: array([False,  True, False]) \n```", "```py\nIn [82]: s1.eq(s2)\nOut[82]: \nA    False\nB     True\nC    False\nD    False\nLength: 4, dtype: bool \n```", "```py\nIn [3]: df1 == df2\nOut[3]:\nValueError: Can only compare identically-labeled DataFrame objects \n```", "```py\nIn [4]: s1 = pd.Series([True, False, True], index=list('ABC'))\nIn [5]: s2 = pd.Series([True, True, True], index=list('ABD'))\nIn [6]: s1 & s2\nOut[6]:\nA     True\nB    False\nC    False\ndtype: bool \n```", "```py\nIn [83]: s1 = pd.Series([True, False, True], index=list(\"ABC\"))\n\nIn [84]: s2 = pd.Series([True, True, True], index=list(\"ABD\"))\n\nIn [85]: s1 & s2\nOut[85]: \nA     True\nB    False\nC    False\nD    False\nLength: 4, dtype: bool \n```", "```py\nIn [86]: s1 & s2.reindex_like(s1)\nOut[86]: \nA     True\nB    False\nC    False\nLength: 3, dtype: bool \n```", "```py\nIn [87]: df1 = pd.DataFrame([True, False, True], index=list(\"ABC\"))\n\nIn [88]: df2 = pd.DataFrame([True, True, True], index=list(\"ABD\"))\n\nIn [89]: df1 & df2\nOut[89]: \n 0\nA   True\nB  False\nC  False\nD  False\n\n[4 rows x 1 columns] \n```", "```py\nIn [90]: s1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n\nIn [91]: s2 = pd.Series([2, 2, 2], index=[\"b\", \"c\", \"d\"])\n\nIn [92]: s1.eq(s2)\nOut[92]: \na    False\nb     True\nc    False\nd    False\nLength: 4, dtype: bool\n\nIn [93]: s1.ge(s2)\nOut[93]: \na    False\nb     True\nc     True\nd    False\nLength: 4, dtype: bool \n```", "```py\nIn [94]: s = pd.Series() \n```", "```py\nIn [2]: s[\"a\"] = pd.Timestamp(\"2016-01-01\")\n\nIn [3]: s[\"b\"] = 3.0\nTypeError: invalid type promotion \n```", "```py\nIn [95]: s[\"a\"] = pd.Timestamp(\"2016-01-01\")\n\nIn [96]: s[\"b\"] = 3.0\n\nIn [97]: s\nOut[97]: \na    2016-01-01 00:00:00\nb                    3.0\nLength: 2, dtype: object\n\nIn [98]: s.dtype\nOut[98]: dtype('O') \n```", "```py\nIn [2]: pd.to_datetime([1, 'foo'], errors='coerce')\nOut[2]: DatetimeIndex(['NaT', 'NaT'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [99]: pd.to_datetime([1, \"foo\"], errors=\"coerce\")\nOut[99]: DatetimeIndex(['1970-01-01 00:00:00.000000001', 'NaT'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [100]: df1 = pd.DataFrame({\"key\": [1], \"v1\": [10]})\n\nIn [101]: df1\nOut[101]: \n key  v1\n0    1  10\n\n[1 rows x 2 columns]\n\nIn [102]: df2 = pd.DataFrame({\"key\": [1, 2], \"v1\": [20, 30]})\n\nIn [103]: df2\nOut[103]: \n key  v1\n0    1  20\n1    2  30\n\n[2 rows x 2 columns] \n```", "```py\nIn [5]: pd.merge(df1, df2, how='outer')\nOut[5]:\n key    v1\n0  1.0  10.0\n1  1.0  20.0\n2  2.0  30.0\n\nIn [6]: pd.merge(df1, df2, how='outer').dtypes\nOut[6]:\nkey    float64\nv1     float64\ndtype: object \n```", "```py\nIn [104]: pd.merge(df1, df2, how=\"outer\")\nOut[104]: \n key  v1\n0    1  10\n1    1  20\n2    2  30\n\n[3 rows x 2 columns]\n\nIn [105]: pd.merge(df1, df2, how=\"outer\").dtypes\nOut[105]: \nkey    int64\nv1     int64\nLength: 2, dtype: object \n```", "```py\nIn [106]: pd.merge(df1, df2, how=\"outer\", on=\"key\")\nOut[106]: \n key  v1_x  v1_y\n0    1  10.0    20\n1    2   NaN    30\n\n[2 rows x 3 columns]\n\nIn [107]: pd.merge(df1, df2, how=\"outer\", on=\"key\").dtypes\nOut[107]: \nkey       int64\nv1_x    float64\nv1_y      int64\nLength: 3, dtype: object \n```", "```py\nIn [108]: s = pd.Series([0, 1, 2, 3, 4])\n\nIn [109]: df = pd.DataFrame([0, 1, 2, 3, 4]) \n```", "```py\nIn [3]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[3]:\ncount     5.000000\nmean      2.000000\nstd       1.581139\nmin       0.000000\n0.0%      0.000400\n0.1%      0.002000\n0.1%      0.004000\n50%       2.000000\n99.9%     3.996000\n100.0%    3.998000\n100.0%    3.999600\nmax       4.000000\ndtype: float64\n\nIn [4]: df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[4]:\n...\nValueError: cannot reindex from a duplicate axis \n```", "```py\nIn [110]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[110]: \ncount     5.000000\nmean      2.000000\nstd       1.581139\nmin       0.000000\n0.01%     0.000400\n0.05%     0.002000\n0.1%      0.004000\n50%       2.000000\n99.9%     3.996000\n99.95%    3.998000\n99.99%    3.999600\nmax       4.000000\nLength: 12, dtype: float64\n\nIn [111]: df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[111]: \n 0\ncount   5.000000\nmean    2.000000\nstd     1.581139\nmin     0.000000\n0.01%   0.000400\n0.05%   0.002000\n0.1%    0.004000\n50%     2.000000\n99.9%   3.996000\n99.95%  3.998000\n99.99%  3.999600\nmax     4.000000\n\n[12 rows x 1 columns] \n```", "```py\nIn [1]: pi = pd.PeriodIndex(['2016-08-01'], freq='D')\n\nIn [2]: pi\nOut[2]: PeriodIndex(['2016-08-01'], dtype='int64', freq='D')\n\nIn [3]: pd.api.types.is_integer_dtype(pi)\nOut[3]: True\n\nIn [4]: pi.dtype\nOut[4]: dtype('int64') \n```", "```py\nIn [112]: pi = pd.PeriodIndex([\"2016-08-01\"], freq=\"D\")\n\nIn [113]: pi\nOut[113]: PeriodIndex(['2016-08-01'], dtype='period[D]')\n\nIn [114]: pd.api.types.is_integer_dtype(pi)\nOut[114]: False\n\nIn [115]: pd.api.types.is_period_dtype(pi)\nOut[115]: True\n\nIn [116]: pi.dtype\nOut[116]: period[D]\n\nIn [117]: type(pi.dtype)\nOut[117]: pandas.core.dtypes.dtypes.PeriodDtype \n```", "```py\nIn [5]: pd.Period('NaT', freq='D')\nOut[5]: Period('NaT', 'D') \n```", "```py\nIn [118]: pd.Period(\"NaT\")\nOut[118]: NaT\n\nIn [119]: pd.Period(None)\nOut[119]: NaT \n```", "```py\nIn [5]: pd.NaT + 1\n...\nValueError: Cannot add integral value to Timestamp without freq. \n```", "```py\nIn [120]: pd.NaT + 1\nOut[120]: NaT\n\nIn [121]: pd.NaT - 1\nOut[121]: NaT \n```", "```py\nIn [6]: pi = pd.PeriodIndex(['2011-01', '2011-02'], freq='M')\nIn [7]: pi.values\nOut[7]: array([492, 493]) \n```", "```py\nIn [122]: pi = pd.PeriodIndex([\"2011-01\", \"2011-02\"], freq=\"M\")\n\nIn [123]: pi.values\nOut[123]: array([Period('2011-01', 'M'), Period('2011-02', 'M')], dtype=object) \n```", "```py\nIn [1]: pd.Index(['a', 'b']) + pd.Index(['a', 'c'])\nFutureWarning: using '+' to provide set union with Indexes is deprecated, use '|' or .union()\nOut[1]: Index(['a', 'b', 'c'], dtype='object') \n```", "```py\nIn [124]: pd.Index([\"a\", \"b\"]) + pd.Index([\"a\", \"c\"])\nOut[124]: Index(['aa', 'bc'], dtype='object') \n```", "```py\nIn [125]: pd.Index([1, 2, 3]) + pd.Index([2, 3, 4])\nOut[125]: Index([3, 5, 7], dtype='int64') \n```", "```py\nIn [1]: (pd.DatetimeIndex(['2016-01-01', '2016-01-02'])\n ...: - pd.DatetimeIndex(['2016-01-02', '2016-01-03']))\nFutureWarning: using '-' to provide set differences with datetimelike Indexes is deprecated, use .difference()\nOut[1]: DatetimeIndex(['2016-01-01'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [126]: (\n .....:    pd.DatetimeIndex([\"2016-01-01\", \"2016-01-02\"])\n .....:    - pd.DatetimeIndex([\"2016-01-02\", \"2016-01-03\"])\n .....: )\n .....: \nOut[126]: TimedeltaIndex(['-1 days', '-1 days'], dtype='timedelta64[ns]', freq=None) \n```", "```py\nIn [127]: idx1 = pd.Index([1, 2, 3, np.nan])\n\nIn [128]: idx2 = pd.Index([0, 1, np.nan]) \n```", "```py\nIn [3]: idx1.difference(idx2)\nOut[3]: Float64Index([nan, 2.0, 3.0], dtype='float64')\n\nIn [4]: idx1.symmetric_difference(idx2)\nOut[4]: Float64Index([0.0, nan, 2.0, 3.0], dtype='float64') \n```", "```py\nIn [129]: idx1.difference(idx2)\nOut[129]: Index([2.0, 3.0], dtype='float64')\n\nIn [130]: idx1.symmetric_difference(idx2)\nOut[130]: Index([0.0, 2.0, 3.0], dtype='float64') \n```", "```py\nIn [1]: pd.Index([1, 2, 3]).unique()\nOut[1]: array([1, 2, 3])\n\nIn [2]: pd.DatetimeIndex(['2011-01-01', '2011-01-02',\n ...:                  '2011-01-03'], tz='Asia/Tokyo').unique()\nOut[2]:\nDatetimeIndex(['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00',\n '2011-01-03 00:00:00+09:00'],\n dtype='datetime64[ns, Asia/Tokyo]', freq=None) \n```", "```py\nIn [131]: pd.Index([1, 2, 3]).unique()\nOut[131]: Index([1, 2, 3], dtype='int64')\n\nIn [132]: pd.DatetimeIndex(\n .....:    [\"2011-01-01\", \"2011-01-02\", \"2011-01-03\"], tz=\"Asia/Tokyo\"\n .....: ).unique()\n .....: \nOut[132]: \nDatetimeIndex(['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00',\n '2011-01-03 00:00:00+09:00'],\n dtype='datetime64[ns, Asia/Tokyo]', freq=None) \n```", "```py\nIn [133]: cat = pd.Categorical([\"a\", \"b\"], categories=list(\"bac\"))\n\nIn [134]: lvl1 = [\"foo\", \"bar\"]\n\nIn [135]: midx = pd.MultiIndex.from_arrays([cat, lvl1])\n\nIn [136]: midx\nOut[136]: \nMultiIndex([('a', 'foo'),\n ('b', 'bar')],\n ) \n```", "```py\nIn [4]: midx.levels[0]\nOut[4]: Index(['b', 'a', 'c'], dtype='object')\n\nIn [5]: midx.get_level_values[0]\nOut[5]: Index(['a', 'b'], dtype='object') \n```", "```py\nIn [137]: midx.levels[0]\nOut[137]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category')\n\nIn [138]: midx.get_level_values(0)\nOut[138]: CategoricalIndex(['a', 'b'], categories=['b', 'a', 'c'], ordered=False, dtype='category') \n```", "```py\nIn [139]: df = pd.DataFrame({\"A\": [0, 1], \"B\": [10, 11], \"C\": cat})\n\nIn [140]: df_grouped = df.groupby(by=[\"A\", \"C\"], observed=False).first()\n\nIn [141]: df_set_idx = df.set_index([\"A\", \"C\"]) \n```", "```py\nIn [11]: df_grouped.index.levels[1]\nOut[11]: Index(['b', 'a', 'c'], dtype='object', name='C')\nIn [12]: df_grouped.reset_index().dtypes\nOut[12]:\nA      int64\nC     object\nB    float64\ndtype: object\n\nIn [13]: df_set_idx.index.levels[1]\nOut[13]: Index(['b', 'a', 'c'], dtype='object', name='C')\nIn [14]: df_set_idx.reset_index().dtypes\nOut[14]:\nA      int64\nC     object\nB      int64\ndtype: object \n```", "```py\nIn [142]: df_grouped.index.levels[1]\nOut[142]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category', name='C')\n\nIn [143]: df_grouped.reset_index().dtypes\nOut[143]: \nA       int64\nC    category\nB     float64\nLength: 3, dtype: object\n\nIn [144]: df_set_idx.index.levels[1]\nOut[144]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category', name='C')\n\nIn [145]: df_set_idx.reset_index().dtypes\nOut[145]: \nA       int64\nC    category\nB       int64\nLength: 3, dtype: object \n```", "```py\nIn [146]: data = \"A,B\\n0,1\\n2,3\\n4,5\\n6,7\" \n```", "```py\nIn [2]: pd.concat(pd.read_csv(StringIO(data), chunksize=2))\nOut[2]:\n A  B\n0  0  1\n1  2  3\n0  4  5\n1  6  7 \n```", "```py\nIn [147]: pd.concat(pd.read_csv(StringIO(data), chunksize=2))\nOut[147]: \n A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n\n[4 rows x 2 columns] \n```", "```py\nIn [1]: pd.SparseArray([1, 2, 0, 0])\nOut[1]:\n[1.0, 2.0, 0.0, 0.0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n\n# specifying int64 dtype, but all values are stored in sp_values because\n# fill_value default is np.nan\nIn [2]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[2]:\n[1, 2, 0, 0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n\nIn [3]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64, fill_value=0)\nOut[3]:\n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32) \n```", "```py\nIn [148]: pd.arrays.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[148]: \n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32)\n\nIn [149]: pd.arrays.SparseArray([True, False, False, False])\nOut[149]: \n[True, False, False, False]\nFill: False\nIntIndex\nIndices: array([0], dtype=int32) \n```", "```py\ns = pd.SparseSeries([0, 2, 0, 1], fill_value=0, dtype=np.int64)\ns.dtype\n\ns + 1 \n```", "```py\ns = pd.SparseSeries([1.0, 0.0, 2.0, 0.0], fill_value=0)\ns\ns.astype(np.int64) \n```", "```py\nIn [7]: pd.SparseSeries([1., np.nan, 2., np.nan], fill_value=np.nan).astype(np.int64)\nOut[7]:\nValueError: unable to coerce current fill_value nan to int64 dtype \n```", "```py\nIn [1]: i = pd.Index(['a', 'b', 'c'])\n\nIn [2]: i.get_indexer(['b', 'b', 'c']).dtype\nOut[2]: dtype('int32') \n```", "```py\nIn [1]: i = pd.Index(['a', 'b', 'c'])\n\nIn [2]: i.get_indexer(['b', 'b', 'c']).dtype\nOut[2]: dtype('int64') \n```", "```py\nIn [1]: left = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\n\nIn [2]: right = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n\nIn [3]: left\nOut[3]: \n a left_val\n0   1        a\n1   5        b\n2  10        c\n\n[3 rows x 2 columns]\n\nIn [4]: right\nOut[4]: \n a  right_val\n0  1          1\n1  2          2\n2  3          3\n3  6          6\n4  7          7\n\n[5 rows x 2 columns] \n```", "```py\nIn [5]: pd.merge_asof(left, right, on=\"a\")\nOut[5]: \n a left_val  right_val\n0   1        a          1\n1   5        b          3\n2  10        c          7\n\n[3 rows x 3 columns] \n```", "```py\nIn [6]: pd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\nOut[6]: \n a left_val  right_val\n0   1        a        NaN\n1   5        b        3.0\n2  10        c        7.0\n\n[3 rows x 3 columns] \n```", "```py\nIn [7]: trades = pd.DataFrame(\n ...:    {\n ...:        \"time\": pd.to_datetime(\n ...:            [\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.038\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.048\",\n ...:            ]\n ...:        ),\n ...:        \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n ...:        \"price\": [51.95, 51.95, 720.77, 720.92, 98.00],\n ...:        \"quantity\": [75, 155, 100, 100, 100],\n ...:    },\n ...:    columns=[\"time\", \"ticker\", \"price\", \"quantity\"],\n ...: )\n ...: \n\nIn [8]: quotes = pd.DataFrame(\n ...:    {\n ...:        \"time\": pd.to_datetime(\n ...:            [\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.030\",\n ...:                \"20160525 13:30:00.041\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.049\",\n ...:                \"20160525 13:30:00.072\",\n ...:                \"20160525 13:30:00.075\",\n ...:            ]\n ...:        ),\n ...:        \"ticker\": [\"GOOG\", \"MSFT\", \"MSFT\", \"MSFT\", \"GOOG\", \"AAPL\", \"GOOG\", \"MSFT\"],\n ...:        \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n ...:        \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],\n ...:    },\n ...:    columns=[\"time\", \"ticker\", \"bid\", \"ask\"],\n ...: )\n ...: \n```", "```py\nIn [9]: trades\nOut[9]: \n time ticker   price  quantity\n0 2016-05-25 13:30:00.023   MSFT   51.95        75\n1 2016-05-25 13:30:00.038   MSFT   51.95       155\n2 2016-05-25 13:30:00.048   GOOG  720.77       100\n3 2016-05-25 13:30:00.048   GOOG  720.92       100\n4 2016-05-25 13:30:00.048   AAPL   98.00       100\n\n[5 rows x 4 columns]\n\nIn [10]: quotes\nOut[10]: \n time ticker     bid     ask\n0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n\n[8 rows x 4 columns] \n```", "```py\nIn [11]: pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\nOut[11]: \n time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n\n[5 rows x 6 columns] \n```", "```py\nIn [12]: dft = pd.DataFrame(\n ....:    {\"B\": [0, 1, 2, np.nan, 4]},\n ....:    index=pd.date_range(\"20130101 09:00:00\", periods=5, freq=\"s\"),\n ....: )\n ....: \n\nIn [13]: dft\nOut[13]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  2.0\n2013-01-01 09:00:03  NaN\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [14]: dft.rolling(2).sum()\nOut[14]: \n B\n2013-01-01 09:00:00  NaN\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  NaN\n2013-01-01 09:00:04  NaN\n\n[5 rows x 1 columns]\n\nIn [15]: dft.rolling(2, min_periods=1).sum()\nOut[15]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [16]: dft.rolling(\"2s\").sum()\nOut[16]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [17]: dft = pd.DataFrame(\n ....:    {\"B\": [0, 1, 2, np.nan, 4]},\n ....:    index=pd.Index(\n ....:        [\n ....:            pd.Timestamp(\"20130101 09:00:00\"),\n ....:            pd.Timestamp(\"20130101 09:00:02\"),\n ....:            pd.Timestamp(\"20130101 09:00:03\"),\n ....:            pd.Timestamp(\"20130101 09:00:05\"),\n ....:            pd.Timestamp(\"20130101 09:00:06\"),\n ....:        ],\n ....:        name=\"foo\",\n ....:    ),\n ....: )\n ....: \n\nIn [18]: dft\nOut[18]: \n B\nfoo \n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0\n\n[5 rows x 1 columns]\n\nIn [19]: dft.rolling(2).sum()\nOut[19]: \n B\nfoo \n2013-01-01 09:00:00  NaN\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  NaN\n\n[5 rows x 1 columns] \n```", "```py\nIn [20]: dft.rolling(\"2s\").sum()\nOut[20]: \n B\nfoo \n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [21]: dft = dft.reset_index()\n\nIn [22]: dft\nOut[22]: \n foo    B\n0 2013-01-01 09:00:00  0.0\n1 2013-01-01 09:00:02  1.0\n2 2013-01-01 09:00:03  2.0\n3 2013-01-01 09:00:05  NaN\n4 2013-01-01 09:00:06  4.0\n\n[5 rows x 2 columns]\n\nIn [23]: dft.rolling(\"2s\", on=\"foo\").sum()\nOut[23]: \n foo    B\n0 2013-01-01 09:00:00  0.0\n1 2013-01-01 09:00:02  1.0\n2 2013-01-01 09:00:03  3.0\n3 2013-01-01 09:00:05  NaN\n4 2013-01-01 09:00:06  4.0\n\n[5 rows x 2 columns] \n```", "```py\nIn [24]: data = \"0,1,2\\n3,4,5\"\n\nIn [25]: names = [\"a\", \"b\", \"a\"] \n```", "```py\nIn [2]: pd.read_csv(StringIO(data), names=names)\nOut[2]:\n a  b  a\n0  2  1  2\n1  5  4  5 \n```", "```py\nIn [26]: pd.read_csv(StringIO(data), names=names)\n---------------------------------------------------------------------------\nValueError  Traceback (most recent call last)\nCell In[26], line 1\n----> 1 pd.read_csv(StringIO(data), names=names)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:617, in _read(filepath_or_buffer, kwds)\n  614 nrows = kwds.get(\"nrows\", None)\n  616 # Check for duplicates in names.\n--> 617 _validate_names(kwds.get(\"names\", None))\n  619 # Create the parser.\n  620 parser = TextFileReader(filepath_or_buffer, **kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:576, in _validate_names(names)\n  574 if names is not None:\n  575     if len(names) != len(set(names)):\n--> 576         raise ValueError(\"Duplicate names are not allowed.\")\n  577     if not (\n  578         is_list_like(names, allow_sets=False) or isinstance(names, abc.KeysView)\n  579     ):\n  580         raise ValueError(\"Names should be an ordered collection.\")\n\nValueError: Duplicate names are not allowed. \n```", "```py\nIn [27]: data = \"\"\"\n ....: col1,col2,col3\n ....: a,b,1\n ....: a,b,2\n ....: c,d,3\n ....: \"\"\"\n ....: \n\nIn [28]: pd.read_csv(StringIO(data))\nOut[28]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\n[3 rows x 3 columns]\n\nIn [29]: pd.read_csv(StringIO(data)).dtypes\nOut[29]: \ncol1    object\ncol2    object\ncol3     int64\nLength: 3, dtype: object\n\nIn [30]: pd.read_csv(StringIO(data), dtype=\"category\").dtypes\nOut[30]: \ncol1    category\ncol2    category\ncol3    category\nLength: 3, dtype: object \n```", "```py\nIn [31]: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\nOut[31]: \ncol1    category\ncol2      object\ncol3       int64\nLength: 3, dtype: object \n```", "```py\nIn [32]: df = pd.read_csv(StringIO(data), dtype=\"category\")\n\nIn [33]: df.dtypes\nOut[33]: \ncol1    category\ncol2    category\ncol3    category\nLength: 3, dtype: object\n\nIn [34]: df[\"col3\"]\nOut[34]: \n0    1\n1    2\n2    3\nName: col3, Length: 3, dtype: category\nCategories (3, object): ['1', '2', '3']\n\nIn [35]: new_categories = pd.to_numeric(df[\"col3\"].cat.categories)\n\nIn [36]: df[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\n\nIn [37]: df[\"col3\"]\nOut[37]: \n0    1\n1    2\n2    3\nName: col3, Length: 3, dtype: category\nCategories (3, int64): [1, 2, 3] \n```", "```py\n    In [38]: from pandas.api.types import union_categoricals\n\n    In [39]: a = pd.Categorical([\"b\", \"c\"])\n\n    In [40]: b = pd.Categorical([\"a\", \"b\"])\n\n    In [41]: union_categoricals([a, b])\n    Out[41]: \n    ['b', 'c', 'a', 'b']\n    Categories (3, object): ['b', 'c', 'a'] \n    ```", "```py\n    In [42]: s1 = pd.Series([\"a\", \"b\"], dtype=\"category\")\n\n    In [43]: s2 = pd.Series([\"b\", \"c\"], dtype=\"category\") \n    ```", "```py\nIn [1]: pd.concat([s1, s2])\nValueError: incompatible categories in categorical concat \n```", "```py\nIn [44]: pd.concat([s1, s2])\nOut[44]: \n0    a\n1    b\n0    b\n1    c\nLength: 4, dtype: object \n```", "```py\nIn [45]: from pandas.tseries.offsets import SemiMonthEnd, SemiMonthBegin \n```", "```py\nIn [46]: pd.Timestamp(\"2016-01-01\") + SemiMonthEnd()\nOut[46]: Timestamp('2016-01-15 00:00:00')\n\nIn [47]: pd.date_range(\"2015-01-01\", freq=\"SM\", periods=4)\nOut[47]: DatetimeIndex(['2015-01-15', '2015-01-31', '2015-02-15', '2015-02-28'], dtype='datetime64[ns]', freq='SM-15') \n```", "```py\nIn [46]: pd.Timestamp(\"2016-01-01\") + SemiMonthBegin()\nOut[46]: Timestamp('2016-01-15 00:00:00')\n\nIn [47]: pd.date_range(\"2015-01-01\", freq=\"SMS\", periods=4)\nOut[47]: DatetimeIndex(['2015-01-01', '2015-01-15', '2015-02-01', '2015-02-15'], dtype='datetime64[ns]', freq='SMS-15') \n```", "```py\nIn [50]: pd.date_range(\"2015-01-01\", freq=\"SMS-16\", periods=4)\nOut[50]: DatetimeIndex(['2015-01-01', '2015-01-16', '2015-02-01', '2015-02-16'], dtype='datetime64[ns]', freq='SMS-16')\n\nIn [51]: pd.date_range(\"2015-01-01\", freq=\"SM-14\", periods=4)\nOut[51]: DatetimeIndex(['2015-01-14', '2015-01-31', '2015-02-14', '2015-02-28'], dtype='datetime64[ns]', freq='SM-14') \n```", "```py\nIn [48]: idx = pd.Index([\"a\", \"b\", \"c\"])\n\nIn [49]: idx.where([True, False, True])\nOut[49]: Index(['a', None, 'c'], dtype='object') \n```", "```py\nIn [50]: idx = pd.Index([1, 2, np.nan, 4])\n\nIn [51]: idx.dropna()\nOut[51]: Index([1.0, 2.0, 4.0], dtype='float64') \n```", "```py\nIn [52]: midx = pd.MultiIndex.from_arrays([[1, 2, np.nan, 4], [1, 2, np.nan, np.nan]])\n\nIn [53]: midx\nOut[53]: \nMultiIndex([(1.0, 1.0),\n (2.0, 2.0),\n (nan, nan),\n (4.0, nan)],\n )\n\nIn [54]: midx.dropna()\nOut[54]: \nMultiIndex([(1, 1),\n (2, 2)],\n )\n\nIn [55]: midx.dropna(how=\"all\")\nOut[55]: \nMultiIndex([(1, 1.0),\n (2, 2.0),\n (4, nan)],\n ) \n```", "```py\nIn [56]: idx = pd.Index([\"a1a2\", \"b1\", \"c1\"])\n\nIn [57]: idx.str.extractall(r\"[ab](?P<digit>\\d)\")\nOut[57]: \n digit\n match \n0 0         1\n 1         2\n1 0         1\n\n[3 rows x 1 columns] \n```", "```py\nIn [1]: pd.get_dummies(['a', 'b', 'a', 'c']).dtypes\n\nOut[1]:\na    float64\nb    float64\nc    float64\ndtype: object \n```", "```py\nIn [58]: pd.get_dummies([\"a\", \"b\", \"a\", \"c\"]).dtypes\nOut[58]: \na    bool\nb    bool\nc    bool\nLength: 3, dtype: object \n```", "```py\nIn [59]: s = [\"1\", 2, 3]\n\nIn [60]: pd.to_numeric(s, downcast=\"unsigned\")\nOut[60]: array([1, 2, 3], dtype=uint8)\n\nIn [61]: pd.to_numeric(s, downcast=\"integer\")\nOut[61]: array([1, 2, 3], dtype=int8) \n```", "```py\nIn [62]: import pprint\n\nIn [63]: from pandas.api import types\n\nIn [64]: funcs = [f for f in dir(types) if not f.startswith(\"_\")]\n\nIn [65]: pprint.pprint(funcs)\n['CategoricalDtype',\n 'DatetimeTZDtype',\n 'IntervalDtype',\n 'PeriodDtype',\n 'infer_dtype',\n 'is_any_real_numeric_dtype',\n 'is_array_like',\n 'is_bool',\n 'is_bool_dtype',\n 'is_categorical_dtype',\n 'is_complex',\n 'is_complex_dtype',\n 'is_datetime64_any_dtype',\n 'is_datetime64_dtype',\n 'is_datetime64_ns_dtype',\n 'is_datetime64tz_dtype',\n 'is_dict_like',\n 'is_dtype_equal',\n 'is_extension_array_dtype',\n 'is_file_like',\n 'is_float',\n 'is_float_dtype',\n 'is_hashable',\n 'is_int64_dtype',\n 'is_integer',\n 'is_integer_dtype',\n 'is_interval',\n 'is_interval_dtype',\n 'is_iterator',\n 'is_list_like',\n 'is_named_tuple',\n 'is_number',\n 'is_numeric_dtype',\n 'is_object_dtype',\n 'is_period_dtype',\n 'is_re',\n 'is_re_compilable',\n 'is_scalar',\n 'is_signed_integer_dtype',\n 'is_sparse',\n 'is_string_dtype',\n 'is_timedelta64_dtype',\n 'is_timedelta64_ns_dtype',\n 'is_unsigned_integer_dtype',\n 'pandas_dtype',\n 'union_categoricals'] \n```", "```py\n    In [66]: pd.Timestamp(2012, 1, 1)\n    Out[66]: Timestamp('2012-01-01 00:00:00')\n\n    In [67]: pd.Timestamp(year=2012, month=1, day=1, hour=8, minute=30)\n    Out[67]: Timestamp('2012-01-01 08:30:00') \n    ```", "```py\n    In [68]: df = pd.DataFrame(\n     ....:    {\"date\": pd.date_range(\"2015-01-01\", freq=\"W\", periods=5), \"a\": np.arange(5)},\n     ....:    index=pd.MultiIndex.from_arrays(\n     ....:        [[1, 2, 3, 4, 5], pd.date_range(\"2015-01-01\", freq=\"W\", periods=5)],\n     ....:        names=[\"v\", \"d\"],\n     ....:    ),\n     ....: )\n     ....: \n\n    In [69]: df\n    Out[69]: \n     date  a\n    v d \n    1 2015-01-04 2015-01-04  0\n    2 2015-01-11 2015-01-11  1\n    3 2015-01-18 2015-01-18  2\n    4 2015-01-25 2015-01-25  3\n    5 2015-02-01 2015-02-01  4\n\n    [5 rows x 2 columns] \n    ```", "```py\n    In [74]: df.resample(\"M\", on=\"date\")[[\"a\"]].sum()\n    Out[74]:\n     a\n    date\n    2015-01-31  6\n    2015-02-28  4\n\n    [2 rows x 1 columns]\n\n    In [75]: df.resample(\"M\", level=\"d\")[[\"a\"]].sum()\n    Out[75]:\n     a\n    d\n    2015-01-31  6\n    2015-02-28  4\n\n    [2 rows x 1 columns] \n    ```", "```py\n    In [70]: df = pd.DataFrame({\"A\": [2, 7], \"B\": [3, 5], \"C\": [4, 8]}, index=[\"row1\", \"row2\"])\n\n    In [71]: df\n    Out[71]: \n     A  B  C\n    row1  2  3  4\n    row2  7  5  8\n\n    [2 rows x 3 columns]\n\n    In [72]: df.sort_values(by=\"row2\", axis=1)\n    Out[72]: \n     B  A  C\n    row1  3  2  4\n    row2  5  7  8\n\n    [2 rows x 3 columns] \n    ```", "```py\nIn [1]: left = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\n\nIn [2]: right = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n\nIn [3]: left\nOut[3]: \n a left_val\n0   1        a\n1   5        b\n2  10        c\n\n[3 rows x 2 columns]\n\nIn [4]: right\nOut[4]: \n a  right_val\n0  1          1\n1  2          2\n2  3          3\n3  6          6\n4  7          7\n\n[5 rows x 2 columns] \n```", "```py\nIn [5]: pd.merge_asof(left, right, on=\"a\")\nOut[5]: \n a left_val  right_val\n0   1        a          1\n1   5        b          3\n2  10        c          7\n\n[3 rows x 3 columns] \n```", "```py\nIn [6]: pd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\nOut[6]: \n a left_val  right_val\n0   1        a        NaN\n1   5        b        3.0\n2  10        c        7.0\n\n[3 rows x 3 columns] \n```", "```py\nIn [7]: trades = pd.DataFrame(\n ...:    {\n ...:        \"time\": pd.to_datetime(\n ...:            [\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.038\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.048\",\n ...:            ]\n ...:        ),\n ...:        \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n ...:        \"price\": [51.95, 51.95, 720.77, 720.92, 98.00],\n ...:        \"quantity\": [75, 155, 100, 100, 100],\n ...:    },\n ...:    columns=[\"time\", \"ticker\", \"price\", \"quantity\"],\n ...: )\n ...: \n\nIn [8]: quotes = pd.DataFrame(\n ...:    {\n ...:        \"time\": pd.to_datetime(\n ...:            [\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.023\",\n ...:                \"20160525 13:30:00.030\",\n ...:                \"20160525 13:30:00.041\",\n ...:                \"20160525 13:30:00.048\",\n ...:                \"20160525 13:30:00.049\",\n ...:                \"20160525 13:30:00.072\",\n ...:                \"20160525 13:30:00.075\",\n ...:            ]\n ...:        ),\n ...:        \"ticker\": [\"GOOG\", \"MSFT\", \"MSFT\", \"MSFT\", \"GOOG\", \"AAPL\", \"GOOG\", \"MSFT\"],\n ...:        \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n ...:        \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],\n ...:    },\n ...:    columns=[\"time\", \"ticker\", \"bid\", \"ask\"],\n ...: )\n ...: \n```", "```py\nIn [9]: trades\nOut[9]: \n time ticker   price  quantity\n0 2016-05-25 13:30:00.023   MSFT   51.95        75\n1 2016-05-25 13:30:00.038   MSFT   51.95       155\n2 2016-05-25 13:30:00.048   GOOG  720.77       100\n3 2016-05-25 13:30:00.048   GOOG  720.92       100\n4 2016-05-25 13:30:00.048   AAPL   98.00       100\n\n[5 rows x 4 columns]\n\nIn [10]: quotes\nOut[10]: \n time ticker     bid     ask\n0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n\n[8 rows x 4 columns] \n```", "```py\nIn [11]: pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\nOut[11]: \n time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n\n[5 rows x 6 columns] \n```", "```py\nIn [12]: dft = pd.DataFrame(\n ....:    {\"B\": [0, 1, 2, np.nan, 4]},\n ....:    index=pd.date_range(\"20130101 09:00:00\", periods=5, freq=\"s\"),\n ....: )\n ....: \n\nIn [13]: dft\nOut[13]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  2.0\n2013-01-01 09:00:03  NaN\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [14]: dft.rolling(2).sum()\nOut[14]: \n B\n2013-01-01 09:00:00  NaN\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  NaN\n2013-01-01 09:00:04  NaN\n\n[5 rows x 1 columns]\n\nIn [15]: dft.rolling(2, min_periods=1).sum()\nOut[15]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [16]: dft.rolling(\"2s\").sum()\nOut[16]: \n B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:04  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [17]: dft = pd.DataFrame(\n ....:    {\"B\": [0, 1, 2, np.nan, 4]},\n ....:    index=pd.Index(\n ....:        [\n ....:            pd.Timestamp(\"20130101 09:00:00\"),\n ....:            pd.Timestamp(\"20130101 09:00:02\"),\n ....:            pd.Timestamp(\"20130101 09:00:03\"),\n ....:            pd.Timestamp(\"20130101 09:00:05\"),\n ....:            pd.Timestamp(\"20130101 09:00:06\"),\n ....:        ],\n ....:        name=\"foo\",\n ....:    ),\n ....: )\n ....: \n\nIn [18]: dft\nOut[18]: \n B\nfoo \n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0\n\n[5 rows x 1 columns]\n\nIn [19]: dft.rolling(2).sum()\nOut[19]: \n B\nfoo \n2013-01-01 09:00:00  NaN\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  NaN\n\n[5 rows x 1 columns] \n```", "```py\nIn [20]: dft.rolling(\"2s\").sum()\nOut[20]: \n B\nfoo \n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0\n\n[5 rows x 1 columns] \n```", "```py\nIn [21]: dft = dft.reset_index()\n\nIn [22]: dft\nOut[22]: \n foo    B\n0 2013-01-01 09:00:00  0.0\n1 2013-01-01 09:00:02  1.0\n2 2013-01-01 09:00:03  2.0\n3 2013-01-01 09:00:05  NaN\n4 2013-01-01 09:00:06  4.0\n\n[5 rows x 2 columns]\n\nIn [23]: dft.rolling(\"2s\", on=\"foo\").sum()\nOut[23]: \n foo    B\n0 2013-01-01 09:00:00  0.0\n1 2013-01-01 09:00:02  1.0\n2 2013-01-01 09:00:03  3.0\n3 2013-01-01 09:00:05  NaN\n4 2013-01-01 09:00:06  4.0\n\n[5 rows x 2 columns] \n```", "```py\nIn [24]: data = \"0,1,2\\n3,4,5\"\n\nIn [25]: names = [\"a\", \"b\", \"a\"] \n```", "```py\nIn [2]: pd.read_csv(StringIO(data), names=names)\nOut[2]:\n a  b  a\n0  2  1  2\n1  5  4  5 \n```", "```py\nIn [26]: pd.read_csv(StringIO(data), names=names)\n---------------------------------------------------------------------------\nValueError  Traceback (most recent call last)\nCell In[26], line 1\n----> 1 pd.read_csv(StringIO(data), names=names)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n  1013 kwds_defaults = _refine_defaults_read(\n  1014     dialect,\n  1015     delimiter,\n   (...)\n  1022     dtype_backend=dtype_backend,\n  1023 )\n  1024 kwds.update(kwds_defaults)\n-> 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:617, in _read(filepath_or_buffer, kwds)\n  614 nrows = kwds.get(\"nrows\", None)\n  616 # Check for duplicates in names.\n--> 617 _validate_names(kwds.get(\"names\", None))\n  619 # Create the parser.\n  620 parser = TextFileReader(filepath_or_buffer, **kwds)\n\nFile ~/work/pandas/pandas/pandas/io/parsers/readers.py:576, in _validate_names(names)\n  574 if names is not None:\n  575     if len(names) != len(set(names)):\n--> 576         raise ValueError(\"Duplicate names are not allowed.\")\n  577     if not (\n  578         is_list_like(names, allow_sets=False) or isinstance(names, abc.KeysView)\n  579     ):\n  580         raise ValueError(\"Names should be an ordered collection.\")\n\nValueError: Duplicate names are not allowed. \n```", "```py\nIn [27]: data = \"\"\"\n ....: col1,col2,col3\n ....: a,b,1\n ....: a,b,2\n ....: c,d,3\n ....: \"\"\"\n ....: \n\nIn [28]: pd.read_csv(StringIO(data))\nOut[28]: \n col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\n[3 rows x 3 columns]\n\nIn [29]: pd.read_csv(StringIO(data)).dtypes\nOut[29]: \ncol1    object\ncol2    object\ncol3     int64\nLength: 3, dtype: object\n\nIn [30]: pd.read_csv(StringIO(data), dtype=\"category\").dtypes\nOut[30]: \ncol1    category\ncol2    category\ncol3    category\nLength: 3, dtype: object \n```", "```py\nIn [31]: pd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\nOut[31]: \ncol1    category\ncol2      object\ncol3       int64\nLength: 3, dtype: object \n```", "```py\nIn [32]: df = pd.read_csv(StringIO(data), dtype=\"category\")\n\nIn [33]: df.dtypes\nOut[33]: \ncol1    category\ncol2    category\ncol3    category\nLength: 3, dtype: object\n\nIn [34]: df[\"col3\"]\nOut[34]: \n0    1\n1    2\n2    3\nName: col3, Length: 3, dtype: category\nCategories (3, object): ['1', '2', '3']\n\nIn [35]: new_categories = pd.to_numeric(df[\"col3\"].cat.categories)\n\nIn [36]: df[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\n\nIn [37]: df[\"col3\"]\nOut[37]: \n0    1\n1    2\n2    3\nName: col3, Length: 3, dtype: category\nCategories (3, int64): [1, 2, 3] \n```", "```py\n    In [38]: from pandas.api.types import union_categoricals\n\n    In [39]: a = pd.Categorical([\"b\", \"c\"])\n\n    In [40]: b = pd.Categorical([\"a\", \"b\"])\n\n    In [41]: union_categoricals([a, b])\n    Out[41]: \n    ['b', 'c', 'a', 'b']\n    Categories (3, object): ['b', 'c', 'a'] \n    ```", "```py\n    In [42]: s1 = pd.Series([\"a\", \"b\"], dtype=\"category\")\n\n    In [43]: s2 = pd.Series([\"b\", \"c\"], dtype=\"category\") \n    ```", "```py\nIn [1]: pd.concat([s1, s2])\nValueError: incompatible categories in categorical concat \n```", "```py\nIn [44]: pd.concat([s1, s2])\nOut[44]: \n0    a\n1    b\n0    b\n1    c\nLength: 4, dtype: object \n```", "```py\nIn [45]: from pandas.tseries.offsets import SemiMonthEnd, SemiMonthBegin \n```", "```py\nIn [46]: pd.Timestamp(\"2016-01-01\") + SemiMonthEnd()\nOut[46]: Timestamp('2016-01-15 00:00:00')\n\nIn [47]: pd.date_range(\"2015-01-01\", freq=\"SM\", periods=4)\nOut[47]: DatetimeIndex(['2015-01-15', '2015-01-31', '2015-02-15', '2015-02-28'], dtype='datetime64[ns]', freq='SM-15') \n```", "```py\nIn [46]: pd.Timestamp(\"2016-01-01\") + SemiMonthBegin()\nOut[46]: Timestamp('2016-01-15 00:00:00')\n\nIn [47]: pd.date_range(\"2015-01-01\", freq=\"SMS\", periods=4)\nOut[47]: DatetimeIndex(['2015-01-01', '2015-01-15', '2015-02-01', '2015-02-15'], dtype='datetime64[ns]', freq='SMS-15') \n```", "```py\nIn [50]: pd.date_range(\"2015-01-01\", freq=\"SMS-16\", periods=4)\nOut[50]: DatetimeIndex(['2015-01-01', '2015-01-16', '2015-02-01', '2015-02-16'], dtype='datetime64[ns]', freq='SMS-16')\n\nIn [51]: pd.date_range(\"2015-01-01\", freq=\"SM-14\", periods=4)\nOut[51]: DatetimeIndex(['2015-01-14', '2015-01-31', '2015-02-14', '2015-02-28'], dtype='datetime64[ns]', freq='SM-14') \n```", "```py\nIn [48]: idx = pd.Index([\"a\", \"b\", \"c\"])\n\nIn [49]: idx.where([True, False, True])\nOut[49]: Index(['a', None, 'c'], dtype='object') \n```", "```py\nIn [50]: idx = pd.Index([1, 2, np.nan, 4])\n\nIn [51]: idx.dropna()\nOut[51]: Index([1.0, 2.0, 4.0], dtype='float64') \n```", "```py\nIn [52]: midx = pd.MultiIndex.from_arrays([[1, 2, np.nan, 4], [1, 2, np.nan, np.nan]])\n\nIn [53]: midx\nOut[53]: \nMultiIndex([(1.0, 1.0),\n (2.0, 2.0),\n (nan, nan),\n (4.0, nan)],\n )\n\nIn [54]: midx.dropna()\nOut[54]: \nMultiIndex([(1, 1),\n (2, 2)],\n )\n\nIn [55]: midx.dropna(how=\"all\")\nOut[55]: \nMultiIndex([(1, 1.0),\n (2, 2.0),\n (4, nan)],\n ) \n```", "```py\nIn [56]: idx = pd.Index([\"a1a2\", \"b1\", \"c1\"])\n\nIn [57]: idx.str.extractall(r\"[ab](?P<digit>\\d)\")\nOut[57]: \n digit\n match \n0 0         1\n 1         2\n1 0         1\n\n[3 rows x 1 columns] \n```", "```py\nIn [1]: pd.get_dummies(['a', 'b', 'a', 'c']).dtypes\n\nOut[1]:\na    float64\nb    float64\nc    float64\ndtype: object \n```", "```py\nIn [58]: pd.get_dummies([\"a\", \"b\", \"a\", \"c\"]).dtypes\nOut[58]: \na    bool\nb    bool\nc    bool\nLength: 3, dtype: object \n```", "```py\nIn [59]: s = [\"1\", 2, 3]\n\nIn [60]: pd.to_numeric(s, downcast=\"unsigned\")\nOut[60]: array([1, 2, 3], dtype=uint8)\n\nIn [61]: pd.to_numeric(s, downcast=\"integer\")\nOut[61]: array([1, 2, 3], dtype=int8) \n```", "```py\nIn [62]: import pprint\n\nIn [63]: from pandas.api import types\n\nIn [64]: funcs = [f for f in dir(types) if not f.startswith(\"_\")]\n\nIn [65]: pprint.pprint(funcs)\n['CategoricalDtype',\n 'DatetimeTZDtype',\n 'IntervalDtype',\n 'PeriodDtype',\n 'infer_dtype',\n 'is_any_real_numeric_dtype',\n 'is_array_like',\n 'is_bool',\n 'is_bool_dtype',\n 'is_categorical_dtype',\n 'is_complex',\n 'is_complex_dtype',\n 'is_datetime64_any_dtype',\n 'is_datetime64_dtype',\n 'is_datetime64_ns_dtype',\n 'is_datetime64tz_dtype',\n 'is_dict_like',\n 'is_dtype_equal',\n 'is_extension_array_dtype',\n 'is_file_like',\n 'is_float',\n 'is_float_dtype',\n 'is_hashable',\n 'is_int64_dtype',\n 'is_integer',\n 'is_integer_dtype',\n 'is_interval',\n 'is_interval_dtype',\n 'is_iterator',\n 'is_list_like',\n 'is_named_tuple',\n 'is_number',\n 'is_numeric_dtype',\n 'is_object_dtype',\n 'is_period_dtype',\n 'is_re',\n 'is_re_compilable',\n 'is_scalar',\n 'is_signed_integer_dtype',\n 'is_sparse',\n 'is_string_dtype',\n 'is_timedelta64_dtype',\n 'is_timedelta64_ns_dtype',\n 'is_unsigned_integer_dtype',\n 'pandas_dtype',\n 'union_categoricals'] \n```", "```py\n    In [66]: pd.Timestamp(2012, 1, 1)\n    Out[66]: Timestamp('2012-01-01 00:00:00')\n\n    In [67]: pd.Timestamp(year=2012, month=1, day=1, hour=8, minute=30)\n    Out[67]: Timestamp('2012-01-01 08:30:00') \n    ```", "```py\n    In [68]: df = pd.DataFrame(\n     ....:    {\"date\": pd.date_range(\"2015-01-01\", freq=\"W\", periods=5), \"a\": np.arange(5)},\n     ....:    index=pd.MultiIndex.from_arrays(\n     ....:        [[1, 2, 3, 4, 5], pd.date_range(\"2015-01-01\", freq=\"W\", periods=5)],\n     ....:        names=[\"v\", \"d\"],\n     ....:    ),\n     ....: )\n     ....: \n\n    In [69]: df\n    Out[69]: \n     date  a\n    v d \n    1 2015-01-04 2015-01-04  0\n    2 2015-01-11 2015-01-11  1\n    3 2015-01-18 2015-01-18  2\n    4 2015-01-25 2015-01-25  3\n    5 2015-02-01 2015-02-01  4\n\n    [5 rows x 2 columns] \n    ```", "```py\n    In [74]: df.resample(\"M\", on=\"date\")[[\"a\"]].sum()\n    Out[74]:\n     a\n    date\n    2015-01-31  6\n    2015-02-28  4\n\n    [2 rows x 1 columns]\n\n    In [75]: df.resample(\"M\", level=\"d\")[[\"a\"]].sum()\n    Out[75]:\n     a\n    d\n    2015-01-31  6\n    2015-02-28  4\n\n    [2 rows x 1 columns] \n    ```", "```py\n    In [70]: df = pd.DataFrame({\"A\": [2, 7], \"B\": [3, 5], \"C\": [4, 8]}, index=[\"row1\", \"row2\"])\n\n    In [71]: df\n    Out[71]: \n     A  B  C\n    row1  2  3  4\n    row2  7  5  8\n\n    [2 rows x 3 columns]\n\n    In [72]: df.sort_values(by=\"row2\", axis=1)\n    Out[72]: \n     B  A  C\n    row1  3  2  4\n    row2  5  7  8\n\n    [2 rows x 3 columns] \n    ```", "```py\nIn [73]: s = pd.Series([1, 2, 3]) \n```", "```py\nIn [7]: type(s.tolist()[0])\nOut[7]:\n <class 'numpy.int64'> \n```", "```py\nIn [74]: type(s.tolist()[0])\nOut[74]: int \n```", "```py\nIn [75]: s1 = pd.Series([1, 2, 3], index=list(\"ABC\"))\n\nIn [76]: s2 = pd.Series([2, 2, 2], index=list(\"ABD\"))\n\nIn [77]: s1 + s2\nOut[77]: \nA    3.0\nB    4.0\nC    NaN\nD    NaN\nLength: 4, dtype: float64\n\nIn [78]: df1 = pd.DataFrame([1, 2, 3], index=list(\"ABC\"))\n\nIn [79]: df2 = pd.DataFrame([2, 2, 2], index=list(\"ABD\"))\n\nIn [80]: df1 + df2\nOut[80]: \n 0\nA  3.0\nB  4.0\nC  NaN\nD  NaN\n\n[4 rows x 1 columns] \n```", "```py\nIn [1]: s1 == s2\nOut[1]:\nA    False\nB     True\nC    False\ndtype: bool \n```", "```py\nIn [2]: s1 == s2\nOut[2]:\nValueError: Can only compare identically-labeled Series objects \n```", "```py\nIn [81]: s1.values == s2.values\nOut[81]: array([False,  True, False]) \n```", "```py\nIn [82]: s1.eq(s2)\nOut[82]: \nA    False\nB     True\nC    False\nD    False\nLength: 4, dtype: bool \n```", "```py\nIn [3]: df1 == df2\nOut[3]:\nValueError: Can only compare identically-labeled DataFrame objects \n```", "```py\nIn [4]: s1 = pd.Series([True, False, True], index=list('ABC'))\nIn [5]: s2 = pd.Series([True, True, True], index=list('ABD'))\nIn [6]: s1 & s2\nOut[6]:\nA     True\nB    False\nC    False\ndtype: bool \n```", "```py\nIn [83]: s1 = pd.Series([True, False, True], index=list(\"ABC\"))\n\nIn [84]: s2 = pd.Series([True, True, True], index=list(\"ABD\"))\n\nIn [85]: s1 & s2\nOut[85]: \nA     True\nB    False\nC    False\nD    False\nLength: 4, dtype: bool \n```", "```py\nIn [86]: s1 & s2.reindex_like(s1)\nOut[86]: \nA     True\nB    False\nC    False\nLength: 3, dtype: bool \n```", "```py\nIn [87]: df1 = pd.DataFrame([True, False, True], index=list(\"ABC\"))\n\nIn [88]: df2 = pd.DataFrame([True, True, True], index=list(\"ABD\"))\n\nIn [89]: df1 & df2\nOut[89]: \n 0\nA   True\nB  False\nC  False\nD  False\n\n[4 rows x 1 columns] \n```", "```py\nIn [90]: s1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n\nIn [91]: s2 = pd.Series([2, 2, 2], index=[\"b\", \"c\", \"d\"])\n\nIn [92]: s1.eq(s2)\nOut[92]: \na    False\nb     True\nc    False\nd    False\nLength: 4, dtype: bool\n\nIn [93]: s1.ge(s2)\nOut[93]: \na    False\nb     True\nc     True\nd    False\nLength: 4, dtype: bool \n```", "```py\nIn [94]: s = pd.Series() \n```", "```py\nIn [2]: s[\"a\"] = pd.Timestamp(\"2016-01-01\")\n\nIn [3]: s[\"b\"] = 3.0\nTypeError: invalid type promotion \n```", "```py\nIn [95]: s[\"a\"] = pd.Timestamp(\"2016-01-01\")\n\nIn [96]: s[\"b\"] = 3.0\n\nIn [97]: s\nOut[97]: \na    2016-01-01 00:00:00\nb                    3.0\nLength: 2, dtype: object\n\nIn [98]: s.dtype\nOut[98]: dtype('O') \n```", "```py\nIn [2]: pd.to_datetime([1, 'foo'], errors='coerce')\nOut[2]: DatetimeIndex(['NaT', 'NaT'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [99]: pd.to_datetime([1, \"foo\"], errors=\"coerce\")\nOut[99]: DatetimeIndex(['1970-01-01 00:00:00.000000001', 'NaT'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [100]: df1 = pd.DataFrame({\"key\": [1], \"v1\": [10]})\n\nIn [101]: df1\nOut[101]: \n key  v1\n0    1  10\n\n[1 rows x 2 columns]\n\nIn [102]: df2 = pd.DataFrame({\"key\": [1, 2], \"v1\": [20, 30]})\n\nIn [103]: df2\nOut[103]: \n key  v1\n0    1  20\n1    2  30\n\n[2 rows x 2 columns] \n```", "```py\nIn [5]: pd.merge(df1, df2, how='outer')\nOut[5]:\n key    v1\n0  1.0  10.0\n1  1.0  20.0\n2  2.0  30.0\n\nIn [6]: pd.merge(df1, df2, how='outer').dtypes\nOut[6]:\nkey    float64\nv1     float64\ndtype: object \n```", "```py\nIn [104]: pd.merge(df1, df2, how=\"outer\")\nOut[104]: \n key  v1\n0    1  10\n1    1  20\n2    2  30\n\n[3 rows x 2 columns]\n\nIn [105]: pd.merge(df1, df2, how=\"outer\").dtypes\nOut[105]: \nkey    int64\nv1     int64\nLength: 2, dtype: object \n```", "```py\nIn [106]: pd.merge(df1, df2, how=\"outer\", on=\"key\")\nOut[106]: \n key  v1_x  v1_y\n0    1  10.0    20\n1    2   NaN    30\n\n[2 rows x 3 columns]\n\nIn [107]: pd.merge(df1, df2, how=\"outer\", on=\"key\").dtypes\nOut[107]: \nkey       int64\nv1_x    float64\nv1_y      int64\nLength: 3, dtype: object \n```", "```py\nIn [108]: s = pd.Series([0, 1, 2, 3, 4])\n\nIn [109]: df = pd.DataFrame([0, 1, 2, 3, 4]) \n```", "```py\nIn [3]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[3]:\ncount     5.000000\nmean      2.000000\nstd       1.581139\nmin       0.000000\n0.0%      0.000400\n0.1%      0.002000\n0.1%      0.004000\n50%       2.000000\n99.9%     3.996000\n100.0%    3.998000\n100.0%    3.999600\nmax       4.000000\ndtype: float64\n\nIn [4]: df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[4]:\n...\nValueError: cannot reindex from a duplicate axis \n```", "```py\nIn [110]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[110]: \ncount     5.000000\nmean      2.000000\nstd       1.581139\nmin       0.000000\n0.01%     0.000400\n0.05%     0.002000\n0.1%      0.004000\n50%       2.000000\n99.9%     3.996000\n99.95%    3.998000\n99.99%    3.999600\nmax       4.000000\nLength: 12, dtype: float64\n\nIn [111]: df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[111]: \n 0\ncount   5.000000\nmean    2.000000\nstd     1.581139\nmin     0.000000\n0.01%   0.000400\n0.05%   0.002000\n0.1%    0.004000\n50%     2.000000\n99.9%   3.996000\n99.95%  3.998000\n99.99%  3.999600\nmax     4.000000\n\n[12 rows x 1 columns] \n```", "```py\nIn [1]: pi = pd.PeriodIndex(['2016-08-01'], freq='D')\n\nIn [2]: pi\nOut[2]: PeriodIndex(['2016-08-01'], dtype='int64', freq='D')\n\nIn [3]: pd.api.types.is_integer_dtype(pi)\nOut[3]: True\n\nIn [4]: pi.dtype\nOut[4]: dtype('int64') \n```", "```py\nIn [112]: pi = pd.PeriodIndex([\"2016-08-01\"], freq=\"D\")\n\nIn [113]: pi\nOut[113]: PeriodIndex(['2016-08-01'], dtype='period[D]')\n\nIn [114]: pd.api.types.is_integer_dtype(pi)\nOut[114]: False\n\nIn [115]: pd.api.types.is_period_dtype(pi)\nOut[115]: True\n\nIn [116]: pi.dtype\nOut[116]: period[D]\n\nIn [117]: type(pi.dtype)\nOut[117]: pandas.core.dtypes.dtypes.PeriodDtype \n```", "```py\nIn [5]: pd.Period('NaT', freq='D')\nOut[5]: Period('NaT', 'D') \n```", "```py\nIn [118]: pd.Period(\"NaT\")\nOut[118]: NaT\n\nIn [119]: pd.Period(None)\nOut[119]: NaT \n```", "```py\nIn [5]: pd.NaT + 1\n...\nValueError: Cannot add integral value to Timestamp without freq. \n```", "```py\nIn [120]: pd.NaT + 1\nOut[120]: NaT\n\nIn [121]: pd.NaT - 1\nOut[121]: NaT \n```", "```py\nIn [6]: pi = pd.PeriodIndex(['2011-01', '2011-02'], freq='M')\nIn [7]: pi.values\nOut[7]: array([492, 493]) \n```", "```py\nIn [122]: pi = pd.PeriodIndex([\"2011-01\", \"2011-02\"], freq=\"M\")\n\nIn [123]: pi.values\nOut[123]: array([Period('2011-01', 'M'), Period('2011-02', 'M')], dtype=object) \n```", "```py\nIn [1]: pd.Index(['a', 'b']) + pd.Index(['a', 'c'])\nFutureWarning: using '+' to provide set union with Indexes is deprecated, use '|' or .union()\nOut[1]: Index(['a', 'b', 'c'], dtype='object') \n```", "```py\nIn [124]: pd.Index([\"a\", \"b\"]) + pd.Index([\"a\", \"c\"])\nOut[124]: Index(['aa', 'bc'], dtype='object') \n```", "```py\nIn [125]: pd.Index([1, 2, 3]) + pd.Index([2, 3, 4])\nOut[125]: Index([3, 5, 7], dtype='int64') \n```", "```py\nIn [1]: (pd.DatetimeIndex(['2016-01-01', '2016-01-02'])\n ...: - pd.DatetimeIndex(['2016-01-02', '2016-01-03']))\nFutureWarning: using '-' to provide set differences with datetimelike Indexes is deprecated, use .difference()\nOut[1]: DatetimeIndex(['2016-01-01'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [126]: (\n .....:    pd.DatetimeIndex([\"2016-01-01\", \"2016-01-02\"])\n .....:    - pd.DatetimeIndex([\"2016-01-02\", \"2016-01-03\"])\n .....: )\n .....: \nOut[126]: TimedeltaIndex(['-1 days', '-1 days'], dtype='timedelta64[ns]', freq=None) \n```", "```py\nIn [127]: idx1 = pd.Index([1, 2, 3, np.nan])\n\nIn [128]: idx2 = pd.Index([0, 1, np.nan]) \n```", "```py\nIn [3]: idx1.difference(idx2)\nOut[3]: Float64Index([nan, 2.0, 3.0], dtype='float64')\n\nIn [4]: idx1.symmetric_difference(idx2)\nOut[4]: Float64Index([0.0, nan, 2.0, 3.0], dtype='float64') \n```", "```py\nIn [129]: idx1.difference(idx2)\nOut[129]: Index([2.0, 3.0], dtype='float64')\n\nIn [130]: idx1.symmetric_difference(idx2)\nOut[130]: Index([0.0, 2.0, 3.0], dtype='float64') \n```", "```py\nIn [1]: pd.Index([1, 2, 3]).unique()\nOut[1]: array([1, 2, 3])\n\nIn [2]: pd.DatetimeIndex(['2011-01-01', '2011-01-02',\n ...:                  '2011-01-03'], tz='Asia/Tokyo').unique()\nOut[2]:\nDatetimeIndex(['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00',\n '2011-01-03 00:00:00+09:00'],\n dtype='datetime64[ns, Asia/Tokyo]', freq=None) \n```", "```py\nIn [131]: pd.Index([1, 2, 3]).unique()\nOut[131]: Index([1, 2, 3], dtype='int64')\n\nIn [132]: pd.DatetimeIndex(\n .....:    [\"2011-01-01\", \"2011-01-02\", \"2011-01-03\"], tz=\"Asia/Tokyo\"\n .....: ).unique()\n .....: \nOut[132]: \nDatetimeIndex(['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00',\n '2011-01-03 00:00:00+09:00'],\n dtype='datetime64[ns, Asia/Tokyo]', freq=None) \n```", "```py\nIn [133]: cat = pd.Categorical([\"a\", \"b\"], categories=list(\"bac\"))\n\nIn [134]: lvl1 = [\"foo\", \"bar\"]\n\nIn [135]: midx = pd.MultiIndex.from_arrays([cat, lvl1])\n\nIn [136]: midx\nOut[136]: \nMultiIndex([('a', 'foo'),\n ('b', 'bar')],\n ) \n```", "```py\nIn [4]: midx.levels[0]\nOut[4]: Index(['b', 'a', 'c'], dtype='object')\n\nIn [5]: midx.get_level_values[0]\nOut[5]: Index(['a', 'b'], dtype='object') \n```", "```py\nIn [137]: midx.levels[0]\nOut[137]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category')\n\nIn [138]: midx.get_level_values(0)\nOut[138]: CategoricalIndex(['a', 'b'], categories=['b', 'a', 'c'], ordered=False, dtype='category') \n```", "```py\nIn [139]: df = pd.DataFrame({\"A\": [0, 1], \"B\": [10, 11], \"C\": cat})\n\nIn [140]: df_grouped = df.groupby(by=[\"A\", \"C\"], observed=False).first()\n\nIn [141]: df_set_idx = df.set_index([\"A\", \"C\"]) \n```", "```py\nIn [11]: df_grouped.index.levels[1]\nOut[11]: Index(['b', 'a', 'c'], dtype='object', name='C')\nIn [12]: df_grouped.reset_index().dtypes\nOut[12]:\nA      int64\nC     object\nB    float64\ndtype: object\n\nIn [13]: df_set_idx.index.levels[1]\nOut[13]: Index(['b', 'a', 'c'], dtype='object', name='C')\nIn [14]: df_set_idx.reset_index().dtypes\nOut[14]:\nA      int64\nC     object\nB      int64\ndtype: object \n```", "```py\nIn [142]: df_grouped.index.levels[1]\nOut[142]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category', name='C')\n\nIn [143]: df_grouped.reset_index().dtypes\nOut[143]: \nA       int64\nC    category\nB     float64\nLength: 3, dtype: object\n\nIn [144]: df_set_idx.index.levels[1]\nOut[144]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category', name='C')\n\nIn [145]: df_set_idx.reset_index().dtypes\nOut[145]: \nA       int64\nC    category\nB       int64\nLength: 3, dtype: object \n```", "```py\nIn [146]: data = \"A,B\\n0,1\\n2,3\\n4,5\\n6,7\" \n```", "```py\nIn [2]: pd.concat(pd.read_csv(StringIO(data), chunksize=2))\nOut[2]:\n A  B\n0  0  1\n1  2  3\n0  4  5\n1  6  7 \n```", "```py\nIn [147]: pd.concat(pd.read_csv(StringIO(data), chunksize=2))\nOut[147]: \n A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n\n[4 rows x 2 columns] \n```", "```py\nIn [1]: pd.SparseArray([1, 2, 0, 0])\nOut[1]:\n[1.0, 2.0, 0.0, 0.0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n\n# specifying int64 dtype, but all values are stored in sp_values because\n# fill_value default is np.nan\nIn [2]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[2]:\n[1, 2, 0, 0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n\nIn [3]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64, fill_value=0)\nOut[3]:\n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32) \n```", "```py\nIn [148]: pd.arrays.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[148]: \n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32)\n\nIn [149]: pd.arrays.SparseArray([True, False, False, False])\nOut[149]: \n[True, False, False, False]\nFill: False\nIntIndex\nIndices: array([0], dtype=int32) \n```", "```py\ns = pd.SparseSeries([0, 2, 0, 1], fill_value=0, dtype=np.int64)\ns.dtype\n\ns + 1 \n```", "```py\ns = pd.SparseSeries([1.0, 0.0, 2.0, 0.0], fill_value=0)\ns\ns.astype(np.int64) \n```", "```py\nIn [7]: pd.SparseSeries([1., np.nan, 2., np.nan], fill_value=np.nan).astype(np.int64)\nOut[7]:\nValueError: unable to coerce current fill_value nan to int64 dtype \n```", "```py\nIn [1]: i = pd.Index(['a', 'b', 'c'])\n\nIn [2]: i.get_indexer(['b', 'b', 'c']).dtype\nOut[2]: dtype('int32') \n```", "```py\nIn [1]: i = pd.Index(['a', 'b', 'c'])\n\nIn [2]: i.get_indexer(['b', 'b', 'c']).dtype\nOut[2]: dtype('int64') \n```", "```py\nIn [73]: s = pd.Series([1, 2, 3]) \n```", "```py\nIn [7]: type(s.tolist()[0])\nOut[7]:\n <class 'numpy.int64'> \n```", "```py\nIn [74]: type(s.tolist()[0])\nOut[74]: int \n```", "```py\nIn [75]: s1 = pd.Series([1, 2, 3], index=list(\"ABC\"))\n\nIn [76]: s2 = pd.Series([2, 2, 2], index=list(\"ABD\"))\n\nIn [77]: s1 + s2\nOut[77]: \nA    3.0\nB    4.0\nC    NaN\nD    NaN\nLength: 4, dtype: float64\n\nIn [78]: df1 = pd.DataFrame([1, 2, 3], index=list(\"ABC\"))\n\nIn [79]: df2 = pd.DataFrame([2, 2, 2], index=list(\"ABD\"))\n\nIn [80]: df1 + df2\nOut[80]: \n 0\nA  3.0\nB  4.0\nC  NaN\nD  NaN\n\n[4 rows x 1 columns] \n```", "```py\nIn [1]: s1 == s2\nOut[1]:\nA    False\nB     True\nC    False\ndtype: bool \n```", "```py\nIn [2]: s1 == s2\nOut[2]:\nValueError: Can only compare identically-labeled Series objects \n```", "```py\nIn [81]: s1.values == s2.values\nOut[81]: array([False,  True, False]) \n```", "```py\nIn [82]: s1.eq(s2)\nOut[82]: \nA    False\nB     True\nC    False\nD    False\nLength: 4, dtype: bool \n```", "```py\nIn [3]: df1 == df2\nOut[3]:\nValueError: Can only compare identically-labeled DataFrame objects \n```", "```py\nIn [4]: s1 = pd.Series([True, False, True], index=list('ABC'))\nIn [5]: s2 = pd.Series([True, True, True], index=list('ABD'))\nIn [6]: s1 & s2\nOut[6]:\nA     True\nB    False\nC    False\ndtype: bool \n```", "```py\nIn [83]: s1 = pd.Series([True, False, True], index=list(\"ABC\"))\n\nIn [84]: s2 = pd.Series([True, True, True], index=list(\"ABD\"))\n\nIn [85]: s1 & s2\nOut[85]: \nA     True\nB    False\nC    False\nD    False\nLength: 4, dtype: bool \n```", "```py\nIn [86]: s1 & s2.reindex_like(s1)\nOut[86]: \nA     True\nB    False\nC    False\nLength: 3, dtype: bool \n```", "```py\nIn [87]: df1 = pd.DataFrame([True, False, True], index=list(\"ABC\"))\n\nIn [88]: df2 = pd.DataFrame([True, True, True], index=list(\"ABD\"))\n\nIn [89]: df1 & df2\nOut[89]: \n 0\nA   True\nB  False\nC  False\nD  False\n\n[4 rows x 1 columns] \n```", "```py\nIn [90]: s1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n\nIn [91]: s2 = pd.Series([2, 2, 2], index=[\"b\", \"c\", \"d\"])\n\nIn [92]: s1.eq(s2)\nOut[92]: \na    False\nb     True\nc    False\nd    False\nLength: 4, dtype: bool\n\nIn [93]: s1.ge(s2)\nOut[93]: \na    False\nb     True\nc     True\nd    False\nLength: 4, dtype: bool \n```", "```py\nIn [75]: s1 = pd.Series([1, 2, 3], index=list(\"ABC\"))\n\nIn [76]: s2 = pd.Series([2, 2, 2], index=list(\"ABD\"))\n\nIn [77]: s1 + s2\nOut[77]: \nA    3.0\nB    4.0\nC    NaN\nD    NaN\nLength: 4, dtype: float64\n\nIn [78]: df1 = pd.DataFrame([1, 2, 3], index=list(\"ABC\"))\n\nIn [79]: df2 = pd.DataFrame([2, 2, 2], index=list(\"ABD\"))\n\nIn [80]: df1 + df2\nOut[80]: \n 0\nA  3.0\nB  4.0\nC  NaN\nD  NaN\n\n[4 rows x 1 columns] \n```", "```py\nIn [1]: s1 == s2\nOut[1]:\nA    False\nB     True\nC    False\ndtype: bool \n```", "```py\nIn [2]: s1 == s2\nOut[2]:\nValueError: Can only compare identically-labeled Series objects \n```", "```py\nIn [81]: s1.values == s2.values\nOut[81]: array([False,  True, False]) \n```", "```py\nIn [82]: s1.eq(s2)\nOut[82]: \nA    False\nB     True\nC    False\nD    False\nLength: 4, dtype: bool \n```", "```py\nIn [3]: df1 == df2\nOut[3]:\nValueError: Can only compare identically-labeled DataFrame objects \n```", "```py\nIn [4]: s1 = pd.Series([True, False, True], index=list('ABC'))\nIn [5]: s2 = pd.Series([True, True, True], index=list('ABD'))\nIn [6]: s1 & s2\nOut[6]:\nA     True\nB    False\nC    False\ndtype: bool \n```", "```py\nIn [83]: s1 = pd.Series([True, False, True], index=list(\"ABC\"))\n\nIn [84]: s2 = pd.Series([True, True, True], index=list(\"ABD\"))\n\nIn [85]: s1 & s2\nOut[85]: \nA     True\nB    False\nC    False\nD    False\nLength: 4, dtype: bool \n```", "```py\nIn [86]: s1 & s2.reindex_like(s1)\nOut[86]: \nA     True\nB    False\nC    False\nLength: 3, dtype: bool \n```", "```py\nIn [87]: df1 = pd.DataFrame([True, False, True], index=list(\"ABC\"))\n\nIn [88]: df2 = pd.DataFrame([True, True, True], index=list(\"ABD\"))\n\nIn [89]: df1 & df2\nOut[89]: \n 0\nA   True\nB  False\nC  False\nD  False\n\n[4 rows x 1 columns] \n```", "```py\nIn [90]: s1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n\nIn [91]: s2 = pd.Series([2, 2, 2], index=[\"b\", \"c\", \"d\"])\n\nIn [92]: s1.eq(s2)\nOut[92]: \na    False\nb     True\nc    False\nd    False\nLength: 4, dtype: bool\n\nIn [93]: s1.ge(s2)\nOut[93]: \na    False\nb     True\nc     True\nd    False\nLength: 4, dtype: bool \n```", "```py\nIn [94]: s = pd.Series() \n```", "```py\nIn [2]: s[\"a\"] = pd.Timestamp(\"2016-01-01\")\n\nIn [3]: s[\"b\"] = 3.0\nTypeError: invalid type promotion \n```", "```py\nIn [95]: s[\"a\"] = pd.Timestamp(\"2016-01-01\")\n\nIn [96]: s[\"b\"] = 3.0\n\nIn [97]: s\nOut[97]: \na    2016-01-01 00:00:00\nb                    3.0\nLength: 2, dtype: object\n\nIn [98]: s.dtype\nOut[98]: dtype('O') \n```", "```py\nIn [2]: pd.to_datetime([1, 'foo'], errors='coerce')\nOut[2]: DatetimeIndex(['NaT', 'NaT'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [99]: pd.to_datetime([1, \"foo\"], errors=\"coerce\")\nOut[99]: DatetimeIndex(['1970-01-01 00:00:00.000000001', 'NaT'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [100]: df1 = pd.DataFrame({\"key\": [1], \"v1\": [10]})\n\nIn [101]: df1\nOut[101]: \n key  v1\n0    1  10\n\n[1 rows x 2 columns]\n\nIn [102]: df2 = pd.DataFrame({\"key\": [1, 2], \"v1\": [20, 30]})\n\nIn [103]: df2\nOut[103]: \n key  v1\n0    1  20\n1    2  30\n\n[2 rows x 2 columns] \n```", "```py\nIn [5]: pd.merge(df1, df2, how='outer')\nOut[5]:\n key    v1\n0  1.0  10.0\n1  1.0  20.0\n2  2.0  30.0\n\nIn [6]: pd.merge(df1, df2, how='outer').dtypes\nOut[6]:\nkey    float64\nv1     float64\ndtype: object \n```", "```py\nIn [104]: pd.merge(df1, df2, how=\"outer\")\nOut[104]: \n key  v1\n0    1  10\n1    1  20\n2    2  30\n\n[3 rows x 2 columns]\n\nIn [105]: pd.merge(df1, df2, how=\"outer\").dtypes\nOut[105]: \nkey    int64\nv1     int64\nLength: 2, dtype: object \n```", "```py\nIn [106]: pd.merge(df1, df2, how=\"outer\", on=\"key\")\nOut[106]: \n key  v1_x  v1_y\n0    1  10.0    20\n1    2   NaN    30\n\n[2 rows x 3 columns]\n\nIn [107]: pd.merge(df1, df2, how=\"outer\", on=\"key\").dtypes\nOut[107]: \nkey       int64\nv1_x    float64\nv1_y      int64\nLength: 3, dtype: object \n```", "```py\nIn [108]: s = pd.Series([0, 1, 2, 3, 4])\n\nIn [109]: df = pd.DataFrame([0, 1, 2, 3, 4]) \n```", "```py\nIn [3]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[3]:\ncount     5.000000\nmean      2.000000\nstd       1.581139\nmin       0.000000\n0.0%      0.000400\n0.1%      0.002000\n0.1%      0.004000\n50%       2.000000\n99.9%     3.996000\n100.0%    3.998000\n100.0%    3.999600\nmax       4.000000\ndtype: float64\n\nIn [4]: df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[4]:\n...\nValueError: cannot reindex from a duplicate axis \n```", "```py\nIn [110]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[110]: \ncount     5.000000\nmean      2.000000\nstd       1.581139\nmin       0.000000\n0.01%     0.000400\n0.05%     0.002000\n0.1%      0.004000\n50%       2.000000\n99.9%     3.996000\n99.95%    3.998000\n99.99%    3.999600\nmax       4.000000\nLength: 12, dtype: float64\n\nIn [111]: df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[111]: \n 0\ncount   5.000000\nmean    2.000000\nstd     1.581139\nmin     0.000000\n0.01%   0.000400\n0.05%   0.002000\n0.1%    0.004000\n50%     2.000000\n99.9%   3.996000\n99.95%  3.998000\n99.99%  3.999600\nmax     4.000000\n\n[12 rows x 1 columns] \n```", "```py\nIn [1]: pi = pd.PeriodIndex(['2016-08-01'], freq='D')\n\nIn [2]: pi\nOut[2]: PeriodIndex(['2016-08-01'], dtype='int64', freq='D')\n\nIn [3]: pd.api.types.is_integer_dtype(pi)\nOut[3]: True\n\nIn [4]: pi.dtype\nOut[4]: dtype('int64') \n```", "```py\nIn [112]: pi = pd.PeriodIndex([\"2016-08-01\"], freq=\"D\")\n\nIn [113]: pi\nOut[113]: PeriodIndex(['2016-08-01'], dtype='period[D]')\n\nIn [114]: pd.api.types.is_integer_dtype(pi)\nOut[114]: False\n\nIn [115]: pd.api.types.is_period_dtype(pi)\nOut[115]: True\n\nIn [116]: pi.dtype\nOut[116]: period[D]\n\nIn [117]: type(pi.dtype)\nOut[117]: pandas.core.dtypes.dtypes.PeriodDtype \n```", "```py\nIn [5]: pd.Period('NaT', freq='D')\nOut[5]: Period('NaT', 'D') \n```", "```py\nIn [118]: pd.Period(\"NaT\")\nOut[118]: NaT\n\nIn [119]: pd.Period(None)\nOut[119]: NaT \n```", "```py\nIn [5]: pd.NaT + 1\n...\nValueError: Cannot add integral value to Timestamp without freq. \n```", "```py\nIn [120]: pd.NaT + 1\nOut[120]: NaT\n\nIn [121]: pd.NaT - 1\nOut[121]: NaT \n```", "```py\nIn [6]: pi = pd.PeriodIndex(['2011-01', '2011-02'], freq='M')\nIn [7]: pi.values\nOut[7]: array([492, 493]) \n```", "```py\nIn [122]: pi = pd.PeriodIndex([\"2011-01\", \"2011-02\"], freq=\"M\")\n\nIn [123]: pi.values\nOut[123]: array([Period('2011-01', 'M'), Period('2011-02', 'M')], dtype=object) \n```", "```py\nIn [1]: pi = pd.PeriodIndex(['2016-08-01'], freq='D')\n\nIn [2]: pi\nOut[2]: PeriodIndex(['2016-08-01'], dtype='int64', freq='D')\n\nIn [3]: pd.api.types.is_integer_dtype(pi)\nOut[3]: True\n\nIn [4]: pi.dtype\nOut[4]: dtype('int64') \n```", "```py\nIn [112]: pi = pd.PeriodIndex([\"2016-08-01\"], freq=\"D\")\n\nIn [113]: pi\nOut[113]: PeriodIndex(['2016-08-01'], dtype='period[D]')\n\nIn [114]: pd.api.types.is_integer_dtype(pi)\nOut[114]: False\n\nIn [115]: pd.api.types.is_period_dtype(pi)\nOut[115]: True\n\nIn [116]: pi.dtype\nOut[116]: period[D]\n\nIn [117]: type(pi.dtype)\nOut[117]: pandas.core.dtypes.dtypes.PeriodDtype \n```", "```py\nIn [5]: pd.Period('NaT', freq='D')\nOut[5]: Period('NaT', 'D') \n```", "```py\nIn [118]: pd.Period(\"NaT\")\nOut[118]: NaT\n\nIn [119]: pd.Period(None)\nOut[119]: NaT \n```", "```py\nIn [5]: pd.NaT + 1\n...\nValueError: Cannot add integral value to Timestamp without freq. \n```", "```py\nIn [120]: pd.NaT + 1\nOut[120]: NaT\n\nIn [121]: pd.NaT - 1\nOut[121]: NaT \n```", "```py\nIn [6]: pi = pd.PeriodIndex(['2011-01', '2011-02'], freq='M')\nIn [7]: pi.values\nOut[7]: array([492, 493]) \n```", "```py\nIn [122]: pi = pd.PeriodIndex([\"2011-01\", \"2011-02\"], freq=\"M\")\n\nIn [123]: pi.values\nOut[123]: array([Period('2011-01', 'M'), Period('2011-02', 'M')], dtype=object) \n```", "```py\nIn [1]: pd.Index(['a', 'b']) + pd.Index(['a', 'c'])\nFutureWarning: using '+' to provide set union with Indexes is deprecated, use '|' or .union()\nOut[1]: Index(['a', 'b', 'c'], dtype='object') \n```", "```py\nIn [124]: pd.Index([\"a\", \"b\"]) + pd.Index([\"a\", \"c\"])\nOut[124]: Index(['aa', 'bc'], dtype='object') \n```", "```py\nIn [125]: pd.Index([1, 2, 3]) + pd.Index([2, 3, 4])\nOut[125]: Index([3, 5, 7], dtype='int64') \n```", "```py\nIn [1]: (pd.DatetimeIndex(['2016-01-01', '2016-01-02'])\n ...: - pd.DatetimeIndex(['2016-01-02', '2016-01-03']))\nFutureWarning: using '-' to provide set differences with datetimelike Indexes is deprecated, use .difference()\nOut[1]: DatetimeIndex(['2016-01-01'], dtype='datetime64[ns]', freq=None) \n```", "```py\nIn [126]: (\n .....:    pd.DatetimeIndex([\"2016-01-01\", \"2016-01-02\"])\n .....:    - pd.DatetimeIndex([\"2016-01-02\", \"2016-01-03\"])\n .....: )\n .....: \nOut[126]: TimedeltaIndex(['-1 days', '-1 days'], dtype='timedelta64[ns]', freq=None) \n```", "```py\nIn [127]: idx1 = pd.Index([1, 2, 3, np.nan])\n\nIn [128]: idx2 = pd.Index([0, 1, np.nan]) \n```", "```py\nIn [3]: idx1.difference(idx2)\nOut[3]: Float64Index([nan, 2.0, 3.0], dtype='float64')\n\nIn [4]: idx1.symmetric_difference(idx2)\nOut[4]: Float64Index([0.0, nan, 2.0, 3.0], dtype='float64') \n```", "```py\nIn [129]: idx1.difference(idx2)\nOut[129]: Index([2.0, 3.0], dtype='float64')\n\nIn [130]: idx1.symmetric_difference(idx2)\nOut[130]: Index([0.0, 2.0, 3.0], dtype='float64') \n```", "```py\nIn [1]: pd.Index([1, 2, 3]).unique()\nOut[1]: array([1, 2, 3])\n\nIn [2]: pd.DatetimeIndex(['2011-01-01', '2011-01-02',\n ...:                  '2011-01-03'], tz='Asia/Tokyo').unique()\nOut[2]:\nDatetimeIndex(['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00',\n '2011-01-03 00:00:00+09:00'],\n dtype='datetime64[ns, Asia/Tokyo]', freq=None) \n```", "```py\nIn [131]: pd.Index([1, 2, 3]).unique()\nOut[131]: Index([1, 2, 3], dtype='int64')\n\nIn [132]: pd.DatetimeIndex(\n .....:    [\"2011-01-01\", \"2011-01-02\", \"2011-01-03\"], tz=\"Asia/Tokyo\"\n .....: ).unique()\n .....: \nOut[132]: \nDatetimeIndex(['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00',\n '2011-01-03 00:00:00+09:00'],\n dtype='datetime64[ns, Asia/Tokyo]', freq=None) \n```", "```py\nIn [133]: cat = pd.Categorical([\"a\", \"b\"], categories=list(\"bac\"))\n\nIn [134]: lvl1 = [\"foo\", \"bar\"]\n\nIn [135]: midx = pd.MultiIndex.from_arrays([cat, lvl1])\n\nIn [136]: midx\nOut[136]: \nMultiIndex([('a', 'foo'),\n ('b', 'bar')],\n ) \n```", "```py\nIn [4]: midx.levels[0]\nOut[4]: Index(['b', 'a', 'c'], dtype='object')\n\nIn [5]: midx.get_level_values[0]\nOut[5]: Index(['a', 'b'], dtype='object') \n```", "```py\nIn [137]: midx.levels[0]\nOut[137]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category')\n\nIn [138]: midx.get_level_values(0)\nOut[138]: CategoricalIndex(['a', 'b'], categories=['b', 'a', 'c'], ordered=False, dtype='category') \n```", "```py\nIn [139]: df = pd.DataFrame({\"A\": [0, 1], \"B\": [10, 11], \"C\": cat})\n\nIn [140]: df_grouped = df.groupby(by=[\"A\", \"C\"], observed=False).first()\n\nIn [141]: df_set_idx = df.set_index([\"A\", \"C\"]) \n```", "```py\nIn [11]: df_grouped.index.levels[1]\nOut[11]: Index(['b', 'a', 'c'], dtype='object', name='C')\nIn [12]: df_grouped.reset_index().dtypes\nOut[12]:\nA      int64\nC     object\nB    float64\ndtype: object\n\nIn [13]: df_set_idx.index.levels[1]\nOut[13]: Index(['b', 'a', 'c'], dtype='object', name='C')\nIn [14]: df_set_idx.reset_index().dtypes\nOut[14]:\nA      int64\nC     object\nB      int64\ndtype: object \n```", "```py\nIn [142]: df_grouped.index.levels[1]\nOut[142]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category', name='C')\n\nIn [143]: df_grouped.reset_index().dtypes\nOut[143]: \nA       int64\nC    category\nB     float64\nLength: 3, dtype: object\n\nIn [144]: df_set_idx.index.levels[1]\nOut[144]: CategoricalIndex(['b', 'a', 'c'], categories=['b', 'a', 'c'], ordered=False, dtype='category', name='C')\n\nIn [145]: df_set_idx.reset_index().dtypes\nOut[145]: \nA       int64\nC    category\nB       int64\nLength: 3, dtype: object \n```", "```py\nIn [146]: data = \"A,B\\n0,1\\n2,3\\n4,5\\n6,7\" \n```", "```py\nIn [2]: pd.concat(pd.read_csv(StringIO(data), chunksize=2))\nOut[2]:\n A  B\n0  0  1\n1  2  3\n0  4  5\n1  6  7 \n```", "```py\nIn [147]: pd.concat(pd.read_csv(StringIO(data), chunksize=2))\nOut[147]: \n A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n\n[4 rows x 2 columns] \n```", "```py\nIn [1]: pd.SparseArray([1, 2, 0, 0])\nOut[1]:\n[1.0, 2.0, 0.0, 0.0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n\n# specifying int64 dtype, but all values are stored in sp_values because\n# fill_value default is np.nan\nIn [2]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[2]:\n[1, 2, 0, 0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n\nIn [3]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64, fill_value=0)\nOut[3]:\n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32) \n```", "```py\nIn [148]: pd.arrays.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[148]: \n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32)\n\nIn [149]: pd.arrays.SparseArray([True, False, False, False])\nOut[149]: \n[True, False, False, False]\nFill: False\nIntIndex\nIndices: array([0], dtype=int32) \n```", "```py\ns = pd.SparseSeries([0, 2, 0, 1], fill_value=0, dtype=np.int64)\ns.dtype\n\ns + 1 \n```", "```py\ns = pd.SparseSeries([1.0, 0.0, 2.0, 0.0], fill_value=0)\ns\ns.astype(np.int64) \n```", "```py\nIn [7]: pd.SparseSeries([1., np.nan, 2., np.nan], fill_value=np.nan).astype(np.int64)\nOut[7]:\nValueError: unable to coerce current fill_value nan to int64 dtype \n```", "```py\nIn [1]: pd.SparseArray([1, 2, 0, 0])\nOut[1]:\n[1.0, 2.0, 0.0, 0.0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n\n# specifying int64 dtype, but all values are stored in sp_values because\n# fill_value default is np.nan\nIn [2]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[2]:\n[1, 2, 0, 0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n\nIn [3]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64, fill_value=0)\nOut[3]:\n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32) \n```", "```py\nIn [148]: pd.arrays.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[148]: \n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32)\n\nIn [149]: pd.arrays.SparseArray([True, False, False, False])\nOut[149]: \n[True, False, False, False]\nFill: False\nIntIndex\nIndices: array([0], dtype=int32) \n```", "```py\ns = pd.SparseSeries([0, 2, 0, 1], fill_value=0, dtype=np.int64)\ns.dtype\n\ns + 1 \n```", "```py\ns = pd.SparseSeries([1.0, 0.0, 2.0, 0.0], fill_value=0)\ns\ns.astype(np.int64) \n```", "```py\nIn [7]: pd.SparseSeries([1., np.nan, 2., np.nan], fill_value=np.nan).astype(np.int64)\nOut[7]:\nValueError: unable to coerce current fill_value nan to int64 dtype \n```", "```py\nIn [1]: i = pd.Index(['a', 'b', 'c'])\n\nIn [2]: i.get_indexer(['b', 'b', 'c']).dtype\nOut[2]: dtype('int32') \n```", "```py\nIn [1]: i = pd.Index(['a', 'b', 'c'])\n\nIn [2]: i.get_indexer(['b', 'b', 'c']).dtype\nOut[2]: dtype('int64') \n```"]