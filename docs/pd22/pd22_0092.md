# `pandas.DataFrame.to_parquet`

> 原文：[`pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)

```py
DataFrame.to_parquet(path=None, *, engine='auto', compression='snappy', index=None, partition_cols=None, storage_options=None, **kwargs)
```

将 DataFrame 写入二进制 parquet 格式。

此函数将数据框写入 [parquet 文件](https://parquet.apache.org/)。您可以选择不同的 parquet 后端，并选择是否压缩。有关更多详细信息，请参阅 用户指南。

参数:

**path**str、path 对象、文件样对象或 None，默认为 None

字符串、路径对象（实现 `os.PathLike[str]`）或实现二进制 `write()` 函数的文件样对象。如果为 None，则结果将返回为 bytes。如果为字符串或路径，则将其用作写入分区数据集时的根目录路径。

**engine**{‘auto’, ‘pyarrow’, ‘fastparquet’}，默认为 'auto'

要使用的 parquet 库。如果为 ‘auto’，则使用选项 `io.parquet.engine`。默认 `io.parquet.engine` 行为是尝试 ‘pyarrow’，如果 ‘pyarrow’ 不可用，则退回到 ‘fastparquet’。

**compression**str 或 None，默认为 'snappy'

要使用的压缩名称。使用 `None` 表示不压缩。支持的选项: ‘snappy’、‘gzip’、‘brotli’、‘lz4’、‘zstd’。

**index**bool，默认为 None

如果为 `True`，则将数据框的索引(es)包含在文件输出中。如果为 `False`，则不会将其写入文件。如果为 `None`，类似于 `True`，将保存数据框的索引(es)。但是，而不是保存为值，RangeIndex 将作为元数据中的范围存储，因此不需要太多空间并且更快。其他索引将包含在文件输出中作为列。

**partition_cols**列表，可选，默认为 None

按照哪些列对数据集进行分区。按给定的顺序分区列。如果路径不是字符串，则必须为 None。

**storage_options**字典，可选

对于特定存储连接有意义的额外选项，例如主机、端口、用户名、密码等。对于 HTTP(S) URL，键值对将作为头选项转发给 `urllib.request.Request`。对于其他 URL（例如以 “s3://” 和 “gcs://” 开头的 URL），键值对将转发给 `fsspec.open`。请参阅 `fsspec` 和 `urllib` 以获取更多详细信息，并参阅此处的存储选项的更多示例 [here](https://pandas.pydata.org/docs/user_guide/io.html?highlight=storage_options#reading-writing-remote-files)。

****kwargs**

传递给 parquet 库的附加参数。有关更多详细信息，请参阅 pandas io。

返回:

如果未提供路径参数，则为 bytes，否则为 None

另请参见

`read_parquet`

读取一个 parquet 文件。

`DataFrame.to_orc`

写入 orc 文件。

`DataFrame.to_csv`

写入 csv 文件。

`DataFrame.to_sql`

写入到一个 SQL 表中。

`DataFrame.to_hdf`

写入到 hdf 中。

注意

此函数需要使用 [fastparquet](https://pypi.org/project/fastparquet) 或 [pyarrow](https://arrow.apache.org/docs/python/) 库。

示例

```py
>>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})
>>> df.to_parquet('df.parquet.gzip',
...               compression='gzip')  
>>> pd.read_parquet('df.parquet.gzip')  
 col1  col2
0     1     3
1     2     4 
```

如果你想要获取一个 parquet 内容的缓冲区，你可以使用一个 io.BytesIO 对象，只要你不使用 partition_cols，这会创建多个文件。

```py
>>> import io
>>> f = io.BytesIO()
>>> df.to_parquet(f)
>>> f.seek(0)
0
>>> content = f.read() 
```
