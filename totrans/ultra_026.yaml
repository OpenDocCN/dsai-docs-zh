- en: 'SAM 2: Segment Anything Model 2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/models/sam-2/`](https://docs.ultralytics.com/models/sam-2/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SAM 2, the successor to Meta's Segment Anything Model (SAM), is a cutting-edge
    tool designed for comprehensive object segmentation in both images and videos.
    It excels in handling complex visual data through a unified, promptable model
    architecture that supports real-time processing and zero-shot generalization.
  prefs: []
  type: TYPE_NORMAL
- en: '![SAM 2 Example Results](img/cc4b63672ee7fc207229c006a771c8ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Key Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unified Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SAM 2 combines the capabilities of image and video segmentation in a single
    model. This unification simplifies deployment and allows for consistent performance
    across different media types. It leverages a flexible prompt-based interface,
    enabling users to specify objects of interest through various prompt types, such
    as points, bounding boxes, or masks.
  prefs: []
  type: TYPE_NORMAL
- en: Real-Time Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model achieves real-time inference speeds, processing approximately 44 frames
    per second. This makes SAM 2 suitable for applications requiring immediate feedback,
    such as video editing and augmented reality.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SAM 2 can segment objects it has never encountered before, demonstrating strong
    zero-shot generalization. This is particularly useful in diverse or evolving visual
    domains where pre-defined categories may not cover all possible objects.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive Refinement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Users can iteratively refine the segmentation results by providing additional
    prompts, allowing for precise control over the output. This interactivity is essential
    for fine-tuning results in applications like video annotation or medical imaging.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Handling of Visual Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SAM 2 includes mechanisms to manage common video segmentation challenges, such
    as object occlusion and reappearance. It uses a sophisticated memory mechanism
    to keep track of objects across frames, ensuring continuity even when objects
    are temporarily obscured or exit and re-enter the scene.
  prefs: []
  type: TYPE_NORMAL
- en: For a deeper understanding of SAM 2's architecture and capabilities, explore
    the [SAM 2 research paper](https://arxiv.org/abs/2401.12741).
  prefs: []
  type: TYPE_NORMAL
- en: Performance and Technical Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SAM 2 sets a new benchmark in the field, outperforming previous models on various
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric | SAM 2 | Previous SOTA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Interactive Video Segmentation** | **Best** | - |'
  prefs: []
  type: TYPE_TB
- en: '| **Human Interactions Required** | **3x fewer** | Baseline |'
  prefs: []
  type: TYPE_TB
- en: '| **Image Segmentation Accuracy** | **Improved** | SAM |'
  prefs: []
  type: TYPE_TB
- en: '| **Inference Speed** | **6x faster** | SAM |'
  prefs: []
  type: TYPE_TB
- en: Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Core Components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Image and Video Encoder**: Utilizes a transformer-based architecture to extract
    high-level features from both images and video frames. This component is responsible
    for understanding the visual content at each timestep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt Encoder**: Processes user-provided prompts (points, boxes, masks)
    to guide the segmentation task. This allows SAM 2 to adapt to user input and target
    specific objects within a scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Mechanism**: Includes a memory encoder, memory bank, and memory attention
    module. These components collectively store and utilize information from past
    frames, enabling the model to maintain consistent object tracking over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mask Decoder**: Generates the final segmentation masks based on the encoded
    image features and prompts. In video, it also uses memory context to ensure accurate
    tracking across frames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![SAM 2 Architecture Diagram](img/e0f99a69f8aa0ebd6d37f97d957a4e05.png)'
  prefs: []
  type: TYPE_IMG
- en: Memory Mechanism and Occlusion Handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The memory mechanism allows SAM 2 to handle temporal dependencies and occlusions
    in video data. As objects move and interact, SAM 2 records their features in a
    memory bank. When an object becomes occluded, the model can rely on this memory
    to predict its position and appearance when it reappears. The occlusion head specifically
    handles scenarios where objects are not visible, predicting the likelihood of
    an object being occluded.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Mask Ambiguity Resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In situations with ambiguity (e.g., overlapping objects), SAM 2 can generate
    multiple mask predictions. This feature is crucial for accurately representing
    complex scenes where a single mask might not sufficiently describe the scene's
    nuances.
  prefs: []
  type: TYPE_NORMAL
- en: SA-V Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SA-V dataset, developed for SAM 2''s training, is one of the largest and
    most diverse video segmentation datasets available. It includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**51,000+ Videos**: Captured across 47 countries, providing a wide range of
    real-world scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**600,000+ Mask Annotations**: Detailed spatio-temporal mask annotations, referred
    to as "masklets," covering whole objects and parts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset Scale**: It features 4.5 times more videos and 53 times more annotations
    than previous largest datasets, offering unprecedented diversity and complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Video Object Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SAM 2 has demonstrated superior performance across major video segmentation
    benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | J&F | J | F |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **DAVIS 2017** | 82.5 | 79.8 | 85.2 |'
  prefs: []
  type: TYPE_TB
- en: '| **YouTube-VOS** | 81.2 | 78.9 | 83.5 |'
  prefs: []
  type: TYPE_TB
- en: Interactive Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In interactive segmentation tasks, SAM 2 shows significant efficiency and accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | NoC@90 | AUC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **DAVIS Interactive** | 1.54 | 0.872 |'
  prefs: []
  type: TYPE_TB
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To install SAM 2, use the following command. All SAM 2 models will automatically
    download on first use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'How to Use SAM 2: Versatility in Image and Video Segmentation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following table details the available SAM 2 models, their pre-trained weights,
    supported tasks, and compatibility with different operating modes like Inference,
    Validation, Training, and Export.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SAM 2 tiny | [sam2_t.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_t.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| SAM 2 small | [sam2_s.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_s.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| SAM 2 base | [sam2_b.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_b.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| SAM 2 large | [sam2_l.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_l.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: SAM 2 Prediction Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SAM 2 can be utilized across a broad spectrum of tasks, including real-time
    video editing, medical imaging, and autonomous systems. Its ability to segment
    both static and dynamic visual data makes it a versatile tool for researchers
    and developers.
  prefs: []
  type: TYPE_NORMAL
- en: Segment with Prompts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Segment with Prompts
  prefs: []
  type: TYPE_NORMAL
- en: Use prompts to segment specific objects in images or videos.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Segment Everything
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Segment Everything
  prefs: []
  type: TYPE_NORMAL
- en: Segment the entire image or video content without specific prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This example demonstrates how SAM 2 can be used to segment the entire content
    of an image or video if no prompts (bboxes/points/masks) are provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAM comparison vs YOLOv8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we compare Meta''s smallest SAM model, SAM-b, with Ultralytics smallest
    segmentation model, YOLOv8n-seg:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size | Parameters | Speed (CPU) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| FastSAM-s with YOLOv8 backbone | 23.7 MB | 11.8 M | 115 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| Ultralytics YOLOv8n-seg | **6.7 MB** (53.4x smaller) | **3.4 M** (27.9x less)
    | **59 ms/im** (866x faster) |'
  prefs: []
  type: TYPE_TB
- en: This comparison shows the order-of-magnitude differences in the model sizes
    and speeds between models. Whereas SAM presents unique capabilities for automatic
    segmenting, it is not a direct competitor to YOLOv8 segment models, which are
    smaller, faster and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tests run on a 2023 Apple M2 Macbook with 16GB of RAM. To reproduce this test:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Auto-Annotation: Efficient Dataset Creation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Auto-annotation is a powerful feature of SAM 2, enabling users to generate segmentation
    datasets quickly and accurately by leveraging pre-trained models. This capability
    is particularly useful for creating large, high-quality datasets without extensive
    manual effort.
  prefs: []
  type: TYPE_NORMAL
- en: How to Auto-Annotate with SAM 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To auto-annotate your dataset using SAM 2, follow this example:'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-Annotation Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '| Argument | Type | Description | Default |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `data` | `str` | Path to a folder containing images to be annotated. |  |'
  prefs: []
  type: TYPE_TB
- en: '| `det_model` | `str`, optional | Pre-trained YOLO detection model. Defaults
    to ''yolov8x.pt''. | `''yolov8x.pt''` |'
  prefs: []
  type: TYPE_TB
- en: '| `sam_model` | `str`, optional | Pre-trained SAM 2 segmentation model. Defaults
    to ''sam2_b.pt''. | `''sam2_b.pt''` |'
  prefs: []
  type: TYPE_TB
- en: '| `device` | `str`, optional | Device to run the models on. Defaults to an
    empty string (CPU or GPU, if available). |  |'
  prefs: []
  type: TYPE_TB
- en: '| `output_dir` | `str`, `None`, optional | Directory to save the annotated
    results. Defaults to a ''labels'' folder in the same directory as ''data''. |
    `None` |'
  prefs: []
  type: TYPE_TB
- en: This function facilitates the rapid creation of high-quality segmentation datasets,
    ideal for researchers and developers aiming to accelerate their projects.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite its strengths, SAM 2 has certain limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tracking Stability**: SAM 2 may lose track of objects during extended sequences
    or significant viewpoint changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object Confusion**: The model can sometimes confuse similar-looking objects,
    particularly in crowded scenes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency with Multiple Objects**: Segmentation efficiency decreases when
    processing multiple objects simultaneously due to the lack of inter-object communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detail Accuracy**: May miss fine details, especially with fast-moving objects.
    Additional prompts can partially address this issue, but temporal smoothness is
    not guaranteed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citations and Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If SAM 2 is a crucial part of your research or development work, please cite
    it using the following reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We extend our gratitude to Meta AI for their contributions to the AI community
    with this groundbreaking model and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is SAM 2 and how does it improve upon the original Segment Anything Model
    (SAM)?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SAM 2, the successor to Meta''s Segment Anything Model (SAM), is a cutting-edge
    tool designed for comprehensive object segmentation in both images and videos.
    It excels in handling complex visual data through a unified, promptable model
    architecture that supports real-time processing and zero-shot generalization.
    SAM 2 offers several improvements over the original SAM, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unified Model Architecture**: Combines image and video segmentation capabilities
    in a single model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-Time Performance**: Processes approximately 44 frames per second, making
    it suitable for applications requiring immediate feedback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-Shot Generalization**: Segments objects it has never encountered before,
    useful in diverse visual domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive Refinement**: Allows users to iteratively refine segmentation
    results by providing additional prompts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced Handling of Visual Challenges**: Manages common video segmentation
    challenges like object occlusion and reappearance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details on SAM 2's architecture and capabilities, explore the [SAM
    2 research paper](https://arxiv.org/abs/2401.12741).
  prefs: []
  type: TYPE_NORMAL
- en: How can I use SAM 2 for real-time video segmentation?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SAM 2 can be utilized for real-time video segmentation by leveraging its promptable
    interface and real-time inference capabilities. Here''s a basic example:'
  prefs: []
  type: TYPE_NORMAL
- en: Segment with Prompts
  prefs: []
  type: TYPE_NORMAL
- en: Use prompts to segment specific objects in images or videos.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For more comprehensive usage, refer to the How to Use SAM 2 section.
  prefs: []
  type: TYPE_NORMAL
- en: What datasets are used to train SAM 2, and how do they enhance its performance?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SAM 2 is trained on the SA-V dataset, one of the largest and most diverse video
    segmentation datasets available. The SA-V dataset includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**51,000+ Videos**: Captured across 47 countries, providing a wide range of
    real-world scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**600,000+ Mask Annotations**: Detailed spatio-temporal mask annotations, referred
    to as "masklets," covering whole objects and parts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset Scale**: Features 4.5 times more videos and 53 times more annotations
    than previous largest datasets, offering unprecedented diversity and complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This extensive dataset allows SAM 2 to achieve superior performance across major
    video segmentation benchmarks and enhances its zero-shot generalization capabilities.
    For more information, see the SA-V Dataset section.
  prefs: []
  type: TYPE_NORMAL
- en: How does SAM 2 handle occlusions and object reappearances in video segmentation?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SAM 2 includes a sophisticated memory mechanism to manage temporal dependencies
    and occlusions in video data. The memory mechanism consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory Encoder and Memory Bank**: Stores features from past frames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Attention Module**: Utilizes stored information to maintain consistent
    object tracking over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Occlusion Head**: Specifically handles scenarios where objects are not visible,
    predicting the likelihood of an object being occluded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This mechanism ensures continuity even when objects are temporarily obscured
    or exit and re-enter the scene. For more details, refer to the Memory Mechanism
    and Occlusion Handling section.
  prefs: []
  type: TYPE_NORMAL
- en: How does SAM 2 compare to other segmentation models like YOLOv8?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SAM 2 and Ultralytics YOLOv8 serve different purposes and excel in different
    areas. While SAM 2 is designed for comprehensive object segmentation with advanced
    features like zero-shot generalization and real-time performance, YOLOv8 is optimized
    for speed and efficiency in object detection and segmentation tasks. Here''s a
    comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size | Parameters | Speed (CPU) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| FastSAM-s with YOLOv8 backbone | 23.7 MB | 11.8 M | 115 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| Ultralytics YOLOv8n-seg | **6.7 MB** (53.4x smaller) | **3.4 M** (27.9x less)
    | **59 ms/im** (866x faster) |'
  prefs: []
  type: TYPE_TB
- en: For more details, see the SAM comparison vs YOLOv8 section.
  prefs: []
  type: TYPE_NORMAL
