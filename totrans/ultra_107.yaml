- en: Triton Inference Server with Ultralytics YOLOv8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/guides/triton-inference-server/`](https://docs.ultralytics.com/guides/triton-inference-server/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)
    (formerly known as TensorRT Inference Server) is an open-source software solution
    developed by NVIDIA. It provides a cloud inference solution optimized for NVIDIA
    GPUs. Triton simplifies the deployment of AI models at scale in production. Integrating
    Ultralytics YOLOv8 with Triton Inference Server allows you to deploy scalable,
    high-performance deep learning inference workloads. This guide provides steps
    to set up and test the integration.
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/NQDtfSi5QF4`](https://www.youtube.com/embed/NQDtfSi5QF4)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** Getting Started with NVIDIA Triton Inference Server.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Triton Inference Server?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Triton Inference Server is designed to deploy a variety of AI models in production.
    It supports a wide range of deep learning and machine learning frameworks, including
    TensorFlow, PyTorch, ONNX Runtime, and many others. Its primary use cases are:'
  prefs: []
  type: TYPE_NORMAL
- en: Serving multiple models from a single server instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic model loading and unloading without server restart.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble inference, allowing multiple models to be used together to achieve
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model versioning for A/B testing and rolling updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensure you have the following prerequisites before proceeding:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker installed on your machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install `tritonclient`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Exporting YOLOv8 to ONNX Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before deploying the model on Triton, it must be exported to the ONNX format.
    ONNX (Open Neural Network Exchange) is a format that allows models to be transferred
    between different deep learning frameworks. Use the `export` function from the
    `YOLO` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Setting Up Triton Model Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Triton Model Repository is a storage location where Triton can access and
    load models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the necessary directory structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move the exported ONNX model to the Triton repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running Triton Inference Server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the Triton Inference Server using Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then run inference using the Triton Server model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Cleanup the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: By following the above steps, you can deploy and run Ultralytics YOLOv8 models
    efficiently on Triton Inference Server, providing a scalable and high-performance
    solution for deep learning inference tasks. If you face any issues or have further
    queries, refer to the [official Triton documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html)
    or reach out to the Ultralytics community for support.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do I set up Ultralytics YOLOv8 with NVIDIA Triton Inference Server?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Setting up [Ultralytics YOLOv8](https://docs.ultralytics.com/models/yolov8)
    with [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)
    involves a few key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Export YOLOv8 to ONNX format**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Set up Triton Model Repository**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Run the Triton Server**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This setup can help you efficiently deploy YOLOv8 models at scale on Triton
    Inference Server for high-performance AI model inference.
  prefs: []
  type: TYPE_NORMAL
- en: What benefits does using Ultralytics YOLOv8 with NVIDIA Triton Inference Server
    offer?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Integrating Ultralytics YOLOv8 with [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)
    provides several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalable AI Inference**: Triton allows serving multiple models from a single
    server instance, supporting dynamic model loading and unloading, making it highly
    scalable for diverse AI workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Performance**: Optimized for NVIDIA GPUs, Triton Inference Server ensures
    high-speed inference operations, perfect for real-time applications such as object
    detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble and Model Versioning**: Triton''s ensemble mode enables combining
    multiple models to improve results, and its model versioning supports A/B testing
    and rolling updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For detailed instructions on setting up and running YOLOv8 with Triton, you
    can refer to the setup guide.
  prefs: []
  type: TYPE_NORMAL
- en: Why should I export my YOLOv8 model to ONNX format before using Triton Inference
    Server?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using ONNX (Open Neural Network Exchange) format for your Ultralytics YOLOv8
    model before deploying it on [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server)
    offers several key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interoperability**: ONNX format supports transfer between different deep
    learning frameworks (such as PyTorch, TensorFlow), ensuring broader compatibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: Many deployment environments, including Triton, optimize
    for ONNX, enabling faster inference and better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of Deployment**: ONNX is widely supported across frameworks and platforms,
    simplifying the deployment process in various operating systems and hardware configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To export your model, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can follow the steps in the exporting guide to complete the process.
  prefs: []
  type: TYPE_NORMAL
- en: Can I run inference using the Ultralytics YOLOv8 model on Triton Inference Server?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Yes, you can run inference using the Ultralytics YOLOv8 model on [NVIDIA Triton
    Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server).
    Once your model is set up in the Triton Model Repository and the server is running,
    you can load and run inference on your model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For an in-depth guide on setting up and running Triton Server with YOLOv8, refer
    to the running triton inference server section.
  prefs: []
  type: TYPE_NORMAL
- en: How does Ultralytics YOLOv8 compare to TensorFlow and PyTorch models for deployment?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Ultralytics YOLOv8](https://docs.ultralytics.com/models/yolov8) offers several
    unique advantages compared to TensorFlow and PyTorch models for deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time Performance**: Optimized for real-time object detection tasks,
    YOLOv8 provides state-of-the-art accuracy and speed, making it ideal for applications
    requiring live video analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of Use**: YOLOv8 integrates seamlessly with Triton Inference Server
    and supports diverse export formats (ONNX, TensorRT, CoreML), making it flexible
    for various deployment scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced Features**: YOLOv8 includes features like dynamic model loading,
    model versioning, and ensemble inference, which are crucial for scalable and reliable
    AI deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details, compare the deployment options in the model deployment guide.
  prefs: []
  type: TYPE_NORMAL
