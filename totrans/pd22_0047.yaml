- en: Scaling to large datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pandas.pydata.org/docs/user_guide/scale.html](https://pandas.pydata.org/docs/user_guide/scale.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: pandas provides data structures for in-memory analytics, which makes using pandas
    to analyze datasets that are larger than memory datasets somewhat tricky. Even
    datasets that are a sizable fraction of memory become unwieldy, as some pandas
    operations need to make intermediate copies.
  prefs: []
  type: TYPE_NORMAL
- en: This document provides a few recommendations for scaling your analysis to larger
    datasets. It’s a complement to [Enhancing performance](enhancingperf.html#enhancingperf),
    which focuses on speeding up analysis for datasets that fit in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Load less data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose our raw dataset on disk has many columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To load the columns we want, we have two options. Option 1 loads in all the
    data and then filters to what we need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Option 2 only loads the columns we request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If we were to measure the memory usage of the two calls, we’d see that specifying
    `columns` uses about 1/10th the memory in this case.
  prefs: []
  type: TYPE_NORMAL
- en: With [`pandas.read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), you can specify `usecols` to limit the columns read into memory.
    Not all file formats that can be read by pandas provide an option to read a subset
    of columns.
  prefs: []
  type: TYPE_NORMAL
- en: Use efficient datatypes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default pandas data types are not the most memory efficient. This is especially
    true for text data columns with relatively few unique values (commonly referred
    to as “low-cardinality” data). By using more efficient data types, you can store
    larger datasets in memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s inspect the data types and memory usage to see where we should focus
    our attention.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `name` column is taking up much more memory than any other. It has just
    a few unique values, so it’s a good candidate for converting to a [`pandas.Categorical`](../reference/api/pandas.Categorical.html#pandas.Categorical
    "pandas.Categorical"). With a [`pandas.Categorical`](../reference/api/pandas.Categorical.html#pandas.Categorical
    "pandas.Categorical"), we store each unique name once and use space-efficient
    integers to know which specific name is used in each row.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can go a bit further and downcast the numeric columns to their smallest types
    using [`pandas.to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In all, we’ve reduced the in-memory footprint of this dataset to 1/5 of its
    original size.
  prefs: []
  type: TYPE_NORMAL
- en: See [Categorical data](categorical.html#categorical) for more on [`pandas.Categorical`](../reference/api/pandas.Categorical.html#pandas.Categorical
    "pandas.Categorical") and [dtypes](basics.html#basics-dtypes) for an overview
    of all of pandas’ dtypes.
  prefs: []
  type: TYPE_NORMAL
- en: Use chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some workloads can be achieved with chunking by splitting a large problem into
    a bunch of small problems. For example, converting an individual CSV file into
    a Parquet file and repeating that for each file in a directory. As long as each
    chunk fits in memory, you can work with datasets that are much larger than memory.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Chunking works well when the operation you’re performing requires zero or minimal
    coordination between chunks. For more complicated workflows, you’re better off
    [using other libraries](#scale-other-libraries).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have an even larger “logical dataset” on disk that’s a directory
    of parquet files. Each file in the directory represents a different year of the
    entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we’ll implement an out-of-core [`pandas.Series.value_counts()`](../reference/api/pandas.Series.value_counts.html#pandas.Series.value_counts
    "pandas.Series.value_counts"). The peak memory usage of this workflow is the single
    largest chunk, plus a small series storing the unique value counts up to this
    point. As long as each individual file fits in memory, this will work for arbitrary-sized
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Some readers, like [`pandas.read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), offer parameters to control the `chunksize` when reading a
    single file.
  prefs: []
  type: TYPE_NORMAL
- en: Manually chunking is an OK option for workflows that don’t require too sophisticated
    of operations. Some operations, like [`pandas.DataFrame.groupby()`](../reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby
    "pandas.DataFrame.groupby"), are much harder to do chunkwise. In these cases,
    you may be better switching to a different library that implements these out-of-core
    algorithms for you.
  prefs: []
  type: TYPE_NORMAL
- en: '## Use Other Libraries'
  prefs: []
  type: TYPE_NORMAL
- en: There are other libraries which provide similar APIs to pandas and work nicely
    with pandas DataFrame, and can give you the ability to scale your large dataset
    processing and analytics by parallel runtime, distributed memory, clustering,
    etc. You can find more information in [the ecosystem page](https://pandas.pydata.org/community/ecosystem.html#out-of-core).
  prefs: []
  type: TYPE_NORMAL
- en: Load less data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose our raw dataset on disk has many columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To load the columns we want, we have two options. Option 1 loads in all the
    data and then filters to what we need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Option 2 only loads the columns we request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If we were to measure the memory usage of the two calls, we’d see that specifying
    `columns` uses about 1/10th the memory in this case.
  prefs: []
  type: TYPE_NORMAL
- en: With [`pandas.read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), you can specify `usecols` to limit the columns read into memory.
    Not all file formats that can be read by pandas provide an option to read a subset
    of columns.
  prefs: []
  type: TYPE_NORMAL
- en: Use efficient datatypes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default pandas data types are not the most memory efficient. This is especially
    true for text data columns with relatively few unique values (commonly referred
    to as “low-cardinality” data). By using more efficient data types, you can store
    larger datasets in memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s inspect the data types and memory usage to see where we should focus
    our attention.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `name` column is taking up much more memory than any other. It has just
    a few unique values, so it’s a good candidate for converting to a [`pandas.Categorical`](../reference/api/pandas.Categorical.html#pandas.Categorical
    "pandas.Categorical"). With a [`pandas.Categorical`](../reference/api/pandas.Categorical.html#pandas.Categorical
    "pandas.Categorical"), we store each unique name once and use space-efficient
    integers to know which specific name is used in each row.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We can go a bit further and downcast the numeric columns to their smallest types
    using [`pandas.to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In all, we’ve reduced the in-memory footprint of this dataset to 1/5 of its
    original size.
  prefs: []
  type: TYPE_NORMAL
- en: See [Categorical data](categorical.html#categorical) for more on [`pandas.Categorical`](../reference/api/pandas.Categorical.html#pandas.Categorical
    "pandas.Categorical") and [dtypes](basics.html#basics-dtypes) for an overview
    of all of pandas’ dtypes.
  prefs: []
  type: TYPE_NORMAL
- en: Use chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some workloads can be achieved with chunking by splitting a large problem into
    a bunch of small problems. For example, converting an individual CSV file into
    a Parquet file and repeating that for each file in a directory. As long as each
    chunk fits in memory, you can work with datasets that are much larger than memory.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Chunking works well when the operation you’re performing requires zero or minimal
    coordination between chunks. For more complicated workflows, you’re better off
    [using other libraries](#scale-other-libraries).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have an even larger “logical dataset” on disk that’s a directory
    of parquet files. Each file in the directory represents a different year of the
    entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now we’ll implement an out-of-core [`pandas.Series.value_counts()`](../reference/api/pandas.Series.value_counts.html#pandas.Series.value_counts
    "pandas.Series.value_counts"). The peak memory usage of this workflow is the single
    largest chunk, plus a small series storing the unique value counts up to this
    point. As long as each individual file fits in memory, this will work for arbitrary-sized
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Some readers, like [`pandas.read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), offer parameters to control the `chunksize` when reading a
    single file.
  prefs: []
  type: TYPE_NORMAL
- en: Manually chunking is an OK option for workflows that don’t require too sophisticated
    of operations. Some operations, like [`pandas.DataFrame.groupby()`](../reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby
    "pandas.DataFrame.groupby"), are much harder to do chunkwise. In these cases,
    you may be better switching to a different library that implements these out-of-core
    algorithms for you.
  prefs: []
  type: TYPE_NORMAL
- en: '## Use Other Libraries'
  prefs: []
  type: TYPE_NORMAL
- en: There are other libraries which provide similar APIs to pandas and work nicely
    with pandas DataFrame, and can give you the ability to scale your large dataset
    processing and analytics by parallel runtime, distributed memory, clustering,
    etc. You can find more information in [the ecosystem page](https://pandas.pydata.org/community/ecosystem.html#out-of-core).
  prefs: []
  type: TYPE_NORMAL
