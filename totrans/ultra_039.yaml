- en: LVIS Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/datasets/detect/lvis/`](https://docs.ultralytics.com/datasets/detect/lvis/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The [LVIS dataset](https://www.lvisdataset.org/) is a large-scale, fine-grained
    vocabulary-level annotation dataset developed and released by Facebook AI Research
    (FAIR). It is primarily used as a research benchmark for object detection and
    instance segmentation with a large vocabulary of categories, aiming to drive further
    advancements in computer vision field.
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/cfTKj96TjSE`](https://www.youtube.com/embed/cfTKj96TjSE)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** YOLO World training workflow with LVIS dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '![LVIS Dataset example images](img/684a611bba29bcc799f8d375ae56ea86.png)'
  prefs: []
  type: TYPE_IMG
- en: Key Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LVIS contains 160k images and 2M instance annotations for object detection,
    segmentation, and captioning tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset comprises 1203 object categories, including common objects like
    cars, bicycles, and animals, as well as more specific categories such as umbrellas,
    handbags, and sports equipment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotations include object bounding boxes, segmentation masks, and captions
    for each image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LVIS provides standardized evaluation metrics like mean Average Precision (mAP)
    for object detection, and mean Average Recall (mAR) for segmentation tasks, making
    it suitable for comparing model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LVIS uses exactly the same images as COCO dataset, but with different splits
    and different annotations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The LVIS dataset is split into three subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train**: This subset contains 100k images for training object detection,
    segmentation, and captioning models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Val**: This subset has 20k images used for validation purposes during model
    training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minival**: This subset is exactly the same as COCO val2017 set which has
    5k images used for validation purposes during model training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test**: This subset consists of 20k images used for testing and benchmarking
    the trained models. Ground truth annotations for this subset are not publicly
    available, and the results are submitted to the [LVIS evaluation server](https://eval.ai/web/challenges/challenge-page/675/overview)
    for performance evaluation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LVIS dataset is widely used for training and evaluating deep learning models
    in object detection (such as YOLO, Faster R-CNN, and SSD), instance segmentation
    (such as Mask R-CNN). The dataset's diverse set of object categories, large number
    of annotated images, and standardized evaluation metrics make it an essential
    resource for computer vision researchers and practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset YAML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A YAML (Yet Another Markup Language) file is used to define the dataset configuration.
    It contains information about the dataset's paths, classes, and other relevant
    information. In the case of the LVIS dataset, the `lvis.yaml` file is maintained
    at [`github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/lvis.yaml`](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/lvis.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: ultralytics/cfg/datasets/lvis.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a YOLOv8n model on the LVIS dataset for 100 epochs with an image size
    of 640, you can use the following code snippets. For a comprehensive list of available
    arguments, refer to the model Training page.
  prefs: []
  type: TYPE_NORMAL
- en: Train Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Sample Images and Annotations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The LVIS dataset contains a diverse set of images with various object categories
    and complex scenes. Here are some examples of images from the dataset, along with
    their corresponding annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![LVIS Dataset sample image](img/5509e501459eedb1ecd24c2fdfef1f51.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Mosaiced Image**: This image demonstrates a training batch composed of mosaiced
    dataset images. Mosaicing is a technique used during training that combines multiple
    images into a single image to increase the variety of objects and scenes within
    each training batch. This helps improve the model''s ability to generalize to
    different object sizes, aspect ratios, and contexts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The example showcases the variety and complexity of the images in the LVIS dataset
    and the benefits of using mosaicing during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Citations and Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you use the LVIS dataset in your research or development work, please cite
    the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We would like to acknowledge the LVIS Consortium for creating and maintaining
    this valuable resource for the computer vision community. For more information
    about the LVIS dataset and its creators, visit the [LVIS dataset website](https://www.lvisdataset.org/).
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the LVIS dataset, and how is it used in computer vision?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [LVIS dataset](https://www.lvisdataset.org/) is a large-scale dataset with
    fine-grained vocabulary-level annotations developed by Facebook AI Research (FAIR).
    It is primarily used for object detection and instance segmentation, featuring
    over 1203 object categories and 2 million instance annotations. Researchers and
    practitioners use it to train and benchmark models like Ultralytics YOLO for advanced
    computer vision tasks. The dataset's extensive size and diversity make it an essential
    resource for pushing the boundaries of model performance in detection and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: How can I train a YOLOv8n model using the LVIS dataset?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train a YOLOv8n model on the LVIS dataset for 100 epochs with an image size
    of 640, follow the example below. This process utilizes Ultralytics' framework,
    which offers comprehensive training features.
  prefs: []
  type: TYPE_NORMAL
- en: Train Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For detailed training configurations, refer to the Training documentation.
  prefs: []
  type: TYPE_NORMAL
- en: How does the LVIS dataset differ from the COCO dataset?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The images in the LVIS dataset are the same as those in the COCO dataset, but
    the two differ in terms of splitting and annotations. LVIS provides a larger and
    more detailed vocabulary with 1203 object categories compared to COCO's 80 categories.
    Additionally, LVIS focuses on annotation completeness and diversity, aiming to
    push the limits of object detection and instance segmentation models by offering
    more nuanced and comprehensive data.
  prefs: []
  type: TYPE_NORMAL
- en: Why should I use Ultralytics YOLO for training on the LVIS dataset?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ultralytics YOLO models, including the latest YOLOv8, are optimized for real-time
    object detection with state-of-the-art accuracy and speed. They support a wide
    range of annotations, such as the fine-grained ones provided by the LVIS dataset,
    making them ideal for advanced computer vision applications. Moreover, Ultralytics
    offers seamless integration with various training, validation, and prediction
    modes, ensuring efficient model development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Can I see some sample annotations from the LVIS dataset?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Yes, the LVIS dataset includes a variety of images with diverse object categories
    and complex scenes. Here is an example of a sample image along with its annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![LVIS Dataset sample image](img/5509e501459eedb1ecd24c2fdfef1f51.png)'
  prefs: []
  type: TYPE_IMG
- en: This mosaiced image demonstrates a training batch composed of multiple dataset
    images combined into one. Mosaicing increases the variety of objects and scenes
    within each training batch, enhancing the model's ability to generalize across
    different contexts. For more details on the LVIS dataset, explore the LVIS dataset
    documentation.
  prefs: []
  type: TYPE_NORMAL
