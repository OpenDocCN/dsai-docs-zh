- en: Model Export with Ultralytics YOLO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/modes/export/`](https://docs.ultralytics.com/modes/export/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Ultralytics YOLO ecosystem and integrations](img/1933b0eeaf180eaa6d0c37f29931fb7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ultimate goal of training a model is to deploy it for real-world applications.
    Export mode in Ultralytics YOLOv8 offers a versatile range of options for exporting
    your trained model to different formats, making it deployable across various platforms
    and devices. This comprehensive guide aims to walk you through the nuances of
    model exporting, showcasing how to achieve maximum compatibility and performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/WbomGeoOT_k?si=aGmuyooWftA0ue9X`](https://www.youtube.com/embed/WbomGeoOT_k?si=aGmuyooWftA0ue9X)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** How To Export Custom Trained Ultralytics YOLOv8 Model and Run Live
    Inference on Webcam.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Choose YOLOv8's Export Mode?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Versatility:** Export to multiple formats including ONNX, TensorRT, CoreML,
    and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance:** Gain up to 5x GPU speedup with TensorRT and 3x CPU speedup
    with ONNX or OpenVINO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility:** Make your model universally deployable across numerous hardware
    and software environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of Use:** Simple CLI and Python API for quick and straightforward model
    exporting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key Features of Export Mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some of the standout functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**One-Click Export:** Simple commands for exporting to different formats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Export:** Export batched-inference capable models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimized Inference:** Exported models are optimized for quicker inference
    times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tutorial Videos:** In-depth guides and tutorials for a smooth exporting experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Export to ONNX or OpenVINO for up to 3x CPU speedup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Export to TensorRT for up to 5x GPU speedup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Export a YOLOv8n model to a different format like ONNX or TensorRT. See Arguments
    section below for a full list of export arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Arguments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This table details the configurations and options available for exporting YOLO
    models to different formats. These settings are critical for optimizing the exported
    model's performance, size, and compatibility across various platforms and environments.
    Proper configuration ensures that the model is ready for deployment in the intended
    application with optimal efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '| Argument | Type | Default | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `format` | `str` | `''torchscript''` | Target format for the exported model,
    such as `''onnx''`, `''torchscript''`, `''tensorflow''`, or others, defining compatibility
    with various deployment environments. |'
  prefs: []
  type: TYPE_TB
- en: '| `imgsz` | `int` or `tuple` | `640` | Desired image size for the model input.
    Can be an integer for square images or a tuple `(height, width)` for specific
    dimensions. |'
  prefs: []
  type: TYPE_TB
- en: '| `keras` | `bool` | `False` | Enables export to Keras format for TensorFlow
    SavedModel, providing compatibility with TensorFlow serving and APIs. |'
  prefs: []
  type: TYPE_TB
- en: '| `optimize` | `bool` | `False` | Applies optimization for mobile devices when
    exporting to TorchScript, potentially reducing model size and improving performance.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `half` | `bool` | `False` | Enables FP16 (half-precision) quantization, reducing
    model size and potentially speeding up inference on supported hardware. |'
  prefs: []
  type: TYPE_TB
- en: '| `int8` | `bool` | `False` | Activates INT8 quantization, further compressing
    the model and speeding up inference with minimal accuracy loss, primarily for
    edge devices. |'
  prefs: []
  type: TYPE_TB
- en: '| `dynamic` | `bool` | `False` | Allows dynamic input sizes for ONNX, TensorRT
    and OpenVINO exports, enhancing flexibility in handling varying image dimensions.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `simplify` | `bool` | `False` | Simplifies the model graph for ONNX exports
    with `onnxslim`, potentially improving performance and compatibility. |'
  prefs: []
  type: TYPE_TB
- en: '| `opset` | `int` | `None` | Specifies the ONNX opset version for compatibility
    with different ONNX parsers and runtimes. If not set, uses the latest supported
    version. |'
  prefs: []
  type: TYPE_TB
- en: '| `workspace` | `float` | `4.0` | Sets the maximum workspace size in GiB for
    TensorRT optimizations, balancing memory usage and performance. |'
  prefs: []
  type: TYPE_TB
- en: '| `nms` | `bool` | `False` | Adds Non-Maximum Suppression (NMS) to the CoreML
    export, essential for accurate and efficient detection post-processing. |'
  prefs: []
  type: TYPE_TB
- en: '| `batch` | `int` | `1` | Specifies export model batch inference size or the
    max number of images the exported model will process concurrently in `predict`
    mode. |'
  prefs: []
  type: TYPE_TB
- en: Adjusting these parameters allows for customization of the export process to
    fit specific requirements, such as deployment environment, hardware constraints,
    and performance targets. Selecting the appropriate format and settings is essential
    for achieving the best balance between model size, speed, and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Export Formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Available YOLOv8 export formats are in the table below. You can export to any
    format using the `format` argument, i.e. `format='onnx'` or `format='engine'`.
    You can predict or validate directly on exported models, i.e. `yolo predict model=yolov8n.onnx`.
    Usage examples are shown for your model after export completes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | `format` Argument | Model | Metadata | Arguments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [PyTorch](https://pytorch.org/) | - | `yolov8n.pt` | ✅ | - |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | `torchscript` | `yolov8n.torchscript` | ✅ | `imgsz`, `optimize`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | `onnx` | `yolov8n.onnx` | ✅ | `imgsz`, `half`, `dynamic`, `simplify`,
    `opset`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | `openvino` | `yolov8n_openvino_model/` | ✅ | `imgsz`, `half`,
    `int8`, `batch`, `dynamic` |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | `engine` | `yolov8n.engine` | ✅ | `imgsz`, `half`, `dynamic`,
    `simplify`, `workspace`, `int8`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| CoreML | `coreml` | `yolov8n.mlpackage` | ✅ | `imgsz`, `half`, `int8`, `nms`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | `saved_model` | `yolov8n_saved_model/` | ✅ | `imgsz`, `keras`,
    `int8`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | `pb` | `yolov8n.pb` | ❌ | `imgsz`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | `tflite` | `yolov8n.tflite` | ✅ | `imgsz`, `half`, `int8`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: '| TF Edge TPU | `edgetpu` | `yolov8n_edgetpu.tflite` | ✅ | `imgsz` |'
  prefs: []
  type: TYPE_TB
- en: '| TF.js | `tfjs` | `yolov8n_web_model/` | ✅ | `imgsz`, `half`, `int8`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | `paddle` | `yolov8n_paddle_model/` | ✅ | `imgsz`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | `ncnn` | `yolov8n_ncnn_model/` | ✅ | `imgsz`, `half`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do I export a YOLOv8 model to ONNX format?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exporting a YOLOv8 model to ONNX format is straightforward with Ultralytics.
    It provides both Python and CLI methods for exporting models.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For more details on the process, including advanced options like handling different
    input sizes, refer to the ONNX section.
  prefs: []
  type: TYPE_NORMAL
- en: What are the benefits of using TensorRT for model export?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using TensorRT for model export offers significant performance improvements.
    YOLOv8 models exported to TensorRT can achieve up to a 5x GPU speedup, making
    it ideal for real-time inference applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Versatility:** Optimize models for a specific hardware setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed:** Achieve faster inference through advanced optimizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility:** Integrate smoothly with NVIDIA hardware.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about integrating TensorRT, see the TensorRT integration guide.
  prefs: []
  type: TYPE_NORMAL
- en: How do I enable INT8 quantization when exporting my YOLOv8 model?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'INT8 quantization is an excellent way to compress the model and speed up inference,
    especially on edge devices. Here''s how you can enable INT8 quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: INT8 quantization can be applied to various formats, such as TensorRT and CoreML.
    More details can be found in the Export section.
  prefs: []
  type: TYPE_NORMAL
- en: Why is dynamic input size important when exporting models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dynamic input size allows the exported model to handle varying image dimensions,
    providing flexibility and optimizing processing efficiency for different use cases.
    When exporting to formats like ONNX or TensorRT, enabling dynamic input size ensures
    that the model can adapt to different input shapes seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable this feature, use the `dynamic=True` flag during export:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For additional context, refer to the dynamic input size configuration.
  prefs: []
  type: TYPE_NORMAL
- en: What are the key export arguments to consider for optimizing model performance?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Understanding and configuring export arguments is crucial for optimizing model
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`format:`** The target format for the exported model (e.g., `onnx`, `torchscript`,
    `tensorflow`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`imgsz:`** Desired image size for the model input (e.g., `640` or `(height,
    width)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`half:`** Enables FP16 quantization, reducing model size and potentially
    speeding up inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`optimize:`** Applies specific optimizations for mobile or constrained environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`int8:`** Enables INT8 quantization, highly beneficial for edge deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a detailed list and explanations of all the export arguments, visit the
    Export Arguments section.
  prefs: []
  type: TYPE_NORMAL
