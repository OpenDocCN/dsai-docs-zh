- en: Pose Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/tasks/pose/`](https://docs.ultralytics.com/tasks/pose/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Pose estimation examples](img/48a7c9a6d42399ce7eddc7553488109a.png)'
  prefs: []
  type: TYPE_IMG
- en: Pose estimation is a task that involves identifying the location of specific
    points in an image, usually referred to as keypoints. The keypoints can represent
    various parts of the object such as joints, landmarks, or other distinctive features.
    The locations of the keypoints are usually represented as a set of 2D `[x, y]`
    or 3D `[x, y, visible]` coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: The output of a pose estimation model is a set of points that represent the
    keypoints on an object in the image, usually along with the confidence scores
    for each point. Pose estimation is a good choice when you need to identify specific
    parts of an object in a scene, and their location in relation to each other.
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/Y28xXQmju64?si=pCY4ZwejZFu6Z4kZ`](https://www.youtube.com/embed/Y28xXQmju64?si=pCY4ZwejZFu6Z4kZ)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** Pose Estimation with Ultralytics YOLOv8. |'
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/aeAX6vWpfR0`](https://www.youtube.com/embed/aeAX6vWpfR0)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** Pose Estimation with Ultralytics HUB. |'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv8 *pose* models use the `-pose` suffix, i.e. `yolov8n-pose.pt`. These models
    are trained on the [COCO keypoints](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco-pose.yaml)
    dataset and are suitable for a variety of pose estimation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the default YOLOv8 pose model, there are 17 keypoints, each representing
    a different part of the human body. Here is the mapping of each index to its respective
    body joint:'
  prefs: []
  type: TYPE_NORMAL
- en: '0: Nose 1: Left Eye 2: Right Eye 3: Left Ear 4: Right Ear 5: Left Shoulder
    6: Right Shoulder 7: Left Elbow 8: Right Elbow 9: Left Wrist 10: Right Wrist 11:
    Left Hip 12: Right Hip 13: Left Knee 14: Right Knee 15: Left Ankle 16: Right Ankle'
  prefs: []
  type: TYPE_NORMAL
- en: '[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models/v8)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YOLOv8 pretrained Pose models are shown here. Detect, Segment and Pose models
    are pretrained on the [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml)
    dataset, while Classify models are pretrained on the [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/ImageNet.yaml)
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models)
    download automatically from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases)
    on first use.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | size ^((pixels)) | mAP^(pose 50-95) | mAP^(pose 50) | Speed ^(CPU
    ONNX'
  prefs: []
  type: TYPE_NORMAL
- en: (ms)) | Speed ^(A100 TensorRT
  prefs: []
  type: TYPE_NORMAL
- en: (ms)) | params ^((M)) | FLOPs ^((B)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8n-pose](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n-pose.pt)
    | 640 | 50.4 | 80.1 | 131.8 | 1.18 | 3.3 | 9.2 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8s-pose](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-pose.pt)
    | 640 | 60.0 | 86.2 | 233.2 | 1.42 | 11.6 | 30.2 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8m-pose](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-pose.pt)
    | 640 | 65.0 | 88.8 | 456.3 | 2.00 | 26.4 | 81.0 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8l-pose](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-pose.pt)
    | 640 | 67.6 | 90.0 | 784.5 | 2.59 | 44.4 | 168.6 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8x-pose](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-pose.pt)
    | 640 | 69.2 | 90.2 | 1607.1 | 3.73 | 69.4 | 263.2 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8x-pose-p6](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-pose-p6.pt)
    | 1280 | 71.6 | 91.2 | 4088.7 | 10.04 | 99.1 | 1066.4 |'
  prefs: []
  type: TYPE_TB
- en: '**mAP^(val)** values are for single-model single-scale on [COCO Keypoints val2017](https://cocodataset.org)
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproduce by `yolo val pose data=coco-pose.yaml device=0`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/)
    instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproduce by `yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train a YOLOv8-pose model on the COCO128-pose dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Dataset format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: YOLO pose dataset format can be found in detail in the Dataset Guide. To convert
    your existing dataset from other formats (like COCO etc.) to YOLO format, please
    use [JSON2YOLO](https://github.com/ultralytics/JSON2YOLO) tool by Ultralytics.
  prefs: []
  type: TYPE_NORMAL
- en: Val
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Validate trained YOLOv8n-pose model accuracy on the COCO128-pose dataset. No
    argument need to passed as the `model` retains its training `data` and arguments
    as model attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Predict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a trained YOLOv8n-pose model to run predictions on images.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: See full `predict` mode details in the Predict page.
  prefs: []
  type: TYPE_NORMAL
- en: Export
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Export a YOLOv8n Pose model to a different format like ONNX, CoreML, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Available YOLOv8-pose export formats are in the table below. You can export
    to any format using the `format` argument, i.e. `format='onnx'` or `format='engine'`.
    You can predict or validate directly on exported models, i.e. `yolo predict model=yolov8n-pose.onnx`.
    Usage examples are shown for your model after export completes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | `format` Argument | Model | Metadata | Arguments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [PyTorch](https://pytorch.org/) | - | `yolov8n-pose.pt` | ✅ | - |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | `torchscript` | `yolov8n-pose.torchscript` | ✅ | `imgsz`, `optimize`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | `onnx` | `yolov8n-pose.onnx` | ✅ | `imgsz`, `half`, `dynamic`, `simplify`,
    `opset`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | `openvino` | `yolov8n-pose_openvino_model/` | ✅ | `imgsz`, `half`,
    `int8`, `batch`, `dynamic` |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | `engine` | `yolov8n-pose.engine` | ✅ | `imgsz`, `half`, `dynamic`,
    `simplify`, `workspace`, `int8`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| CoreML | `coreml` | `yolov8n-pose.mlpackage` | ✅ | `imgsz`, `half`, `int8`,
    `nms`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | `saved_model` | `yolov8n-pose_saved_model/` | ✅ | `imgsz`,
    `keras`, `int8`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | `pb` | `yolov8n-pose.pb` | ❌ | `imgsz`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | `tflite` | `yolov8n-pose.tflite` | ✅ | `imgsz`, `half`, `int8`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF Edge TPU | `edgetpu` | `yolov8n-pose_edgetpu.tflite` | ✅ | `imgsz` |'
  prefs: []
  type: TYPE_TB
- en: '| TF.js | `tfjs` | `yolov8n-pose_web_model/` | ✅ | `imgsz`, `half`, `int8`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | `paddle` | `yolov8n-pose_paddle_model/` | ✅ | `imgsz`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | `ncnn` | `yolov8n-pose_ncnn_model/` | ✅ | `imgsz`, `half`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: See full `export` details in the Export page.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is Pose Estimation with Ultralytics YOLOv8 and how does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pose estimation with Ultralytics YOLOv8 involves identifying specific points,
    known as keypoints, in an image. These keypoints typically represent joints or
    other important features of the object. The output includes the `[x, y]` coordinates
    and confidence scores for each point. YOLOv8-pose models are specifically designed
    for this task and use the `-pose` suffix, such as `yolov8n-pose.pt`. These models
    are pre-trained on datasets like [COCO keypoints](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco-pose.yaml)
    and can be used for various pose estimation tasks. For more information, visit
    the Pose Estimation Page.
  prefs: []
  type: TYPE_NORMAL
- en: How can I train a YOLOv8-pose model on a custom dataset?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training a YOLOv8-pose model on a custom dataset involves loading a model, either
    a new model defined by a YAML file or a pre-trained model. You can then start
    the training process using your specified dataset and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For comprehensive details on training, refer to the Train Section.
  prefs: []
  type: TYPE_NORMAL
- en: How do I validate a trained YOLOv8-pose model?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Validation of a YOLOv8-pose model involves assessing its accuracy using the
    same dataset parameters retained during training. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For more information, visit the Val Section.
  prefs: []
  type: TYPE_NORMAL
- en: Can I export a YOLOv8-pose model to other formats, and how?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Yes, you can export a YOLOv8-pose model to various formats like ONNX, CoreML,
    TensorRT, and more. This can be done using either Python or the Command Line Interface
    (CLI).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Refer to the Export Section for more details.
  prefs: []
  type: TYPE_NORMAL
- en: What are the available Ultralytics YOLOv8-pose models and their performance
    metrics?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ultralytics YOLOv8 offers various pretrained pose models such as YOLOv8n-pose,
    YOLOv8s-pose, YOLOv8m-pose, among others. These models differ in size, accuracy
    (mAP), and speed. For instance, the YOLOv8n-pose model achieves a mAP^(pose)50-95
    of 50.4 and an mAP^(pose)50 of 80.1\. For a complete list and performance details,
    visit the Models Section.
  prefs: []
  type: TYPE_NORMAL
