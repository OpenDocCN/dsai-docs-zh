# `scipy.optimize.isotonic_regression`

> 原文链接：[`docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.isotonic_regression.html#scipy.optimize.isotonic_regression`](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.isotonic_regression.html#scipy.optimize.isotonic_regression)

```py
scipy.optimize.isotonic_regression(y, *, weights=None, increasing=True)
```

非参数等温回归。

通过池相邻违反者算法（PAVA）计算出与*y*长度相同的（不严格）单调递增数组*x*，参见[[1]](#rddcb72c1ad4d-1)。更多细节请参见注释部分。

参数：

**y**(N,) array_like

响应变量。

**weights**(N,) array_like or None

案例权重。

**increasing**bool

如果为 True，则拟合单调递增，即等温，回归。如果为 False，则拟合单调递减，即反等温，回归。默认为 True。

返回：

**res**OptimizeResult

优化结果表示为`OptimizeResult`对象。重要属性包括：

+   `x`：等温回归解，即与 y 长度相同的递增（或递减）数组，元素范围从 min(y)到 max(y)。

+   `weights`：每个块（或池）B 的案例权重总和的数组。

+   `blocks`：长度为 B+1 的数组，其中包含每个块（或池）B 的起始位置的索引。第 j 个块由`x[blocks[j]:blocks[j+1]]`给出，其中所有值都相同。

注释

给定数据\(y\)和案例权重\(w\)，等温回归解决了以下优化问题：

\[\operatorname{argmin}_{x_i} \sum_i w_i (y_i - x_i)² \quad \text{subject to } x_i \leq x_j \text{ whenever } i \leq j \,.\]

对于每个输入值\(y_i\)，它生成一个值\(x_i\)，使得\(x\)是递增的（但不是严格的），即\(x_i \leq x_{i+1}\)。这是通过 PAVA 完成的。解决方案由池或块组成，即\(x\)的相邻元素，例如\(x_i\)和\(x_{i+1}\)，它们都具有相同的值。

最有趣的是，如果将平方损失替换为广泛的 Bregman 函数类，那么解决方案将保持不变，这些函数是均值的唯一一类严格一致的评分函数，参见[[2]](#rddcb72c1ad4d-2)及其中的参考文献。

根据[[1]](#rddcb72c1ad4d-1)实现的 PAVA 版本，其计算复杂度为 O(N)，其中 N 为输入大小。

参考文献

[1] (1,2)

Busing, F. M. T. A. (2022). 单调回归：简单快速的 O(n) PAVA 实现。《统计软件杂志》，代码片段，102(1)，1-25。[DOI:10.18637/jss.v102.c01](https://doi.org/10.18637/jss.v102.c01)

[2]

Jordan, A.I., Mühlemann, A. & Ziegel, J.F. 表征可识别函数的等温回归问题的最优解。《统计数学研究所通报》74，489-514 (2022)。[DOI:10.1007/s10463-021-00808-0](https://doi.org/10.1007/s10463-021-00808-0)

示例

该示例演示了`isotonic_regression`确实解决了一个受限制的优化问题。

```py
>>> import numpy as np
>>> from scipy.optimize import isotonic_regression, minimize
>>> y = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]
>>> def objective(yhat, y):
...     return np.sum((yhat - y)**2)
>>> def constraint(yhat, y):
...     # This is for a monotonically increasing regression.
...     return np.diff(yhat)
>>> result = minimize(objective, x0=y, args=(y,),
...                   constraints=[{'type': 'ineq',
...                                 'fun': lambda x: constraint(x, y)}])
>>> result.x
array([1.25      , 1.25      , 4\.        , 5.56666667, 5.56666667,
 5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,
 9.25      ])
>>> result = isotonic_regression(y)
>>> result.x
array([1.25      , 1.25      , 4\.        , 5.56666667, 5.56666667,
 5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,
 9.25      ]) 
```

相对于调用`minimize`，`isotonic_regression`的一个巨大优势在于它更加用户友好，即无需定义目标和约束函数，并且速度快上几个数量级。在普通硬件（2023 年）上，对长度为 1000 的正态分布输入 y 进行优化，最小化器大约需要 4 秒，而`isotonic_regression`只需大约 200 微秒。
