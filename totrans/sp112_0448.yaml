- en: scipy.optimize.least_squares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Solve a nonlinear least-squares problem with bounds on the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the residuals f(x) (an m-D real function of n real variables) and the
    loss function rho(s) (a scalar function), [`least_squares`](#scipy.optimize.least_squares
    "scipy.optimize.least_squares") finds a local minimum of the cost function F(x):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The purpose of the loss function rho(s) is to reduce the influence of outliers
    on the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**fun**callable'
  prefs: []
  type: TYPE_NORMAL
- en: Function which computes the vector of residuals, with the signature `fun(x,
    *args, **kwargs)`, i.e., the minimization proceeds with respect to its first argument.
    The argument `x` passed to this function is an ndarray of shape (n,) (never a
    scalar, even for n=1). It must allocate and return a 1-D array_like of shape (m,)
    or a scalar. If the argument `x` is complex or the function `fun` returns complex
    residuals, it must be wrapped in a real function of real arguments, as shown at
    the end of the Examples section.
  prefs: []
  type: TYPE_NORMAL
- en: '**x0**array_like with shape (n,) or float'
  prefs: []
  type: TYPE_NORMAL
- en: Initial guess on independent variables. If float, it will be treated as a 1-D
    array with one element. When *method* is ‘trf’, the initial guess might be slightly
    adjusted to lie sufficiently within the given *bounds*.
  prefs: []
  type: TYPE_NORMAL
- en: '**jac**{‘2-point’, ‘3-point’, ‘cs’, callable}, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Method of computing the Jacobian matrix (an m-by-n matrix, where element (i,
    j) is the partial derivative of f[i] with respect to x[j]). The keywords select
    a finite difference scheme for numerical estimation. The scheme ‘3-point’ is more
    accurate, but requires twice as many operations as ‘2-point’ (default). The scheme
    ‘cs’ uses complex steps, and while potentially the most accurate, it is applicable
    only when *fun* correctly handles complex inputs and can be analytically continued
    to the complex plane. Method ‘lm’ always uses the ‘2-point’ scheme. If callable,
    it is used as `jac(x, *args, **kwargs)` and should return a good approximation
    (or the exact value) for the Jacobian as an array_like (np.atleast_2d is applied),
    a sparse matrix (csr_matrix preferred for performance) or a [`scipy.sparse.linalg.LinearOperator`](scipy.sparse.linalg.LinearOperator.html#scipy.sparse.linalg.LinearOperator
    "scipy.sparse.linalg.LinearOperator").
  prefs: []
  type: TYPE_NORMAL
- en: '**bounds**2-tuple of array_like or [`Bounds`](scipy.optimize.Bounds.html#scipy.optimize.Bounds
    "scipy.optimize.Bounds"), optional'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to specify bounds:'
  prefs: []
  type: TYPE_NORMAL
- en: Instance of [`Bounds`](scipy.optimize.Bounds.html#scipy.optimize.Bounds "scipy.optimize.Bounds")
    class
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Lower and upper bounds on independent variables. Defaults to no bounds. Each
    array must match the size of *x0* or be a scalar, in the latter case a bound will
    be the same for all variables. Use `np.inf` with an appropriate sign to disable
    bounds on all or some variables.
  prefs:
  - PREF_BQ
  - PREF_OL
  type: TYPE_NORMAL
- en: '**method**{‘trf’, ‘dogbox’, ‘lm’}, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm to perform minimization.
  prefs: []
  type: TYPE_NORMAL
- en: '‘trf’ : Trust Region Reflective algorithm, particularly suitable for large
    sparse problems with bounds. Generally robust method.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '‘dogbox’ : dogleg algorithm with rectangular trust regions, typical use case
    is small problems with bounds. Not recommended for problems with rank-deficient
    Jacobian.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '‘lm’ : Levenberg-Marquardt algorithm as implemented in MINPACK. Doesn’t handle
    bounds and sparse Jacobians. Usually the most efficient method for small unconstrained
    problems.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Default is ‘trf’. See Notes for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '**ftol**float or None, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Tolerance for termination by the change of the cost function. Default is 1e-8\.
    The optimization process is stopped when `dF < ftol * F`, and there was an adequate
    agreement between a local quadratic model and the true model in the last step.
  prefs: []
  type: TYPE_NORMAL
- en: If None and ‘method’ is not ‘lm’, the termination by this condition is disabled.
    If ‘method’ is ‘lm’, this tolerance must be higher than machine epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: '**xtol**float or None, optional'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tolerance for termination by the change of the independent variables. Default
    is 1e-8\. The exact condition depends on the *method* used:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For ‘trf’ and ‘dogbox’ : `norm(dx) < xtol * (xtol + norm(x))`.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For ‘lm’ : `Delta < xtol * norm(xs)`, where `Delta` is a trust-region radius
    and `xs` is the value of `x` scaled according to *x_scale* parameter (see below).'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: If None and ‘method’ is not ‘lm’, the termination by this condition is disabled.
    If ‘method’ is ‘lm’, this tolerance must be higher than machine epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: '**gtol**float or None, optional'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tolerance for termination by the norm of the gradient. Default is 1e-8. The
    exact condition depends on a *method* used:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For ‘trf’ : `norm(g_scaled, ord=np.inf) < gtol`, where `g_scaled` is the value
    of the gradient scaled to account for the presence of the bounds [[STIR]](#r20fc1df64af7-stir).'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For ‘dogbox’ : `norm(g_free, ord=np.inf) < gtol`, where `g_free` is the gradient
    with respect to the variables which are not in the optimal state on the boundary.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For ‘lm’ : the maximum absolute value of the cosine of angles between columns
    of the Jacobian and the residual vector is less than *gtol*, or the residual vector
    is zero.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: If None and ‘method’ is not ‘lm’, the termination by this condition is disabled.
    If ‘method’ is ‘lm’, this tolerance must be higher than machine epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: '**x_scale**array_like or ‘jac’, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Characteristic scale of each variable. Setting *x_scale* is equivalent to reformulating
    the problem in scaled variables `xs = x / x_scale`. An alternative view is that
    the size of a trust region along jth dimension is proportional to `x_scale[j]`.
    Improved convergence may be achieved by setting *x_scale* such that a step of
    a given size along any of the scaled variables has a similar effect on the cost
    function. If set to ‘jac’, the scale is iteratively updated using the inverse
    norms of the columns of the Jacobian matrix (as described in [[JJMore]](#r20fc1df64af7-jjmore)).
  prefs: []
  type: TYPE_NORMAL
- en: '**loss**str or callable, optional'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determines the loss function. The following keyword values are allowed:'
  prefs: []
  type: TYPE_NORMAL
- en: '‘linear’ (default) : `rho(z) = z`. Gives a standard least-squares problem.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '‘soft_l1’ : `rho(z) = 2 * ((1 + z)**0.5 - 1)`. The smooth approximation of
    l1 (absolute value) loss. Usually a good choice for robust least squares.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '‘huber’ : `rho(z) = z if z <= 1 else 2*z**0.5 - 1`. Works similarly to ‘soft_l1’.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '‘cauchy’ : `rho(z) = ln(1 + z)`. Severely weakens outliers influence, but may
    cause difficulties in optimization process.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '‘arctan’ : `rho(z) = arctan(z)`. Limits a maximum loss on a single residual,
    has properties similar to ‘cauchy’.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: If callable, it must take a 1-D ndarray `z=f**2` and return an array_like with
    shape (3, m) where row 0 contains function values, row 1 contains first derivatives
    and row 2 contains second derivatives. Method ‘lm’ supports only ‘linear’ loss.
  prefs: []
  type: TYPE_NORMAL
- en: '**f_scale**float, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Value of soft margin between inlier and outlier residuals, default is 1.0\.
    The loss function is evaluated as follows `rho_(f**2) = C**2 * rho(f**2 / C**2)`,
    where `C` is *f_scale*, and `rho` is determined by *loss* parameter. This parameter
    has no effect with `loss='linear'`, but for other *loss* values it is of crucial
    importance.
  prefs: []
  type: TYPE_NORMAL
- en: '**max_nfev**None or int, optional'
  prefs: []
  type: TYPE_NORMAL
- en: 'Maximum number of function evaluations before the termination. If None (default),
    the value is chosen automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For ‘trf’ and ‘dogbox’ : 100 * n.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For ‘lm’ : 100 * n if *jac* is callable and 100 * n * (n + 1) otherwise (because
    ‘lm’ counts function calls in Jacobian estimation).'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '**diff_step**None or array_like, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Determines the relative step size for the finite difference approximation of
    the Jacobian. The actual step is computed as `x * diff_step`. If None (default),
    then *diff_step* is taken to be a conventional “optimal” power of machine epsilon
    for the finite difference scheme used [[NR]](#r20fc1df64af7-nr).
  prefs: []
  type: TYPE_NORMAL
- en: '**tr_solver**{None, ‘exact’, ‘lsmr’}, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Method for solving trust-region subproblems, relevant only for ‘trf’ and ‘dogbox’
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: ‘exact’ is suitable for not very large problems with dense Jacobian matrices.
    The computational complexity per iteration is comparable to a singular value decomposition
    of the Jacobian matrix.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ‘lsmr’ is suitable for problems with sparse and large Jacobian matrices. It
    uses the iterative procedure [`scipy.sparse.linalg.lsmr`](scipy.sparse.linalg.lsmr.html#scipy.sparse.linalg.lsmr
    "scipy.sparse.linalg.lsmr") for finding a solution of a linear least-squares problem
    and only requires matrix-vector product evaluations.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: If None (default), the solver is chosen based on the type of Jacobian returned
    on the first iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '**tr_options**dict, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Keyword options passed to trust-region solver.
  prefs: []
  type: TYPE_NORMAL
- en: '`tr_solver=''exact''`: *tr_options* are ignored.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '`tr_solver=''lsmr''`: options for [`scipy.sparse.linalg.lsmr`](scipy.sparse.linalg.lsmr.html#scipy.sparse.linalg.lsmr
    "scipy.sparse.linalg.lsmr"). Additionally, `method=''trf''` supports ‘regularize’
    option (bool, default is True), which adds a regularization term to the normal
    equation, which improves convergence if the Jacobian is rank-deficient [[Byrd]](#r20fc1df64af7-byrd)
    (eq. 3.4).'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '**jac_sparsity**{None, array_like, sparse matrix}, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the sparsity structure of the Jacobian matrix for finite difference
    estimation, its shape must be (m, n). If the Jacobian has only few non-zero elements
    in *each* row, providing the sparsity structure will greatly speed up the computations
    [[Curtis]](#r20fc1df64af7-curtis). A zero entry means that a corresponding element
    in the Jacobian is identically zero. If provided, forces the use of ‘lsmr’ trust-region
    solver. If None (default), then dense differencing will be used. Has no effect
    for ‘lm’ method.
  prefs: []
  type: TYPE_NORMAL
- en: '**verbose**{0, 1, 2}, optional'
  prefs: []
  type: TYPE_NORMAL
- en: 'Level of algorithm’s verbosity:'
  prefs: []
  type: TYPE_NORMAL
- en: '0 (default) : work silently.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '1 : display a termination report.'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '2 : display progress during iterations (not supported by ‘lm’ method).'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '**args, kwargs**tuple and dict, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Additional arguments passed to *fun* and *jac*. Both empty by default. The calling
    signature is `fun(x, *args, **kwargs)` and the same for *jac*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**result**OptimizeResult'
  prefs: []
  type: TYPE_NORMAL
- en: '[`OptimizeResult`](scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult
    "scipy.optimize.OptimizeResult") with the following fields defined:'
  prefs: []
  type: TYPE_NORMAL
- en: xndarray, shape (n,)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Solution found.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: costfloat
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Value of the cost function at the solution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: funndarray, shape (m,)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vector of residuals at the solution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: jacndarray, sparse matrix or LinearOperator, shape (m, n)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Modified Jacobian matrix at the solution, in the sense that J^T J is a Gauss-Newton
    approximation of the Hessian of the cost function. The type is the same as the
    one used by the algorithm.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: gradndarray, shape (m,)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gradient of the cost function at the solution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: optimalityfloat
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: First-order optimality measure. In unconstrained problems, it is always the
    uniform norm of the gradient. In constrained problems, it is the quantity which
    was compared with *gtol* during iterations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: active_maskndarray of int, shape (n,)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Each component shows whether a corresponding constraint is active (that is,
    whether a variable is at the bound):'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '0 : a constraint is not active.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '-1 : a lower bound is active.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '1 : an upper bound is active.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Might be somewhat arbitrary for ‘trf’ method as it generates a sequence of strictly
    feasible iterates and *active_mask* is determined within a tolerance threshold.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: nfevint
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Number of function evaluations done. Methods ‘trf’ and ‘dogbox’ do not count
    function calls for numerical Jacobian approximation, as opposed to ‘lm’ method.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: njevint or None
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Number of Jacobian evaluations done. If numerical Jacobian approximation is
    used in ‘lm’ method, it is set to None.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: statusint
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The reason for algorithm termination:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '-1 : improper input parameters status returned from MINPACK.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '0 : the maximum number of function evaluations is exceeded.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '1 : *gtol* termination condition is satisfied.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '2 : *ftol* termination condition is satisfied.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '3 : *xtol* termination condition is satisfied.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '4 : Both *ftol* and *xtol* termination conditions are satisfied.'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: messagestr
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Verbal description of the termination reason.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: successbool
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: True if one of the convergence criteria is satisfied (*status* > 0).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See also
  prefs: []
  type: TYPE_NORMAL
- en: '[`leastsq`](scipy.optimize.leastsq.html#scipy.optimize.leastsq "scipy.optimize.leastsq")'
  prefs: []
  type: TYPE_NORMAL
- en: A legacy wrapper for the MINPACK implementation of the Levenberg-Marquadt algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[`curve_fit`](scipy.optimize.curve_fit.html#scipy.optimize.curve_fit "scipy.optimize.curve_fit")'
  prefs: []
  type: TYPE_NORMAL
- en: Least-squares minimization applied to a curve-fitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs: []
  type: TYPE_NORMAL
- en: Method ‘lm’ (Levenberg-Marquardt) calls a wrapper over least-squares algorithms
    implemented in MINPACK (lmder, lmdif). It runs the Levenberg-Marquardt algorithm
    formulated as a trust-region type algorithm. The implementation is based on paper
    [[JJMore]](#r20fc1df64af7-jjmore), it is very robust and efficient with a lot
    of smart tricks. It should be your first choice for unconstrained problems. Note
    that it doesn’t support bounds. Also, it doesn’t work when m < n.
  prefs: []
  type: TYPE_NORMAL
- en: Method ‘trf’ (Trust Region Reflective) is motivated by the process of solving
    a system of equations, which constitute the first-order optimality condition for
    a bound-constrained minimization problem as formulated in [[STIR]](#r20fc1df64af7-stir).
    The algorithm iteratively solves trust-region subproblems augmented by a special
    diagonal quadratic term and with trust-region shape determined by the distance
    from the bounds and the direction of the gradient. This enhancements help to avoid
    making steps directly into bounds and efficiently explore the whole space of variables.
    To further improve convergence, the algorithm considers search directions reflected
    from the bounds. To obey theoretical requirements, the algorithm keeps iterates
    strictly feasible. With dense Jacobians trust-region subproblems are solved by
    an exact method very similar to the one described in [[JJMore]](#r20fc1df64af7-jjmore)
    (and implemented in MINPACK). The difference from the MINPACK implementation is
    that a singular value decomposition of a Jacobian matrix is done once per iteration,
    instead of a QR decomposition and series of Givens rotation eliminations. For
    large sparse Jacobians a 2-D subspace approach of solving trust-region subproblems
    is used [[STIR]](#r20fc1df64af7-stir), [[Byrd]](#r20fc1df64af7-byrd). The subspace
    is spanned by a scaled gradient and an approximate Gauss-Newton solution delivered
    by [`scipy.sparse.linalg.lsmr`](scipy.sparse.linalg.lsmr.html#scipy.sparse.linalg.lsmr
    "scipy.sparse.linalg.lsmr"). When no constraints are imposed the algorithm is
    very similar to MINPACK and has generally comparable performance. The algorithm
    works quite robust in unbounded and bounded problems, thus it is chosen as a default
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Method ‘dogbox’ operates in a trust-region framework, but considers rectangular
    trust regions as opposed to conventional ellipsoids [[Voglis]](#r20fc1df64af7-voglis).
    The intersection of a current trust region and initial bounds is again rectangular,
    so on each iteration a quadratic minimization problem subject to bound constraints
    is solved approximately by Powell’s dogleg method [[NumOpt]](#r20fc1df64af7-numopt).
    The required Gauss-Newton step can be computed exactly for dense Jacobians or
    approximately by [`scipy.sparse.linalg.lsmr`](scipy.sparse.linalg.lsmr.html#scipy.sparse.linalg.lsmr
    "scipy.sparse.linalg.lsmr") for large sparse Jacobians. The algorithm is likely
    to exhibit slow convergence when the rank of Jacobian is less than the number
    of variables. The algorithm often outperforms ‘trf’ in bounded problems with a
    small number of variables.
  prefs: []
  type: TYPE_NORMAL
- en: Robust loss functions are implemented as described in [[BA]](#r20fc1df64af7-ba).
    The idea is to modify a residual vector and a Jacobian matrix on each iteration
    such that computed gradient and Gauss-Newton Hessian approximation match the true
    gradient and Hessian approximation of the cost function. Then the algorithm proceeds
    in a normal way, i.e., robust loss functions are implemented as a simple wrapper
    over standard least-squares algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 0.17.0.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs: []
  type: TYPE_NORMAL
- en: '[STIR] ([1](#id1),[2](#id7),[3](#id9))'
  prefs: []
  type: TYPE_NORMAL
- en: M. A. Branch, T. F. Coleman, and Y. Li, “A Subspace, Interior, and Conjugate
    Gradient Method for Large-Scale Bound-Constrained Minimization Problems,” SIAM
    Journal on Scientific Computing, Vol. 21, Number 1, pp 1-23, 1999.
  prefs: []
  type: TYPE_NORMAL
- en: '[[NR](#id3)]'
  prefs: []
  type: TYPE_NORMAL
- en: William H. Press et. al., “Numerical Recipes. The Art of Scientific Computing.
    3rd edition”, Sec. 5.7.
  prefs: []
  type: TYPE_NORMAL
- en: '[Byrd] ([1](#id4),[2](#id10))'
  prefs: []
  type: TYPE_NORMAL
- en: R. H. Byrd, R. B. Schnabel and G. A. Shultz, “Approximate solution of the trust
    region problem by minimization over two-dimensional subspaces”, Math. Programming,
    40, pp. 247-263, 1988.
  prefs: []
  type: TYPE_NORMAL
- en: '[[Curtis](#id5)]'
  prefs: []
  type: TYPE_NORMAL
- en: A. Curtis, M. J. D. Powell, and J. Reid, “On the estimation of sparse Jacobian
    matrices”, Journal of the Institute of Mathematics and its Applications, 13, pp.
    117-120, 1974.
  prefs: []
  type: TYPE_NORMAL
- en: '[JJMore] ([1](#id2),[2](#id6),[3](#id8))'
  prefs: []
  type: TYPE_NORMAL
- en: 'J. J. More, “The Levenberg-Marquardt Algorithm: Implementation and Theory,”
    Numerical Analysis, ed. G. A. Watson, Lecture Notes in Mathematics 630, Springer
    Verlag, pp. 105-116, 1977.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[Voglis](#id11)]'
  prefs: []
  type: TYPE_NORMAL
- en: C. Voglis and I. E. Lagaris, “A Rectangular Trust Region Dogleg Approach for
    Unconstrained and Bound Constrained Nonlinear Optimization”, WSEAS International
    Conference on Applied Mathematics, Corfu, Greece, 2004.
  prefs: []
  type: TYPE_NORMAL
- en: '[[NumOpt](#id12)]'
  prefs: []
  type: TYPE_NORMAL
- en: J. Nocedal and S. J. Wright, “Numerical optimization, 2nd edition”, Chapter
    4.
  prefs: []
  type: TYPE_NORMAL
- en: '[[BA](#id13)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'B. Triggs et. al., “Bundle Adjustment - A Modern Synthesis”, Proceedings of
    the International Workshop on Vision Algorithms: Theory and Practice, pp. 298-372,
    1999.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: In this example we find a minimum of the Rosenbrock function without bounds
    on independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we only provide the vector of the residuals. The algorithm constructs
    the cost function as a sum of squares of the residuals, which gives the Rosenbrock
    function. The exact minimum is at `x = [1.0, 1.0]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We now constrain the variables, in such a way that the previous solution becomes
    infeasible. Specifically, we require that `x[1] >= 1.5`, and `x[0]` left unconstrained.
    To this end, we specify the *bounds* parameter to [`least_squares`](#scipy.optimize.least_squares
    "scipy.optimize.least_squares") in the form `bounds=([-np.inf, 1.5], np.inf)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also provide the analytic Jacobian:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting this all together, we see that the new solution lies on the bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we solve a system of equations (i.e., the cost function should be zero
    at a minimum) for a Broyden tridiagonal vector-valued function of 100000 variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding Jacobian matrix is sparse. We tell the algorithm to estimate
    it by finite differences and provide the sparsity structure of Jacobian to significantly
    speed up this process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let’s also solve a curve fitting problem using robust loss function to take
    care of outliers in the data. Define the model function as `y = a + b * exp(c
    * t)`, where t is a predictor variable, y is an observation and a, b, c are parameters
    to estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the function which generates the data with noise and outliers,
    define the model parameters, and generate data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Define function for computing residuals and initial estimate of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute a standard least-squares solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now compute two solutions with two different robust loss functions. The parameter
    *f_scale* is set to 0.1, meaning that inlier residuals should not significantly
    exceed 0.1 (the noise level used).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: And, finally, plot all the curves. We see that by selecting an appropriate *loss*
    we can get estimates close to optimal even in the presence of strong outliers.
    But keep in mind that generally it is recommended to try ‘soft_l1’ or ‘huber’
    losses first (if at all necessary) as the other two options may cause difficulties
    in optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/scipy-optimize-least_squares-1_00_00.png](../Images/2ae49de882c02b5c0b611ae4394a5dd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next example, we show how complex-valued residual functions of complex
    variables can be optimized with `least_squares()`. Consider the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We wrap it into a function of real variables that returns real residuals by
    simply handling the real and imaginary parts as independent variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, instead of the original m-D complex function of n complex variables we
    optimize a 2m-D real function of 2n real variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
