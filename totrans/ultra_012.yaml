- en: Instance Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/tasks/segment/`](https://docs.ultralytics.com/tasks/segment/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Instance segmentation examples](img/1b1a79830659e97d5d415d67b83defc0.png)'
  prefs: []
  type: TYPE_IMG
- en: Instance segmentation goes a step further than object detection and involves
    identifying individual objects in an image and segmenting them from the rest of
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: The output of an instance segmentation model is a set of masks or contours that
    outline each object in the image, along with class labels and confidence scores
    for each object. Instance segmentation is useful when you need to know not only
    where objects are in an image, but also what their exact shape is.
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/o4Zd-IeMlSY?si=37nusCzDTd74Obsp`](https://www.youtube.com/embed/o4Zd-IeMlSY?si=37nusCzDTd74Obsp)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** Run Segmentation with Pre-Trained Ultralytics YOLOv8 Model in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv8 Segment models use the `-seg` suffix, i.e. `yolov8n-seg.pt` and are pretrained
    on [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: '[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models/v8)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YOLOv8 pretrained Segment models are shown here. Detect, Segment and Pose models
    are pretrained on the [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml)
    dataset, while Classify models are pretrained on the [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/ImageNet.yaml)
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models)
    download automatically from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases)
    on first use.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | size ^((pixels)) | mAP^(box 50-95) | mAP^(mask 50-95) | Speed ^(CPU
    ONNX'
  prefs: []
  type: TYPE_NORMAL
- en: (ms)) | Speed ^(A100 TensorRT
  prefs: []
  type: TYPE_NORMAL
- en: (ms)) | params ^((M)) | FLOPs ^((B)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8n-seg](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n-seg.pt)
    | 640 | 36.7 | 30.5 | 96.1 | 1.21 | 3.4 | 12.6 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8s-seg](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-seg.pt)
    | 640 | 44.6 | 36.8 | 155.7 | 1.47 | 11.8 | 42.6 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8m-seg](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-seg.pt)
    | 640 | 49.9 | 40.8 | 317.0 | 2.18 | 27.3 | 110.2 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8l-seg](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-seg.pt)
    | 640 | 52.3 | 42.6 | 572.4 | 2.79 | 46.0 | 220.5 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8x-seg](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-seg.pt)
    | 640 | 53.4 | 43.4 | 712.1 | 4.02 | 71.8 | 344.1 |'
  prefs: []
  type: TYPE_TB
- en: '**mAP^(val)** values are for single-model single-scale on [COCO val2017](https://cocodataset.org)
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproduce by `yolo val segment data=coco.yaml device=0`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/)
    instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproduce by `yolo val segment data=coco8-seg.yaml batch=1 device=0|cpu`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train YOLOv8n-seg on the COCO128-seg dataset for 100 epochs at image size 640\.
    For a full list of available arguments see the Configuration page.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Dataset format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: YOLO segmentation dataset format can be found in detail in the Dataset Guide.
    To convert your existing dataset from other formats (like COCO etc.) to YOLO format,
    please use [JSON2YOLO](https://github.com/ultralytics/JSON2YOLO) tool by Ultralytics.
  prefs: []
  type: TYPE_NORMAL
- en: Val
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Validate trained YOLOv8n-seg model accuracy on the COCO128-seg dataset. No argument
    need to passed as the `model` retains its training `data` and arguments as model
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Predict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a trained YOLOv8n-seg model to run predictions on images.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: See full `predict` mode details in the Predict page.
  prefs: []
  type: TYPE_NORMAL
- en: Export
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Export a YOLOv8n-seg model to a different format like ONNX, CoreML, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Available YOLOv8-seg export formats are in the table below. You can export to
    any format using the `format` argument, i.e. `format='onnx'` or `format='engine'`.
    You can predict or validate directly on exported models, i.e. `yolo predict model=yolov8n-seg.onnx`.
    Usage examples are shown for your model after export completes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | `format` Argument | Model | Metadata | Arguments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [PyTorch](https://pytorch.org/) | - | `yolov8n-seg.pt` | ✅ | - |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | `torchscript` | `yolov8n-seg.torchscript` | ✅ | `imgsz`, `optimize`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | `onnx` | `yolov8n-seg.onnx` | ✅ | `imgsz`, `half`, `dynamic`, `simplify`,
    `opset`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | `openvino` | `yolov8n-seg_openvino_model/` | ✅ | `imgsz`, `half`,
    `int8`, `batch`, `dynamic` |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | `engine` | `yolov8n-seg.engine` | ✅ | `imgsz`, `half`, `dynamic`,
    `simplify`, `workspace`, `int8`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| CoreML | `coreml` | `yolov8n-seg.mlpackage` | ✅ | `imgsz`, `half`, `int8`,
    `nms`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | `saved_model` | `yolov8n-seg_saved_model/` | ✅ | `imgsz`,
    `keras`, `int8`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | `pb` | `yolov8n-seg.pb` | ❌ | `imgsz`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | `tflite` | `yolov8n-seg.tflite` | ✅ | `imgsz`, `half`, `int8`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF Edge TPU | `edgetpu` | `yolov8n-seg_edgetpu.tflite` | ✅ | `imgsz` |'
  prefs: []
  type: TYPE_TB
- en: '| TF.js | `tfjs` | `yolov8n-seg_web_model/` | ✅ | `imgsz`, `half`, `int8`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | `paddle` | `yolov8n-seg_paddle_model/` | ✅ | `imgsz`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | `ncnn` | `yolov8n-seg_ncnn_model/` | ✅ | `imgsz`, `half`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: See full `export` details in the Export page.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do I train a YOLOv8 segmentation model on a custom dataset?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train a YOLOv8 segmentation model on a custom dataset, you first need to
    prepare your dataset in the YOLO segmentation format. You can use tools like [JSON2YOLO](https://github.com/ultralytics/JSON2YOLO)
    to convert datasets from other formats. Once your dataset is ready, you can train
    the model using Python or CLI commands:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Check the Configuration page for more available arguments.
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between object detection and instance segmentation in
    YOLOv8?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object detection identifies and localizes objects within an image by drawing
    bounding boxes around them, whereas instance segmentation not only identifies
    the bounding boxes but also delineates the exact shape of each object. YOLOv8
    instance segmentation models provide masks or contours that outline each detected
    object, which is particularly useful for tasks where knowing the precise shape
    of objects is important, such as medical imaging or autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: Why use YOLOv8 for instance segmentation?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ultralytics YOLOv8 is a state-of-the-art model recognized for its high accuracy
    and real-time performance, making it ideal for instance segmentation tasks. YOLOv8
    Segment models come pretrained on the [COCO dataset](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml),
    ensuring robust performance across a variety of objects. Additionally, YOLOv8
    supports training, validation, prediction, and export functionalities with seamless
    integration, making it highly versatile for both research and industry applications.
  prefs: []
  type: TYPE_NORMAL
- en: How do I load and validate a pretrained YOLOv8 segmentation model?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Loading and validating a pretrained YOLOv8 segmentation model is straightforward.
    Here''s how you can do it using both Python and CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These steps will provide you with validation metrics like Mean Average Precision
    (mAP), crucial for assessing model performance.
  prefs: []
  type: TYPE_NORMAL
- en: How can I export a YOLOv8 segmentation model to ONNX format?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exporting a YOLOv8 segmentation model to ONNX format is simple and can be done
    using Python or CLI commands:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For more details on exporting to various formats, refer to the Export page.
  prefs: []
  type: TYPE_NORMAL
