- en: 'SAM 2: Segment Anything Model 2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SAM 2：Segment Anything Model 2
- en: 原文：[`docs.ultralytics.com/models/sam-2/`](https://docs.ultralytics.com/models/sam-2/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`docs.ultralytics.com/models/sam-2/`](https://docs.ultralytics.com/models/sam-2/)
- en: SAM 2, the successor to Meta's Segment Anything Model (SAM), is a cutting-edge
    tool designed for comprehensive object segmentation in both images and videos.
    It excels in handling complex visual data through a unified, promptable model
    architecture that supports real-time processing and zero-shot generalization.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2，Meta 的 Segment Anything Model (SAM) 的后继者，是一个为图像和视频中全面对象分割而设计的前沿工具。通过支持实时处理和零射击泛化的统一、可提示的模型架构，它在处理复杂视觉数据方面表现出色。
- en: '![SAM 2 Example Results](img/cc4b63672ee7fc207229c006a771c8ee.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![SAM 2 示例结果](img/cc4b63672ee7fc207229c006a771c8ee.png)'
- en: Key Features
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键特性
- en: Unified Model Architecture
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统一模型架构
- en: SAM 2 combines the capabilities of image and video segmentation in a single
    model. This unification simplifies deployment and allows for consistent performance
    across different media types. It leverages a flexible prompt-based interface,
    enabling users to specify objects of interest through various prompt types, such
    as points, bounding boxes, or masks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2 将图像和视频分割的能力结合到单一模型中。这种统一简化了部署，并允许在不同媒体类型上保持一致的性能。它利用灵活的基于提示的接口，使用户能够通过各种提示类型（如点、边界框或掩码）指定感兴趣的对象。
- en: Real-Time Performance
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时性能
- en: The model achieves real-time inference speeds, processing approximately 44 frames
    per second. This makes SAM 2 suitable for applications requiring immediate feedback,
    such as video editing and augmented reality.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型实现了实时推断速度，每秒处理约44帧。这使得SAM 2 适用于需要即时反馈的应用，如视频编辑和增强现实。
- en: Zero-Shot Generalization
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 零射击泛化
- en: SAM 2 can segment objects it has never encountered before, demonstrating strong
    zero-shot generalization. This is particularly useful in diverse or evolving visual
    domains where pre-defined categories may not cover all possible objects.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2 能够分割它从未遇到过的对象，展示出强大的零射击泛化能力。这在多样化或不断变化的视觉领域特别有用，预定义的类别可能无法涵盖所有可能的对象。
- en: Interactive Refinement
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交互式完善
- en: Users can iteratively refine the segmentation results by providing additional
    prompts, allowing for precise control over the output. This interactivity is essential
    for fine-tuning results in applications like video annotation or medical imaging.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以通过提供额外提示来逐步完善分割结果，从而精确控制输出。这种互动性对于在视频注释或医学成像等应用中微调结果至关重要。
- en: Advanced Handling of Visual Challenges
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级视觉挑战处理
- en: SAM 2 includes mechanisms to manage common video segmentation challenges, such
    as object occlusion and reappearance. It uses a sophisticated memory mechanism
    to keep track of objects across frames, ensuring continuity even when objects
    are temporarily obscured or exit and re-enter the scene.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2 包括管理常见视频分割挑战的机制，如对象遮挡和重新出现。它使用先进的内存机制跟踪对象跨帧，确保即使对象暂时被遮挡或退出再进入场景，也能保持连续性。
- en: For a deeper understanding of SAM 2's architecture and capabilities, explore
    the [SAM 2 research paper](https://arxiv.org/abs/2401.12741).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解 SAM 2 的架构和能力更深入的理解，请查阅 [SAM 2 研究论文](https://arxiv.org/abs/2401.12741)。
- en: Performance and Technical Details
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能和技术细节
- en: 'SAM 2 sets a new benchmark in the field, outperforming previous models on various
    metrics:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2 在领域中树立了新的标准，在各种指标上优于先前的模型：
- en: '| Metric | SAM 2 | Previous SOTA |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | SAM 2 | 先前的SOTA |'
- en: '| --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Interactive Video Segmentation** | **Best** | - |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **交互式视频分割** | **最佳** | - |'
- en: '| **Human Interactions Required** | **3x fewer** | Baseline |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **需要人类交互** | **少3倍** | 基准 |'
- en: '| **Image Segmentation Accuracy** | **Improved** | SAM |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **图像分割准确性** | **提升** | SAM |'
- en: '| **Inference Speed** | **6x faster** | SAM |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **推断速度** | **快6倍** | SAM |'
- en: Model Architecture
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: Core Components
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心组件
- en: '**Image and Video Encoder**: Utilizes a transformer-based architecture to extract
    high-level features from both images and video frames. This component is responsible
    for understanding the visual content at each timestep.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像和视频编码器**：利用基于Transformer的架构从图像和视频帧中提取高级特征。该组件负责理解每个时间步的视觉内容。'
- en: '**Prompt Encoder**: Processes user-provided prompts (points, boxes, masks)
    to guide the segmentation task. This allows SAM 2 to adapt to user input and target
    specific objects within a scene.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示编码器**：处理用户提供的提示（点、框、掩码）以指导分割任务。这使得SAM 2 能够适应用户输入并针对场景中特定对象进行目标定位。'
- en: '**Memory Mechanism**: Includes a memory encoder, memory bank, and memory attention
    module. These components collectively store and utilize information from past
    frames, enabling the model to maintain consistent object tracking over time.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储机制**：包括内存编码器、内存存储器和内存注意模块。这些组件共同存储并利用过去帧的信息，使模型能够随时间保持一致的对象跟踪。'
- en: '**Mask Decoder**: Generates the final segmentation masks based on the encoded
    image features and prompts. In video, it also uses memory context to ensure accurate
    tracking across frames.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掩模解码器**：根据编码图像特征和提示生成最终的分割掩模。在视频中，它还使用内存上下文确保跨帧的准确跟踪。'
- en: '![SAM 2 Architecture Diagram](img/e0f99a69f8aa0ebd6d37f97d957a4e05.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![SAM 2 架构图](img/e0f99a69f8aa0ebd6d37f97d957a4e05.png)'
- en: Memory Mechanism and Occlusion Handling
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储机制和遮挡处理
- en: The memory mechanism allows SAM 2 to handle temporal dependencies and occlusions
    in video data. As objects move and interact, SAM 2 records their features in a
    memory bank. When an object becomes occluded, the model can rely on this memory
    to predict its position and appearance when it reappears. The occlusion head specifically
    handles scenarios where objects are not visible, predicting the likelihood of
    an object being occluded.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 存储机制使得 SAM 2 能够处理视频数据中的时间依赖性和遮挡。当对象移动和交互时，SAM 2 将它们的特征记录在内存存储器中。当一个对象被遮挡时，模型可以依靠这些记忆来预测它再次出现时的位置和外观。遮挡头特别处理对象不可见的情况，预测对象被遮挡的可能性。
- en: Multi-Mask Ambiguity Resolution
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多掩模歧义解析
- en: In situations with ambiguity (e.g., overlapping objects), SAM 2 can generate
    multiple mask predictions. This feature is crucial for accurately representing
    complex scenes where a single mask might not sufficiently describe the scene's
    nuances.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在歧义的情况下（例如重叠对象），SAM 2 可以生成多个掩模预测。这一功能对于准确表达复杂场景至关重要，其中单个掩模可能不足以描述场景的细微差别。
- en: SA-V Dataset
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SA-V 数据集
- en: 'The SA-V dataset, developed for SAM 2''s training, is one of the largest and
    most diverse video segmentation datasets available. It includes:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为 SAM 2 的训练而开发的 SA-V 数据集是目前最大和最多样化的视频分割数据集之一。它包括：
- en: '**51,000+ Videos**: Captured across 47 countries, providing a wide range of
    real-world scenarios.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**51,000+ 视频**：来自47个国家，提供了广泛的真实场景。'
- en: '**600,000+ Mask Annotations**: Detailed spatio-temporal mask annotations, referred
    to as "masklets," covering whole objects and parts.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**600,000+ 掩模标注**：详细的时空掩模标注，称为“掩模片段”，涵盖了整个对象和部分对象。'
- en: '**Dataset Scale**: It features 4.5 times more videos and 53 times more annotations
    than previous largest datasets, offering unprecedented diversity and complexity.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集规模**：比之前最大的数据集包含的视频多了4.5倍，标注多了53倍，提供了前所未有的多样性和复杂性。'
- en: Benchmarks
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试
- en: Video Object Segmentation
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视频对象分割
- en: 'SAM 2 has demonstrated superior performance across major video segmentation
    benchmarks:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2 在主要视频分割基准测试中展示了优越的性能：
- en: '| Dataset | J&F | J | F |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | J&F | J | F |'
- en: '| --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **DAVIS 2017** | 82.5 | 79.8 | 85.2 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **DAVIS 2017** | 82.5 | 79.8 | 85.2 |'
- en: '| **YouTube-VOS** | 81.2 | 78.9 | 83.5 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **YouTube-VOS** | 81.2 | 78.9 | 83.5 |'
- en: Interactive Segmentation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交互式分割
- en: 'In interactive segmentation tasks, SAM 2 shows significant efficiency and accuracy:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在交互式分割任务中，SAM 2 表现出显著的效率和准确性：
- en: '| Dataset | NoC@90 | AUC |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | NoC@90 | AUC |'
- en: '| --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **DAVIS Interactive** | 1.54 | 0.872 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **DAVIS Interactive** | 1.54 | 0.872 |'
- en: Installation
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: To install SAM 2, use the following command. All SAM 2 models will automatically
    download on first use.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 SAM 2，请使用以下命令。首次使用时，所有 SAM 2 模型将自动下载。
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'How to Use SAM 2: Versatility in Image and Video Segmentation'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 SAM 2：在图像和视频分割中的多功能性
- en: The following table details the available SAM 2 models, their pre-trained weights,
    supported tasks, and compatibility with different operating modes like Inference,
    Validation, Training, and Export.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下表详细说明了可用的 SAM 2 模型、它们的预训练权重、支持的任务以及与推断、验证、训练和导出等不同操作模式的兼容性。
- en: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 模型类型 | 预训练权重 | 支持的任务 | 推断 | 验证 | 训练 | 导出 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| SAM 2 tiny | [sam2_t.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_t.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SAM 2 tiny | [sam2_t.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_t.pt)
    | 实例分割 | ✅ | ❌ | ❌ | ❌ |'
- en: '| SAM 2 small | [sam2_s.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_s.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| SAM 2 小型 | [sam2_s.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_s.pt)
    | 实例分割 | ✅ | ❌ | ❌ | ❌ |'
- en: '| SAM 2 base | [sam2_b.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_b.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| SAM 2 基础 | [sam2_b.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_b.pt)
    | 实例分割 | ✅ | ❌ | ❌ | ❌ |'
- en: '| SAM 2 large | [sam2_l.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_l.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| SAM 2 大型 | [sam2_l.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam2_l.pt)
    | 实例分割 | ✅ | ❌ | ❌ | ❌ |'
- en: SAM 2 Prediction Examples
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SAM 2 预测示例
- en: SAM 2 can be utilized across a broad spectrum of tasks, including real-time
    video editing, medical imaging, and autonomous systems. Its ability to segment
    both static and dynamic visual data makes it a versatile tool for researchers
    and developers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2 可在广泛的任务范围内使用，包括实时视频编辑、医学成像和自主系统。其分割静态和动态视觉数据的能力使其成为研究人员和开发人员的多功能工具。
- en: Segment with Prompts
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分段提示
- en: Segment with Prompts
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分段提示
- en: Use prompts to segment specific objects in images or videos.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提示来分割图像或视频中的特定对象。
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Segment Everything
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分割所有内容
- en: Segment Everything
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 分割所有内容
- en: Segment the entire image or video content without specific prompts.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 分割整个图像或视频内容，无需特定提示。
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This example demonstrates how SAM 2 can be used to segment the entire content
    of an image or video if no prompts (bboxes/points/masks) are provided.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该示例演示了如果不提供提示（边界框/点/掩模），如何使用 SAM 2 分割图像或视频的整个内容。
- en: SAM comparison vs YOLOv8
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SAM 与 YOLOv8 的比较
- en: 'Here we compare Meta''s smallest SAM model, SAM-b, with Ultralytics smallest
    segmentation model, YOLOv8n-seg:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们比较了 Meta 最小的 SAM 模型 SAM-b 和 Ultralytics 最小的分割模型 YOLOv8n-seg：
- en: '| Model | Size | Parameters | Speed (CPU) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | 参数 | 速度（CPU） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
- en: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
- en: '| FastSAM-s with YOLOv8 backbone | 23.7 MB | 11.8 M | 115 ms/im |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 基于 YOLOv8 骨干的 FastSAM-s | 23.7 MB | 11.8 M | 115 ms/im |'
- en: '| Ultralytics YOLOv8n-seg | **6.7 MB** (53.4x smaller) | **3.4 M** (27.9x less)
    | **59 ms/im** (866x faster) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Ultralytics YOLOv8n-seg | **6.7 MB** (53.4x 更小) | **3.4 M** (27.9x 较少) |
    **59 ms/im** (866x 更快) |'
- en: This comparison shows the order-of-magnitude differences in the model sizes
    and speeds between models. Whereas SAM presents unique capabilities for automatic
    segmenting, it is not a direct competitor to YOLOv8 segment models, which are
    smaller, faster and more efficient.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此比较显示了模型大小和速度之间数量级的差异。虽然 SAM 提供了自动分割的独特能力，但它并非 YOLOv8 分割模型的直接竞争对手，后者更小、更快且更高效。
- en: 'Tests run on a 2023 Apple M2 Macbook with 16GB of RAM. To reproduce this test:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 测试在 2023 年配备 16GB RAM 的 Apple M2 MacBook 上运行。要重现此测试：
- en: Example
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Auto-Annotation: Efficient Dataset Creation'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动注释：高效数据集创建
- en: Auto-annotation is a powerful feature of SAM 2, enabling users to generate segmentation
    datasets quickly and accurately by leveraging pre-trained models. This capability
    is particularly useful for creating large, high-quality datasets without extensive
    manual effort.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 自动注释是 SAM 2 的一个强大功能，通过利用预训练模型，使用户能够快速准确地生成分段数据集。这一功能特别适用于创建大规模、高质量的数据集，无需大量手动操作。
- en: How to Auto-Annotate with SAM 2
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用 SAM 2 进行自动注释
- en: 'To auto-annotate your dataset using SAM 2, follow this example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 若要使用 SAM 2 自动注释您的数据集，请参考此示例：
- en: Auto-Annotation Example
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 自动注释示例
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '| Argument | Type | Description | Default |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 | 默认值 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `data` | `str` | Path to a folder containing images to be annotated. |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `data` | `str` | 包含要注释的图像的文件夹路径。 |  |'
- en: '| `det_model` | `str`, optional | Pre-trained YOLO detection model. Defaults
    to ''yolov8x.pt''. | `''yolov8x.pt''` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `det_model` | `str`, 可选 | 预训练的 YOLO 检测模型。默认为 ''yolov8x.pt''。 | `''yolov8x.pt''`
    |'
- en: '| `sam_model` | `str`, optional | Pre-trained SAM 2 segmentation model. Defaults
    to ''sam2_b.pt''. | `''sam2_b.pt''` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `sam_model` | `str`, 可选 | 预训练的 SAM 2 分割模型。默认为 ''sam2_b.pt''。 | `''sam2_b.pt''`
    |'
- en: '| `device` | `str`, optional | Device to run the models on. Defaults to an
    empty string (CPU or GPU, if available). |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `device` | `str`, 可选 | 运行模型的设备。默认为空字符串（CPU 或 GPU，如果可用）。 |  |'
- en: '| `output_dir` | `str`, `None`, optional | Directory to save the annotated
    results. Defaults to a ''labels'' folder in the same directory as ''data''. |
    `None` |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `output_dir` | `str`, `None`, 可选 | 保存注释结果的目录。默认为与 ''data'' 目录同级的 ''labels''
    文件夹。 | `None` |'
- en: This function facilitates the rapid creation of high-quality segmentation datasets,
    ideal for researchers and developers aiming to accelerate their projects.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能有助于快速创建高质量的分割数据集，非常适合希望加速项目的研究人员和开发者。
- en: Limitations
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'Despite its strengths, SAM 2 has certain limitations:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其优点，SAM 2 也有一些限制：
- en: '**Tracking Stability**: SAM 2 may lose track of objects during extended sequences
    or significant viewpoint changes.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪稳定性**：在长序列或视角显著变化期间，SAM 2可能会丢失对象的跟踪。'
- en: '**Object Confusion**: The model can sometimes confuse similar-looking objects,
    particularly in crowded scenes.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象混淆**：模型有时会在拥挤场景中混淆看起来相似的对象。'
- en: '**Efficiency with Multiple Objects**: Segmentation efficiency decreases when
    processing multiple objects simultaneously due to the lack of inter-object communication.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对象效率**：由于缺乏对象间通信，处理多个对象时，分段效率会降低。'
- en: '**Detail Accuracy**: May miss fine details, especially with fast-moving objects.
    Additional prompts can partially address this issue, but temporal smoothness is
    not guaranteed.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细节精度**：可能会错过细小的细节，特别是在快速移动对象时。额外的提示可以部分解决这个问题，但不能保证时间上的平滑。'
- en: Citations and Acknowledgements
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用和致谢
- en: 'If SAM 2 is a crucial part of your research or development work, please cite
    it using the following reference:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 SAM 2 对你的研究或开发工作至关重要，请引用以下参考文献：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We extend our gratitude to Meta AI for their contributions to the AI community
    with this groundbreaking model and dataset.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Meta AI 为AI社区做出的贡献，使用这一开创性的模型和数据集。
- en: FAQ
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见问题解答
- en: What is SAM 2 and how does it improve upon the original Segment Anything Model
    (SAM)?
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SAM 2是什么，它如何改进原始的Segment Anything Model（SAM）？
- en: 'SAM 2, the successor to Meta''s Segment Anything Model (SAM), is a cutting-edge
    tool designed for comprehensive object segmentation in both images and videos.
    It excels in handling complex visual data through a unified, promptable model
    architecture that supports real-time processing and zero-shot generalization.
    SAM 2 offers several improvements over the original SAM, including:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2 是Meta的Segment Anything Model（SAM）的继任者，是一种专为图像和视频中全面对象分割而设计的尖端工具。它通过统一的可提示模型架构，在处理复杂视觉数据方面表现出色，支持实时处理和零样本泛化。SAM
    2相比原始SAM有多项改进，包括：
- en: '**Unified Model Architecture**: Combines image and video segmentation capabilities
    in a single model.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一模型架构**：将图像和视频分割能力结合在单一模型中。'
- en: '**Real-Time Performance**: Processes approximately 44 frames per second, making
    it suitable for applications requiring immediate feedback.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时性能**：每秒处理约44帧，适用于需要即时反馈的应用程序。'
- en: '**Zero-Shot Generalization**: Segments objects it has never encountered before,
    useful in diverse visual domains.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零样本泛化**：能够分割它从未遇到过的对象，在各种视觉领域非常有用。'
- en: '**Interactive Refinement**: Allows users to iteratively refine segmentation
    results by providing additional prompts.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式优化**：允许用户通过提供额外的提示来迭代地优化分割结果。'
- en: '**Advanced Handling of Visual Challenges**: Manages common video segmentation
    challenges like object occlusion and reappearance.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级视觉挑战处理**：管理常见的视频分割挑战，如对象遮挡和重新出现。'
- en: For more details on SAM 2's architecture and capabilities, explore the [SAM
    2 research paper](https://arxiv.org/abs/2401.12741).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SAM 2的架构和能力的更多细节，请查看[《SAM 2研究论文》](https://arxiv.org/abs/2401.12741)。
- en: How can I use SAM 2 for real-time video segmentation?
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何在实时视频分割中使用SAM 2？
- en: 'SAM 2 can be utilized for real-time video segmentation by leveraging its promptable
    interface and real-time inference capabilities. Here''s a basic example:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用其可提示的界面和实时推断能力，SAM 2可以用于实时视频分割。以下是一个基本示例：
- en: Segment with Prompts
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 带提示的分段
- en: Use prompts to segment specific objects in images or videos.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提示来分割图像或视频中的特定对象。
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For more comprehensive usage, refer to the How to Use SAM 2 section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更详细的使用方法，请参阅如何使用SAM 2部分。
- en: What datasets are used to train SAM 2, and how do they enhance its performance?
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于训练SAM 2的数据集是什么，它们如何提升其性能？
- en: 'SAM 2 is trained on the SA-V dataset, one of the largest and most diverse video
    segmentation datasets available. The SA-V dataset includes:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2 是在SA-V数据集上训练的，这是目前可用的最大和最多样化的视频分割数据集之一。SA-V数据集包括：
- en: '**51,000+ Videos**: Captured across 47 countries, providing a wide range of
    real-world scenarios.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**51,000+ 视频**：涵盖了47个国家，提供了广泛的真实场景。'
- en: '**600,000+ Mask Annotations**: Detailed spatio-temporal mask annotations, referred
    to as "masklets," covering whole objects and parts.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超过600,000个掩码注释**：详细的时空掩码注释，称为“掩码片”，覆盖整个对象和部分。'
- en: '**Dataset Scale**: Features 4.5 times more videos and 53 times more annotations
    than previous largest datasets, offering unprecedented diversity and complexity.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集规模**：比之前最大数据集的视频数量多4.5倍，注释数量多53倍，提供前所未有的多样性和复杂性。'
- en: This extensive dataset allows SAM 2 to achieve superior performance across major
    video segmentation benchmarks and enhances its zero-shot generalization capabilities.
    For more information, see the SA-V Dataset section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这一广泛的数据集使SAM 2能够在主要视频分割基准测试中取得卓越性能，并增强其零样本泛化能力。有关更多信息，请参阅SA-V数据集部分。
- en: How does SAM 2 handle occlusions and object reappearances in video segmentation?
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SAM 2如何处理视频分割中的遮挡和物体再出现问题？
- en: 'SAM 2 includes a sophisticated memory mechanism to manage temporal dependencies
    and occlusions in video data. The memory mechanism consists of:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2包含一种复杂的内存机制，用于管理视频数据中的时间依赖性和遮挡。该内存机制包括：
- en: '**Memory Encoder and Memory Bank**: Stores features from past frames.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存编码器和内存存储库**：存储来自过去帧的特征。'
- en: '**Memory Attention Module**: Utilizes stored information to maintain consistent
    object tracking over time.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存注意模块**：利用存储的信息维持随时间的一致对象追踪。'
- en: '**Occlusion Head**: Specifically handles scenarios where objects are not visible,
    predicting the likelihood of an object being occluded.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遮挡头**：专门处理物体不可见的情况，预测物体被遮挡的可能性。'
- en: This mechanism ensures continuity even when objects are temporarily obscured
    or exit and re-enter the scene. For more details, refer to the Memory Mechanism
    and Occlusion Handling section.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对象暂时被遮挡或退出并重新进入场景，该机制也确保连续性。更多详情请参阅内存机制和遮挡处理部分。
- en: How does SAM 2 compare to other segmentation models like YOLOv8?
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SAM 2与YOLOv8等其他分割模型相比如何？
- en: 'SAM 2 and Ultralytics YOLOv8 serve different purposes and excel in different
    areas. While SAM 2 is designed for comprehensive object segmentation with advanced
    features like zero-shot generalization and real-time performance, YOLOv8 is optimized
    for speed and efficiency in object detection and segmentation tasks. Here''s a
    comparison:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 2和Ultralytics YOLOv8在不同领域有不同的优势和用途。SAM 2专为具有零样本泛化和实时性能等先进功能的全面对象分割而设计，而YOLOv8则专为物体检测和分割任务中的速度和效率进行了优化。以下是比较：
- en: '| Model | Size | Parameters | Speed (CPU) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | 参数 | 速度（CPU） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
- en: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
- en: '| FastSAM-s with YOLOv8 backbone | 23.7 MB | 11.8 M | 115 ms/im |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| FastSAM-s（采用YOLOv8骨干） | 23.7 MB | 11.8 M | 115 ms/im |'
- en: '| Ultralytics YOLOv8n-seg | **6.7 MB** (53.4x smaller) | **3.4 M** (27.9x less)
    | **59 ms/im** (866x faster) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Ultralytics YOLOv8n-seg | **6.7 MB**（缩小53.4倍） | **3.4 M**（减少27.9倍） | **59
    ms/im**（快了866倍） |'
- en: For more details, see the SAM comparison vs YOLOv8 section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详情，请参阅SAM与YOLOv8的比较部分。
