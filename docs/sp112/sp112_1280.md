# scipy.stats.entropy

> 原文链接：[https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.stats.entropy.html#scipy.stats.entropy](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.stats.entropy.html#scipy.stats.entropy)

```py
scipy.stats.entropy(pk, qk=None, base=None, axis=0, *, nan_policy='propagate', keepdims=False)
```

计算给定分布的Shannon熵/相对熵。

如果仅提供了概率*pk*，则香农熵计算为`H = -sum(pk * log(pk))`。

如果*qk*不为None，则计算相对熵`D = sum(pk * log(pk / qk))`。这个量也被称为Kullback-Leibler散度。

如果*pk*和*qk*的和不为1，则此例程将对它们进行标准化。

参数：

**pk**array_like

定义（离散）分布。对于`pk`的每个轴切片，元素`i`是事件`i`的（可能未标准化的）概率。

**qk**array_like，可选

用于计算相对熵的序列。应与*pk*具有相同的格式。

**base**float，可选

要使用的对数基数，默认为`e`（自然对数）。

**axis**int或None，默认值：0

如果是整数，则是计算统计量的输入轴。输入的每个轴切片（例如行）的统计量将出现在输出的相应元素中。如果为`None`，则在计算统计量之前将对输入进行展平。

**nan_policy**{‘propagate’, ‘omit’, ‘raise’}

定义如何处理输入的NaN值。

+   `propagate`：如果在计算统计量的轴切片（例如行）中存在NaN值，则输出的相应条目将为NaN。

+   `omit`：在执行计算时将忽略NaN值。如果沿着计算统计量的轴切片中剩余的数据不足，输出的相应条目将为NaN。

+   `raise`：如果存在NaN值，则会引发`ValueError`。

**keepdims**bool，默认值：False

如果设置为True，则减少的轴将作为大小为1的维度保留在结果中。选择此选项后，结果将正确地与输入数组进行广播。

返回值：

**S**{float, array_like}

计算得到的熵。

注意事项

通俗地讲，香农熵量化了离散随机变量可能结果的预期不确定性。例如，如果要对由一组符号序列组成的消息进行编码并通过无噪声信道传输，则香农熵`H(pk)`给出了每个符号所需的信息单位数的平均下界，如果符号的发生频率由离散分布*pk*控制[[1]](#r7a63479d8f91-1)。基数的选择确定了单位的选择；例如，自然对数`e`用于nats，`2`用于bits，等等。

相对熵 `D(pk|qk)` 量化了如果编码针对概率分布 *qk* 而不是真实分布 *pk* 进行了优化，则每个符号所需的平均信息单位数的增加量。非正式地，相对熵量化了在真实分布实际为 *pk* 时，但人们认为其为 *qk* 时所经历的预期惊讶的过量。

相关量，交叉熵 `CE(pk, qk)`，满足方程 `CE(pk, qk) = H(pk) + D(pk|qk)`，也可以用公式 `CE = -sum(pk * log(qk))` 计算。如果编码针对概率分布 *qk* 进行了优化，当真实分布为 *pk* 时，它给出每个符号所需的平均信息单位数。它不是直接由 [`entropy`](#scipy.stats.entropy "scipy.stats.entropy") 计算的，但可以通过两次调用函数来计算（见示例）。

更多信息请参见 [[2]](#r7a63479d8f91-2)。

从 SciPy 1.9 开始，`np.matrix` 输入（不建议在新代码中使用）在进行计算之前会转换为 `np.ndarray`。在这种情况下，输出将是一个标量或适当形状的 `np.ndarray`，而不是二维的 `np.matrix`。类似地，虽然忽略了掩码数组的掩码元素，但输出将是一个标量或 `np.ndarray`，而不是带有 `mask=False` 的掩码数组。

参考文献

[[1](#id1)]

Shannon, C.E. (1948)，A Mathematical Theory of Communication. Bell System Technical Journal, 27: 379-423. [https://doi.org/10.1002/j.1538-7305.1948.tb01338.x](https://doi.org/10.1002/j.1538-7305.1948.tb01338.x)

[[2](#id2)]

Thomas M. Cover 和 Joy A. Thomas. 2006\. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA.

示例

公平硬币的结果是最不确定的：

```py
>>> import numpy as np
>>> from scipy.stats import entropy
>>> base = 2  # work in units of bits
>>> pk = np.array([1/2, 1/2])  # fair coin
>>> H = entropy(pk, base=base)
>>> H
1.0
>>> H == -np.sum(pk * np.log(pk)) / np.log(base)
True 
```

有偏硬币的结果不那么不确定：

```py
>>> qk = np.array([9/10, 1/10])  # biased coin
>>> entropy(qk, base=base)
0.46899559358928117 
```

公平硬币和有偏硬币之间的相对熵计算如下：

```py
>>> D = entropy(pk, qk, base=base)
>>> D
0.7369655941662062
>>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)
True 
```

交叉熵可以计算为熵和相对熵的总和`：

```py
>>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)
>>> CE
1.736965594166206
>>> CE == -np.sum(pk * np.log(qk)) / np.log(base)
True 
```
