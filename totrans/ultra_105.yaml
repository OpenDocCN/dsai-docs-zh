- en: 'Quick Start Guide: NVIDIA Jetson with Ultralytics YOLOv8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速入门指南：NVIDIA Jetson与Ultralytics YOLOv8
- en: 原文：[`docs.ultralytics.com/guides/nvidia-jetson/`](https://docs.ultralytics.com/guides/nvidia-jetson/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`docs.ultralytics.com/guides/nvidia-jetson/`](https://docs.ultralytics.com/guides/nvidia-jetson/)
- en: This comprehensive guide provides a detailed walkthrough for deploying Ultralytics
    YOLOv8 on [NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)
    devices. Additionally, it showcases performance benchmarks to demonstrate the
    capabilities of YOLOv8 on these small and powerful devices.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本详细指南提供了在[NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/)设备上部署Ultralytics
    YOLOv8的详细步骤。此外，它展示了性能基准，以展示YOLOv8在这些小巧且强大的设备上的能力。
- en: '[`www.youtube.com/embed/mUybgOlSxxA`](https://www.youtube.com/embed/mUybgOlSxxA)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[`www.youtube.com/embed/mUybgOlSxxA`](https://www.youtube.com/embed/mUybgOlSxxA)'
- en: '**Watch:** How to Setup NVIDIA Jetson with Ultralytics YOLOv8'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 如何设置NVIDIA Jetson与Ultralytics YOLOv8'
- en: '![NVIDIA Jetson Ecosystem](img/7e331b3dafcec8f50a767f1c09c15a5c.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![NVIDIA Jetson生态系统](img/7e331b3dafcec8f50a767f1c09c15a5c.png)'
- en: Note
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This guide has been tested with both [Seeed Studio reComputer J4012](https://www.seeedstudio.com/reComputer-J4012-p-5586.html)
    which is based on NVIDIA Jetson Orin NX 16GB running the latest stable JetPack
    release of [JP6.0](https://developer.nvidia.com/embedded/jetpack-sdk-60), JetPack
    release of [JP5.1.3](https://developer.nvidia.com/embedded/jetpack-sdk-513) and
    [Seeed Studio reComputer J1020 v2](https://www.seeedstudio.com/reComputer-J1020-v2-p-5498.html)
    which is based on NVIDIA Jetson Nano 4GB running JetPack release of [JP4.6.1](https://developer.nvidia.com/embedded/jetpack-sdk-461).
    It is expected to work across all the NVIDIA Jetson hardware lineup including
    latest and legacy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此指南已在基于NVIDIA Jetson Orin NX 16GB的[Seeed Studio reComputer J4012](https://www.seeedstudio.com/reComputer-J4012-p-5586.html)（运行最新稳定的JetPack版本[JP6.0](https://developer.nvidia.com/embedded/jetpack-sdk-60)）、基于NVIDIA
    Jetson Nano 4GB的[Seeed Studio reComputer J1020 v2](https://www.seeedstudio.com/reComputer-J1020-v2-p-5498.html)（运行JetPack版本[JP4.6.1](https://developer.nvidia.com/embedded/jetpack-sdk-461)）上经过测试。预计可以在包括最新和传统硬件在内的所有NVIDIA
    Jetson硬件系列上运行。
- en: What is NVIDIA Jetson?
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是NVIDIA Jetson？
- en: NVIDIA Jetson is a series of embedded computing boards designed to bring accelerated
    AI (artificial intelligence) computing to edge devices. These compact and powerful
    devices are built around NVIDIA's GPU architecture and are capable of running
    complex AI algorithms and deep learning models directly on the device, without
    needing to rely on cloud computing resources. Jetson boards are often used in
    robotics, autonomous vehicles, industrial automation, and other applications where
    AI inference needs to be performed locally with low latency and high efficiency.
    Additionally, these boards are based on the ARM64 architecture and runs on lower
    power compared to traditional GPU computing devices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA Jetson是一系列嵌入式计算板，旨在将加速AI（人工智能）计算引入边缘设备。这些小巧而强大的设备围绕NVIDIA的GPU架构构建，并能够在设备上直接运行复杂的AI算法和深度学习模型，无需依赖云计算资源。Jetson板常用于机器人技术、自动驾驶车辆、工业自动化及其他需要在本地进行低延迟和高效率AI推理的应用。此外，这些板基于ARM64架构，与传统的GPU计算设备相比，功耗更低。
- en: NVIDIA Jetson Series Comparison
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NVIDIA Jetson系列比较
- en: '[Jetson Orin](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)
    is the latest iteration of the NVIDIA Jetson family based on NVIDIA Ampere architecture
    which brings drastically improved AI performance when compared to the previous
    generations. Below table compared few of the Jetson devices in the ecosystem.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jetson Orin](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)是基于NVIDIA
    Ampere架构的NVIDIA Jetson系列的最新版本，与前几代相比，显著提升了AI性能。下表比较了生态系统中几款Jetson设备。'
- en: '|  | Jetson AGX Orin 64GB | Jetson Orin NX 16GB | Jetson Orin Nano 8GB | Jetson
    AGX Xavier | Jetson Xavier NX | Jetson Nano |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '|  | Jetson AGX Orin 64GB | Jetson Orin NX 16GB | Jetson Orin Nano 8GB | Jetson
    AGX Xavier | Jetson Xavier NX | Jetson Nano |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| AI Performance | 275 TOPS | 100 TOPS | 40 TOPs | 32 TOPS | 21 TOPS | 472
    GFLOPS |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| AI Performance | 275 TOPS | 100 TOPS | 40 TOPs | 32 TOPS | 21 TOPS | 472
    GFLOPS |'
- en: '| GPU | 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores | 1024-core
    NVIDIA Ampere architecture GPU with 32 Tensor Cores | 1024-core NVIDIA Ampere
    architecture GPU with 32 Tensor Cores | 512-core NVIDIA Volta architecture GPU
    with 64 Tensor Cores | 384-core NVIDIA Volta™ architecture GPU with 48 Tensor
    Cores | 128-core NVIDIA Maxwell™ architecture GPU |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| GPU | 2048核 NVIDIA 安培架构 GPU，带有 64 个张量核心 | 1024核 NVIDIA 安培架构 GPU，带有 32 个张量核心
    | 1024核 NVIDIA 安培架构 GPU，带有 32 个张量核心 | 512核 NVIDIA 伏特架构 GPU，带有 64 个张量核心 | 384核
    NVIDIA 伏特架构 GPU，带有 48 个张量核心 | 128核 NVIDIA 麦克斯韦架构 GPU |'
- en: '| GPU Max Frequency | 1.3 GHz | 918 MHz | 625 MHz | 1377 MHz | 1100 MHz | 921MHz
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| GPU 最大频率 | 1.3 GHz | 918 MHz | 625 MHz | 1377 MHz | 1100 MHz | 921MHz |'
- en: '| CPU | 12-core NVIDIA Arm® Cortex A78AE v8.2 64-bit CPU 3MB L2 + 6MB L3 |
    8-core NVIDIA Arm® Cortex A78AE v8.2 64-bit CPU 2MB L2 + 4MB L3 | 6-core Arm®
    Cortex®-A78AE v8.2 64-bit CPU 1.5MB L2 + 4MB L3 | 8-core NVIDIA Carmel Arm®v8.2
    64-bit CPU 8MB L2 + 4MB L3 | 6-core NVIDIA Carmel Arm®v8.2 64-bit CPU 6MB L2 +
    4MB L3 | Quad-Core Arm® Cortex®-A57 MPCore processor |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| CPU | 12核 NVIDIA Arm® Cortex A78AE v8.2 64位 CPU，3MB L2 + 6MB L3 | 8核 NVIDIA
    Arm® Cortex A78AE v8.2 64位 CPU，2MB L2 + 4MB L3 | 6核 Arm® Cortex®-A78AE v8.2 64位
    CPU，1.5MB L2 + 4MB L3 | 8核 NVIDIA Carmel Arm®v8.2 64位 CPU，8MB L2 + 4MB L3 | 6核
    NVIDIA Carmel Arm®v8.2 64位 CPU，6MB L2 + 4MB L3 | 四核 Arm® Cortex®-A57 MPCore 处理器
    |'
- en: '| CPU Max Frequency | 2.2 GHz | 2.0 GHz | 1.5 GHz | 2.2 GHz | 1.9 GHz | 1.43GHz
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| CPU 最大频率 | 2.2 GHz | 2.0 GHz | 1.5 GHz | 2.2 GHz | 1.9 GHz | 1.43GHz |'
- en: '| Memory | 64GB 256-bit LPDDR5 204.8GB/s | 16GB 128-bit LPDDR5 102.4GB/s |
    8GB 128-bit LPDDR5 68 GB/s | 32GB 256-bit LPDDR4x 136.5GB/s | 8GB 128-bit LPDDR4x
    59.7GB/s | 4GB 64-bit LPDDR4 25.6GB/s" |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 内存 | 64GB 256位 LPDDR5，204.8GB/s | 16GB 128位 LPDDR5，102.4GB/s | 8GB 128位 LPDDR5，68
    GB/s | 32GB 256位 LPDDR4x，136.5GB/s | 8GB 128位 LPDDR4x，59.7GB/s | 4GB 64位 LPDDR4，25.6GB/s"
    |'
- en: For a more detailed comparison table, please visit the **Technical Specifications**
    section of [official NVIDIA Jetson page](https://developer.nvidia.com/embedded/jetson-modules).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 欲查看更详细的比较表，请访问[官方 NVIDIA Jetson 页面的技术规格部分](https://developer.nvidia.com/embedded/jetson-modules)。
- en: What is NVIDIA JetPack?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NVIDIA JetPack 是什么？
- en: '[NVIDIA JetPack SDK](https://developer.nvidia.com/embedded/jetpack) powering
    the Jetson modules is the most comprehensive solution and provides full development
    environment for building end-to-end accelerated AI applications and shortens time
    to market. JetPack includes Jetson Linux with bootloader, Linux kernel, Ubuntu
    desktop environment, and a complete set of libraries for acceleration of GPU computing,
    multimedia, graphics, and computer vision. It also includes samples, documentation,
    and developer tools for both host computer and developer kit, and supports higher
    level SDKs such as DeepStream for streaming video analytics, Isaac for robotics,
    and Riva for conversational AI.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[NVIDIA JetPack SDK](https://developer.nvidia.com/embedded/jetpack)，为 Jetson
    模块提供动力的最全面解决方案，为构建端到端加速 AI 应用提供完整的开发环境，并缩短上市时间。JetPack 包括 Jetson Linux 与引导加载程序、Linux
    内核、Ubuntu 桌面环境，以及一整套用于加速 GPU 计算、多媒体、图形和计算机视觉的库。它还包括样本、文档和主机计算机及开发套件的开发工具，并支持更高级别的
    SDK，如用于流媒体视频分析的 DeepStream、用于机器人技术的 Isaac 和用于对话 AI 的 Riva。'
- en: Flash JetPack to NVIDIA Jetson
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 JetPack 刷写到 NVIDIA Jetson
- en: The first step after getting your hands on an NVIDIA Jetson device is to flash
    NVIDIA JetPack to the device. There are several different way of flashing NVIDIA
    Jetson devices.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 获得 NVIDIA Jetson 设备后的第一步是将 NVIDIA JetPack 刷写到设备上。有多种不同的刷写 NVIDIA Jetson 设备的方法。
- en: If you own an official NVIDIA Development Kit such as the Jetson Orin Nano Developer
    Kit, you can [download an image and prepare an SD card with JetPack for booting
    the device](https://developer.nvidia.com/embedded/learn/get-started-jetson-orin-nano-devkit).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您拥有官方的 NVIDIA 开发套件，如 Jetson Orin Nano 开发套件，可以[下载映像并准备一张 JetPack 启动设备的 SD 卡](https://developer.nvidia.com/embedded/learn/get-started-jetson-orin-nano-devkit)。
- en: If you own any other NVIDIA Development Kit, you can [flash JetPack to the device
    using SDK Manager](https://docs.nvidia.com/sdk-manager/install-with-sdkm-jetson/index.html).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您拥有其他任何 NVIDIA 开发套件，可以使用 SDK 管理器[将 JetPack 刷写到设备上](https://docs.nvidia.com/sdk-manager/install-with-sdkm-jetson/index.html)。
- en: If you own a Seeed Studio reComputer J4012 device, you can [flash JetPack to
    the included SSD](https://wiki.seeedstudio.com/reComputer_J4012_Flash_Jetpack)
    and if you own a Seeed Studio reComputer J1020 v2 device, you can [flash JetPack
    to the eMMC/ SSD](https://wiki.seeedstudio.com/reComputer_J2021_J202_Flash_Jetpack).
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您拥有 Seeed Studio reComputer J4012 设备，可以[将 JetPack 刷写到包含的 SSD 上](https://wiki.seeedstudio.com/reComputer_J4012_Flash_Jetpack)，如果您拥有
    Seeed Studio reComputer J1020 v2 设备，可以[将 JetPack 刷写到 eMMC/ SSD 上](https://wiki.seeedstudio.com/reComputer_J2021_J202_Flash_Jetpack)。
- en: If you own any other third party device powered by the NVIDIA Jetson module,
    it is recommended to follow [command-line flashing](https://docs.nvidia.com/jetson/archives/r35.5.0/DeveloperGuide/IN/QuickStart.html).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: For methods 3 and 4 above, after flashing the system and booting the device,
    please enter "sudo apt update && sudo apt install nvidia-jetpack -y" on the device
    terminal to install all the remaining JetPack components needed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: JetPack Support Based on Jetson Device
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The below table highlights NVIDIA JetPack versions supported by different NVIDIA
    Jetson devices.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '|  | JetPack 4 | JetPack 5 | JetPack 6 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| Jetson Nano | ✅ | ❌ | ❌ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| Jetson TX2 | ✅ | ❌ | ❌ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| Jetson Xavier NX | ✅ | ✅ | ❌ |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| Jetson AGX Xavier | ✅ | ✅ | ❌ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| Jetson AGX Orin | ❌ | ✅ | ✅ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Jetson Orin NX | ❌ | ✅ | ✅ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Jetson Orin Nano | ❌ | ✅ | ✅ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: Quick Start with Docker
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fastest way to get started with Ultralytics YOLOv8 on NVIDIA Jetson is to
    run with pre-built docker images for Jetson. Refer to the table above and choose
    the JetPack version according to the Jetson device you own.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After this is done, skip to Use TensorRT on NVIDIA Jetson section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Start with Native Installation
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a native installation without Docker, please refer to the steps below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Run on JetPack 6.x
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Install Ultralytics Package
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here we will install Ultralytics package on the Jetson with optional dependencies
    so that we can export the PyTorch models to other different formats. We will mainly
    focus on NVIDIA TensorRT exports because TensorRT will make sure we can get the
    maximum performance out of the Jetson devices.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Update packages list, install pip and upgrade to latest
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Install `ultralytics` pip package with optional dependencies
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Reboot the device
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Install PyTorch and Torchvision
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The above ultralytics installation will install Torch and Torchvision. However,
    these 2 packages installed via pip are not compatible to run on Jetson platform
    which is based on ARM64 architecture. Therefore, we need to manually install pre-built
    PyTorch pip wheel and compile/ install Torchvision from source.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Install `torch 2.3.0` and `torchvision 0.18` according to JP6.0
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Visit the [PyTorch for Jetson page](https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048)
    to access all different versions of PyTorch for different JetPack versions. For
    a more detailed list on the PyTorch, Torchvision compatibility, visit the [PyTorch
    and Torchvision compatibility page](https://github.com/pytorch/vision).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Install `onnxruntime-gpu`
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The [onnxruntime-gpu](https://pypi.org/project/onnxruntime-gpu/) package hosted
    in PyPI does not have `aarch64` binaries for the Jetson. So we need to manually
    install this package. This package is needed for some of the exports.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: All different `onnxruntime-gpu` packages corresponding to different JetPack
    and Python versions are listed [here](https://elinux.org/Jetson_Zoo#ONNX_Runtime).
    However, here we will download and install `onnxruntime-gpu 1.18.0` with `Python3.10`
    support.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有不同 JetPack 和 Python 版本对应的 `onnxruntime-gpu` 包都列在 [这里](https://elinux.org/Jetson_Zoo#ONNX_Runtime)。然而，在这里我们将下载并安装支持
    `Python3.10` 的 `onnxruntime-gpu 1.18.0`。
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`onnxruntime-gpu` will automatically revert back the numpy version to latest.
    So we need to reinstall numpy to `1.23.5` to fix an issue by executing:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`onnxruntime-gpu` 将自动将 numpy 版本恢复到最新版本。因此，我们需要重新安装 numpy 到 `1.23.5` 以修复一个问题，执行以下命令：'
- en: '`pip install numpy==1.23.5`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install numpy==1.23.5`'
- en: Run on JetPack 5.x
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 JetPack 5.x 上运行
- en: Install Ultralytics Package
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装 Ultralytics 包
- en: Here we will install Ultralytics package on the Jetson with optional dependencies
    so that we can export the PyTorch models to other different formats. We will mainly
    focus on NVIDIA TensorRT exports because TensorRT will make sure we can get the
    maximum performance out of the Jetson devices.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jetson 上安装 Ultralytics 包及其可选依赖项，以便我们可以将 PyTorch 模型导出到其他不同的格式。我们将主要关注 NVIDIA
    TensorRT 的导出，因为 TensorRT 能确保我们从 Jetson 设备中获得最佳性能。
- en: Update packages list, install pip and upgrade to latest
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新软件包列表，安装 pip 并升级到最新版本
- en: '[PRE8]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Install `ultralytics` pip package with optional dependencies
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装带有可选依赖项的 `ultralytics` pip 包
- en: '[PRE9]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Reboot the device
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启设备
- en: '[PRE10]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Install PyTorch and Torchvision
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装 PyTorch 和 Torchvision
- en: The above ultralytics installation will install Torch and Torchvision. However,
    these 2 packages installed via pip are not compatible to run on Jetson platform
    which is based on ARM64 architecture. Therefore, we need to manually install pre-built
    PyTorch pip wheel and compile/ install Torchvision from source.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 Ultralytics 安装将安装 Torch 和 Torchvision。然而，通过 pip 安装的这两个包不兼容运行在基于 ARM64 架构的
    Jetson 平台上。因此，我们需要手动安装预构建的 PyTorch pip wheel，并从源代码编译/安装 Torchvision。
- en: Uninstall currently installed PyTorch and Torchvision
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卸载当前安装的 PyTorch 和 Torchvision
- en: '[PRE11]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Install PyTorch 2.1.0 according to JP5.1.3
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 JP5.1.3 安装 PyTorch 2.1.0
- en: '[PRE12]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Install Torchvision v0.16.2 according to PyTorch v2.1.0
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 PyTorch v2.1.0 安装 Torchvision v0.16.2
- en: '[PRE13]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Visit the [PyTorch for Jetson page](https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048)
    to access all different versions of PyTorch for different JetPack versions. For
    a more detailed list on the PyTorch, Torchvision compatibility, visit the [PyTorch
    and Torchvision compatibility page](https://github.com/pytorch/vision).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [Jetson 上的 PyTorch 页面](https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048)
    以访问不同 JetPack 版本的所有 PyTorch 不同版本。有关 PyTorch 和 Torchvision 兼容性的更详细列表，请访问 [PyTorch
    和 Torchvision 兼容性页面](https://github.com/pytorch/vision)。
- en: Install `onnxruntime-gpu`
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装 `onnxruntime-gpu`
- en: The [onnxruntime-gpu](https://pypi.org/project/onnxruntime-gpu/) package hosted
    in PyPI does not have `aarch64` binaries for the Jetson. So we need to manually
    install this package. This package is needed for some of the exports.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 托管在 PyPI 上的 [onnxruntime-gpu](https://pypi.org/project/onnxruntime-gpu/) 包没有适用于
    Jetson 的 `aarch64` 二进制文件。因此，我们需要手动安装此包。此包在某些导出中是必需的。
- en: All different `onnxruntime-gpu` packages corresponding to different JetPack
    and Python versions are listed [here](https://elinux.org/Jetson_Zoo#ONNX_Runtime).
    However, here we will download and install `onnxruntime-gpu 1.17.0` with `Python3.8`
    support.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所有不同 JetPack 和 Python 版本对应的 `onnxruntime-gpu` 包都列在 [这里](https://elinux.org/Jetson_Zoo#ONNX_Runtime)。然而，在这里我们将下载并安装支持
    `Python3.8` 的 `onnxruntime-gpu 1.17.0`。
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`onnxruntime-gpu` will automatically revert back the numpy version to latest.
    So we need to reinstall numpy to `1.23.5` to fix an issue by executing:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`onnxruntime-gpu` 将自动将 numpy 版本恢复到最新版本。因此，我们需要重新安装 numpy 到 `1.23.5` 以修复一个问题，执行以下命令：'
- en: '`pip install numpy==1.23.5`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install numpy==1.23.5`'
- en: Use TensorRT on NVIDIA Jetson
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 NVIDIA Jetson 上使用 TensorRT
- en: Out of all the model export formats supported by Ultralytics, TensorRT delivers
    the best inference performance when working with NVIDIA Jetson devices and our
    recommendation is to use TensorRT with Jetson. We also have a detailed document
    on TensorRT here.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ultralytics 支持的所有模型导出格式中，TensorRT 在与 NVIDIA Jetson 设备一起使用时提供了最佳推断性能。我们建议在
    Jetson 上使用 TensorRT。我们还有一个关于 TensorRT 的详细文档 [here](https://example.org/tensorrt_document)。
- en: Convert Model to TensorRT and Run Inference
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型转换为 TensorRT 并运行推断
- en: The YOLOv8n model in PyTorch format is converted to TensorRT to run inference
    with the exported model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将 PyTorch 格式中的 YOLOv8n 模型转换为 TensorRT，以便使用导出模型进行推断。
- en: Example
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Visit the Export page to access additional arguments when exporting models to
    different model formats
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 访问导出页面以获取导出模型到不同模型格式时的额外参数
- en: NVIDIA Jetson Orin YOLOv8 Benchmarks
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YOLOv8 benchmarks were run by the Ultralytics team on 10 different model formats
    measuring speed and accuracy: PyTorch, TorchScript, ONNX, OpenVINO, TensorRT,
    TF SavedModel, TF GraphDef, TF Lite, PaddlePaddle, NCNN. Benchmarks were run on
    Seeed Studio reComputer J4012 powered by Jetson Orin NX 16GB device at FP32 precision
    with default input image size of 640.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Comparison Chart
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though all model exports are working with NVIDIA Jetson, we have only included
    **PyTorch, TorchScript, TensorRT** for the comparison chart below because, they
    make use of the GPU on the Jetson and are guaranteed to produce the best results.
    All the other exports only utilize the CPU and the performance is not as good
    as the above three. You can find benchmarks for all exports in the section after
    this chart.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![NVIDIA Jetson Ecosystem](img/de74d093f481acb6f94a952545a46535.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Detailed Comparison Table
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The below table represents the benchmark results for five different models (YOLOv8n,
    YOLOv8s, YOLOv8m, YOLOv8l, YOLOv8x) across ten different formats (PyTorch, TorchScript,
    ONNX, OpenVINO, TensorRT, TF SavedModel, TF GraphDef, TF Lite, PaddlePaddle, NCNN),
    giving us the status, size, mAP50-95(B) metric, and inference time for each combination.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 6.2 | 0.6381 | 14.3 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | ✅ | 12.4 | 0.6117 | 13.3 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| ONNX | ✅ | 12.2 | 0.6092 | 70.6 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | ✅ | 12.3 | 0.6092 | 104.2 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | ✅ | 13.6 | 0.6117 | 8.9 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | ✅ | 30.6 | 0.6092 | 141.74 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | ✅ | 12.3 | 0.6092 | 199.93 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | ✅ | 12.3 | 0.6092 | 349.18 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | ✅ | 24.4 | 0.6030 | 555 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| NCNN | ✅ | 12.2 | 0.6092 | 32 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 21.5 | 0.6967 | 18 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | ✅ | 43.0 | 0.7136 | 23.81 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| ONNX | ✅ | 42.8 | 0.7136 | 185.55 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | ✅ | 42.9 | 0.7136 | 243.97 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | ✅ | 44.0 | 0.7136 | 14.82 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | ✅ | 107 | 0.7136 | 260.03 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | ✅ | 42.8 | 0.7136 | 423.4 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | ✅ | 42.8 | 0.7136 | 1046.64 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | ✅ | 85.5 | 0.7140 | 1464 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| NCNN | ✅ | 42.7 | 0.7200 | 63 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 49.7 | 0.7370 | 36.4 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | ✅ | 99.2 | 0.7285 | 53.58 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| ONNX | ✅ | 99 | 0.7280 | 452.09 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | ✅ | 99.1 | 0.7280 | 544.36 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | ✅ | 100.3 | 0.7285 | 33.21 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | ✅ | 247.5 | 0.7280 | 543.65 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | ✅ | 99 | 0.7280 | 906.63 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | ✅ | 99 | 0.7280 | 2758.08 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | ✅ | 197.9 | 0.7280 | 3678 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| NCNN | ✅ | 98.9 | 0.7260 | 135 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| PyTorch | ✅ | 83.7 | 0.7768 | 61.3 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | ✅ | 83.7 | 0.7768 | 61.3 |'
- en: '| TorchScript | ✅ | 167.2 | 0.7554 | 87.9 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| TorchScript | ✅ | 167.2 | 0.7554 | 87.9 |'
- en: '| ONNX | ✅ | 166.8 | 0.7551 | 852.29 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| ONNX | ✅ | 166.8 | 0.7551 | 852.29 |'
- en: '| OpenVINO | ✅ | 167 | 0.7551 | 1012.6 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| OpenVINO | ✅ | 167 | 0.7551 | 1012.6 |'
- en: '| TensorRT | ✅ | 168.4 | 0.7554 | 51.23 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| TensorRT | ✅ | 168.4 | 0.7554 | 51.23 |'
- en: '| TF SavedModel | ✅ | 417.2 | 0.7551 | 990.45 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| TF SavedModel | ✅ | 417.2 | 0.7551 | 990.45 |'
- en: '| TF GraphDef | ✅ | 166.9 | 0.7551 | 1649.86 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| TF GraphDef | ✅ | 166.9 | 0.7551 | 1649.86 |'
- en: '| TF Lite | ✅ | 166.9 | 0.7551 | 5652.37 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| TF Lite | ✅ | 166.9 | 0.7551 | 5652.37 |'
- en: '| PaddlePaddle | ✅ | 333.6 | 0.7551 | 7114.67 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| PaddlePaddle | ✅ | 333.6 | 0.7551 | 7114.67 |'
- en: '| NCNN | ✅ | 166.8 | 0.7685 | 231.9 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| NCNN | ✅ | 166.8 | 0.7685 | 231.9 |'
- en: '| Format | Status | Size on disk (MB) | mAP50-95(B) | Inference time (ms/im)
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 状态 | 磁盘大小（MB） | mAP50-95(B) | 推理时间（ms/图像） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PyTorch | ✅ | 130.5 | 0.7759 | 93 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | ✅ | 130.5 | 0.7759 | 93 |'
- en: '| TorchScript | ✅ | 260.7 | 0.7472 | 135.1 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| TorchScript | ✅ | 260.7 | 0.7472 | 135.1 |'
- en: '| ONNX | ✅ | 260.4 | 0.7479 | 1296.13 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| ONNX | ✅ | 260.4 | 0.7479 | 1296.13 |'
- en: '| OpenVINO | ✅ | 260.6 | 0.7479 | 1502.15 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| OpenVINO | ✅ | 260.6 | 0.7479 | 1502.15 |'
- en: '| TensorRT | ✅ | 261.8 | 0.7469 | 84.53 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| TensorRT | ✅ | 261.8 | 0.7469 | 84.53 |'
- en: '| TF SavedModel | ✅ | 651.1 | 0.7479 | 1451.76 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| TF SavedModel | ✅ | 651.1 | 0.7479 | 1451.76 |'
- en: '| TF GraphDef | ✅ | 260.5 | 0.7479 | 4029.36 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| TF GraphDef | ✅ | 260.5 | 0.7479 | 4029.36 |'
- en: '| TF Lite | ✅ | 260.4 | 0.7479 | 8772.86 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| TF Lite | ✅ | 260.4 | 0.7479 | 8772.86 |'
- en: '| PaddlePaddle | ✅ | 520.8 | 0.7479 | 10619.53 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| PaddlePaddle | ✅ | 520.8 | 0.7479 | 10619.53 |'
- en: '| NCNN | ✅ | 260.4 | 0.7646 | 376.38 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| NCNN | ✅ | 260.4 | 0.7646 | 376.38 |'
- en: '[Explore more benchmarking efforts by Seeed Studio](https://www.seeedstudio.com/blog/2023/03/30/yolov8-performance-benchmarks-on-nvidia-jetson-devices)
    running on different versions of NVIDIA Jetson hardware.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看由Seeed Studio进行的更多基准测试工作](https://www.seeedstudio.com/blog/2023/03/30/yolov8-performance-benchmarks-on-nvidia-jetson-devices)，适用于不同版本的NVIDIA
    Jetson硬件。'
- en: Reproduce Our Results
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复现我们的结果
- en: 'To reproduce the above Ultralytics benchmarks on all export formats run this
    code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要在所有导出格式上复现以上Ultralytics基准，请运行此代码：
- en: Example
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE17]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that benchmarking results might vary based on the exact hardware and software
    configuration of a system, as well as the current workload of the system at the
    time the benchmarks are run. For the most reliable results use a dataset with
    a large number of images, i.e. `data='coco8.yaml' (4 val images), or`data='coco.yaml'`
    (5000 val images).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，基准测试结果可能会因系统的精确硬件和软件配置以及进行基准测试时系统当前工作负载的不同而有所变化。要获得最可靠的结果，请使用具有大量图像的数据集，例如`data='coco8.yaml'（4个val图像）`或`data='coco.yaml'（5000个val图像）`。
- en: Best Practices when using NVIDIA Jetson
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NVIDIA Jetson时的最佳实践
- en: When using NVIDIA Jetson, there are a couple of best practices to follow in
    order to enable maximum performance on the NVIDIA Jetson running YOLOv8.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NVIDIA Jetson时，有几个最佳实践需要遵循，以确保在运行YOLOv8的NVIDIA Jetson上实现最佳性能。
- en: Enable MAX Power Mode
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用MAX功率模式
- en: Enabling MAX Power Mode on the Jetson will make sure all CPU, GPU cores are
    turned on.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在Jetson上启用MAX功率模式将确保所有CPU和GPU核心都处于启动状态。
- en: '[PRE19]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Enable Jetson Clocks
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用Jetson时钟
- en: Enabling Jetson Clocks will make sure all CPU, GPU cores are clocked at their
    maximum frequency.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启用Jetson时钟将确保所有CPU和GPU核心以其最大频率时钟运行。
- en: '[PRE20]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Install Jetson Stats Application
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Jetson Stats应用程序
- en: We can use jetson stats application to monitor the temperatures of the system
    components and check other system details such as view CPU, GPU, RAM utilization,
    change power modes, set to max clocks, check JetPack information
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以使用Jetson Stats应用程序监视系统组件的温度并检查其他系统详细信息，例如查看CPU、GPU、RAM利用率，更改功率模式，设置为最大时钟，检查JetPack信息。
- en: '[PRE21]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Jetson Stats](img/eca9687add440601967d5a07e832db7b.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![Jetson Stats](img/eca9687add440601967d5a07e832db7b.png)'
- en: Next Steps
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后续步骤
- en: Congratulations on successfully setting up YOLOv8 on your NVIDIA Jetson! For
    further learning and support, visit more guide at Ultralytics YOLOv8 Docs!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 祝贺您成功在NVIDIA Jetson上设置了YOLOv8！如需进一步了解和支持，请访问Ultralytics YOLOv8文档的更多指南！
- en: FAQ
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见问题解答
- en: How do I deploy Ultralytics YOLOv8 on NVIDIA Jetson devices?
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何在NVIDIA Jetson设备上部署Ultralytics YOLOv8？
- en: Deploying Ultralytics YOLOv8 on NVIDIA Jetson devices is a straightforward process.
    First, flash your Jetson device with the NVIDIA JetPack SDK. Then, either use
    a pre-built Docker image for quick setup or manually install the required packages.
    Detailed steps for each approach can be found in sections Quick Start with Docker
    and Start with Native Installation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在NVIDIA Jetson上部署Ultralytics YOLOv8是一个简单直接的过程。首先，使用NVIDIA JetPack SDK刷新您的Jetson设备。然后，可以使用预构建的Docker镜像进行快速设置，或者手动安装所需的软件包。每种方法的详细步骤可以在“使用Docker快速入门”和“开始本地安装”部分找到。
- en: What performance benchmarks can I expect from YOLOv8 models on NVIDIA Jetson
    devices?
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我可以在NVIDIA Jetson设备上的YOLOv8模型中期望什么性能基准？
- en: YOLOv8 models have been benchmarked on various NVIDIA Jetson devices showing
    significant performance improvements. For example, the TensorRT format delivers
    the best inference performance. The table in the Detailed Comparison Table section
    provides a comprehensive view of performance metrics like mAP50-95 and inference
    time across different model formats.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8模型在各种NVIDIA Jetson设备上进行了基准测试，显示出显著的性能改进。例如，TensorRT格式提供了最佳的推理性能。详细比较表部分的表格提供了跨不同模型格式的mAP50-95和推理时间等性能指标的全面视图。
- en: Why should I use TensorRT for deploying YOLOv8 on NVIDIA Jetson?
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在NVIDIA Jetson上部署YOLOv8时，为什么应该使用TensorRT？
- en: TensorRT is highly recommended for deploying YOLOv8 models on NVIDIA Jetson
    due to its optimal performance. It accelerates inference by leveraging the Jetson's
    GPU capabilities, ensuring maximum efficiency and speed. Learn more about how
    to convert to TensorRT and run inference in the Use TensorRT on NVIDIA Jetson
    section.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其优化的性能，强烈推荐在NVIDIA Jetson上部署YOLOv8模型使用TensorRT。它通过利用Jetson的GPU能力加速推理，确保最大的效率和速度。了解更多有关如何转换为TensorRT并在NVIDIA
    Jetson上运行推理的信息，请参阅在NVIDIA Jetson上使用TensorRT部分。
- en: How can I install PyTorch and Torchvision on NVIDIA Jetson?
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我如何在NVIDIA Jetson上安装PyTorch和Torchvision？
- en: To install PyTorch and Torchvision on NVIDIA Jetson, first uninstall any existing
    versions that may have been installed via pip. Then, manually install the compatible
    PyTorch and Torchvision versions for the Jetson's ARM64 architecture. Detailed
    instructions for this process are provided in the Install PyTorch and Torchvision
    section.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要在NVIDIA Jetson上安装PyTorch和Torchvision，首先卸载可能通过pip安装的任何现有版本。然后，为Jetson的ARM64架构手动安装兼容的PyTorch和Torchvision版本。有关此过程的详细说明，请参阅安装PyTorch和Torchvision部分。
- en: What are the best practices for maximizing performance on NVIDIA Jetson when
    using YOLOv8?
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在使用YOLOv8时，如何最大化NVIDIA Jetson的性能的最佳实践是什么？
- en: 'To maximize performance on NVIDIA Jetson with YOLOv8, follow these best practices:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在NVIDIA Jetson上最大化YOLOv8的性能，请遵循以下最佳实践：
- en: Enable MAX Power Mode to utilize all CPU and GPU cores.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用最大功率模式以利用所有CPU和GPU核心。
- en: Enable Jetson Clocks to run all cores at their maximum frequency.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用Jetson Clocks，以使所有核心运行在其最大频率。
- en: Install the Jetson Stats application for monitoring system metrics.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Jetson Stats应用程序以监视系统指标。
- en: For commands and additional details, refer to the Best Practices when using
    NVIDIA Jetson section.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于命令和额外的细节，请参考使用NVIDIA Jetson时的最佳实践部分。
