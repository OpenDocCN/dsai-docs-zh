- en: Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/tasks/classify/`](https://docs.ultralytics.com/tasks/classify/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Image classification examples](img/d9a956cdec314a5204b6ea5e05fc9381.png)'
  prefs: []
  type: TYPE_IMG
- en: Image classification is the simplest of the three tasks and involves classifying
    an entire image into one of a set of predefined classes.
  prefs: []
  type: TYPE_NORMAL
- en: The output of an image classifier is a single class label and a confidence score.
    Image classification is useful when you need to know only what class an image
    belongs to and don't need to know where objects of that class are located or what
    their exact shape is.
  prefs: []
  type: TYPE_NORMAL
- en: '[`www.youtube.com/embed/5BO0Il_YYAg`](https://www.youtube.com/embed/5BO0Il_YYAg)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch:** Explore Ultralytics YOLO Tasks: Image Classification using Ultralytics
    HUB'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv8 Classify models use the `-cls` suffix, i.e. `yolov8n-cls.pt` and are
    pretrained on [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/ImageNet.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: '[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models/v8)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YOLOv8 pretrained Classify models are shown here. Detect, Segment and Pose models
    are pretrained on the [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml)
    dataset, while Classify models are pretrained on the [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/ImageNet.yaml)
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models)
    download automatically from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases)
    on first use.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | size ^((pixels)) | acc ^(top1) | acc ^(top5) | Speed ^(CPU ONNX'
  prefs: []
  type: TYPE_NORMAL
- en: (ms)) | Speed ^(A100 TensorRT
  prefs: []
  type: TYPE_NORMAL
- en: (ms)) | params ^((M)) | FLOPs ^((B) at 640) |
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8n-cls](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n-cls.pt)
    | 224 | 69.0 | 88.3 | 12.9 | 0.31 | 2.7 | 4.3 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8s-cls](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-cls.pt)
    | 224 | 73.8 | 91.7 | 23.4 | 0.35 | 6.4 | 13.5 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8m-cls](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-cls.pt)
    | 224 | 76.8 | 93.5 | 85.4 | 0.62 | 17.0 | 42.7 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8l-cls](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-cls.pt)
    | 224 | 76.8 | 93.5 | 163.0 | 0.87 | 37.5 | 99.7 |'
  prefs: []
  type: TYPE_TB
- en: '| [YOLOv8x-cls](https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-cls.pt)
    | 224 | 79.0 | 94.6 | 232.0 | 1.01 | 57.4 | 154.8 |'
  prefs: []
  type: TYPE_TB
- en: '**acc** values are model accuracies on the [ImageNet](https://www.image-net.org/)
    dataset validation set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproduce by `yolo val classify data=path/to/ImageNet device=0`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Speed** averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/)
    instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproduce by `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train YOLOv8n-cls on the MNIST160 dataset for 100 epochs at image size 64\.
    For a full list of available arguments see the Configuration page.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Dataset format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: YOLO classification dataset format can be found in detail in the Dataset Guide.
  prefs: []
  type: TYPE_NORMAL
- en: Val
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Validate trained YOLOv8n-cls model accuracy on the MNIST160 dataset. No argument
    need to passed as the `model` retains its training `data` and arguments as model
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Predict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use a trained YOLOv8n-cls model to run predictions on images.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: See full `predict` mode details in the Predict page.
  prefs: []
  type: TYPE_NORMAL
- en: Export
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Export a YOLOv8n-cls model to a different format like ONNX, CoreML, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Available YOLOv8-cls export formats are in the table below. You can export to
    any format using the `format` argument, i.e. `format='onnx'` or `format='engine'`.
    You can predict or validate directly on exported models, i.e. `yolo predict model=yolov8n-cls.onnx`.
    Usage examples are shown for your model after export completes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Format | `format` Argument | Model | Metadata | Arguments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [PyTorch](https://pytorch.org/) | - | `yolov8n-cls.pt` | ✅ | - |'
  prefs: []
  type: TYPE_TB
- en: '| TorchScript | `torchscript` | `yolov8n-cls.torchscript` | ✅ | `imgsz`, `optimize`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | `onnx` | `yolov8n-cls.onnx` | ✅ | `imgsz`, `half`, `dynamic`, `simplify`,
    `opset`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| OpenVINO | `openvino` | `yolov8n-cls_openvino_model/` | ✅ | `imgsz`, `half`,
    `int8`, `batch`, `dynamic` |'
  prefs: []
  type: TYPE_TB
- en: '| TensorRT | `engine` | `yolov8n-cls.engine` | ✅ | `imgsz`, `half`, `dynamic`,
    `simplify`, `workspace`, `int8`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| CoreML | `coreml` | `yolov8n-cls.mlpackage` | ✅ | `imgsz`, `half`, `int8`,
    `nms`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF SavedModel | `saved_model` | `yolov8n-cls_saved_model/` | ✅ | `imgsz`,
    `keras`, `int8`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF GraphDef | `pb` | `yolov8n-cls.pb` | ❌ | `imgsz`, `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF Lite | `tflite` | `yolov8n-cls.tflite` | ✅ | `imgsz`, `half`, `int8`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| TF Edge TPU | `edgetpu` | `yolov8n-cls_edgetpu.tflite` | ✅ | `imgsz` |'
  prefs: []
  type: TYPE_TB
- en: '| TF.js | `tfjs` | `yolov8n-cls_web_model/` | ✅ | `imgsz`, `half`, `int8`,
    `batch` |'
  prefs: []
  type: TYPE_TB
- en: '| PaddlePaddle | `paddle` | `yolov8n-cls_paddle_model/` | ✅ | `imgsz`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: '| NCNN | `ncnn` | `yolov8n-cls_ncnn_model/` | ✅ | `imgsz`, `half`, `batch`
    |'
  prefs: []
  type: TYPE_TB
- en: See full `export` details in the Export page.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the purpose of YOLOv8 in image classification?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: YOLOv8 models, such as `yolov8n-cls.pt`, are designed for efficient image classification.
    They assign a single class label to an entire image along with a confidence score.
    This is particularly useful for applications where knowing the specific class
    of an image is sufficient, rather than identifying the location or shape of objects
    within the image.
  prefs: []
  type: TYPE_NORMAL
- en: How do I train a YOLOv8 model for image classification?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To train a YOLOv8 model, you can use either Python or CLI commands. For example,
    to train a `yolov8n-cls` model on the MNIST160 dataset for 100 epochs at an image
    size of 64:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For more configuration options, visit the Configuration page.
  prefs: []
  type: TYPE_NORMAL
- en: Where can I find pretrained YOLOv8 classification models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pretrained YOLOv8 classification models can be found in the [Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models/v8)
    section. Models like `yolov8n-cls.pt`, `yolov8s-cls.pt`, `yolov8m-cls.pt`, etc.,
    are pretrained on the [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/ImageNet.yaml)
    dataset and can be easily downloaded and used for various image classification
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: How can I export a trained YOLOv8 model to different formats?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can export a trained YOLOv8 model to various formats using Python or CLI
    commands. For instance, to export a model to ONNX format:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For detailed export options, refer to the Export page.
  prefs: []
  type: TYPE_NORMAL
- en: How do I validate a trained YOLOv8 classification model?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To validate a trained model''s accuracy on a dataset like MNIST160, you can
    use the following Python or CLI commands:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For more information, visit the Validate section.
  prefs: []
  type: TYPE_NORMAL
