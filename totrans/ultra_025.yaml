- en: Segment Anything Model (SAM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/models/sam/`](https://docs.ultralytics.com/models/sam/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Welcome to the frontier of image segmentation with the Segment Anything Model,
    or SAM. This revolutionary model has changed the game by introducing promptable
    image segmentation with real-time performance, setting new standards in the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction to SAM: The Segment Anything Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Segment Anything Model, or SAM, is a cutting-edge image segmentation model
    that allows for promptable segmentation, providing unparalleled versatility in
    image analysis tasks. SAM forms the heart of the Segment Anything initiative,
    a groundbreaking project that introduces a novel model, task, and dataset for
    image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: SAM's advanced design allows it to adapt to new image distributions and tasks
    without prior knowledge, a feature known as zero-shot transfer. Trained on the
    expansive [SA-1B dataset](https://ai.facebook.com/datasets/segment-anything/),
    which contains more than 1 billion masks spread over 11 million carefully curated
    images, SAM has displayed impressive zero-shot performance, surpassing previous
    fully supervised results in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset sample image](img/af0aee249f56af250a272709d2c33ce9.png) **SA-1B Example
    images.** Dataset images overlaid masks from the newly introduced SA-1B dataset.
    SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting
    images and 1.1B high-quality segmentation masks. These masks were annotated fully
    automatically by SAM, and as verified by human ratings and numerous experiments,
    are of high quality and diversity. Images are grouped by number of masks per image
    for visualization (there are ∼100 masks per image on average).'
  prefs: []
  type: TYPE_NORMAL
- en: Key Features of the Segment Anything Model (SAM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Promptable Segmentation Task:** SAM was designed with a promptable segmentation
    task in mind, allowing it to generate valid segmentation masks from any given
    prompt, such as spatial or text clues identifying an object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced Architecture:** The Segment Anything Model employs a powerful image
    encoder, a prompt encoder, and a lightweight mask decoder. This unique architecture
    enables flexible prompting, real-time mask computation, and ambiguity awareness
    in segmentation tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The SA-1B Dataset:** Introduced by the Segment Anything project, the SA-1B
    dataset features over 1 billion masks on 11 million images. As the largest segmentation
    dataset to date, it provides SAM with a diverse and large-scale training data
    source.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-Shot Performance:** SAM displays outstanding zero-shot performance across
    various segmentation tasks, making it a ready-to-use tool for diverse applications
    with minimal need for prompt engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For an in-depth look at the Segment Anything Model and the SA-1B dataset, please
    visit the [Segment Anything website](https://segment-anything.com) and check out
    the research paper [Segment Anything](https://arxiv.org/abs/2304.02643).
  prefs: []
  type: TYPE_NORMAL
- en: Available Models, Supported Tasks, and Operating Modes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This table presents the available models with their specific pre-trained weights,
    the tasks they support, and their compatibility with different operating modes
    like Inference, Validation, Training, and Export, indicated by ✅ emojis for supported
    modes and ❌ emojis for unsupported modes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SAM base | [sam_b.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam_b.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: '| SAM large | [sam_l.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam_l.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  prefs: []
  type: TYPE_TB
- en: 'How to Use SAM: Versatility and Power in Image Segmentation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Segment Anything Model can be employed for a multitude of downstream tasks
    that go beyond its training data. This includes edge detection, object proposal
    generation, instance segmentation, and preliminary text-to-mask prediction. With
    prompt engineering, SAM can swiftly adapt to new tasks and data distributions
    in a zero-shot manner, establishing it as a versatile and potent tool for all
    your image segmentation needs.
  prefs: []
  type: TYPE_NORMAL
- en: SAM prediction example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Segment with prompts
  prefs: []
  type: TYPE_NORMAL
- en: Segment image with given prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Segment everything
  prefs: []
  type: TYPE_NORMAL
- en: Segment the whole image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The logic here is to segment the whole image if you don't pass any prompts(bboxes/points/masks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAMPredictor example
  prefs: []
  type: TYPE_NORMAL
- en: This way you can set image once and run prompts inference multiple times without
    running image encoder multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Segment everything with additional args.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All the returned `results` in above examples are Results object which allows
    access predicted masks and source image easily.
  prefs: []
  type: TYPE_NORMAL
- en: More additional args for `Segment everything` see `Predictor/generate` Reference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAM comparison vs YOLOv8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we compare Meta''s smallest SAM model, SAM-b, with Ultralytics smallest
    segmentation model, YOLOv8n-seg:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size | Parameters | Speed (CPU) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| FastSAM-s with YOLOv8 backbone | 23.7 MB | 11.8 M | 115 ms/im |'
  prefs: []
  type: TYPE_TB
- en: '| Ultralytics YOLOv8n-seg | **6.7 MB** (53.4x smaller) | **3.4 M** (27.9x less)
    | **59 ms/im** (866x faster) |'
  prefs: []
  type: TYPE_TB
- en: This comparison shows the order-of-magnitude differences in the model sizes
    and speeds between models. Whereas SAM presents unique capabilities for automatic
    segmenting, it is not a direct competitor to YOLOv8 segment models, which are
    smaller, faster and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tests run on a 2023 Apple M2 Macbook with 16GB of RAM. To reproduce this test:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Auto-Annotation: A Quick Path to Segmentation Datasets'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Auto-annotation is a key feature of SAM, allowing users to generate a segmentation
    dataset using a pre-trained detection model. This feature enables rapid and accurate
    annotation of a large number of images, bypassing the need for time-consuming
    manual labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Generate Your Segmentation Dataset Using a Detection Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To auto-annotate your dataset with the Ultralytics framework, use the `auto_annotate`
    function as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '| Argument | Type | Description | Default |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `data` | `str` | Path to a folder containing images to be annotated. |  |'
  prefs: []
  type: TYPE_TB
- en: '| `det_model` | `str`, optional | Pre-trained YOLO detection model. Defaults
    to ''yolov8x.pt''. | `''yolov8x.pt''` |'
  prefs: []
  type: TYPE_TB
- en: '| `sam_model` | `str`, optional | Pre-trained SAM segmentation model. Defaults
    to ''sam_b.pt''. | `''sam_b.pt''` |'
  prefs: []
  type: TYPE_TB
- en: '| `device` | `str`, optional | Device to run the models on. Defaults to an
    empty string (CPU or GPU, if available). |  |'
  prefs: []
  type: TYPE_TB
- en: '| `output_dir` | `str`, None, optional | Directory to save the annotated results.
    Defaults to a ''labels'' folder in the same directory as ''data''. | `None` |'
  prefs: []
  type: TYPE_TB
- en: The `auto_annotate` function takes the path to your images, with optional arguments
    for specifying the pre-trained detection and SAM segmentation models, the device
    to run the models on, and the output directory for saving the annotated results.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-annotation with pre-trained models can dramatically cut down the time and
    effort required for creating high-quality segmentation datasets. This feature
    is especially beneficial for researchers and developers dealing with large image
    collections, as it allows them to focus on model development and evaluation rather
    than manual annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Citations and Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you find SAM useful in your research or development work, please consider
    citing our paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We would like to express our gratitude to Meta AI for creating and maintaining
    this valuable resource for the computer vision community.
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is the Segment Anything Model (SAM) by Ultralytics?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Segment Anything Model (SAM) by Ultralytics is a revolutionary image segmentation
    model designed for promptable segmentation tasks. It leverages advanced architecture,
    including image and prompt encoders combined with a lightweight mask decoder,
    to generate high-quality segmentation masks from various prompts such as spatial
    or text cues. Trained on the expansive [SA-1B dataset](https://ai.facebook.com/datasets/segment-anything/),
    SAM excels in zero-shot performance, adapting to new image distributions and tasks
    without prior knowledge. Learn more here.
  prefs: []
  type: TYPE_NORMAL
- en: How can I use the Segment Anything Model (SAM) for image segmentation?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can use the Segment Anything Model (SAM) for image segmentation by running
    inference with various prompts such as bounding boxes or points. Here''s an example
    using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can run inference with SAM in the command line interface
    (CLI):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For more detailed usage instructions, visit the Segmentation section.
  prefs: []
  type: TYPE_NORMAL
- en: How do SAM and YOLOv8 compare in terms of performance?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to YOLOv8, SAM models like SAM-b and FastSAM-s are larger and slower
    but offer unique capabilities for automatic segmentation. For instance, Ultralytics
    YOLOv8n-seg is 53.4 times smaller and 866 times faster than SAM-b. However, SAM's
    zero-shot performance makes it highly flexible and efficient in diverse, untrained
    tasks. Learn more about performance comparisons between SAM and YOLOv8 here.
  prefs: []
  type: TYPE_NORMAL
- en: How can I auto-annotate my dataset using SAM?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ultralytics'' SAM offers an auto-annotation feature that allows generating
    segmentation datasets using a pre-trained detection model. Here''s an example
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This function takes the path to your images and optional arguments for pre-trained
    detection and SAM segmentation models, along with device and output directory
    specifications. For a complete guide, see Auto-Annotation.
  prefs: []
  type: TYPE_NORMAL
- en: What datasets are used to train the Segment Anything Model (SAM)?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SAM is trained on the extensive [SA-1B dataset](https://ai.facebook.com/datasets/segment-anything/)
    which comprises over 1 billion masks across 11 million images. SA-1B is the largest
    segmentation dataset to date, providing high-quality and diverse training data,
    ensuring impressive zero-shot performance in varied segmentation tasks. For more
    details, visit the Dataset section.
  prefs: []
  type: TYPE_NORMAL
