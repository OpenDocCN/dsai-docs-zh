["```py\n>>> import numpy as np\n>>> from scipy.optimize import minimize \n```", "```py\n>>> def rosen(x):\n...  \"\"\"The Rosenbrock function\"\"\"\n...     return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0) \n```", "```py\n>>> x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n>>> res = minimize(rosen, x0, method='nelder-mead',\n...                options={'xatol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 339\n Function evaluations: 571 \n```", "```py\n>>> print(res.x)\n[1\\. 1\\. 1\\. 1\\. 1.] \n```", "```py\n>>> def rosen_with_args(x, a, b):\n...  \"\"\"The Rosenbrock function with additional arguments\"\"\"\n...     return sum(a*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0) + b \n```", "```py\n>>> x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n>>> res = minimize(rosen_with_args, x0, method='nelder-mead',\n...                args=(0.5, 1.), options={'xatol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 1.000000\n Iterations: 319 # may vary\n Function evaluations: 525 # may vary \n```", "```py\n>>> print(res.x)\n[1\\.         1\\.         1\\.         1\\.         0.99999999] \n```", "```py\n>>> def rosen_with_args(x, a, *, b):  # b is a keyword-only argument\n...     return sum(a*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0) + b\n>>> def wrapped_rosen_without_args(x):\n...     return rosen_with_args(x, 0.5, b=1.)  # pass in `a` and `b`\n>>> x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n>>> res = minimize(wrapped_rosen_without_args, x0, method='nelder-mead',\n...                options={'xatol': 1e-8,})\n>>> print(res.x)\n[1\\.         1\\.         1\\.         1\\.         0.99999999] \n```", "```py\n>>> from functools import partial\n>>> partial_rosen = partial(rosen_with_args, a=0.5, b=1.)\n>>> res = minimize(partial_rosen, x0, method='nelder-mead',\n...                options={'xatol': 1e-8,})\n>>> print(res.x)\n[1\\.         1\\.         1\\.         1\\.         0.99999999] \n```", "```py\n>>> def rosen_der(x):\n...     xm = x[1:-1]\n...     xm_m1 = x[:-2]\n...     xm_p1 = x[2:]\n...     der = np.zeros_like(x)\n...     der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n...     der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n...     der[-1] = 200*(x[-1]-x[-2]**2)\n...     return der \n```", "```py\n>>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n...                options={'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 25                     # may vary\n Function evaluations: 30\n Gradient evaluations: 30\n>>> res.x\narray([1., 1., 1., 1., 1.]) \n```", "```py\n>>> def f(x):\n...    return -expensive(x[0])**2\n>>>\n>>> def df(x):\n...     return -2 * expensive(x[0]) * dexpensive(x[0])\n>>>\n>>> def expensive(x):\n...     # this function is computationally expensive!\n...     expensive.count += 1  # let's keep track of how many times it runs\n...     return np.sin(x)\n>>> expensive.count = 0\n>>>\n>>> def dexpensive(x):\n...     return np.cos(x)\n>>>\n>>> res = minimize(f, 0.5, jac=df)\n>>> res.fun\n-0.9999999999999174\n>>> res.nfev, res.njev\n6, 6\n>>> expensive.count\n12 \n```", "```py\n>>> def f_and_df(x):\n...     expensive_value = expensive(x[0])\n...     return (-expensive_value**2,  # objective function\n...             -2*expensive_value*dexpensive(x[0]))  # gradient\n>>>\n>>> expensive.count = 0  # reset the counter\n>>> res = minimize(f_and_df, 0.5, jac=True)\n>>> res.fun\n-0.9999999999999174\n>>> expensive.count\n6 \n```", "```py\n>>> from functools import lru_cache\n>>> expensive.count = 0  # reset the counter\n>>> expensive = lru_cache(expensive)\n>>> res = minimize(f, 0.5, jac=df)\n>>> res.fun\n-0.9999999999999174\n>>> expensive.count\n6 \n```", "```py\n>>> def rosen_hess(x):\n...     x = np.asarray(x)\n...     H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n...     diagonal = np.zeros_like(x)\n...     diagonal[0] = 1200*x[0]**2-400*x[1]+2\n...     diagonal[-1] = 200\n...     diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n...     H = H + np.diag(diagonal)\n...     return H \n```", "```py\n>>> res = minimize(rosen, x0, method='Newton-CG',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'xtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 19                       # may vary\n Function evaluations: 22\n Gradient evaluations: 19\n Hessian evaluations: 19\n>>> res.x\narray([1.,  1.,  1.,  1.,  1.]) \n```", "```py\n>>> def rosen_hess_p(x, p):\n...     x = np.asarray(x)\n...     Hp = np.zeros_like(x)\n...     Hp[0] = (1200*x[0]**2 - 400*x[1] + 2)*p[0] - 400*x[0]*p[1]\n...     Hp[1:-1] = -400*x[:-2]*p[:-2]+(202+1200*x[1:-1]**2-400*x[2:])*p[1:-1] \\\n...                -400*x[1:-1]*p[2:]\n...     Hp[-1] = -400*x[-2]*p[-2] + 200*p[-1]\n...     return Hp \n```", "```py\n>>> res = minimize(rosen, x0, method='Newton-CG',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'xtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 20                    # may vary\n Function evaluations: 23\n Gradient evaluations: 20\n Hessian evaluations: 44\n>>> res.x\narray([1., 1., 1., 1., 1.]) \n```", "```py\n>>> res = minimize(rosen, x0, method='trust-ncg',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 20                    # may vary\n Function evaluations: 21\n Gradient evaluations: 20\n Hessian evaluations: 19\n>>> res.x\narray([1., 1., 1., 1., 1.]) \n```", "```py\n>>> res = minimize(rosen, x0, method='trust-ncg',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 20                    # may vary\n Function evaluations: 21\n Gradient evaluations: 20\n Hessian evaluations: 0\n>>> res.x\narray([1., 1., 1., 1., 1.]) \n```", "```py\n>>> res = minimize(rosen, x0, method='trust-krylov',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 19                    # may vary\n Function evaluations: 20\n Gradient evaluations: 20\n Hessian evaluations: 18\n>>> res.x\narray([1., 1., 1., 1., 1.]) \n```", "```py\n>>> res = minimize(rosen, x0, method='trust-krylov',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 19                    # may vary\n Function evaluations: 20\n Gradient evaluations: 20\n Hessian evaluations: 0\n>>> res.x\narray([1., 1., 1., 1., 1.]) \n```", "```py\n>>> res = minimize(rosen, x0, method='trust-exact',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n Current function value: 0.000000\n Iterations: 13                    # may vary\n Function evaluations: 14\n Gradient evaluations: 13\n Hessian evaluations: 14\n>>> res.x\narray([1., 1., 1., 1., 1.]) \n```", "```py\n>>> from scipy.optimize import Bounds\n>>> bounds = Bounds([0, -0.5], [1.0, 2.0]) \n```", "```py\n>>> from scipy.optimize import LinearConstraint\n>>> linear_constraint = LinearConstraint([[1, 2], [2, 1]], [-np.inf, 1], [1, 1]) \n```", "```py\n>>> def cons_f(x):\n...     return [x[0]**2 + x[1], x[0]**2 - x[1]]\n>>> def cons_J(x):\n...     return [[2*x[0], 1], [2*x[0], -1]]\n>>> def cons_H(x, v):\n...     return v[0]*np.array([[2, 0], [0, 0]]) + v[1]*np.array([[2, 0], [0, 0]])\n>>> from scipy.optimize import NonlinearConstraint\n>>> nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess=cons_H) \n```", "```py\n>>> from scipy.sparse import csc_matrix\n>>> def cons_H_sparse(x, v):\n...     return v[0]*csc_matrix([[2, 0], [0, 0]]) + v[1]*csc_matrix([[2, 0], [0, 0]])\n>>> nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1,\n...                                            jac=cons_J, hess=cons_H_sparse) \n```", "```py\n>>> from scipy.sparse.linalg import LinearOperator\n>>> def cons_H_linear_operator(x, v):\n...     def matvec(p):\n...         return np.array([p[0]*2*(v[0]+v[1]), 0])\n...     return LinearOperator((2, 2), matvec=matvec)\n>>> nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1,\n...                                           jac=cons_J, hess=cons_H_linear_operator) \n```", "```py\n>>> from scipy.optimize import BFGS\n>>> nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess=BFGS()) \n```", "```py\n>>> nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess='2-point') \n```", "```py\n>>> nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac='2-point', hess=BFGS()) \n```", "```py\n>>> x0 = np.array([0.5, 0])\n>>> res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.016 s.\n>>> print(res.x)\n[0.41494531 0.17010937] \n```", "```py\n>>> def rosen_hess_linop(x):\n...     def matvec(p):\n...         return rosen_hess_p(x, p)\n...     return LinearOperator((2, 2), matvec=matvec)\n>>> res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess_linop,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.018 s.\n>>> print(res.x)\n[0.41494531 0.17010937] \n```", "```py\n>>> res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hessp=rosen_hess_p,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.018 s.\n>>> print(res.x)\n[0.41494531 0.17010937] \n```", "```py\n>>> from scipy.optimize import SR1\n>>> res = minimize(rosen, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(),\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 24, CG iterations: 7, optimality: 4.48e-09, constraint violation: 0.00e+00, execution time: 0.016 s.\n>>> print(res.x)\n[0.41494531 0.17010937] \n```", "```py\n>>> ineq_cons = {'type': 'ineq',\n...              'fun' : lambda x: np.array([1 - x[0] - 2*x[1],\n...                                          1 - x[0]**2 - x[1],\n...                                          1 - x[0]**2 + x[1]]),\n...              'jac' : lambda x: np.array([[-1.0, -2.0],\n...                                          [-2*x[0], -1.0],\n...                                          [-2*x[0], 1.0]])}\n>>> eq_cons = {'type': 'eq',\n...            'fun' : lambda x: np.array([2*x[0] + x[1] - 1]),\n...            'jac' : lambda x: np.array([2.0, 1.0])} \n```", "```py\n>>> x0 = np.array([0.5, 0])\n>>> res = minimize(rosen, x0, method='SLSQP', jac=rosen_der,\n...                constraints=[eq_cons, ineq_cons], options={'ftol': 1e-9, 'disp': True},\n...                bounds=bounds)\n# may vary\nOptimization terminated successfully.    (Exit mode 0)\n Current function value: 0.342717574857755\n Iterations: 5\n Function evaluations: 6\n Gradient evaluations: 5\n>>> print(res.x)\n[0.41494475 0.1701105 ] \n```", "```py\n>>> def eggholder(x):\n...     return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1]  + 47))))\n...             -x[0] * np.sin(np.sqrt(abs(x[0] - (x[1]  + 47)))))\n\n>>> bounds = [(-512, 512), (-512, 512)] \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> from mpl_toolkits.mplot3d import Axes3D\n\n>>> x = np.arange(-512, 513)\n>>> y = np.arange(-512, 513)\n>>> xgrid, ygrid = np.meshgrid(x, y)\n>>> xy = np.stack([xgrid, ygrid])\n\n>>> fig = plt.figure()\n>>> ax = fig.add_subplot(111, projection='3d')\n>>> ax.view_init(45, -45)\n>>> ax.plot_surface(xgrid, ygrid, eggholder(xy), cmap='terrain')\n>>> ax.set_xlabel('x')\n>>> ax.set_ylabel('y')\n>>> ax.set_zlabel('eggholder(x, y)')\n>>> plt.show() \n```", "```py\n>>> from scipy import optimize\n>>> results = dict()\n>>> results['shgo'] = optimize.shgo(eggholder, bounds)\n>>> results['shgo']\n fun: -935.3379515604197  # may vary\n funl: array([-935.33795156])\n message: 'Optimization terminated successfully.'\n nfev: 42\n nit: 2\n nlfev: 37\n nlhev: 0\n nljev: 9\n success: True\n x: array([439.48096952, 453.97740589])\n xl: array([[439.48096952, 453.97740589]]) \n```", "```py\n>>> results['DA'] = optimize.dual_annealing(eggholder, bounds)\n>>> results['DA']\n fun: -956.9182316237413  # may vary\n message: ['Maximum number of iteration reached']\n nfev: 4091\n nhev: 0\n nit: 1000\n njev: 0\n x: array([482.35324114, 432.87892901]) \n```", "```py\n>>> results['DE'] = optimize.differential_evolution(eggholder, bounds) \n```", "```py\n>>> results['shgo_sobol'] = optimize.shgo(eggholder, bounds, n=200, iters=5,\n...                                       sampling_method='sobol') \n```", "```py\n>>> fig = plt.figure()\n>>> ax = fig.add_subplot(111)\n>>> im = ax.imshow(eggholder(xy), interpolation='bilinear', origin='lower',\n...                cmap='gray')\n>>> ax.set_xlabel('x')\n>>> ax.set_ylabel('y')\n>>>\n>>> def plot_point(res, marker='o', color=None):\n...     ax.plot(512+res.x[0], 512+res.x[1], marker=marker, color=color, ms=10)\n\n>>> plot_point(results['DE'], color='c')  # differential_evolution - cyan\n>>> plot_point(results['DA'], color='w')  # dual_annealing.        - white\n\n>>> # SHGO produces multiple minima, plot them all (with a smaller marker size)\n>>> plot_point(results['shgo'], color='r', marker='+')\n>>> plot_point(results['shgo_sobol'], color='r', marker='x')\n>>> for i in range(results['shgo_sobol'].xl.shape[0]):\n...     ax.plot(512 + results['shgo_sobol'].xl[i, 0],\n...             512 + results['shgo_sobol'].xl[i, 1],\n...             'ro', ms=2)\n\n>>> ax.set_xlim([-4, 514*2])\n>>> ax.set_ylim([-4, 514*2])\n>>> plt.show() \n```", "```py\n>>> from scipy.optimize import least_squares \n```", "```py\n>>> def model(x, u):\n...     return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3]) \n```", "```py\n>>> def fun(x, u, y):\n...     return model(x, u) - y \n```", "```py\n>>> def jac(x, u, y):\n...     J = np.empty((u.size, x.size))\n...     den = u ** 2 + x[2] * u + x[3]\n...     num = u ** 2 + x[1] * u\n...     J[:, 0] = num / den\n...     J[:, 1] = x[0] * u / den\n...     J[:, 2] = -x[0] * num * u / den ** 2\n...     J[:, 3] = -x[0] * num / den ** 2\n...     return J \n```", "```py\n>>> u = np.array([4.0, 2.0, 1.0, 5.0e-1, 2.5e-1, 1.67e-1, 1.25e-1, 1.0e-1,\n...               8.33e-2, 7.14e-2, 6.25e-2])\n>>> y = np.array([1.957e-1, 1.947e-1, 1.735e-1, 1.6e-1, 8.44e-2, 6.27e-2,\n...               4.56e-2, 3.42e-2, 3.23e-2, 2.35e-2, 2.46e-2])\n>>> x0 = np.array([2.5, 3.9, 4.15, 3.9])\n>>> res = least_squares(fun, x0, jac=jac, bounds=(0, 100), args=(u, y), verbose=1)\n# may vary\n`ftol` termination condition is satisfied.\nFunction evaluations 130, initial cost 4.4383e+00, final cost 1.5375e-04, first-order optimality 4.92e-08.\n>>> res.x\narray([ 0.19280596,  0.19130423,  0.12306063,  0.13607247]) \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> u_test = np.linspace(0, 5)\n>>> y_test = model(res.x, u_test)\n>>> plt.plot(u, y, 'o', markersize=4, label='data')\n>>> plt.plot(u_test, y_test, label='fitted model')\n>>> plt.xlabel(\"u\")\n>>> plt.ylabel(\"y\")\n>>> plt.legend(loc='lower right')\n>>> plt.show() \n```", "```py\n>>> from scipy.optimize import minimize_scalar\n>>> f = lambda x: (x - 2) * (x + 1)**2\n>>> res = minimize_scalar(f, method='brent')\n>>> print(res.x)\n1.0 \n```", "```py\n>>> from scipy.special import j1\n>>> res = minimize_scalar(j1, bounds=(4, 7), method='bounded')\n>>> res.x\n5.33144184241 \n```", "```py\n>>> from scipy.optimize import OptimizeResult\n>>> def custmin(fun, x0, args=(), maxfev=None, stepsize=0.1,\n...         maxiter=100, callback=None, **options):\n...     bestx = x0\n...     besty = fun(x0)\n...     funcalls = 1\n...     niter = 0\n...     improved = True\n...     stop = False\n...\n...     while improved and not stop and niter < maxiter:\n...         improved = False\n...         niter += 1\n...         for dim in range(np.size(x0)):\n...             for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]:\n...                 testx = np.copy(bestx)\n...                 testx[dim] = s\n...                 testy = fun(testx, *args)\n...                 funcalls += 1\n...                 if testy < besty:\n...                     besty = testy\n...                     bestx = testx\n...                     improved = True\n...             if callback is not None:\n...                 callback(bestx)\n...             if maxfev is not None and funcalls >= maxfev:\n...                 stop = True\n...                 break\n...\n...     return OptimizeResult(fun=besty, x=bestx, nit=niter,\n...                           nfev=funcalls, success=(niter > 1))\n>>> x0 = [1.35, 0.9, 0.8, 1.1, 1.2]\n>>> res = minimize(rosen, x0, method=custmin, options=dict(stepsize=0.05))\n>>> res.x\narray([1., 1., 1., 1., 1.]) \n```", "```py\n>>> def custmin(fun, bracket, args=(), maxfev=None, stepsize=0.1,\n...         maxiter=100, callback=None, **options):\n...     bestx = (bracket[1] + bracket[0]) / 2.0\n...     besty = fun(bestx)\n...     funcalls = 1\n...     niter = 0\n...     improved = True\n...     stop = False\n...\n...     while improved and not stop and niter < maxiter:\n...         improved = False\n...         niter += 1\n...         for testx in [bestx - stepsize, bestx + stepsize]:\n...             testy = fun(testx, *args)\n...             funcalls += 1\n...             if testy < besty:\n...                 besty = testy\n...                 bestx = testx\n...                 improved = True\n...         if callback is not None:\n...             callback(bestx)\n...         if maxfev is not None and funcalls >= maxfev:\n...             stop = True\n...             break\n...\n...     return OptimizeResult(fun=besty, x=bestx, nit=niter,\n...                           nfev=funcalls, success=(niter > 1))\n>>> def f(x):\n...    return (x - 2)**2 * (x + 2)**2\n>>> res = minimize_scalar(f, bracket=(-3.5, 0), method=custmin,\n...                       options=dict(stepsize = 0.05))\n>>> res.x\n-2.0 \n```", "```py\n>>> import numpy as np\n>>> from scipy.optimize import root\n>>> def func(x):\n...     return x + 2 * np.cos(x)\n>>> sol = root(func, 0.3)\n>>> sol.x\narray([-1.02986653])\n>>> sol.fun\narray([ -6.66133815e-16]) \n```", "```py\n>>> def func2(x):\n...     f = [x[0] * np.cos(x[1]) - 4,\n...          x[1]*x[0] - x[1] - 5]\n...     df = np.array([[np.cos(x[1]), -x[0] * np.sin(x[1])],\n...                    [x[1], x[0] - 1]])\n...     return f, df\n>>> sol = root(func2, [1, 1], jac=True, method='lm')\n>>> sol.x\narray([ 6.50409711,  0.90841421]) \n```", "```py\nimport numpy as np\nfrom scipy.optimize import root\nfrom numpy import cosh, zeros_like, mgrid, zeros\n\n# parameters\nnx, ny = 75, 75\nhx, hy = 1./(nx-1), 1./(ny-1)\n\nP_left, P_right = 0, 0\nP_top, P_bottom = 1, 0\n\ndef residual(P):\n   d2x = zeros_like(P)\n   d2y = zeros_like(P)\n\n   d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2]) / hx/hx\n   d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n   d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n\n   d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n   d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n   d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n\n   return d2x + d2y + 5*cosh(P).mean()**2\n\n# solve\nguess = zeros((nx, ny), float)\nsol = root(residual, guess, method='krylov', options={'disp': True})\n#sol = root(residual, guess, method='broyden2', options={'disp': True, 'max_rank': 50})\n#sol = root(residual, guess, method='anderson', options={'disp': True, 'M': 10})\nprint('Residual: %g' % abs(residual(sol.x)).max())\n\n# visualize\nimport matplotlib.pyplot as plt\nx, y = mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\nplt.pcolormesh(x, y, sol.x, shading='gouraud')\nplt.colorbar()\nplt.show() \n```", "```py\nfrom scipy.optimize import root\nfrom scipy.sparse import spdiags, kron\nfrom scipy.sparse.linalg import spilu, LinearOperator\nfrom numpy import cosh, zeros_like, mgrid, zeros, eye\n\n# parameters\nnx, ny = 75, 75\nhx, hy = 1./(nx-1), 1./(ny-1)\n\nP_left, P_right = 0, 0\nP_top, P_bottom = 1, 0\n\ndef get_preconditioner():\n  \"\"\"Compute the preconditioner M\"\"\"\n    diags_x = zeros((3, nx))\n    diags_x[0,:] = 1/hx/hx\n    diags_x[1,:] = -2/hx/hx\n    diags_x[2,:] = 1/hx/hx\n    Lx = spdiags(diags_x, [-1,0,1], nx, nx)\n\n    diags_y = zeros((3, ny))\n    diags_y[0,:] = 1/hy/hy\n    diags_y[1,:] = -2/hy/hy\n    diags_y[2,:] = 1/hy/hy\n    Ly = spdiags(diags_y, [-1,0,1], ny, ny)\n\n    J1 = kron(Lx, eye(ny)) + kron(eye(nx), Ly)\n\n    # Now we have the matrix `J_1`. We need to find its inverse `M` --\n    # however, since an approximate inverse is enough, we can use\n    # the *incomplete LU* decomposition\n\n    J1_ilu = spilu(J1)\n\n    # This returns an object with a method .solve() that evaluates\n    # the corresponding matrix-vector product. We need to wrap it into\n    # a LinearOperator before it can be passed to the Krylov methods:\n\n    M = LinearOperator(shape=(nx*ny, nx*ny), matvec=J1_ilu.solve)\n    return M\n\ndef solve(preconditioning=True):\n  \"\"\"Compute the solution\"\"\"\n    count = [0]\n\n    def residual(P):\n        count[0] += 1\n\n        d2x = zeros_like(P)\n        d2y = zeros_like(P)\n\n        d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2])/hx/hx\n        d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n        d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n\n        d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n        d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n        d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n\n        return d2x + d2y + 5*cosh(P).mean()**2\n\n    # preconditioner\n    if preconditioning:\n        M = get_preconditioner()\n    else:\n        M = None\n\n    # solve\n    guess = zeros((nx, ny), float)\n\n    sol = root(residual, guess, method='krylov',\n               options={'disp': True,\n                        'jac_options': {'inner_M': M}})\n    print('Residual', abs(residual(sol.x)).max())\n    print('Evaluations', count[0])\n\n    return sol.x\n\ndef main():\n    sol = solve(preconditioning=True)\n\n    # visualize\n    import matplotlib.pyplot as plt\n    x, y = mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\n    plt.clf()\n    plt.pcolor(x, y, sol)\n    plt.clim(0, 1)\n    plt.colorbar()\n    plt.show()\n\nif __name__ == \"__main__\":\n    main() \n```", "```py\n0:  |F(x)| = 803.614; step 1; tol 0.000257947\n1:  |F(x)| = 345.912; step 1; tol 0.166755\n2:  |F(x)| = 139.159; step 1; tol 0.145657\n3:  |F(x)| = 27.3682; step 1; tol 0.0348109\n4:  |F(x)| = 1.03303; step 1; tol 0.00128227\n5:  |F(x)| = 0.0406634; step 1; tol 0.00139451\n6:  |F(x)| = 0.00344341; step 1; tol 0.00645373\n7:  |F(x)| = 0.000153671; step 1; tol 0.00179246\n8:  |F(x)| = 6.7424e-06; step 1; tol 0.00173256\nResidual 3.57078908664e-07\nEvaluations 317 \n```", "```py\n0:  |F(x)| = 136.993; step 1; tol 7.49599e-06\n1:  |F(x)| = 4.80983; step 1; tol 0.00110945\n2:  |F(x)| = 0.195942; step 1; tol 0.00149362\n3:  |F(x)| = 0.000563597; step 1; tol 7.44604e-06\n4:  |F(x)| = 1.00698e-09; step 1; tol 2.87308e-12\nResidual 9.29603061195e-11\nEvaluations 77 \n```", "```py\n>>> import numpy as np\n>>> from scipy.optimize import linprog\n>>> c = np.array([-29.0, -45.0, 0.0, 0.0])\n>>> A_ub = np.array([[1.0, -1.0, -3.0, 0.0],\n...                 [-2.0, 3.0, 7.0, -3.0]])\n>>> b_ub = np.array([5.0, -10.0])\n>>> A_eq = np.array([[2.0, 8.0, 1.0, 0.0],\n...                 [4.0, 4.0, 0.0, 1.0]])\n>>> b_eq = np.array([60.0, 60.0])\n>>> x0_bounds = (0, None)\n>>> x1_bounds = (0, 5.0)\n>>> x2_bounds = (-np.inf, 0.5)  # +/- np.inf can be used instead of None\n>>> x3_bounds = (-3.0, None)\n>>> bounds = [x0_bounds, x1_bounds, x2_bounds, x3_bounds]\n>>> result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)\n>>> print(result.message)\nThe problem is infeasible. (HiGHS Status 8: model_status is Infeasible; primal_status is At lower/fixed bound) \n```", "```py\n>>> x1_bounds = (0, 6)\n>>> bounds = [x0_bounds, x1_bounds, x2_bounds, x3_bounds]\n>>> result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)\n>>> print(result.message)\nOptimization terminated successfully. (HiGHS Status 7: Optimal) \n```", "```py\n>>> x = np.array(result.x)\n>>> obj = result.fun\n>>> print(c @ x)\n-505.97435889013434  # may vary\n>>> print(obj)\n-505.97435889013434  # may vary \n```", "```py\n>>> print(b_ub - (A_ub @ x).flatten())  # this is equivalent to result.slack\n[ 6.52747190e-10, -2.26730279e-09]  # may vary\n>>> print(b_eq - (A_eq @ x).flatten())  # this is equivalent to result.con\n[ 9.78840831e-09, 1.04662945e-08]]  # may vary\n>>> print([0 <= result.x[0], 0 <= result.x[1] <= 6.0, result.x[2] <= 0.5, -3.0 <= result.x[3]])\n[True, True, True, True] \n```", "```py\n>>> import numpy as np\n>>> cost = np.array([[43.5, 45.5, 43.4, 46.5, 46.3],\n...                  [47.1, 42.1, 39.1, 44.1, 47.8],\n...                  [48.4, 49.6, 42.1, 44.5, 50.4],\n...                  [38.2, 36.8, 43.2, 41.2, 37.2]]) \n```", "```py\n>>> from scipy.optimize import linear_sum_assignment\n>>> row_ind, col_ind = linear_sum_assignment(cost) \n```", "```py\n>>> row_ind\narray([0, 1, 2, 3])\n>>> col_ind\narray([0, 2, 3, 1]) \n```", "```py\n>>> styles = np.array([\"backstroke\", \"breaststroke\", \"butterfly\", \"freestyle\"])[row_ind]\n>>> students = np.array([\"A\", \"B\", \"C\", \"D\", \"E\"])[col_ind]\n>>> dict(zip(styles, students))\n{'backstroke': 'A', 'breaststroke': 'C', 'butterfly': 'D', 'freestyle': 'B'} \n```", "```py\n>>> cost[row_ind, col_ind].sum()\n163.89999999999998 \n```", "```py\n>>> np.min(cost, axis=1).sum()\n161.39999999999998 \n```", "```py\n>>> import numpy as np\n>>> from scipy import optimize\n>>> sizes = np.array([21, 11, 15, 9, 34, 25, 41, 52])\n>>> values = np.array([22, 12, 16, 10, 35, 26, 42, 53]) \n```", "```py\n>>> bounds = optimize.Bounds(0, 1)  # 0 <= x_i <= 1\n>>> integrality = np.full_like(values, True)  # x_i are integers \n```", "```py\n>>> capacity = 100\n>>> constraints = optimize.LinearConstraint(A=sizes, lb=0, ub=capacity) \n```", "```py\n>>> from scipy.optimize import milp\n>>> res = milp(c=-values, constraints=constraints,\n...            integrality=integrality, bounds=bounds) \n```", "```py\n>>> res.success\nTrue\n>>> res.x\narray([1., 1., 0., 1., 1., 1., 0., 0.]) \n```", "```py\n>>> from scipy.optimize import milp\n>>> res = milp(c=-values, constraints=constraints,\n...            integrality=False, bounds=bounds)\n>>> res.x\narray([1\\.        , 1\\.        , 1\\.        , 1\\.        ,\n 0.55882353, 1\\.        , 0\\.        , 0\\.        ]) \n```"]