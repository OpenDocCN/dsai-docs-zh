- en: IO tools (text, CSV, HDF5, …)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://pandas.pydata.org/docs/user_guide/io.html](https://pandas.pydata.org/docs/user_guide/io.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The pandas I/O API is a set of top level `reader` functions accessed like [`pandas.read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") that generally return a pandas object. The corresponding `writer`
    functions are object methods that are accessed like [`DataFrame.to_csv()`](../reference/api/pandas.DataFrame.to_csv.html#pandas.DataFrame.to_csv
    "pandas.DataFrame.to_csv"). Below is a table containing available `readers` and
    `writers`.
  prefs: []
  type: TYPE_NORMAL
- en: '| Format Type | Data Description | Reader | Writer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| text | [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) | [read_csv](#io-read-csv-table)
    | [to_csv](#io-store-in-csv) |'
  prefs: []
  type: TYPE_TB
- en: '| text | Fixed-Width Text File | [read_fwf](#io-fwf-reader) |  |'
  prefs: []
  type: TYPE_TB
- en: '| text | [JSON](https://www.json.org/) | [read_json](#io-json-reader) | [to_json](#io-json-writer)
    |'
  prefs: []
  type: TYPE_TB
- en: '| text | [HTML](https://en.wikipedia.org/wiki/HTML) | [read_html](#io-read-html)
    | [to_html](#io-html) |'
  prefs: []
  type: TYPE_TB
- en: '| text | [LaTeX](https://en.wikipedia.org/wiki/LaTeX) |  | [Styler.to_latex](#io-latex)
    |'
  prefs: []
  type: TYPE_TB
- en: '| text | [XML](https://www.w3.org/standards/xml/core) | [read_xml](#io-read-xml)
    | [to_xml](#io-xml) |'
  prefs: []
  type: TYPE_TB
- en: '| text | Local clipboard | [read_clipboard](#io-clipboard) | [to_clipboard](#io-clipboard)
    |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [MS Excel](https://en.wikipedia.org/wiki/Microsoft_Excel) | [read_excel](#io-excel-reader)
    | [to_excel](#io-excel-writer) |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [OpenDocument](http://opendocumentformat.org) | [read_excel](#io-ods)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [HDF5 Format](https://support.hdfgroup.org/HDF5/whatishdf5.html)
    | [read_hdf](#io-hdf5) | [to_hdf](#io-hdf5) |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [Feather Format](https://github.com/wesm/feather) | [read_feather](#io-feather)
    | [to_feather](#io-feather) |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [Parquet Format](https://parquet.apache.org/) | [read_parquet](#io-parquet)
    | [to_parquet](#io-parquet) |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [ORC Format](https://orc.apache.org/) | [read_orc](#io-orc) | [to_orc](#io-orc)
    |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [Stata](https://en.wikipedia.org/wiki/Stata) | [read_stata](#io-stata-reader)
    | [to_stata](#io-stata-writer) |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [SAS](https://en.wikipedia.org/wiki/SAS_(software)) | [read_sas](#io-sas-reader)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [SPSS](https://en.wikipedia.org/wiki/SPSS) | [read_spss](#io-spss-reader)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| binary | [Python Pickle Format](https://docs.python.org/3/library/pickle.html)
    | [read_pickle](#io-pickle) | [to_pickle](#io-pickle) |'
  prefs: []
  type: TYPE_TB
- en: '| SQL | [SQL](https://en.wikipedia.org/wiki/SQL) | [read_sql](#io-sql) | [to_sql](#io-sql)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SQL | [Google BigQuery](https://en.wikipedia.org/wiki/BigQuery) | [read_gbq](#io-bigquery)
    | [to_gbq](#io-bigquery) |'
  prefs: []
  type: TYPE_TB
- en: '[Here](#io-perf) is an informal performance comparison for some of these IO
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For examples that use the `StringIO` class, make sure you import it with `from
    io import StringIO` for Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: '## CSV & text files'
  prefs: []
  type: TYPE_NORMAL
- en: The workhorse function for reading text files (a.k.a. flat files) is [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"). See the [cookbook](cookbook.html#cookbook-csv) for some advanced
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    accepts the following common arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: filepath_or_buffervarious
  prefs: []
  type: TYPE_NORMAL
- en: Either a path to a file (a [`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)"), or `py:py._path.local.LocalPath`), URL (including http,
    ftp, and S3 locations), or any object with a `read()` method (such as an open
    file or [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO "(in
    Python v3.12)")).
  prefs: []
  type: TYPE_NORMAL
- en: sepstr, defaults to `','` for [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), `\t` for [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
  prefs: []
  type: TYPE_NORMAL
- en: 'Delimiter to use. If sep is `None`, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will be used
    and automatically detect the separator by Python’s builtin sniffer tool, [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)"). In addition, separators longer than 1 character and different
    from `''\s+''` will be interpreted as regular expressions and will also force
    the use of the Python parsing engine. Note that regex delimiters are prone to
    ignoring quoted data. Regex example: `''\\r\\t''`.'
  prefs: []
  type: TYPE_NORMAL
- en: delimiterstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Alternative argument name for sep.
  prefs: []
  type: TYPE_NORMAL
- en: delim_whitespaceboolean, default False
  prefs: []
  type: TYPE_NORMAL
- en: Specifies whether or not whitespace (e.g. `' '` or `'\t'`) will be used as the
    delimiter. Equivalent to setting `sep='\s+'`. If this option is set to `True`,
    nothing should be passed in for the `delimiter` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Column and index locations and names
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: headerint or list of ints, default `'infer'`
  prefs: []
  type: TYPE_NORMAL
- en: 'Row number(s) to use as the column names, and the start of the data. Default
    behavior is to infer the column names: if no names are passed the behavior is
    identical to `header=0` and column names are inferred from the first line of the
    file, if column names are passed explicitly then the behavior is identical to
    `header=None`. Explicitly pass `header=0` to be able to replace existing names.'
  prefs: []
  type: TYPE_NORMAL
- en: The header can be a list of ints that specify row locations for a MultiIndex
    on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this parameter ignores
    commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes
    the first line of data rather than the first line of the file.
  prefs: []
  type: TYPE_NORMAL
- en: namesarray-like, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: List of column names to use. If file contains no header row, then you should
    explicitly pass `header=None`. Duplicates in this list are not allowed.
  prefs: []
  type: TYPE_NORMAL
- en: index_colint, str, sequence of int / str, or False, optional, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Column(s) to use as the row labels of the `DataFrame`, either given as string
    name or column index. If a sequence of int / str is given, a MultiIndex is used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`index_col=False` can be used to force pandas to *not* use the first column
    as the index, e.g. when you have a malformed file with delimiters at the end of
    each line.'
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `None` instructs pandas to guess. If the number of fields
    in the column header row is equal to the number of fields in the body of the data
    file, then a default index is used. If it is larger, then the first columns are
    used as index so that the remaining number of fields in the body are equal to
    the number of fields in the header.
  prefs: []
  type: TYPE_NORMAL
- en: The first row after the header is used to determine the number of columns, which
    will go into the index. If the subsequent rows contain less columns than the first
    row, they are filled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: This can be avoided through `usecols`. This ensures that the columns are taken
    as is and the trailing data are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: usecolslist-like or callable, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Return a subset of the columns. If list-like, all elements must either be positional
    (i.e. integer indices into the document columns) or strings that correspond to
    column names provided either by the user in `names` or inferred from the document
    header row(s). If `names` are given, the document header row(s) are not taken
    into account. For example, a valid list-like `usecols` parameter would be `[0,
    1, 2]` or `['foo', 'bar', 'baz']`.
  prefs: []
  type: TYPE_NORMAL
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate
    a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo',
    'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the column names,
    returning names where the callable function evaluates to True:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using this parameter results in much faster parsing time and lower memory usage
    when using the c engine. The Python engine loads the data first before deciding
    which columns to drop.
  prefs: []
  type: TYPE_NORMAL
- en: General parsing configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: dtypeType name or dict of column -> type, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'Data type for data or columns. E.g. `{''a'': np.float64, ''b'': np.int32, ''c'':
    ''Int64''}` Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype. If converters are specified, they will be
    applied INSTEAD of dtype conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 1.5.0: Support for defaultdict was added. Specify a defaultdict
    as input where the default determines the dtype of the columns which are not explicitly
    listed.'
  prefs: []
  type: TYPE_NORMAL
- en: dtype_backend{“numpy_nullable”, “pyarrow”}, defaults to NumPy backed DataFrames
  prefs: []
  type: TYPE_NORMAL
- en: Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
    nullable dtypes are used for all dtypes that have a nullable implementation when
    “numpy_nullable” is set, pyarrow is used for all dtypes if “pyarrow” is set.
  prefs: []
  type: TYPE_NORMAL
- en: The dtype_backends are still experimential.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: engine{`'c'`, `'python'`, `'pyarrow'`}
  prefs: []
  type: TYPE_NORMAL
- en: Parser engine to use. The C and pyarrow engines are faster, while the python
    engine is currently more feature-complete. Multithreading is currently only supported
    by the pyarrow engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 1.4.0: The “pyarrow” engine was added as an *experimental* engine,
    and some features are unsupported, or may not work correctly, with this engine.'
  prefs: []
  type: TYPE_NORMAL
- en: convertersdict, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.
  prefs: []
  type: TYPE_NORMAL
- en: true_valueslist, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Values to consider as `True`.
  prefs: []
  type: TYPE_NORMAL
- en: false_valueslist, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Values to consider as `False`.
  prefs: []
  type: TYPE_NORMAL
- en: skipinitialspaceboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Skip spaces after delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: skiprowslist-like or integer, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
    of the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: skipfooterint, default `0`
  prefs: []
  type: TYPE_NORMAL
- en: Number of lines at bottom of file to skip (unsupported with engine=’c’).
  prefs: []
  type: TYPE_NORMAL
- en: nrowsint, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Number of rows of file to read. Useful for reading pieces of large files.
  prefs: []
  type: TYPE_NORMAL
- en: low_memoryboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: Internally process the file in chunks, resulting in lower memory use while parsing,
    but possibly mixed type inference. To ensure no mixed types either set `False`,
    or specify the type with the `dtype` parameter. Note that the entire file is read
    into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter
    to return the data in chunks. (Only valid with C parser)
  prefs: []
  type: TYPE_NORMAL
- en: memory_mapboolean, default False
  prefs: []
  type: TYPE_NORMAL
- en: If a filepath is provided for `filepath_or_buffer`, map the file object directly
    onto memory and access the data directly from there. Using this option can improve
    performance because there is no longer any I/O overhead.
  prefs: []
  type: TYPE_NORMAL
- en: NA and missing data handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: na_valuesscalar, str, list-like, or dict, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Additional strings to recognize as NA/NaN. If dict passed, specific per-column
    NA values. See [na values const](#io-navaluesconst) below for a list of the values
    interpreted as NaN by default.
  prefs: []
  type: TYPE_NORMAL
- en: keep_default_naboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether or not to include the default NaN values when parsing the data. Depending
    on whether `na_values` is passed in, the behavior is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is
    appended to the default NaN values used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are not specified, only the
    default NaN values are used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are specified, only the NaN
    values specified `na_values` are used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are not specified, no strings
    will be parsed as NaN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if `na_filter` is passed in as `False`, the `keep_default_na` and
    `na_values` parameters will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: na_filterboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: Detect missing value markers (empty strings and the value of na_values). In
    data without any NAs, passing `na_filter=False` can improve the performance of
    reading a large file.
  prefs: []
  type: TYPE_NORMAL
- en: verboseboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Indicate number of NA values placed in non-numeric columns.
  prefs: []
  type: TYPE_NORMAL
- en: skip_blank_linesboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: If `True`, skip over blank lines rather than interpreting as NaN values.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Datetime handling'
  prefs: []
  type: TYPE_NORMAL
- en: parse_datesboolean or list of ints or names or list of lists or dict, default
    `False`.
  prefs: []
  type: TYPE_NORMAL
- en: If `True` -> try parsing the index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `{''foo'': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A fast-path exists for iso8601-formatted dates.
  prefs: []
  type: TYPE_NORMAL
- en: infer_datetime_formatboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates is enabled for a column, attempt to infer the datetime
    format to speed up the processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: A strict version of this argument is now the
    default, passing it has no effect.'
  prefs: []
  type: TYPE_NORMAL
- en: keep_date_colboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates specifies combining multiple columns then keep the
    original columns.
  prefs: []
  type: TYPE_NORMAL
- en: date_parserfunction, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'Function to use for converting a sequence of string columns to an array of
    datetime instances. The default uses `dateutil.parser.parser` to do the conversion.
    pandas will try to call date_parser in three different ways, advancing to the
    next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates)
    as arguments; 2) concatenate (row-wise) the string values from the columns defined
    by parse_dates into a single array and pass that; and 3) call date_parser once
    for each row using one or more strings (corresponding to the columns defined by
    parse_dates) as arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: Use `date_format` instead, or read in as `object`
    and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.'
  prefs: []
  type: TYPE_NORMAL
- en: date_formatstr or dict of column -> format, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: If used in conjunction with `parse_dates`, will parse dates according to this
    format. For anything more complex, please read in as `object` and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: dayfirstboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: DD/MM format dates, international and European format.
  prefs: []
  type: TYPE_NORMAL
- en: cache_datesboolean, default True
  prefs: []
  type: TYPE_NORMAL
- en: If True, use a cache of unique, converted dates to apply the datetime conversion.
    May produce significant speed-up when parsing duplicate date strings, especially
    ones with timezone offsets.
  prefs: []
  type: TYPE_NORMAL
- en: Iteration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: iteratorboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.
  prefs: []
  type: TYPE_NORMAL
- en: chunksizeint, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration. See [iterating and chunking](#io-chunking)
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Quoting, compression, and file format
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: compression{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`,
    `dict`}, default `'infer'`
  prefs: []
  type: TYPE_NORMAL
- en: 'For on-the-fly decompression of on-disk data. If ‘infer’, then use gzip, bz2,
    zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in ‘.gz’, ‘.bz2’,
    ‘.zip’, ‘.xz’, ‘.zst’, respectively, and no decompression otherwise. If using
    ‘zip’, the ZIP file must contain only one data file to be read in. Set to `None`
    for no decompression. Can also be a dict with key `''method''` set to one of {`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd''`} and other key-value pairs are forwarded to
    `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`.
    As an example, the following could be passed for faster compression and to create
    a reproducible gzip archive: `compression={''method'': ''gzip'', ''compresslevel'':
    1, ''mtime'': 1}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’
    to `gzip.open`.'
  prefs: []
  type: TYPE_NORMAL
- en: thousandsstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Thousands separator.
  prefs: []
  type: TYPE_NORMAL
- en: decimalstr, default `'.'`
  prefs: []
  type: TYPE_NORMAL
- en: Character to recognize as decimal point. E.g. use `','` for European data.
  prefs: []
  type: TYPE_NORMAL
- en: float_precisionstring, default None
  prefs: []
  type: TYPE_NORMAL
- en: Specifies which converter the C engine should use for floating-point values.
    The options are `None` for the ordinary converter, `high` for the high-precision
    converter, and `round_trip` for the round-trip converter.
  prefs: []
  type: TYPE_NORMAL
- en: lineterminatorstr (length 1), default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Character to break file into lines. Only valid with C parser.
  prefs: []
  type: TYPE_NORMAL
- en: quotecharstr (length 1)
  prefs: []
  type: TYPE_NORMAL
- en: The character used to denote the start and end of a quoted item. Quoted items
    can include the delimiter and it will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: quotingint or `csv.QUOTE_*` instance, default `0`
  prefs: []
  type: TYPE_NORMAL
- en: Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL`
    (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).
  prefs: []
  type: TYPE_NORMAL
- en: doublequoteboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether
    or not to interpret two consecutive `quotechar` elements **inside** a field as
    a single `quotechar` element.
  prefs: []
  type: TYPE_NORMAL
- en: escapecharstr (length 1), default `None`
  prefs: []
  type: TYPE_NORMAL
- en: One-character string used to escape delimiter when quoting is `QUOTE_NONE`.
  prefs: []
  type: TYPE_NORMAL
- en: commentstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Indicates remainder of line should not be parsed. If found at the beginning
    of a line, the line will be ignored altogether. This parameter must be a single
    character. Like empty lines (as long as `skip_blank_lines=True`), fully commented
    lines are ignored by the parameter `header` but not by `skiprows`. For example,
    if `comment='#'`, parsing ‘#empty\na,b,c\n1,2,3’ with `header=0` will result in
    ‘a,b,c’ being treated as the header.
  prefs: []
  type: TYPE_NORMAL
- en: encodingstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python
    standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  prefs: []
  type: TYPE_NORMAL
- en: dialectstr or [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") instance, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'If provided, this parameter will override values (default or not) for the following
    parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`,
    and `quoting`. If it is necessary to override values, a ParserWarning will be
    issued. See [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") documentation for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: on_bad_lines(‘error’, ‘warn’, ‘skip’), default ‘error’
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :'
  prefs: []
  type: TYPE_NORMAL
- en: ‘error’, raise an ParserError when a bad line is encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘warn’, print a warning when a bad line is encountered and skip that line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘skip’, skip bad lines without raising or warning when they are encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: '### Specifying column data types'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can indicate the data type for the whole `DataFrame` or individual columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Fortunately, pandas offers more than one way to ensure that your column(s) contain
    only one `dtype`. If you’re unfamiliar with these concepts, you can see [here](basics.html#basics-dtypes)
    to learn more about dtypes, and [here](basics.html#basics-object-conversion) to
    learn more about `object` conversion in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, you can use the `converters` argument of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Or you can use the [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") function to coerce the dtypes after reading in the data,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: which will convert all valid parsing to floats, leaving the invalid parsing
    as `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, how you deal with reading in columns containing mixed dtypes depends
    on your specific needs. In the case above, if you wanted to `NaN` out the data
    anomalies, then [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") is probably your best option. However, if you wanted for
    all the data to be coerced, no matter the type, then using the `converters` argument
    of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    would certainly be worth trying.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, reading in abnormal data with columns containing mixed dtypes
    will result in an inconsistent dataset. If you rely on pandas to infer the dtypes
    of your columns, the parsing engine will go and infer the dtypes for different
    chunks of the data, rather than the whole dataset at once. Consequently, you can
    end up with column(s) with mixed dtypes. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: will result with `mixed_df` containing an `int` dtype for certain chunks of
    the column, and `str` for others due to the mixed dtypes from the data that was
    read in. It is important to note that the overall column will be marked with a
    `dtype` of `object`, which is used for columns with mixed dtypes.
  prefs: []
  type: TYPE_NORMAL
- en: Setting `dtype_backend="numpy_nullable"` will result in nullable dtypes for
    every column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]  ### Specifying categorical dtype'
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` columns can be parsed directly by specifying `dtype=''category''`
    or `dtype=CategoricalDtype(categories, ordered)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Individual columns can be parsed as a `Categorical` using a dict specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Specifying `dtype='category'` will result in an unordered `Categorical` whose
    `categories` are the unique values observed in the data. For more control on the
    categories and order, create a `CategoricalDtype` ahead of time, and pass that
    for that column’s `dtype`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When using `dtype=CategoricalDtype`, “unexpected” values outside of `dtype.categories`
    are treated as missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This matches the behavior of `Categorical.set_categories()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `dtype='category'`, the resulting categories will always be parsed as strings
    (object dtype). If the categories are numeric they can be converted using the
    [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric "pandas.to_numeric")
    function, or as appropriate, another converter such as [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime").
  prefs: []
  type: TYPE_NORMAL
- en: When `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all numeric,
    all datetimes, etc.), the conversion is done automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Naming and using columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Handling column names'
  prefs: []
  type: TYPE_NORMAL
- en: 'A file may or may not have a header row. pandas assumes the first row should
    be used as the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying the `names` argument in conjunction with `header` you can indicate
    other names to use and whether or not to throw away the header row (if any):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If the header is in a row other than the first, pass the row number to `header`.
    This will skip the preceding rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Default behavior is to infer the column names: if no names are passed the behavior
    is identical to `header=0` and column names are inferred from the first non-blank
    line of the file, if column names are passed explicitly then the behavior is identical
    to `header=None`.  ### Duplicate names parsing'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the file or header contains duplicate names, pandas will by default distinguish
    between them so as to prevent overwriting data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There is no more duplicate data because duplicate columns ‘X’, …, ‘X’ become
    ‘X’, ‘X.1’, …, ‘X.N’.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Filtering columns (`usecols`)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `usecols` argument allows you to select any subset of the columns in a
    file, either using the column names, position numbers or a callable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `usecols` argument can also be used to specify which columns not to use
    in the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the callable is specifying that we exclude the “a” and “c” columns
    from the output.
  prefs: []
  type: TYPE_NORMAL
- en: Comments and empty lines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Ignoring line comments and empty lines'
  prefs: []
  type: TYPE_NORMAL
- en: If the `comment` parameter is specified, then completely commented lines will
    be ignored. By default, completely blank lines will be ignored as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'The presence of ignored lines might create ambiguities involving line numbers;
    the parameter `header` uses row numbers (ignoring commented/empty lines), while
    `skiprows` uses line numbers (including commented/empty lines):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If both `header` and `skiprows` are specified, `header` will be relative to
    the end of `skiprows`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]  #### Comments'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes comments or meta data may be included in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the parser includes the comments in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can suppress the comments using the `comment` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]  ### Dealing with Unicode data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `encoding` argument should be used for encoded unicode data, which will
    result in byte strings being decoded to unicode in the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Some formats which encode all characters as multiple bytes, like UTF-16, won’t
    parse correctly at all without specifying the encoding. [Full list of Python standard
    encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).  ###
    Index columns and trailing delimiters'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a file has one more column of data than the number of column names, the
    first column will be used as the `DataFrame`’s row names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Ordinarily, you can achieve this behavior using the `index_col` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some exception cases when a file has been prepared with delimiters
    at the end of each data line, confusing the parser. To explicitly disable the
    index column inference and discard the last column, pass `index_col=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If a subset of data is being parsed using the `usecols` option, the `index_col`
    specification is based on that subset, not the original data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]  ### Date Handling'
  prefs: []
  type: TYPE_NORMAL
- en: Specifying date columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To better facilitate working with datetime data, [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") uses the keyword arguments `parse_dates` and `date_format`
    to allow users to specify a variety of columns and date/time formats to turn the
    input text data into `datetime` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest case is to just pass in `parse_dates=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: It is often the case that we may want to store date and time data separately,
    or store various date fields separately. the `parse_dates` keyword can be used
    to specify a combination of columns to parse the dates and/or times from.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a list of column lists to `parse_dates`, the resulting date
    columns will be prepended to the output (so as to not affect the existing column
    order) and the new column names will be the concatenation of the component column
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'By default the parser removes the component date columns, but you can choose
    to retain them via the `keep_date_col` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you wish to combine multiple columns into a single date column,
    a nested list must be used. In other words, `parse_dates=[1, 2]` indicates that
    the second and third columns should each be parsed as separate date columns while
    `parse_dates=[[1, 2]]` means the two columns should be parsed into a single column.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use a dict to specify custom name columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to remember that if multiple text columns are to be parsed
    into a single date column, then a new column is prepended to the data. The `index_col`
    specification is based off of this new set of columns rather than the original
    data columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a column or index contains an unparsable date, the entire column or index
    will be returned unaltered as an object data type. For non-standard datetime parsing,
    use [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") after `pd.read_csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g
    “2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your
    data to store datetimes in this format, load times will be significantly faster,
    ~20x has been observed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.2.0: Combining date columns inside read_csv is deprecated.
    Use `pd.to_datetime` on the relevant result columns instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Date parsing functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, the parser allows you to specify a custom `date_format`. Performance-wise,
    you should try these methods of parsing dates in order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or
    `date_format={column_name: "%d/%m/%Y"}`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you different formats for different columns, or want to pass any extra options
    (such as `utc`) to `to_datetime`, then you should read in your data as `object`
    dtype, and then use `to_datetime`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### Parsing a CSV with mixed timezones'
  prefs: []
  type: TYPE_NORMAL
- en: pandas cannot natively represent a column or index with mixed timezones. If
    your CSV file contains columns with a mixture of timezones, the default result
    will be an object-dtype column with strings, even with `parse_dates`. To parse
    the mixed-timezone values as a datetime column, read in as `object` dtype and
    then call [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") with `utc=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]  #### Inferring datetime format'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of datetime strings that can be guessed (all representing
    December 30th, 2011 at 00:00:00):'
  prefs: []
  type: TYPE_NORMAL
- en: “20111230”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “2011/12/30”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “20111230 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “12/30/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/Dec/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/December/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`,
    it will guess “01/12/2011” to be December 1st. With `dayfirst=False` (default)
    it will guess “01/12/2011” to be January 12th.
  prefs: []
  type: TYPE_NORMAL
- en: If you try to parse a column of date strings, pandas will attempt to guess the
    format from the first non-NaN element, and will then parse the rest of the column
    with that format. If pandas fails to guess the format (for example if your first
    string is `'01 December US/Pacific 2000'`), then a warning will be raised and
    each row will be parsed individually by `dateutil.parser.parse`. The safest way
    to parse dates is to explicitly set `format=`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the case that you have mixed datetime formats within the same column, you
    can pass `format='mixed'`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'or, if your datetime formats are all ISO8601 (possibly not identically-formatted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: International date formats
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While US date formats tend to be MM/DD/YYYY, many international formats use
    DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Writing CSVs to binary file objects
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  prefs: []
  type: TYPE_NORMAL
- en: '`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary
    mode. In most cases, it is not necessary to specify `mode` as Pandas will auto-detect
    whether the file object is opened in text or binary mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]  ### Specifying method for floating-point conversion'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `float_precision` can be specified in order to use a specific
    floating-point converter during parsing with the C engine. The options are the
    ordinary converter, the high-precision converter, and the round-trip converter
    (which is guaranteed to round-trip values after writing to a file). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]  ### Thousand separators'
  prefs: []
  type: TYPE_NORMAL
- en: 'For large numbers that have been written with a thousands separator, you can
    set the `thousands` keyword to a string of length 1 so that integers will be parsed
    correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, numbers with a thousands separator will be parsed as strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `thousands` keyword allows integers to be parsed correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]  ### NA values'
  prefs: []
  type: TYPE_NORMAL
- en: To control which values are parsed as missing values (which are signified by
    `NaN`), specify a string in `na_values`. If you specify a list of strings, then
    all values in it are considered to be missing values. If you specify a number
    (a `float`, like `5.0` or an `integer` like `5`), the corresponding equivalent
    values will also imply a missing value (in this case effectively `[5.0, 5]` are
    recognized as `NaN`).
  prefs: []
  type: TYPE_NORMAL
- en: To completely override the default values that are recognized as missing, specify
    `keep_default_na=False`.
  prefs: []
  type: TYPE_NORMAL
- en: The default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',
    '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN',
    '-NaN', 'nan', '-nan', 'None', '']`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In the example above `5` and `5.0` will be recognized as `NaN`, in addition
    to the defaults. A string will first be interpreted as a numerical `5`, then as
    a `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Above, only an empty field will be recognized as `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Above, both `NA` and `0` as strings are `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The default values, in addition to the string `"Nope"` are recognized as `NaN`.  ###
    Infinity'
  prefs: []
  type: TYPE_NORMAL
- en: '`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf`
    as `-np.inf` (negative infinity). These will ignore the case of the value, meaning
    `Inf`, will also be parsed as `np.inf`.  ### Boolean values'
  prefs: []
  type: TYPE_NORMAL
- en: 'The common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as
    boolean. Occasionally you might want to recognize other values as being boolean.
    To do this, use the `true_values` and `false_values` options as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]  ### Handling “bad” lines'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some files may have malformed lines with too few fields or too many. Lines
    with too few fields will have NA values filled in the trailing fields. Lines with
    too many fields will raise an error by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'You can elect to skip bad lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: New in version 1.4.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Or pass a callable function to handle the bad line if `engine="python"`. The
    bad line will be a list of strings that was split by the `sep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The callable function will handle only a line with too many fields. Bad lines
    caused by other errors will be silently skipped.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The line was not processed in this case, as a “bad line” here is caused by an
    escape character.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `usecols` parameter to eliminate extraneous column data
    that appear in some lines but not others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: In case you want to keep all data including the lines with too many fields,
    you can specify a sufficient number of `names`. This ensures that lines with not
    enough fields are filled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]  ### Dialect'
  prefs: []
  type: TYPE_NORMAL
- en: The `dialect` keyword gives greater flexibility in specifying the file format.
    By default it uses the Excel dialect but you can specify either the dialect name
    or a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect "(in
    Python v3.12)") instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you had data with unenclosed quotes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: By default, `read_csv` uses the Excel dialect and treats the double quote as
    the quote character, which causes it to fail when it finds a newline before it
    finds the closing double quote.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get around this using `dialect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'All of the dialect options can be specified separately by keyword arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Another common dialect option is `skipinitialspace`, to skip any whitespace
    after a delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The parsers make every attempt to “do the right thing” and not be fragile.
    Type inference is a pretty big deal. If a column can be coerced to integer dtype
    without altering the contents, the parser will do so. Any non-numeric columns
    will come through as object dtype as with the rest of pandas objects.  ### Quoting
    and Escape Characters'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quotes (and other escape characters) in embedded fields can be handled in any
    number of ways. One way is to use backslashes; to properly parse this data, you
    should pass the `escapechar` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]  ### Files with fixed width columns'
  prefs: []
  type: TYPE_NORMAL
- en: 'While [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") reads delimited data, the [`read_fwf()`](../reference/api/pandas.read_fwf.html#pandas.read_fwf
    "pandas.read_fwf") function works with data files that have known and fixed column
    widths. The function parameters to `read_fwf` are largely the same as `read_csv`
    with two extra parameters, and a different usage of the `delimiter` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`colspecs`: A list of pairs (tuples) giving the extents of the fixed-width
    fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’
    can be used to instruct the parser to try detecting the column specifications
    from the first 100 rows of the data. Default behavior, if not specified, is to
    infer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`widths`: A list of field widths which can be used instead of ‘colspecs’ if
    the intervals are contiguous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delimiter`: Characters to consider as filler characters in the fixed-width
    file. Can be used to specify the filler character of the fields if it is not spaces
    (e.g., ‘~’).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider a typical fixed-width data file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to parse this file into a `DataFrame`, we simply need to supply the
    column specifications to the `read_fwf` function along with the file name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how the parser automatically picks column names X.<column number> when
    `header=None` argument is specified. Alternatively, you can supply just the column
    widths for contiguous columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The parser will take care of extra white spaces around the columns so it’s ok
    to have extra separation between the columns in the file.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `read_fwf` will try to infer the file’s `colspecs` by using the
    first 100 rows of the file. It can do it only in cases when the columns are aligned
    and correctly separated by the provided `delimiter` (default delimiter is whitespace).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '`read_fwf` supports the `dtype` parameter for specifying the types of parsed
    columns to be different from the inferred type.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Files with an “implicit” index column
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a file with one less entry in the header than the number of data column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'In this special case, `read_csv` assumes that the first column is to be used
    as the index of the `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the dates weren’t automatically parsed. In that case you would need
    to do as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Reading an index with a `MultiIndex`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you have data indexed by two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The `index_col` argument to `read_csv` can take a list of column numbers to
    turn multiple columns into a `MultiIndex` for the index of the returned object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '#### Reading columns with a `MultiIndex`'
  prefs: []
  type: TYPE_NORMAL
- en: By specifying list of row locations for the `header` argument, you can read
    in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the
    intervening rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '`read_csv` is also able to interpret a more common format of multi-columns
    indices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If an `index_col` is not specified (e.g. you don’t have an index, or wrote
    it with `df.to_csv(..., index=False)`, then any `names` on the columns index will
    be *lost*.  ### Automatically “sniffing” the delimiter'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_csv` is capable of inferring delimited (not necessarily comma-separated)
    files, as pandas uses the [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)") class of the csv module. For this, you have to specify `sep=None`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]  ### Reading multiple files to create a single DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s best to use [`concat()`](../reference/api/pandas.concat.html#pandas.concat
    "pandas.concat") to combine multiple files. See the [cookbook](cookbook.html#cookbook-csv-multiple-files)
    for an example.  ### Iterating through files chunk by chunk'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you wish to iterate through a (potentially very large) file lazily
    rather than reading the entire file into memory, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying a `chunksize` to `read_csv`, the return value will be an iterable
    object of type `TextFileReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Changed in version 1.2: `read_csv/json/sas` return a context-manager when iterating
    through a file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifying `iterator=True` will also return the `TextFileReader` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Specifying the parser engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pandas currently supports three engines, the C engine, the python engine, and
    an experimental pyarrow engine (requires the `pyarrow` package). In general, the
    pyarrow engine is fastest on larger workloads and is equivalent in speed to the
    C engine on most other workloads. The python engine tends to be slower than the
    pyarrow and C engines on most workloads. However, the pyarrow engine is much less
    robust than the C engine, which lacks a few features compared to the Python engine.
  prefs: []
  type: TYPE_NORMAL
- en: Where possible, pandas uses the C parser (specified as `engine='c'`), but it
    may fall back to Python if C-unsupported options are specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, options unsupported by the C and pyarrow engines include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sep` other than a single character (e.g. regex separators)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipfooter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep=None` with `delim_whitespace=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying any of the above options will produce a `ParserWarning` unless the
    python engine is selected explicitly using `engine='python'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Options that are unsupported by the pyarrow engine which are not covered by
    the list above include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`float_precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nrows`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thousands`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_map`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dialect`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`on_bad_lines`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delim_whitespace`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`converters`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decimal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iterator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dayfirst`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`infer_datetime_format`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipinitialspace`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_memory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying these options with `engine='pyarrow'` will raise a `ValueError`.
  prefs: []
  type: TYPE_NORMAL
- en: '### Reading/writing remote files'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pass in a URL to read or write remote files to many of pandas’ IO functions
    - the following example shows reading a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'A custom header can be sent alongside HTTP(s) requests by passing a dictionary
    of header key value mappings to the `storage_options` keyword argument as shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'All URLs which are not local files or HTTP(s) are handled by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/),
    if installed, and its various filesystem implementations (including Amazon S3,
    Google Cloud, SSH, FTP, webHDFS…). Some of these implementations will require
    additional packages to be installed, for example S3 URLs require the [s3fs](https://pypi.org/project/s3fs/)
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: When dealing with remote storage systems, you might need extra configuration
    with environment variables or config files in special locations. For example,
    to access data in your S3 bucket, you will need to define credentials in one of
    the several ways listed in the [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/#credentials).
    The same is true for several of the storage backends, and you should follow the
    links at [fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations)
    for implementations built into `fsspec` and [fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)
    for those not included in the main `fsspec` distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also pass parameters directly to the backend driver. Since `fsspec`
    does not utilize the `AWS_S3_HOST` environment variable, we can directly define
    a dictionary containing the endpoint_url and pass the object into the storage
    option parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: More sample configurations and documentation can be found at [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage).
  prefs: []
  type: TYPE_NORMAL
- en: If you do *not* have S3 credentials, you can still access public data by specifying
    an anonymous connection, such as
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '`fsspec` also allows complex URLs, for accessing data in compressed archives,
    local caching of files, and more. To locally cache the above example, you would
    modify the call to'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: where we specify that the “anon” parameter is meant for the “s3” part of the
    implementation, not to the caching implementation. Note that this caches to a
    temporary directory for the duration of the session only, but you can also specify
    a permanent store.
  prefs: []
  type: TYPE_NORMAL
- en: Writing out data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Writing to CSV format'
  prefs: []
  type: TYPE_NORMAL
- en: The `Series` and `DataFrame` objects have an instance method `to_csv` which
    allows storing the contents of the object as a comma-separated-values file. The
    function takes a number of arguments. Only the first is required.
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf`: A string path to the file to write or a file object. If a file
    object it must be opened with `newline=''''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep` : Field delimiter for the output file (default “,”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep`: A string representation of a missing value (default ‘’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format`: Format string for floating point numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: Columns to write (default None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header`: Whether to write out the column names (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index`: whether to write row (index) names (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_label`: Column label(s) for index column(s) if desired. If None (default),
    and `header` and `index` are True, then the index names are used. (A sequence
    should be given if the `DataFrame` uses MultiIndex).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : Python write mode, default ‘w’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding`: a string representing the encoding to use if the contents are non-ASCII,
    for Python versions prior to 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`: Character sequence denoting line end (default `os.linesep`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).
    Note that if you have set a `float_format` then floats are converted to strings
    and csv.QUOTE_NONNUMERIC will treat them as non-numeric'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quotechar`: Character used to quote fields (default ‘”’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doublequote`: Control quoting of `quotechar` in fields (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`escapechar`: Character used to escape `sep` and `quotechar` when appropriate
    (default None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`: Number of rows to write at a time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_format`: Format string for datetime objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a formatted string
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `DataFrame` object has an instance method `to_string` which allows control
    over the string representation of the object. All arguments are optional:'
  prefs: []
  type: TYPE_NORMAL
- en: '`buf` default None, for example a StringIO object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` default None, which columns to write'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col_space` default None, minimum width of each column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep` default `NaN`, representation of NA value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formatters` default None, a dictionary (by column) of functions each of which
    takes a single argument and returns a formatted string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format` default None, a function which takes a single (float) argument
    and returns a formatted string; to be applied to floats in the `DataFrame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparsify` default True, set to False for a `DataFrame` with a hierarchical
    index to print every MultiIndex key at each row.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_names` default True, will print the names of the indices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` default True, will print the index (ie, row labels)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header` default True, will print the column labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`justify` default `left`, will print column headers left- or right-justified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Series` object also has a `to_string` method, but with only the `buf`,
    `na_rep`, `float_format` arguments. There is also a `length` argument which, if
    set to `True`, will additionally output the length of the Series.  ## JSON'
  prefs: []
  type: TYPE_NORMAL
- en: Read and write `JSON` format files and strings.
  prefs: []
  type: TYPE_NORMAL
- en: '### Writing JSON'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Series` or `DataFrame` can be converted to a valid JSON string. Use `to_json`
    with optional parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` : the pathname or buffer to write the output. This can be `None`
    in which case a JSON string is returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Series`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`date_format` : string, type of date conversion, ‘epoch’ for timestamp, ‘iso’
    for ISO8601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`double_precision` : The number of decimal places to use when encoding floating
    point values, default 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_ascii` : force encoded string to be ASCII, default True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : The time unit to encode to, governs timestamp and ISO8601 precision.
    One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds
    respectively. Default ‘ms’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_handler` : The handler to call if an object cannot otherwise be converted
    to a suitable format for JSON. Takes a single argument, which is the object to
    convert, and returns a serializable object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : If `records` orient, then will write each record per line as json.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : string, writer mode when writing to path. ‘w’ for write, ‘a’ for append.
    Default ‘w’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note `NaN`’s, `NaT`’s and `None` will be converted to `null` and `datetime`
    objects will be converted based on the `date_format` and `date_unit` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Orient options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a number of different options for the format of the resulting JSON
    file / string. Consider the following `DataFrame` and `Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '**Column oriented** (the default for `DataFrame`) serializes the data as nested
    JSON objects with column labels acting as the primary index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '**Index oriented** (the default for `Series`) similar to column oriented but
    the index labels are now primary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '**Record oriented** serializes the data to a JSON array of column -> value
    records, index labels are not included. This is useful for passing `DataFrame`
    data to plotting libraries, for example the JavaScript library `d3.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '**Value oriented** is a bare-bones option which serializes to nested JSON arrays
    of values only, column and index labels are not included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '**Split oriented** serializes to a JSON object containing separate entries
    for values, index and columns. Name is also included for `Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/),
    allowing for the preservation of metadata including but not limited to dtypes
    and index names.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Any orient option that encodes to a JSON object will not preserve the ordering
    of index and column labels during round-trip serialization. If you wish to preserve
    label ordering use the `split` option as it uses ordered containers.
  prefs: []
  type: TYPE_NORMAL
- en: Date handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Writing in ISO date format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing in ISO date format, with microseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Epoch timestamps, in seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing to a file, with a date index and a date column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Fallback behavior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the JSON serializer cannot handle the container contents directly it will
    fall back in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,
    if provided, will be called for each value, otherwise an exception is raised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'if an object is unsupported it will attempt the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check if the object has defined a `toDict` method and call it. A `toDict` method
    should return a `dict` which will then be JSON serialized.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: invoke the `default_handler` if one was provided.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: convert the object to a `dict` by traversing its contents. However this will
    often fail with an `OverflowError` or give unexpected results.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general the best approach for unsupported objects or dtypes is to provide
    a `default_handler`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'can be dealt with by specifying a simple `default_handler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]  ### Reading JSON'
  prefs: []
  type: TYPE_NORMAL
- en: Reading a JSON string to pandas object can take a number of parameters. The
    parser will try to parse a `DataFrame` if `typ` is not supplied or is `None`.
    To explicitly force `Series` parsing, pass `typ=series`
  prefs: []
  type: TYPE_NORMAL
- en: '`filepath_or_buffer` : a **VALID** JSON string or file handle / StringIO. The
    string could be a URL. Valid URL schemes include http, ftp, S3, and file. For
    file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typ` : type of object to recover (series or frame), default ‘frame’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Series :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`dtype` : if True, infer dtypes, if a dict of column to dtype, then use those,
    if `False`, then don’t infer dtypes at all, default is True, apply only to the
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_axes` : boolean, try to convert the axes to the proper dtypes, default
    is `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_dates` : a list of columns to parse for dates; If `True`, then try
    to parse date-like columns, default is `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_default_dates` : boolean, default `True`. If parsing dates, then parse
    the default date-like columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`precise_float` : boolean, default `False`. Set to enable usage of higher precision
    (strtod) function when decoding string to double values. Default (`False`) is
    to use fast but less precise builtin functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : string, the timestamp unit to detect if converting dates. Default
    None. By default the timestamp precision will be detected, if this is not desired
    then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force timestamp precision to seconds,
    milliseconds, microseconds or nanoseconds respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : reads file as one json object per line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding` : The encoding to use to decode py3 bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize` : when used in combination with `lines=True`, return a `pandas.api.typing.JsonReader`
    which reads in `chunksize` lines per iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`engine`: Either `"ujson"`, the built-in JSON parser, or `"pyarrow"` which
    dispatches to pyarrow’s `pyarrow.json.read_json`. The `"pyarrow"` is only available
    when `lines=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parser will raise one of `ValueError/TypeError/AssertionError` if the JSON
    is not parseable.
  prefs: []
  type: TYPE_NORMAL
- en: If a non-default `orient` was used when encoding to JSON be sure to pass the
    same option here so that decoding produces sensible results, see [Orient Options](#orient-options)
    for an overview.
  prefs: []
  type: TYPE_NORMAL
- en: Data conversion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will
    try to parse the axes, and all of the data into appropriate types, including dates.
    If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes`
    should only be set to `False` if you need to preserve string-like numbers (e.g.
    ‘1’, ‘2’) in an axes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Large integer values may be converted to dates if `convert_dates=True` and
    the data and / or column labels appear ‘date-like’. The exact threshold depends
    on the `date_unit` specified. ‘date-like’ means that the column label meets one
    of the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: it ends with `'_at'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it ends with `'_time'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it begins with `'timestamp'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'modified'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'date'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'When reading JSON data, automatic coercing into dtypes has some quirks:'
  prefs: []
  type: TYPE_NORMAL
- en: an index can be reconstructed in a different order from serialization, that
    is, the returned order is not guaranteed to be the same as before serialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a column that was `float` data will be converted to `integer` if it can be done
    safely, e.g. a column of `1.`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bool columns will be converted to `integer` on reconstruction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus there are times where you may want to specify specific dtypes via the `dtype`
    keyword argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading from a JSON string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Reading from a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t convert any data (but still convert axes and dates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify dtypes for conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Preserve string indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Dates written in nanoseconds need to be read back in nanoseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]  ### Normalization'
  prefs: []
  type: TYPE_NORMAL
- en: pandas provides a utility function to take a dict or list of dicts and *normalize*
    this semi-structured data into a flat table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: The max_level parameter provides more control over which level to end normalization.
    With max_level=1 the following snippet normalizes until 1st nesting level of the
    provided dict.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]  ### Line delimited json'
  prefs: []
  type: TYPE_NORMAL
- en: pandas is able to read and write line-delimited json files that are common in
    data processing pipelines using Hadoop or Spark.
  prefs: []
  type: TYPE_NORMAL
- en: For line-delimited json files, pandas can also return an iterator which reads
    in `chunksize` lines at a time. This can be useful for large files or to read
    from a stream.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Line-limited json can also be read using the pyarrow reader by specifying `engine="pyarrow"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'New in version 2.0.0.  ### Table schema'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table Schema](https://specs.frictionlessdata.io/table-schema/) is a spec for
    describing tabular datasets as a JSON object. The JSON includes information on
    the field names, types, and other attributes. You can use the orient `table` to
    build a JSON string with two fields, `schema` and `data`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: The `schema` field contains the `fields` key, which itself contains a list of
    column name to type pairs, including the `Index` or `MultiIndex` (see below for
    a list of types). The `schema` field also contains a `primaryKey` field if the
    (Multi)index is unique.
  prefs: []
  type: TYPE_NORMAL
- en: The second field, `data`, contains the serialized data with the `records` orient.
    The index is included, and any datetimes are ISO 8601 formatted, as required by
    the Table Schema spec.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of types supported are described in the Table Schema spec. This
    table shows the mapping from pandas types:'
  prefs: []
  type: TYPE_NORMAL
- en: '| pandas type | Table Schema type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| int64 | integer |'
  prefs: []
  type: TYPE_TB
- en: '| float64 | number |'
  prefs: []
  type: TYPE_TB
- en: '| bool | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | datetime |'
  prefs: []
  type: TYPE_TB
- en: '| timedelta64[ns] | duration |'
  prefs: []
  type: TYPE_TB
- en: '| categorical | any |'
  prefs: []
  type: TYPE_TB
- en: '| object | str |'
  prefs: []
  type: TYPE_TB
- en: 'A few notes on the generated table schema:'
  prefs: []
  type: TYPE_NORMAL
- en: The `schema` object contains a `pandas_version` field. This contains the version
    of pandas’ dialect of the schema, and will be incremented with each revision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All dates are converted to UTC when serializing. Even timezone naive values,
    which are treated as UTC with an offset of 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: datetimes with a timezone (before serializing), include an additional field
    `tz` with the time zone name (e.g. `'US/Central'`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Periods are converted to timestamps before serialization, and so have the same
    behavior of being converted to UTC. In addition, periods will contain and additional
    field `freq` with the period’s frequency, e.g. `'A-DEC'`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Categoricals use the `any` type and an `enum` constraint listing the set of
    possible values. Additionally, an `ordered` field is included:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A `primaryKey` field, containing an array of labels, is included *if the index
    is unique*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `primaryKey` behavior is the same with MultiIndexes, but in this case the
    `primaryKey` is an array:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The default naming roughly follows these rules:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For series, the `object.name` is used. If that’s none, then the name is `values`
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `DataFrames`, the stringified version of the column name is used
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `Index` (not `MultiIndex`), `index.name` is used, with a fallback to `index`
    if that is None.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `MultiIndex`, `mi.names` is used. If any level has no name, then `level_<i>`
    is used.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_json` also accepts `orient=''table''` as an argument. This allows for
    the preservation of metadata such as dtypes and index names in a round-trippable
    manner.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the literal string ‘index’ as the name of an [`Index`](../reference/api/pandas.Index.html#pandas.Index
    "pandas.Index") is not round-trippable, nor are any names beginning with `'level_'`
    within a [`MultiIndex`](../reference/api/pandas.MultiIndex.html#pandas.MultiIndex
    "pandas.MultiIndex"). These are used by default in [`DataFrame.to_json()`](../reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json
    "pandas.DataFrame.to_json") to indicate missing values and the subsequent read
    cannot distinguish the intent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: When using `orient='table'` along with user-defined `ExtensionArray`, the generated
    schema will contain an additional `extDtype` key in the respective `fields` element.
    This extra key is not standard but does enable JSON roundtrips for extension types
    (e.g. `read_json(df.to_json(orient="table"), orient="table")`).
  prefs: []
  type: TYPE_NORMAL
- en: The `extDtype` key carries the name of the extension, if you have properly registered
    the `ExtensionDtype`, pandas will use said name to perform a lookup into the registry
    and re-convert the serialized data into your custom dtype.
  prefs: []
  type: TYPE_NORMAL
- en: HTML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### Reading HTML content'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: We **highly encourage** you to read the [HTML Table Parsing gotchas](#io-html-gotchas)
    below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_html()` function can accept an HTML string/file/URL and
    will parse HTML tables into list of pandas `DataFrames`. Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`read_html` returns a `list` of `DataFrame` objects, even if there is only
    a single table contained in the HTML content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL with no options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The data from the above URL changes every Monday so the resulting data above
    may be slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL while passing headers alongside the HTTP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We see above that the headers we passed are reflected in the HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read in the content of the file from the above URL and pass it to `read_html`
    as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'You can even pass in an instance of `StringIO` if you so desire:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The following examples are not run by the IPython evaluator due to the fact
    that having so many network-accessing functions slows down the documentation build.
    If you spot an error or an example that doesn’t run, please do not hesitate to
    report it over on [pandas GitHub issues page](https://github.com/pandas-dev/pandas/issues).
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL and match a table that contains specific text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Specify a header row (by default `<th>` or `<td>` elements located within a
    `<thead>` are used to form the column index, if multiple rows are contained within
    `<thead>` then a MultiIndex is created); if specified, the header row is taken
    from the data minus the parsed header elements (`<th>` elements).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify an index column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a number of rows to skip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a number of rows to skip using a list (`range` works as well):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify an HTML attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify values that should be converted to NaN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify whether to keep the default set of NaN values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: Specify converters for columns. This is useful for numerical text data that
    has leading zeros. By default columns that are numerical are cast to numeric types
    and the leading zeros are lost. To avoid this, we can convert these columns to
    strings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'Use some combination of the above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in pandas `to_html` output (with some loss of floating point precision):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lxml` backend will raise an error on a failed parse if that is the only
    parser you provide. If you only have a single parser you can provide just a string,
    but it is considered good practice to pass a list with one string if, for example,
    the function expects a sequence of strings. You may use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you could pass `flavor=''lxml''` without a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: However, if you have bs4 and html5lib installed and pass `None` or `['lxml',
    'bs4']` then the parse will most likely succeed. Note that *as soon as a parse
    succeeds, the function will return*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Links can be extracted from cells along with the text using `extract_links="all"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'New in version 1.5.0.  ### Writing to HTML files'
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_html` which renders the contents
    of the `DataFrame` as an HTML table. The function arguments are as in the method
    `to_string` described above.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Not all of the possible options for `DataFrame.to_html` are shown here for brevity’s
    sake. See `DataFrame.to_html()` for the full set of options.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In an HTML-rendering supported environment like a Jupyter Notebook, `display(HTML(...))``
    will render the raw HTML into the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'The `columns` argument will limit the columns shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '`float_format` takes a Python callable to control the precision of floating
    point values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '`bold_rows` will make the row labels bold by default, but you can turn that
    off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: The `classes` argument provides the ability to give the resulting HTML table
    CSS classes. Note that these classes are *appended* to the existing `'dataframe'`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: The `render_links` argument provides the ability to add hyperlinks to cells
    that contain URLs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the `escape` argument allows you to control whether the “<”, “>” and
    “&” characters escaped in the resulting HTML (by default it is `True`). So to
    get the HTML without escaped characters pass `escape=False`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'Escaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: 'Not escaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Some browsers may not show a difference in the rendering of the previous two
    HTML tables.  ### HTML Table Parsing Gotchas'
  prefs: []
  type: TYPE_NORMAL
- en: There are some versioning issues surrounding the libraries that are used to
    parse HTML tables in the top-level pandas io function `read_html`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Issues with** [**lxml**](https://lxml.de)'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) is very fast.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) requires Cython to install correctly.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) does *not* make any guarantees about the results
    of its parse *unless* it is given [**strictly valid markup**](https://validator.w3.org/docs/help.html#validation_basics).'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: In light of the above, we have chosen to allow you, the user, to use the [**lxml**](https://lxml.de)
    backend, but **this backend will use** [**html5lib**](https://github.com/html5lib/html5lib-python)
    if [**lxml**](https://lxml.de) fails to parse
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: It is therefore *highly recommended* that you install both [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    and [**html5lib**](https://github.com/html5lib/html5lib-python), so that you will
    still get a valid result (provided everything else is valid) even if [**lxml**](https://lxml.de)
    fails.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**lxml**](https://lxml.de) **as a backend**'
  prefs: []
  type: TYPE_NORMAL
- en: The above issues hold here as well since [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    is essentially just a wrapper around a parser backend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**html5lib**](https://github.com/html5lib/html5lib-python) **as a backend**'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is far more lenient
    than [**lxml**](https://lxml.de) and consequently deals with *real-life markup*
    in a much saner way rather than just, e.g., dropping an element without notifying
    you.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) *generates valid
    HTML5 markup from invalid markup automatically*. This is extremely important for
    parsing HTML tables, since it guarantees a valid document. However, that does
    NOT mean that it is “correct”, since the process of fixing markup does not have
    a single definition.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is pure Python
    and requires no additional build steps beyond its own installation.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The biggest drawback to using [**html5lib**](https://github.com/html5lib/html5lib-python)
    is that it is slow as molasses. However consider the fact that many tables on
    the web are not big enough for the parsing algorithm runtime to matter. It is
    more likely that the bottleneck will be in the process of reading the raw text
    from the URL over the web, i.e., IO (input-output). For very large tables, this
    might not be true.  ## LaTeX'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: Currently there are no methods to read from LaTeX, only output methods.
  prefs: []
  type: TYPE_NORMAL
- en: Writing to LaTeX files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame *and* Styler objects currently have a `to_latex` method. We recommend
    using the [Styler.to_latex()](../reference/api/pandas.io.formats.style.Styler.to_latex.html)
    method over [DataFrame.to_latex()](../reference/api/pandas.DataFrame.to_latex.html)
    due to the former’s greater flexibility with conditional styling, and the latter’s
    possible future deprecation.
  prefs: []
  type: TYPE_NORMAL
- en: Review the documentation for [Styler.to_latex](../reference/api/pandas.io.formats.style.Styler.to_latex.html),
    which gives examples of conditional styling and explains the operation of its
    keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: For simple application the following pattern is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: To format values before output, chain the [Styler.format](../reference/api/pandas.io.formats.style.Styler.format.html)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: XML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### Reading XML'
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_xml()` function can accept an XML string/file/URL and will
    parse nodes and attributes into a pandas `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since there is no standard XML structure where design types can vary in many
    ways, `read_xml` works best with flatter, shallow versions. If an XML document
    is deeply nested, use the `stylesheet` feature to transform XML into a flatter
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read an XML string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'Read a URL with no options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the content of the “books.xml” file and pass it to `read_xml` as a
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the content of the “books.xml” as instance of `StringIO` or `BytesIO`
    and pass it to `read_xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'Even read XML from AWS S3 buckets such as NIH NCBI PMC Article Datasets providing
    Biomedical and Life Science Jorurnals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: 'With [lxml](https://lxml.de) as default `parser`, you access the full-featured
    XML library that extends Python’s ElementTree API. One powerful tool is ability
    to query nodes selectively or conditionally with more expressive XPath:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify only elements or only attributes to parse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: XML documents can have namespaces with prefixes and default namespaces without
    prefixes both of which are denoted with a special attribute `xmlns`. In order
    to parse by node under a namespace context, `xpath` must reference a prefix.
  prefs: []
  type: TYPE_NORMAL
- en: For example, below XML contains a namespace with prefix, `doc`, and URI at `https://example.com`.
    In order to parse `doc:row` nodes, `namespaces` must be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, an XML document can have a default namespace without prefix. Failing
    to assign a temporary prefix will return no nodes and raise a `ValueError`. But
    assigning *any* temporary name to correct URI allows parsing by nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: However, if XPath does not reference node names such as default, `/*`, then
    `namespaces` is not required.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since `xpath` identifies the parent of content to be parsed, only immediate
    desendants which include child nodes or current attributes are parsed. Therefore,
    `read_xml` will not parse the text of grandchildren or other descendants and will
    not parse attributes of any descendant. To retrieve lower level content, adjust
    xpath to lower level. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: shows the attribute `sides` on `shape` element was not parsed as expected since
    this attribute resides on the child of `row` element and not `row` element itself.
    In other words, `sides` attribute is a grandchild level descendant of `row` element.
    However, the `xpath` targets `row` element which covers only its children and
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: With [lxml](https://lxml.de) as parser, you can flatten nested XML documents
    with an XSLT script which also can be string/file/URL types. As background, [XSLT](https://www.w3.org/TR/xslt/)
    is a special-purpose language written in a special XML file that can transform
    original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using
    an XSLT processor.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider this somewhat nested structure of Chicago “L” Rides where
    station and rides elements encapsulate data in their own sections. With below
    XSLT, `lxml` can transform original nested document into a flatter output (as
    shown below for demonstration) for easier parse into `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: For very large XML files that can range in hundreds of megabytes to gigabytes,
    [`pandas.read_xml()`](../reference/api/pandas.read_xml.html#pandas.read_xml "pandas.read_xml")
    supports parsing such sizeable files using [lxml’s iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk)
    and [etree’s iterparse](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse)
    which are memory-efficient methods to iterate through an XML tree and extract
    specific elements and attributes. without holding entire tree in memory.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.5.0.
  prefs: []
  type: TYPE_NORMAL
- en: To use this feature, you must pass a physical XML file path into `read_xml`
    and use the `iterparse` argument. Files should not be compressed or point to online
    sources but stored on local disk. Also, `iterparse` should be a dictionary where
    the key is the repeating nodes in document (which become the rows) and the value
    is a list of any element or attribute that is a descendant (i.e., child, grandchild)
    of repeating node. Since XPath is not used in this method, descendants do not
    need to share same relationship with one another. Below shows example of reading
    in Wikipedia’s very large (12 GB+) latest article data dump.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]  ### Writing XML'
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_xml` which renders the contents
    of the `DataFrame` as an XML document.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This method does not support special properties of XML including DTD, CData,
    XSD schemas, processing instructions, comments, and others. Only namespaces at
    the root level is supported. However, `stylesheet` allows design changes after
    initial output.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write an XML without options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with new root and row name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an attribute-centric XML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a mix of elements and attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: 'Any `DataFrames` with hierarchical columns will be flattened for XML element
    names with levels delimited by underscores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with default namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with namespace prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML without declaration or pretty print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML and transform with stylesheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: XML Final Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All XML documents adhere to [W3C specifications](https://www.w3.org/TR/xml/).
    Both `etree` and `lxml` parsers will fail to parse any markup document that is
    not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document
    unless it follows XHTML specs. However, other popular markup types including KML,
    XAML, RSS, MusicML, MathML are compliant [XML schemas](https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For above reason, if your application builds XML prior to pandas operations,
    use appropriate DOM libraries like `etree` and `lxml` to build the necessary document
    and not by string concatenation or regex adjustments. Always remember XML is a
    *special* text file with markup rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With very large XML files (several hundred MBs to GBs), XPath and XSLT can become
    memory-intensive operations. Be sure to have enough available RAM for reading
    and writing to large XML files (roughly about 5 times the size of text).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because XSLT is a programming language, use it with caution since such scripts
    can pose a security risk in your environment and can run large or infinite recursive
    operations. Always test scripts on small fragments before full run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [etree](https://docs.python.org/3/library/xml.etree.elementtree.html) parser
    supports all functionality of both `read_xml` and `to_xml` except for complex
    XPath and any XSLT. Though limited in features, `etree` is still a reliable and
    capable parser and tree builder. Its performance may trail `lxml` to a certain
    degree for larger files but relatively unnoticeable on small to medium size files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '## Excel files'
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read Excel 2007+ (`.xlsx`) files using the `openpyxl`
    Python module. Excel 2003 (`.xls`) files can be read using `xlrd`. Binary Excel
    (`.xlsb`) files can be read using `pyxlsb`. All formats can be read using [calamine](#io-calamine)
    engine. The [`to_excel()`](../reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel
    "pandas.DataFrame.to_excel") instance method is used for saving a `DataFrame`
    to Excel. Generally the semantics are similar to working with [csv](#io-read-csv-table)
    data. See the [cookbook](cookbook.html#cookbook-excel) for some advanced strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When `engine=None`, the following logic will be used to determine the engine:'
  prefs: []
  type: TYPE_NORMAL
- en: If `path_or_buffer` is an OpenDocument format (.odf, .ods, .odt), then [odf](https://pypi.org/project/odfpy/)
    will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise if `path_or_buffer` is an xls format, `xlrd` will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise if `path_or_buffer` is in xlsb format, `pyxlsb` will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise `openpyxl` will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### Reading Excel files'
  prefs: []
  type: TYPE_NORMAL
- en: In the most basic use-case, `read_excel` takes a path to an Excel file, and
    the `sheet_name` indicating which sheet to parse.
  prefs: []
  type: TYPE_NORMAL
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.load_workbook()` to read
    in (`.xlsx`) and (`.xlsm`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlrd, pandas is using `xlrd.open_workbook()` to read in (`.xls`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine pyxlsb, pandas is using `pyxlsb.open_workbook()` to read in (`.xlsb`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.load()` to read in (`.ods`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine calamine, pandas is using `python_calamine.load_workbook()` to
    read in (`.xlsx`), (`.xlsm`), (`.xls`), (`.xlsb`), (`.ods`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '#### `ExcelFile` class'
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate working with multiple sheets from the same file, the `ExcelFile`
    class can be used to wrap the file and can be passed into `read_excel` There will
    be a performance benefit for reading multiple sheets as the file is read into
    memory only once.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: The `ExcelFile` class can also be used as a context manager.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: The `sheet_names` property will generate a list of the sheet names in the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary use-case for an `ExcelFile` is parsing multiple sheets with different
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: Note that if the same parsing parameters are used for all sheets, a list of
    sheet names can simply be passed to `read_excel` with no loss in performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.
    This allows the user to control how the excel file is read. For example, sheets
    can be loaded on demand by calling `xlrd.open_workbook()` with `on_demand=True`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]  #### Specifying sheets'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: An ExcelFile’s attribute `sheet_names` provides access to a list of sheets.
  prefs: []
  type: TYPE_NORMAL
- en: The arguments `sheet_name` allows specifying the sheet or sheets to read.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default value for `sheet_name` is 0, indicating to read the first sheet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a string to refer to the name of a particular sheet in the workbook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass an integer to refer to the index of a sheet. Indices follow Python convention,
    beginning at 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a list of either strings or integers, to return a dictionary of specified
    sheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a `None` to return a dictionary of all available sheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the sheet index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: 'Using all default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: 'Using None to get all sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a list to get multiple sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '`read_excel` can read more than one sheet, by setting `sheet_name` to either
    a list of sheet names, a list of sheet positions, or `None` to read all sheets.
    Sheets can be specified by sheet index or sheet name, using an integer or string,
    respectively.  #### Reading a `MultiIndex`'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_excel` can read a `MultiIndex` index, by passing a list of columns to
    `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If
    either the `index` or `columns` have serialized level names those will be read
    in as well by specifying the rows/columns that make up the levels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to read in a `MultiIndex` index without names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: If the index has level names, they will parsed as well, using the same parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: 'If the source file has both `MultiIndex` index and columns, lists specifying
    each should be passed to `index_col` and `header`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: Missing values in columns specified in `index_col` will be forward filled to
    allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward
    filling the missing values use `set_index` after reading the data instead of `index_col`.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing specific columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is often the case that users will insert columns to do temporary computations
    in Excel and you may not want to read in those columns. `read_excel` takes a `usecols`
    keyword to allow you to specify a subset of columns to parse.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a comma-delimited set of Excel columns and ranges as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: If `usecols` is a list of integers, then it is assumed to be the file column
    indices to be parsed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `usecols` is a list of strings, it is assumed that each string corresponds
    to a column name provided either by the user in `names` or inferred from the document
    header row(s). Those strings define which columns will be parsed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: Element order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',
    'baz']`.
  prefs: []
  type: TYPE_NORMAL
- en: If `usecols` is callable, the callable function will be evaluated against the
    column names, returning names where the callable function evaluates to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: Parsing dates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Datetime-like values are normally automatically converted to the appropriate
    dtype when reading the excel file. But if you have a column of strings that *look*
    like dates (but are not actually formatted as dates in excel), you can use the
    `parse_dates` keyword to parse those strings to datetimes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: Cell converters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is possible to transform the contents of Excel cells via the `converters`
    option. For instance, to convert a column to boolean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: 'This options handles missing values and treats exceptions in the converters
    as missing data. Transformations are applied cell by cell rather than to the column
    as a whole, so the array dtype is not guaranteed. For instance, a column of integers
    with missing values cannot be transformed to an array with integer dtype, because
    NaN is strictly a float. You can manually mask missing data to recover integer
    dtype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: Dtype specifications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to converters, the type for an entire column can be specified
    using the `dtype` keyword, which takes a dictionary mapping column names to types.
    To interpret data with no type inference, use the type `str` or `object`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]  ### Writing Excel files'
  prefs: []
  type: TYPE_NORMAL
- en: Writing Excel files to disk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To write a `DataFrame` object to a sheet of an Excel file, you can use the
    `to_excel` instance method. The arguments are largely the same as `to_csv` described
    above, the first argument being the name of the excel file, and the optional second
    argument the name of the sheet to which the `DataFrame` should be written. For
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: Files with a `.xlsx` extension will be written using `xlsxwriter` (if available)
    or `openpyxl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataFrame` will be written in a way that tries to mimic the REPL output.
    The `index_label` will be placed in the second row instead of the first. You can
    place it in the first row by setting the `merge_cells` option in `to_excel()`
    to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: In order to write separate `DataFrames` to separate sheets in a single Excel
    file, one can pass an `ExcelWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.Workbook()` to create a new
    sheet and `openpyxl.load_workbook()` to append data to an existing sheet. The
    openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlsxwriter, pandas is using `xlsxwriter.Workbook()` to write
    to (`.xlsx`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.OpenDocumentSpreadsheet()`
    to write to (`.ods`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Excel files to memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: pandas supports writing Excel files to buffer-like objects such as `StringIO`
    or `BytesIO` using `ExcelWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`engine` is optional but recommended. Setting the engine determines the version
    of workbook produced. Setting `engine=''xlrd''` will produce an Excel 2003-format
    workbook (xls). Using either `''openpyxl''` or `''xlsxwriter''` will produce an
    Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook
    is produced.  ### Excel writer engines'
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas chooses an Excel writer via two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: the `engine` keyword argument
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the filename extension (via the default specified in config options)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, pandas uses the [XlsxWriter](https://xlsxwriter.readthedocs.io)
    for `.xlsx`, [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsm`. If you
    have multiple engines installed, you can set the default engine through [setting
    the config options](options.html#options) `io.excel.xlsx.writer` and `io.excel.xls.writer`.
    pandas will fall back on [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsx`
    files if [Xlsxwriter](https://xlsxwriter.readthedocs.io) is not available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify which writer you want to use, you can pass an engine keyword argument
    to `to_excel` and to `ExcelWriter`. The built-in engines are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`openpyxl`: version 2.4 or higher is required'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xlsxwriter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE195]  ### Style and formatting'
  prefs: []
  type: TYPE_NORMAL
- en: The look and feel of Excel worksheets created from pandas can be modified using
    the following parameters on the `DataFrame`’s `to_excel` method.
  prefs: []
  type: TYPE_NORMAL
- en: '`float_format` : Format string for floating point numbers (default `None`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_panes` : A tuple of two integers representing the bottommost row and
    rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will
    freeze the first row and first column (default `None`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the [Xlsxwriter](https://xlsxwriter.readthedocs.io) engine provides many
    options for controlling the format of an Excel worksheet created with the `to_excel`
    method. Excellent examples can be found in the [Xlsxwriter](https://xlsxwriter.readthedocs.io)
    documentation here: [https://xlsxwriter.readthedocs.io/working_with_pandas.html](https://xlsxwriter.readthedocs.io/working_with_pandas.html)  ##
    OpenDocument Spreadsheets'
  prefs: []
  type: TYPE_NORMAL
- en: The io methods for [Excel files](#excel-files) also support reading and writing
    OpenDocument spreadsheets using the [odfpy](https://pypi.org/project/odfpy/) module.
    The semantics and features for reading and writing OpenDocument spreadsheets match
    what can be done for [Excel files](#excel-files) using `engine='odf'`. The optional
    dependency ‘odfpy’ needs to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read OpenDocument spreadsheets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, the `to_excel()` method can write OpenDocument spreadsheets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE197]  ## Binary Excel (.xlsb) files'
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can also read binary Excel files using the `pyxlsb`
    module. The semantics and features for reading binary Excel files mostly match
    what can be done for [Excel files](#excel-files) using `engine='pyxlsb'`. `pyxlsb`
    does not recognize datetime types in files and will return floats instead (you
    can use [calamine](#io-calamine) if you need recognize datetime types).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently pandas only supports *reading* binary Excel files. Writing is not
    implemented.  ## Calamine (Excel and ODS files)'
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read Excel file (`.xlsx`, `.xlsm`, `.xls`, `.xlsb`)
    and OpenDocument spreadsheets (`.ods`) using the `python-calamine` module. This
    module is a binding for Rust library [calamine](https://crates.io/crates/calamine)
    and is faster than other engines in most cases. The optional dependency ‘python-calamine’
    needs to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE199]  ## Clipboard'
  prefs: []
  type: TYPE_NORMAL
- en: 'A handy way to grab data is to use the `read_clipboard()` method, which takes
    the contents of the clipboard buffer and passes them to the `read_csv` method.
    For instance, you can copy the following text to the clipboard (CTRL-C on many
    operating systems):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: 'And then import the data directly to a `DataFrame` by calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: The `to_clipboard` method can be used to write the contents of a `DataFrame`
    to the clipboard. Following which you can paste the clipboard contents into other
    applications (CTRL-V on many operating systems). Here we illustrate writing a
    `DataFrame` into clipboard and reading it back.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we got the same content back, which we had earlier written to
    the clipboard.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux
    to use these methods.  ## Pickling'
  prefs: []
  type: TYPE_NORMAL
- en: All pandas objects are equipped with `to_pickle` methods which use Python’s
    `cPickle` module to save data structures to disk using the pickle format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: 'The `read_pickle` function in the `pandas` namespace can be used to load any
    pickled pandas object (or any other pickled object) from file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Loading pickled data received from untrusted sources can be unsafe.
  prefs: []
  type: TYPE_NORMAL
- en: 'See: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle") is only guaranteed backwards compatible back to a few minor
    release.'
  prefs: []
  type: TYPE_NORMAL
- en: '### Compressed pickle files'
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle"), [`DataFrame.to_pickle()`](../reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle
    "pandas.DataFrame.to_pickle") and [`Series.to_pickle()`](../reference/api/pandas.Series.to_pickle.html#pandas.Series.to_pickle
    "pandas.Series.to_pickle") can read and write compressed pickle files. The compression
    types of `gzip`, `bz2`, `xz`, `zstd` are supported for reading and writing. The
    `zip` file format only supports reading and must contain only one data file to
    be read.'
  prefs: []
  type: TYPE_NORMAL
- en: The compression type can be an explicit parameter or be inferred from the file
    extension. If ‘infer’, then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename
    ends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The compression parameter can also be a `dict` in order to pass options to the
    compression protocol. It must have a `'method'` key set to the name of the compression
    protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`, `'xz'`, `'zstd'`}.
    All other key-value pairs are passed to the underlying compression library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: 'Using an explicit compression type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: 'Inferring compression type from the extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: 'The default is to ‘infer’:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: 'Passing options to the compression protocol in order to speed up compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE209]  ## msgpack'
  prefs: []
  type: TYPE_NORMAL
- en: pandas support for `msgpack` has been removed in version 1.0.0\. It is recommended
    to use [pickle](#io-pickle) instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can also the Arrow IPC serialization format for on-the-wire
    transmission of pandas objects. For documentation on pyarrow, see [here](https://arrow.apache.org/docs/python/ipc.html).  ##
    HDF5 (PyTables)'
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is a dict-like object which reads and writes pandas using the high
    performance HDF5 format using the excellent [PyTables](https://www.pytables.org/)
    library. See the [cookbook](cookbook.html#cookbook-hdf) for some advanced strategies'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: pandas uses PyTables for reading and writing HDF5 files, which allows serializing
    object-dtype data with pickle. Loading pickled data received from untrusted sources
    can be unsafe.
  prefs: []
  type: TYPE_NORMAL
- en: 'See: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)
    for more.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: 'Objects can be written to the file just like adding key-value pairs to a dict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: 'In a current or later Python session, you can retrieve stored objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: 'Deletion of the object specified by the key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: 'Closing a Store and using a context manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: Read/write API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf`
    for writing, similar to how `read_csv` and `to_csv` work.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: HDFStore will by default not drop rows that are all missing. This behavior can
    be changed by setting `dropna=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: '### Fixed format'
  prefs: []
  type: TYPE_NORMAL
- en: The examples above show storing using `put`, which write the HDF5 to `PyTables`
    in a fixed array format, called the `fixed` format. These types of stores are
    **not** appendable once written (though you can simply remove them and rewrite).
    Nor are they **queryable**; they must be retrieved in their entirety. They also
    do not support dataframes with non-unique column names. The `fixed` format stores
    offer very fast writing and slightly faster reading than `table` stores. This
    format is specified by default when using `put` or `to_hdf` or by `format='fixed'`
    or `format='f'`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'A `fixed` format will raise a `TypeError` if you try to retrieve using a `where`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE217]  ### Table format'
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` supports another `PyTables` format on disk, the `table` format.
    Conceptually a `table` is shaped very much like a DataFrame, with rows and columns.
    A `table` may be appended to in the same or other sessions. In addition, delete
    and query type operations are supported. This format is specified by `format=''table''`
    or `format=''t''` to `append` or `put` or `to_hdf`.'
  prefs: []
  type: TYPE_NORMAL
- en: This format can be set as an option as well `pd.set_option('io.hdf.default_format','table')`
    to enable `put/append/to_hdf` to by default store in the `table` format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also create a `table` by passing `format=''table''` or `format=''t''`
    to a `put` operation.  ### Hierarchical keys'
  prefs: []
  type: TYPE_NORMAL
- en: Keys to a store can be specified as a string. These can be in a hierarchical
    path-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of
    sub-stores (or `Groups` in PyTables parlance). Keys can be specified without the
    leading ‘/’ and are **always** absolute (e.g. ‘foo’ refers to ‘/foo’). Removal
    operations can remove everything in the sub-store and **below**, so be *careful*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: You can walk through the group hierarchy using the `walk` method which will
    yield a tuple for each group key along with the relative keys of its contents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical keys cannot be retrieved as dotted (attribute) access as described
    above for items stored under the root node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, use explicit string based keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE223]  ### Storing types'
  prefs: []
  type: TYPE_NORMAL
- en: Storing mixed types in a table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing mixed-dtype data is supported. Strings are stored as a fixed-width using
    the maximum size of the appended column. Subsequent attempts at appending longer
    strings will raise a `ValueError`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Passing `min_itemsize={`values`: size}` as a parameter to append will set a
    larger minimum for the string columns. Storing `floats, strings, ints, bools,
    datetime64` are currently supported. For string columns, passing `nan_rep = ''nan''`
    to append will change the default nan representation on disk (which converts to/from
    `np.nan`), this defaults to `nan`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: Storing MultiIndex DataFrames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting
    from homogeneous index `DataFrames`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The `index` keyword is reserved and cannot be use as a level name.  ### Querying'
  prefs: []
  type: TYPE_NORMAL
- en: Querying a table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`select` and `delete` operations have an optional criterion that can be specified
    to select/delete only a subset of the data. This allows one to have a very large
    on-disk table and retrieve only a portion of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: A query is specified using the `Term` class under the hood, as a boolean expression.
  prefs: []
  type: TYPE_NORMAL
- en: '`index` and `columns` are supported indexers of `DataFrames`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `data_columns` are specified, these can be used as additional indexers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: level name in a MultiIndex, with default name `level_0`, `level_1`, … if not
    provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valid comparison operators are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`=, ==, !=, >, >=, <, <=`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid boolean expressions are combined with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`|` : or'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`&` : and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(` and `)` : for grouping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are similar to how boolean expressions are used in pandas for indexing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`=` will be automatically expanded to the comparison operator `==`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~` is the not operator, but can only be used in very limited circumstances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a list/tuple of expressions is passed they will be combined via `&`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are valid expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''index >= date''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns = [''A'', ''D'']"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns in [''A'', ''D'']"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns = A''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns == A''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"~(columns = [''A'', ''B''])"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''index > df.index[3] & string = "bar"''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"ts >= Timestamp(''2012-02-01'')"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"major_axis>=20130101"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `indexers` are on the left-hand side of the sub-expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '`columns`, `major_axis`, `ts`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-hand side of the sub-expression (after a comparison operator) can
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strings, e.g. `"bar"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: date-like, e.g. `20130101`, or `"20130101"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lists, e.g. `"['A', 'B']"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variables that are defined in the local names space, e.g. `date`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Passing a string to a query by interpolating it into the query expression is
    not recommended. Simply assign the string of interest to a variable and use that
    variable in an expression. For example, do this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: instead of this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: The latter will **not** work and will raise a `SyntaxError`.Note that there’s
    a single quote followed by a double quote in the `string` variable.
  prefs: []
  type: TYPE_NORMAL
- en: If you *must* interpolate, use the `'%r'` format specifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: which will quote `string`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: Use boolean expressions, with in-line function evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: Use inline column reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: 'The `columns` keyword can be supplied to select a list of columns to be returned,
    this is equivalent to passing a `''columns=list_of_columns_to_filter''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: '`start` and `stop` parameters can be specified to limit the total search space.
    These are in terms of the total number of rows in a table.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `ValueError` if the query expression has an unknown variable
    reference. Usually this means that you are trying to select on a column that is
    **not** a data_column.'
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `SyntaxError` if the query expression is not valid.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query timedelta64[ns]'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can store and query using the `timedelta64[ns]` type. Terms can be specified
    in the format: `<float>(<unit>)`, where float may be signed (and fractional),
    and unit can be `D,s,ms,us,ns` for the timedelta. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE233]  #### Query MultiIndex'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting from a `MultiIndex` can be achieved by using the name of the level.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: If the `MultiIndex` levels names are `None`, the levels are automatically made
    available via the `level_n` keyword with `n` the level of the `MultiIndex` you
    want to select from.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: Indexing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can create/modify an index for a table with `create_table_index` after data
    is already in the table (after and `append/put` operation). Creating a table index
    is **highly** encouraged. This will speed your queries a great deal when you use
    a `select` with the indexed dimension as the `where`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Indexes are automagically created on the indexables and any data columns you
    specify. This behavior can be turned off by passing `index=False` to `append`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: Oftentimes when appending large amounts of data to a store, it is useful to
    turn off index creation for each append, then recreate at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: Then create the index when finished appending.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: See [here](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)
    for how to create a completely-sorted-index (CSI) on an existing store.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query via data columns'
  prefs: []
  type: TYPE_NORMAL
- en: You can designate (and index) certain columns that you want to be able to perform
    queries (other than the `indexable` columns, which you can always query). For
    instance say you want to perform this common operation, on-disk, and return just
    the frame that matches this query. You can specify `data_columns = True` to force
    all columns to be `data_columns`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: There is some performance degradation by making lots of columns into `data columns`,
    so it is up to the user to designate these. In addition, you cannot change data
    columns (nor indexables) after the first append/put operation (Of course you can
    simply read in the data and create a new table!).
  prefs: []
  type: TYPE_NORMAL
- en: Iterator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and
    `select_as_multiple` to return an iterator on the results. The default is 50,000
    rows returned in a chunk.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the iterator with `read_hdf` which will open, then automatically
    close the store when finished iterating.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: Note, that the chunksize keyword applies to the **source** rows. So if you are
    doing a query, then the chunksize will subdivide the total rows in the table and
    the query applied, returning an iterator on potentially unequal sized chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a recipe for generating a query and using it to create equal sized return
    chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: Advanced queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Select a single column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: '##### Selecting coordinates'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE244]  ##### Selecting using a where mask'
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: Storer object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: Multiple table queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The methods `append_to_multiple` and `select_as_multiple` can perform appending/selecting
    from multiple tables at once. The idea is to have one table (call it the selector
    table) that you index most/all of the columns, and perform your queries. The other
    table(s) are data tables with an index matching the selector table’s index. You
    can then perform a very fast query on the selector table, yet get lots of data
    back. This method is similar to having a very wide table, but enables more efficient
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: The `append_to_multiple` method splits a given single DataFrame into multiple
    tables according to `d`, a dictionary that maps the table names to a list of ‘columns’
    you want in that table. If `None` is used in place of a list, that table will
    have the remaining unspecified columns of the given DataFrame. The argument `selector`
    defines which table is the selector table (which you can make queries from). The
    argument `dropna` will drop rows from the input `DataFrame` to ensure tables are
    synchronized. This means that if a row for one of the tables being written to
    is entirely `np.nan`, that row will be dropped from all tables.
  prefs: []
  type: TYPE_NORMAL
- en: If `dropna` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
    Remember that entirely `np.Nan` rows are not written to the HDFStore, so if you
    choose to call `dropna=False`, some tables may have more rows than others, and
    therefore `select_as_multiple` may not work or it may return unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: Delete from a table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can delete from a table selectively by specifying a `where`. In deleting
    rows, it is important to understand the `PyTables` deletes rows by erasing the
    rows, then **moving** the following data. Thus deleting can potentially be a very
    expensive operation depending on the orientation of your data. To get optimal
    performance, it’s worthwhile to have the dimension you are deleting be the first
    of the `indexables`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is ordered (on the disk) in terms of the `indexables`. Here’s a simple
    use case. You store panel-type data, with dates in the `major_axis` and ids in
    the `minor_axis`. The data is then interleaved like this:'
  prefs: []
  type: TYPE_NORMAL
- en: date_1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_2
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: date_2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be clear that a delete operation on the `major_axis` will be fairly
    quick, as one chunk is removed, then the following data moved. On the other hand
    a delete operation on the `minor_axis` will be very expensive. In this case it
    would almost certainly be faster to rewrite the table using a `where` that selects
    all but the missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files automatically.
    Thus, repeatedly deleting (or removing nodes) and adding again, **WILL TEND TO
    INCREASE THE FILE SIZE**.
  prefs: []
  type: TYPE_NORMAL
- en: To *repack and clean* the file, use [ptrepack](#io-hdf5-ptrepack).
  prefs: []
  type: TYPE_NORMAL
- en: '### Notes & caveats'
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`PyTables` allows the stored data to be compressed. This applies to all kinds
    of stores, not just tables. Two parameters are used to control compression: `complevel`
    and `complib`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`complevel` specifies if and how hard data is to be compressed. `complevel=0`
    and `complevel=None` disables compression and `0<complevel<10` enables compression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`complib` specifies which compression library to use. If nothing is specified
    the default library `zlib` is used. A compression library usually optimizes for
    either good compression rates or speed and the results will depend on the type
    of data. Which type of compression to choose depends on your specific needs and
    data. The list of supported compression libraries:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[zlib](https://zlib.net/): The default compression library. A classic in terms
    of compression, achieves good compression rates but is somewhat slow.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lzo](https://www.oberhumer.com/opensource/lzo/): Fast compression and decompression.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bzip2](https://sourceware.org/bzip2/): Good compression rates.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc](https://www.blosc.org/): Fast compression and decompression.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for alternative blosc compressors:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[blosc:blosclz](https://www.blosc.org/) This is the default compressor for
    `blosc`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html): A compact, very
    popular and fast compressor.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html): A tweaked version
    of LZ4, produces better compression ratios at the expense of speed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:snappy](https://google.github.io/snappy/): A popular compressor used
    in many places.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zlib](https://zlib.net/): A classic; somewhat slower than the previous
    ones, but achieving better compression ratios.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zstd](https://facebook.github.io/zstd/): An extremely well balanced
    codec; it provides the best compression ratios among the others above, and at
    reasonably fast speed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `complib` is defined as something other than the listed libraries a `ValueError`
    exception is issued.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the library specified with the `complib` option is missing on your platform,
    compression defaults to `zlib` without further ado.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable compression for all objects within the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: 'Or on-the-fly compression (this only applies to tables) in stores where compression
    is not enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: '#### ptrepack'
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` offers better write performance when tables are compressed after
    they are written, as opposed to turning on compression at the very beginning.
    You can use the supplied `PyTables` utility `ptrepack`. In addition, `ptrepack`
    can change compression levels after the fact.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow you to
    reuse previously deleted space. Alternatively, one can simply remove the file
    and write again, or use the `copy` method.  #### Caveats'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is **not-threadsafe for writing**. The underlying `PyTables` only
    supports concurrent reads (via threading or processes). If you need reading and
    writing *at the same time*, you need to serialize these operations in a single
    thread in a single process. You will corrupt your data otherwise. See the ([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397)) for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: If you use locks to manage write access between multiple processes, you may
    want to use [`fsync()`](https://docs.python.org/3/library/os.html#os.fsync "(in
    Python v3.12)") before releasing write locks. For convenience you can use `store.flush(fsync=True)`
    to do this for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a `table` is created columns (DataFrame) are fixed; only exactly the same
    columns can be appended
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not necessarily
    equal across timezone versions. So if data is localized to a specific timezone
    in the HDFStore using one version of a timezone library and that data is updated
    with another version, the data will be converted to UTC since these timezones
    are not considered equal. Either use the same version of timezone library or use
    `tz_convert` with the updated timezone definition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` will show a `NaturalNameWarning` if a column name cannot be used
    as an attribute selector. *Natural* identifiers contain only letters, numbers,
    and underscores, and may not begin with a number. Other identifiers cannot be
    used in a `where` clause and are generally a bad idea.  ### DataTypes'
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` will map an object dtype to the `PyTables` underlying dtype. This
    means the following types are known to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Represents missing values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| floating : `float64, float32, float16` | `np.nan` |'
  prefs: []
  type: TYPE_TB
- en: '| integer : `int64, int32, int8, uint64,uint32, uint8` |  |'
  prefs: []
  type: TYPE_TB
- en: '| boolean |  |'
  prefs: []
  type: TYPE_TB
- en: '| `datetime64[ns]` | `NaT` |'
  prefs: []
  type: TYPE_TB
- en: '| `timedelta64[ns]` | `NaT` |'
  prefs: []
  type: TYPE_TB
- en: '| categorical : see the section below |  |'
  prefs: []
  type: TYPE_TB
- en: '| object : `strings` | `np.nan` |'
  prefs: []
  type: TYPE_TB
- en: '`unicode` columns are not supported, and **WILL FAIL**.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  prefs: []
  type: TYPE_NORMAL
- en: You can write data that contains `category` dtypes to a `HDFStore`. Queries
    work the same as if it was an object array. However, the `category` dtyped data
    is stored in a more efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: String columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**min_itemsize**'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation of `HDFStore` uses a fixed column width (itemsize)
    for string columns. A string column itemsize is calculated as the maximum of the
    length of data (for that column) that is passed to the `HDFStore`, **in the first
    append**. Subsequent appends, may introduce a string for a column **larger** than
    the column can hold, an Exception will be raised (otherwise you could have a silent
    truncation of these columns, leading to loss of information). In the future we
    may relax this and allow a user-specified truncation to occur.
  prefs: []
  type: TYPE_NORMAL
- en: Pass `min_itemsize` on the first table creation to a-priori specify the minimum
    length of a particular string column. `min_itemsize` can be an integer, or a dict
    mapping a column name to an integer. You can pass `values` as a key to allow all
    *indexables* or *data_columns* to have this min_itemsize.
  prefs: []
  type: TYPE_NORMAL
- en: Passing a `min_itemsize` dict will cause all passed columns to be created as
    *data_columns* automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are not passing any `data_columns`, then the `min_itemsize` will be the
    maximum of the length of any string passed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: '**nan_rep**'
  prefs: []
  type: TYPE_NORMAL
- en: String columns will serialize a `np.nan` (a missing value) with the `nan_rep`
    string representation. This defaults to the string value `nan`. You could inadvertently
    turn an actual `nan` value into a missing value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`tables` format come with a writing performance penalty as compared to `fixed`
    stores. The benefit is the ability to append/delete and query (potentially very
    large amounts of data). Write times are generally longer as compared with regular
    stores. Query times can be quite fast, especially on an indexed axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `chunksize=<int>` to `append`, specifying the write chunksize (default
    is 50000). This will significantly lower your memory usage on writing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `expectedrows=<int>` to the first `append`, to set the TOTAL number
    of rows that `PyTables` will expect. This will optimize read/write performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate rows can be written to tables, but are filtered out in selection (with
    the last items being selected; thus a table is unique on major, minor pairs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `PerformanceWarning` will be raised if you are attempting to store types
    that will be pickled by PyTables (rather than stored as endemic types). See [Here](https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190)
    for more information and some solutions.  ## Feather'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feather provides binary columnar serialization for data frames. It is designed
    to make reading and writing data frames efficient, and to make sharing data across
    data analysis languages easy.
  prefs: []
  type: TYPE_NORMAL
- en: Feather is designed to faithfully serialize and de-serialize DataFrames, supporting
    all of the pandas dtypes, including extension dtypes such as categorical and datetime
    with tz.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: The format will NOT write an `Index`, or `MultiIndex` for the `DataFrame` and
    will raise an error if a non-default one is provided. You can `.reset_index()`
    to store the index or `.reset_index(drop=True)` to ignore it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate column names and non-string columns names are not supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actual Python objects in object dtype columns are not supported. These will
    raise a helpful error message on an attempt at serialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See the [Full Documentation](https://github.com/wesm/feather).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: Write to a feather file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: Read from a feather file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE256]  ## Parquet'
  prefs: []
  type: TYPE_NORMAL
- en: '[Apache Parquet](https://parquet.apache.org/) provides a partitioned binary
    columnar serialization for data frames. It is designed to make reading and writing
    data frames efficient, and to make sharing data across data analysis languages
    easy. Parquet can use a variety of compression techniques to shrink the file size
    as much as possible while still maintaining good read performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Parquet is designed to faithfully serialize and de-serialize `DataFrame` s,
    supporting all of the pandas dtypes, including extension dtypes such as datetime
    with tz.
  prefs: []
  type: TYPE_NORMAL
- en: Several caveats.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate column names and non-string columns names are not supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine always writes the index to the output, but `fastparquet`
    only writes non-default indexes. This extra column can cause problems for non-pandas
    consumers that are not expecting it. You can force including or omitting indexes
    with the `index` argument, regardless of the underlying engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index level names, if specified, must be strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `pyarrow` engine, categorical dtypes for non-string types can be serialized
    to parquet, but will de-serialize as their primitive dtype.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine preserves the `ordered` flag of categorical dtypes with
    string types. `fastparquet` does not preserve the `ordered` flag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non supported types include `Interval` and actual Python object types. These
    will raise a helpful error message on an attempt at serialization. `Period` type
    is supported with pyarrow >= 0.16.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine preserves extension data types such as the nullable integer
    and string data type (requiring pyarrow >= 0.16.0, and requiring the extension
    type to implement the needed protocols, see the [extension types documentation](../development/extending.html#extending-extension-arrow)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can specify an `engine` to direct the serialization. This can be one of
    `pyarrow`, or `fastparquet`, or `auto`. If the engine is NOT specified, then the
    `pd.options.io.parquet.engine` option is checked; if this is also `auto`, then
    `pyarrow` is tried, and falling back to `fastparquet`.
  prefs: []
  type: TYPE_NORMAL
- en: See the documentation for [pyarrow](https://arrow.apache.org/docs/python/) and
    [fastparquet](https://fastparquet.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: These engines are very similar and should read/write nearly identical parquet
    format files. `pyarrow>=8.0.0` supports timedelta data, `fastparquet>=0.1.4` supports
    timezone aware datetimes. These libraries differ by having different underlying
    dependencies (`fastparquet` by using `numba`, while `pyarrow` uses a c-library).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: Write to a parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: Read from a parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is not supported for `fastparquet`.
  prefs: []
  type: TYPE_NORMAL
- en: Read only certain columns of a parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: Handling indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Serializing a `DataFrame` to parquet may include the implicit index as one
    or more columns in the output file. Thus, this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: 'creates a parquet file with *three* columns if you use `pyarrow` for serialization:
    `a`, `b`, and `__index_level_0__`. If you’re using `fastparquet`, the index [may
    or may not](https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write)
    be written to the file.'
  prefs: []
  type: TYPE_NORMAL
- en: This unexpected extra column causes some databases like Amazon Redshift to reject
    the file, because that column doesn’t exist in the target table.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to omit a dataframe’s indexes when writing, pass `index=False`
    to [`to_parquet()`](../reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet
    "pandas.DataFrame.to_parquet"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: This creates a parquet file with just the two expected columns, `a` and `b`.
    If your `DataFrame` has a custom index, you won’t get it back when you load this
    file into a `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Passing `index=True` will *always* write the index, even if that’s not the underlying
    engine’s default behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning Parquet files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parquet supports partitioning of data based on the values of one or more columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: 'The `path` specifies the parent directory to which data will be saved. The
    `partition_cols` are the column names by which the dataset will be partitioned.
    Columns are partitioned in the order they are given. The partition splits are
    determined by the unique values in the partition columns. The above example creates
    a partitioned dataset that may look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE265]  ## ORC'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the [parquet](#io-parquet) format, the [ORC Format](https://orc.apache.org/)
    is a binary columnar serialization for data frames. It is designed to make reading
    data frames efficient. pandas provides both the reader and the writer for the
    ORC format, [`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc
    "pandas.read_orc") and [`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc"). This requires the [pyarrow](https://arrow.apache.org/docs/python/)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: It is *highly recommended* to install pyarrow using conda due to some issues
    occurred by pyarrow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc") requires pyarrow>=7.0.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc "pandas.read_orc")
    and [`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc") are not supported on Windows yet, you can find valid
    environments on [install optional dependencies](../getting_started/install.html#install-warn-orc).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For supported dtypes please refer to [supported ORC features in Arrow](https://arrow.apache.org/docs/cpp/orc.html#data-types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently timezones in datetime columns are not preserved when a dataframe is
    converted into ORC files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: Write to an orc file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: Read from an orc file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: Read only certain columns of an orc file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE269]  ## SQL queries'
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas.io.sql` module provides a collection of query wrappers to both facilitate
    data retrieval and to reduce dependency on DB-specific API.
  prefs: []
  type: TYPE_NORMAL
- en: Where available, users may first want to opt for [Apache Arrow ADBC](https://arrow.apache.org/adbc/current/index.html)
    drivers. These drivers should provide the best performance, null handling, and
    type detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 2.2.0: Added native support for ADBC drivers'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For a full list of ADBC drivers and their development status, see the [ADBC
    Driver Implementation Status](https://arrow.apache.org/adbc/current/driver/status.html)
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Where an ADBC driver is not available or may be missing functionality, users
    should opt for installing SQLAlchemy alongside their database driver library.
    Examples of such drivers are [psycopg2](https://www.psycopg.org/) for PostgreSQL
    or [pymysql](https://github.com/PyMySQL/PyMySQL) for MySQL. For [SQLite](https://docs.python.org/3/library/sqlite3.html)
    this is included in Python’s standard library by default. You can find an overview
    of supported drivers for each SQL dialect in the [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/dialects/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: If SQLAlchemy is not installed, you can use a [`sqlite3.Connection`](https://docs.python.org/3/library/sqlite3.html#sqlite3.Connection
    "(in Python v3.12)") in place of a SQLAlchemy engine, connection, or URI string.
  prefs: []
  type: TYPE_NORMAL
- en: See also some [cookbook examples](cookbook.html#cookbook-sql) for some advanced
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`read_sql_table`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table")(table_name, con[, schema, ...]) | Read SQL database table
    into a DataFrame. |'
  prefs: []
  type: TYPE_TB
- en: '| [`read_sql_query`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query")(sql, con[, index_col, ...]) | Read SQL query into a DataFrame.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`read_sql`](../reference/api/pandas.read_sql.html#pandas.read_sql "pandas.read_sql")(sql, con[, index_col, ...])
    | Read SQL query or database table into a DataFrame. |'
  prefs: []
  type: TYPE_TB
- en: '| [`DataFrame.to_sql`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql")(name, con, *[, schema, ...]) | Write records stored
    in a DataFrame to a SQL database. |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The function [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") is a convenience wrapper around [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") (and for backward compatibility) and will delegate to
    specific function depending on the provided input (database table name or sql
    query). Table names do not need to be quoted if they have special characters.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we use the [SQlite](https://www.sqlite.org/index.html)
    SQL database engine. You can use a temporary SQLite database where data are stored
    in “memory”.
  prefs: []
  type: TYPE_NORMAL
- en: To connect using an ADBC driver you will want to install the `adbc_driver_sqlite`
    using your package manager. Once installed, you can use the DBAPI interface provided
    by the ADBC driver to connect to your database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to. For more information on `create_engine()` and
    the URI formatting, see the examples below and the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: If you want to manage your own connections you can pass one of those instead.
    The example below opens a connection to the database using a Python context manager
    that automatically closes the connection after the block has completed. See the
    [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/core/connections.html#basic-usage)
    for an explanation of how the database connection is handled.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: When you open a connection to a database you are also responsible for closing
    it. Side effects of leaving a connection open may include locking the database
    or other breaking behaviour.
  prefs: []
  type: TYPE_NORMAL
- en: Writing DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assuming the following data is in a `DataFrame` `data`, we can insert it into
    the database using [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql").
  prefs: []
  type: TYPE_NORMAL
- en: '| id | Date | Col_1 | Col_2 | Col_3 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 2012-10-18 | X | 25.7 | True |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | 2012-10-19 | Y | -12.4 | False |'
  prefs: []
  type: TYPE_TB
- en: '| 63 | 2012-10-20 | Z | 5.73 | True |'
  prefs: []
  type: TYPE_TB
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: 'With some databases, writing large DataFrames can result in errors due to packet
    size limitations being exceeded. This can be avoided by setting the `chunksize`
    parameter when calling `to_sql`. For example, the following writes `data` to the
    database in batches of 1000 rows at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: SQL data types
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensuring consistent data type management across SQL databases is challenging.
    Not every SQL database offers the same types, and even when they do the implementation
    of a given type can vary in ways that have subtle effects on how types can be
    preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the best odds at preserving database types users are advised to use ADBC
    drivers when available. The Arrow type system offers a wider array of types that
    more closely match database types than the historical pandas/NumPy type system.
    To illustrate, note this (non-exhaustive) listing of types available in different
    databases and pandas backends:'
  prefs: []
  type: TYPE_NORMAL
- en: '| numpy/pandas | arrow | postgres | sqlite |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| int32/Int32 | int32 | INTEGER | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| int64/Int64 | int64 | BIGINT | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| float32 | float32 | REAL | REAL |'
  prefs: []
  type: TYPE_TB
- en: '| float64 | float64 | DOUBLE PRECISION | REAL |'
  prefs: []
  type: TYPE_TB
- en: '| object | string | TEXT | TEXT |'
  prefs: []
  type: TYPE_TB
- en: '| bool | `bool_` | BOOLEAN |  |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | date32 | DATE |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | month_day_nano_interval | INTERVAL |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | binary | BINARY | BLOB |'
  prefs: []
  type: TYPE_TB
- en: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | list | ARRAY [[1]](#f1) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | struct |'
  prefs: []
  type: TYPE_TB
- en: COMPOSITE TYPE
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]](#f1)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: Footnotes
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in preserving database types as best as possible throughout
    the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"`
    argument of [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: This will prevent your data from being converted to the traditional pandas/NumPy
    type system, which often converts SQL types in ways that make them impossible
    to round-trip.
  prefs: []
  type: TYPE_NORMAL
- en: In case an ADBC driver is not available, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") will try to map your data to an appropriate SQL data
    type based on the dtype of the data. When you have columns of dtype `object`,
    pandas will try to infer the data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always override the default type by specifying the desired SQL type
    of any of the columns by using the `dtype` argument. This argument needs a dictionary
    mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback
    mode). For example, specifying to use the sqlalchemy `String` type instead of
    the default `Text` type for string columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limited support for timedelta’s in the different database flavors,
    columns with type `timedelta64` will be written as integer values as nanoseconds
    to the database and a warning will be raised. The only exception to this is when
    using the ADBC PostgreSQL driver in which case a timedelta will be written to
    the database as an `INTERVAL`
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Columns of `category` dtype will be converted to the dense representation as
    you would get with `np.asarray(categorical)` (e.g. for string categories this
    gives an array of strings). Because of this, reading the database table back in
    does **not** generate a categorical.
  prefs: []
  type: TYPE_NORMAL
- en: '### Datetime data types'
  prefs: []
  type: TYPE_NORMAL
- en: Using ADBC or SQLAlchemy, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") is capable of writing datetime data that is timezone
    naive or timezone aware. However, the resulting data stored in the database ultimately
    depends on the supported data type for datetime data of the database system being
    used.
  prefs: []
  type: TYPE_NORMAL
- en: The following table lists supported data types for datetime data for some common
    databases. Other database dialects may have different data types for datetime
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '| Database | SQL Datetime Types | Timezone Support |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SQLite | `TEXT` | No |'
  prefs: []
  type: TYPE_TB
- en: '| MySQL | `TIMESTAMP` or `DATETIME` | No |'
  prefs: []
  type: TYPE_TB
- en: '| PostgreSQL | `TIMESTAMP` or `TIMESTAMP WITH TIME ZONE` | Yes |'
  prefs: []
  type: TYPE_TB
- en: When writing timezone aware data to databases that do not support timezones,
    the data will be written as timezone naive timestamps that are in local time with
    respect to the timezone.
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") is also capable of reading datetime data that is timezone
    aware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will convert
    the data to UTC.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Insertion method'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `method` controls the SQL insertion clause used. Possible values
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`None`: Uses standard SQL `INSERT` clause (one per row).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''multi''`: Pass multiple values in a single `INSERT` clause. It uses a *special*
    SQL syntax not supported by all backends. This usually provides better performance
    for analytic databases like *Presto* and *Redshift*, but has worse performance
    for traditional SQL backend if the table contains many columns. For more information
    check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'callable with signature `(pd_table, conn, keys, data_iter)`: This can be used
    to implement a more performant insertion method based on specific backend dialect
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE277]'
  prefs: []
  type: TYPE_PRE
- en: Reading tables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") will read a database table given the table name and optionally
    a subset of columns to read.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In order to use [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table"), you **must** have the ADBC driver or SQLAlchemy optional
    dependency installed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE278]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: ADBC drivers will map database types directly back to arrow types. For other
    drivers note that pandas infers column dtypes from query outputs, and not by looking
    up data types in the physical database schema. For example, assume `userid` is
    an integer column in a table. Then, intuitively, `select userid ...` will return
    integer-valued series, while `select cast(userid as text) ...` will return object-valued
    (str) series. Accordingly, if the query output is empty, then all resulting columns
    will be returned as object-valued (since they are most general). If you foresee
    that your query will sometimes generate an empty result, you may want to explicitly
    typecast afterwards to ensure dtype integrity.
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify the name of the column as the `DataFrame` index, and specify
    a subset of columns to be read.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE279]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can explicitly force columns to be parsed as dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE280]'
  prefs: []
  type: TYPE_PRE
- en: 'If needed you can explicitly specify a format string, or a dict of arguments
    to pass to [`pandas.to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE281]'
  prefs: []
  type: TYPE_PRE
- en: You can check if a table exists using `has_table()`
  prefs: []
  type: TYPE_NORMAL
- en: Schema support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reading from and writing to different schema’s is supported through the `schema`
    keyword in the [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") functions. Note however that this depends on the database
    flavor (sqlite does not have schema’s). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE282]'
  prefs: []
  type: TYPE_PRE
- en: Querying
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can query using raw SQL in the [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function. In this case you must use the SQL variant appropriate
    for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression
    language constructs, which are database-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE283]'
  prefs: []
  type: TYPE_PRE
- en: Of course, you can specify a more “complex” query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE284]'
  prefs: []
  type: TYPE_PRE
- en: 'The [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function supports a `chunksize` argument. Specifying
    this will return an iterator through chunks of the query result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE285]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE286]'
  prefs: []
  type: TYPE_PRE
- en: Engine connection examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE287]'
  prefs: []
  type: TYPE_PRE
- en: For more information see the examples the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  prefs: []
  type: TYPE_NORMAL
- en: Advanced SQLAlchemy queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use SQLAlchemy constructs to describe your query.
  prefs: []
  type: TYPE_NORMAL
- en: Use `sqlalchemy.text()` to specify query parameters in a backend-neutral way
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE288]'
  prefs: []
  type: TYPE_PRE
- en: If you have an SQLAlchemy description of your database you can express where
    conditions using SQLAlchemy expressions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE289]'
  prefs: []
  type: TYPE_PRE
- en: You can combine SQLAlchemy expressions with parameters passed to [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") using `sqlalchemy.bindparam()`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE290]'
  prefs: []
  type: TYPE_PRE
- en: Sqlite fallback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of sqlite is supported without using SQLAlchemy. This mode requires
    a Python database adapter which respect the [Python DB-API](https://www.python.org/dev/peps/pep-0249/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create connections like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE291]'
  prefs: []
  type: TYPE_PRE
- en: 'And then issue the following queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE292]  ## Google BigQuery'
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas-gbq` package provides functionality to read/write from Google BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: pandas integrates with this external package. if `pandas-gbq` is installed,
    you can use the pandas methods `pd.read_gbq` and `DataFrame.to_gbq`, which will
    call the respective functions from `pandas-gbq`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Full documentation can be found [here](https://pandas-gbq.readthedocs.io/en/latest/).  ##
    Stata format'
  prefs: []
  type: TYPE_NORMAL
- en: '### Writing to stata format'
  prefs: []
  type: TYPE_NORMAL
- en: The method `DataFrame.to_stata()` will write a DataFrame into a .dta file. The
    format version of this file is always 115 (Stata 12).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE293]'
  prefs: []
  type: TYPE_PRE
- en: '*Stata* data files have limited data type support; only strings with 244 or
    fewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be stored
    in `.dta` files. Additionally, *Stata* reserves certain values to represent missing
    data. Exporting a non-missing value that is outside of the permitted range in
    Stata for a particular data type will retype the variable to the next larger size.
    For example, `int8` values are restricted to lie between -127 and 100 in Stata,
    and so variables with values above 100 will trigger a conversion to `int16`. `nan`
    values in floating points data types are stored as the basic missing data type
    (`.` in *Stata*).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is not possible to export missing data values for integer data types.
  prefs: []
  type: TYPE_NORMAL
- en: The *Stata* writer gracefully handles other data types including `int64`, `bool`,
    `uint8`, `uint16`, `uint32` by casting to the smallest supported type that can
    represent the data. For example, data with a type of `uint8` will be cast to `int8`
    if all values are less than 100 (the upper bound for non-missing `int8` data in
    *Stata*), or, if values are outside of this range, the variable is cast to `int16`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from `int64` to `float64` may result in a loss of precision if `int64`
    values are larger than 2**53.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`StataWriter` and `DataFrame.to_stata()` only support fixed width strings containing
    up to 244 characters, a limitation imposed by the version 115 dta file format.
    Attempting to write *Stata* dta files with strings longer than 244 characters
    raises a `ValueError`.  ### Reading from Stata format'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function `read_stata` will read a dta file and return either a
    `DataFrame` or a `pandas.api.typing.StataReader` that can be used to read the
    file incrementally.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE294]'
  prefs: []
  type: TYPE_PRE
- en: Specifying a `chunksize` yields a `pandas.api.typing.StataReader` instance that
    can be used to read `chunksize` lines from the file at a time. The `StataReader`
    object can be used as an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE295]'
  prefs: []
  type: TYPE_PRE
- en: For more fine-grained control, use `iterator=True` and specify `chunksize` with
    each call to `read()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE296]'
  prefs: []
  type: TYPE_PRE
- en: Currently the `index` is retrieved as a column.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_categoricals` indicates whether value labels should be
    read and used to create a `Categorical` variable from them. Value labels can also
    be retrieved by the function `value_labels`, which requires `read()` to be called
    before use.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_missing` indicates whether missing value representations
    in Stata should be preserved. If `False` (the default), missing values are represented
    as `np.nan`. If `True`, missing values are represented using `StataMissingValue`
    objects, and columns containing missing values will have `object` data type.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") and `StataReader` support .dta formats 113-115 (Stata 10-12),
    117 (Stata 13), and 118 (Stata 14).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `preserve_dtypes=False` will upcast to the standard pandas data types:
    `int64` for all integer types and `float64` for floating point data. By default,
    the Stata data types are preserved when importing.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All `StataReader` objects, whether created by [`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") (when using `iterator=True` or `chunksize`) or instantiated
    by hand, must be used as context managers (e.g. the `with` statement). While the
    `close()` method is available, its use is unsupported. It is not part of the public
    API and will be removed in with future without warning.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` data can be exported to *Stata* data files as value labeled data.
    The exported data consists of the underlying category codes as integer data values
    and the categories as value labels. *Stata* does not have an explicit equivalent
    to a `Categorical` and information about *whether* the variable is ordered is
    lost when exporting.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* only supports string value labels, and so `str` is called on the categories
    when exporting data. Exporting `Categorical` variables with non-string categories
    produces a warning, and can result a loss of information if the `str` representations
    of the categories are not unique.'
  prefs: []
  type: TYPE_NORMAL
- en: Labeled data can similarly be imported from *Stata* data files as `Categorical`
    variables using the keyword argument `convert_categoricals` (`True` by default).
    The keyword argument `order_categoricals` (`True` by default) determines whether
    imported `Categorical` variables are ordered.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since `Categorical` variables always use integer data
    types between `-1` and `n-1` where `n` is the number of categories. If the original
    values in the *Stata* data file are required, these can be imported by setting
    `convert_categoricals=False`, which will import original data (but not the variable
    labels). The original values can be matched to the imported categorical data since
    there is a simple mapping between the original *Stata* data values and the category
    codes of imported Categorical variables: missing values are assigned code `-1`,
    and the smallest original value is assigned `0`, the second smallest is assigned
    `1` and so on until the largest original value is assigned the code `n-1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a `Categorical` with string categories for the values that are labeled and numeric
    categories for values with no label.  ## SAS formats'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function [`read_sas()`](../reference/api/pandas.read_sas.html#pandas.read_sas
    "pandas.read_sas") can read (but not write) SAS XPORT (.xpt) and SAS7BDAT (.sas7bdat)
    format files.
  prefs: []
  type: TYPE_NORMAL
- en: 'SAS files only contain two value types: ASCII text and floating point values
    (usually 8 bytes but sometimes truncated). For xport files, there is no automatic
    type conversion to integers, dates, or categoricals. For SAS7BDAT files, the format
    codes may allow date variables to be automatically converted to dates. By default
    the whole file is read and returned as a `DataFrame`.'
  prefs: []
  type: TYPE_NORMAL
- en: Specify a `chunksize` or use `iterator=True` to obtain reader objects (`XportReader`
    or `SAS7BDATReader`) for incrementally reading the file. The reader objects also
    have attributes that contain additional information about the file and its variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a SAS7BDAT file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE297]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtain an iterator and read an XPORT file 100,000 lines at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE298]'
  prefs: []
  type: TYPE_PRE
- en: The [specification](https://support.sas.com/content/dam/SAS/support/en/technical-papers/record-layout-of-a-sas-version-5-or-6-data-set-in-sas-transport-xport-format.pdf)
    for the xport file format is available from the SAS web site.
  prefs: []
  type: TYPE_NORMAL
- en: 'No official documentation is available for the SAS7BDAT format.  ## SPSS formats'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function [`read_spss()`](../reference/api/pandas.read_spss.html#pandas.read_spss
    "pandas.read_spss") can read (but not write) SPSS SAV (.sav) and ZSAV (.zsav)
    format files.
  prefs: []
  type: TYPE_NORMAL
- en: SPSS files contain column names. By default the whole file is read, categorical
    columns are converted into `pd.Categorical`, and a `DataFrame` with all columns
    is returned.
  prefs: []
  type: TYPE_NORMAL
- en: Specify the `usecols` parameter to obtain a subset of columns. Specify `convert_categoricals=False`
    to avoid converting categorical columns into `pd.Categorical`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read an SPSS file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE299]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract a subset of columns contained in `usecols` from an SPSS file and avoid
    converting categorical columns into `pd.Categorical`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE300]'
  prefs: []
  type: TYPE_PRE
- en: 'More information about the SAV and ZSAV file formats is available [here](https://www.ibm.com/docs/en/spss-statistics/22.0.0).  ##
    Other file formats'
  prefs: []
  type: TYPE_NORMAL
- en: pandas itself only supports IO with a limited set of file formats that map cleanly
    to its tabular data model. For reading and writing other file formats into and
    from pandas, we recommend these packages from the broader community.
  prefs: []
  type: TYPE_NORMAL
- en: netCDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[xarray](https://xarray.pydata.org/en/stable/) provides data structures inspired
    by the pandas `DataFrame` for working with multi-dimensional datasets, with a
    focus on the netCDF file format and easy conversion to and from pandas.  ## Performance
    considerations'
  prefs: []
  type: TYPE_NORMAL
- en: This is an informal comparison of various IO methods, using pandas 0.24.2\.
    Timings are machine dependent and small differences should be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE301]'
  prefs: []
  type: TYPE_PRE
- en: 'The following test functions will be used below to compare the performance
    of several IO methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE302]'
  prefs: []
  type: TYPE_PRE
- en: When writing, the top three functions in terms of speed are `test_feather_write`,
    `test_hdf_fixed_write` and `test_hdf_fixed_write_compress`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE303]'
  prefs: []
  type: TYPE_PRE
- en: When reading, the top three functions in terms of speed are `test_feather_read`,
    `test_pickle_read` and `test_hdf_fixed_read`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE304]'
  prefs: []
  type: TYPE_PRE
- en: The files `test.pkl.compress`, `test.parquet` and `test.feather` took the least
    space on disk (in bytes).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE305]  ## CSV & text files'
  prefs: []
  type: TYPE_NORMAL
- en: The workhorse function for reading text files (a.k.a. flat files) is [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"). See the [cookbook](cookbook.html#cookbook-csv) for some advanced
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    accepts the following common arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: filepath_or_buffervarious
  prefs: []
  type: TYPE_NORMAL
- en: Either a path to a file (a [`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)"), or `py:py._path.local.LocalPath`), URL (including http,
    ftp, and S3 locations), or any object with a `read()` method (such as an open
    file or [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO "(in
    Python v3.12)")).
  prefs: []
  type: TYPE_NORMAL
- en: sepstr, defaults to `','` for [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), `\t` for [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
  prefs: []
  type: TYPE_NORMAL
- en: 'Delimiter to use. If sep is `None`, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will be used
    and automatically detect the separator by Python’s builtin sniffer tool, [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)"). In addition, separators longer than 1 character and different
    from `''\s+''` will be interpreted as regular expressions and will also force
    the use of the Python parsing engine. Note that regex delimiters are prone to
    ignoring quoted data. Regex example: `''\\r\\t''`.'
  prefs: []
  type: TYPE_NORMAL
- en: delimiterstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Alternative argument name for sep.
  prefs: []
  type: TYPE_NORMAL
- en: delim_whitespaceboolean, default False
  prefs: []
  type: TYPE_NORMAL
- en: Specifies whether or not whitespace (e.g. `' '` or `'\t'`) will be used as the
    delimiter. Equivalent to setting `sep='\s+'`. If this option is set to `True`,
    nothing should be passed in for the `delimiter` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Column and index locations and names
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: headerint or list of ints, default `'infer'`
  prefs: []
  type: TYPE_NORMAL
- en: 'Row number(s) to use as the column names, and the start of the data. Default
    behavior is to infer the column names: if no names are passed the behavior is
    identical to `header=0` and column names are inferred from the first line of the
    file, if column names are passed explicitly then the behavior is identical to
    `header=None`. Explicitly pass `header=0` to be able to replace existing names.'
  prefs: []
  type: TYPE_NORMAL
- en: The header can be a list of ints that specify row locations for a MultiIndex
    on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this parameter ignores
    commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes
    the first line of data rather than the first line of the file.
  prefs: []
  type: TYPE_NORMAL
- en: namesarray-like, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: List of column names to use. If file contains no header row, then you should
    explicitly pass `header=None`. Duplicates in this list are not allowed.
  prefs: []
  type: TYPE_NORMAL
- en: index_colint, str, sequence of int / str, or False, optional, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Column(s) to use as the row labels of the `DataFrame`, either given as string
    name or column index. If a sequence of int / str is given, a MultiIndex is used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`index_col=False` can be used to force pandas to *not* use the first column
    as the index, e.g. when you have a malformed file with delimiters at the end of
    each line.'
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `None` instructs pandas to guess. If the number of fields
    in the column header row is equal to the number of fields in the body of the data
    file, then a default index is used. If it is larger, then the first columns are
    used as index so that the remaining number of fields in the body are equal to
    the number of fields in the header.
  prefs: []
  type: TYPE_NORMAL
- en: The first row after the header is used to determine the number of columns, which
    will go into the index. If the subsequent rows contain less columns than the first
    row, they are filled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: This can be avoided through `usecols`. This ensures that the columns are taken
    as is and the trailing data are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: usecolslist-like or callable, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Return a subset of the columns. If list-like, all elements must either be positional
    (i.e. integer indices into the document columns) or strings that correspond to
    column names provided either by the user in `names` or inferred from the document
    header row(s). If `names` are given, the document header row(s) are not taken
    into account. For example, a valid list-like `usecols` parameter would be `[0,
    1, 2]` or `['foo', 'bar', 'baz']`.
  prefs: []
  type: TYPE_NORMAL
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate
    a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo',
    'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the column names,
    returning names where the callable function evaluates to True:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE306]'
  prefs: []
  type: TYPE_PRE
- en: Using this parameter results in much faster parsing time and lower memory usage
    when using the c engine. The Python engine loads the data first before deciding
    which columns to drop.
  prefs: []
  type: TYPE_NORMAL
- en: General parsing configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: dtypeType name or dict of column -> type, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'Data type for data or columns. E.g. `{''a'': np.float64, ''b'': np.int32, ''c'':
    ''Int64''}` Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype. If converters are specified, they will be
    applied INSTEAD of dtype conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 1.5.0: Support for defaultdict was added. Specify a defaultdict
    as input where the default determines the dtype of the columns which are not explicitly
    listed.'
  prefs: []
  type: TYPE_NORMAL
- en: dtype_backend{“numpy_nullable”, “pyarrow”}, defaults to NumPy backed DataFrames
  prefs: []
  type: TYPE_NORMAL
- en: Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
    nullable dtypes are used for all dtypes that have a nullable implementation when
    “numpy_nullable” is set, pyarrow is used for all dtypes if “pyarrow” is set.
  prefs: []
  type: TYPE_NORMAL
- en: The dtype_backends are still experimential.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: engine{`'c'`, `'python'`, `'pyarrow'`}
  prefs: []
  type: TYPE_NORMAL
- en: Parser engine to use. The C and pyarrow engines are faster, while the python
    engine is currently more feature-complete. Multithreading is currently only supported
    by the pyarrow engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 1.4.0: The “pyarrow” engine was added as an *experimental* engine,
    and some features are unsupported, or may not work correctly, with this engine.'
  prefs: []
  type: TYPE_NORMAL
- en: convertersdict, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.
  prefs: []
  type: TYPE_NORMAL
- en: true_valueslist, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Values to consider as `True`.
  prefs: []
  type: TYPE_NORMAL
- en: false_valueslist, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Values to consider as `False`.
  prefs: []
  type: TYPE_NORMAL
- en: skipinitialspaceboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Skip spaces after delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: skiprowslist-like or integer, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
    of the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE307]'
  prefs: []
  type: TYPE_PRE
- en: skipfooterint, default `0`
  prefs: []
  type: TYPE_NORMAL
- en: Number of lines at bottom of file to skip (unsupported with engine=’c’).
  prefs: []
  type: TYPE_NORMAL
- en: nrowsint, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Number of rows of file to read. Useful for reading pieces of large files.
  prefs: []
  type: TYPE_NORMAL
- en: low_memoryboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: Internally process the file in chunks, resulting in lower memory use while parsing,
    but possibly mixed type inference. To ensure no mixed types either set `False`,
    or specify the type with the `dtype` parameter. Note that the entire file is read
    into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter
    to return the data in chunks. (Only valid with C parser)
  prefs: []
  type: TYPE_NORMAL
- en: memory_mapboolean, default False
  prefs: []
  type: TYPE_NORMAL
- en: If a filepath is provided for `filepath_or_buffer`, map the file object directly
    onto memory and access the data directly from there. Using this option can improve
    performance because there is no longer any I/O overhead.
  prefs: []
  type: TYPE_NORMAL
- en: NA and missing data handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: na_valuesscalar, str, list-like, or dict, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Additional strings to recognize as NA/NaN. If dict passed, specific per-column
    NA values. See [na values const](#io-navaluesconst) below for a list of the values
    interpreted as NaN by default.
  prefs: []
  type: TYPE_NORMAL
- en: keep_default_naboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether or not to include the default NaN values when parsing the data. Depending
    on whether `na_values` is passed in, the behavior is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is
    appended to the default NaN values used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are not specified, only the
    default NaN values are used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are specified, only the NaN
    values specified `na_values` are used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are not specified, no strings
    will be parsed as NaN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if `na_filter` is passed in as `False`, the `keep_default_na` and
    `na_values` parameters will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: na_filterboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: Detect missing value markers (empty strings and the value of na_values). In
    data without any NAs, passing `na_filter=False` can improve the performance of
    reading a large file.
  prefs: []
  type: TYPE_NORMAL
- en: verboseboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Indicate number of NA values placed in non-numeric columns.
  prefs: []
  type: TYPE_NORMAL
- en: skip_blank_linesboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: If `True`, skip over blank lines rather than interpreting as NaN values.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Datetime handling'
  prefs: []
  type: TYPE_NORMAL
- en: parse_datesboolean or list of ints or names or list of lists or dict, default
    `False`.
  prefs: []
  type: TYPE_NORMAL
- en: If `True` -> try parsing the index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `{''foo'': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A fast-path exists for iso8601-formatted dates.
  prefs: []
  type: TYPE_NORMAL
- en: infer_datetime_formatboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates is enabled for a column, attempt to infer the datetime
    format to speed up the processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: A strict version of this argument is now the
    default, passing it has no effect.'
  prefs: []
  type: TYPE_NORMAL
- en: keep_date_colboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates specifies combining multiple columns then keep the
    original columns.
  prefs: []
  type: TYPE_NORMAL
- en: date_parserfunction, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'Function to use for converting a sequence of string columns to an array of
    datetime instances. The default uses `dateutil.parser.parser` to do the conversion.
    pandas will try to call date_parser in three different ways, advancing to the
    next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates)
    as arguments; 2) concatenate (row-wise) the string values from the columns defined
    by parse_dates into a single array and pass that; and 3) call date_parser once
    for each row using one or more strings (corresponding to the columns defined by
    parse_dates) as arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: Use `date_format` instead, or read in as `object`
    and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.'
  prefs: []
  type: TYPE_NORMAL
- en: date_formatstr or dict of column -> format, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: If used in conjunction with `parse_dates`, will parse dates according to this
    format. For anything more complex, please read in as `object` and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: dayfirstboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: DD/MM format dates, international and European format.
  prefs: []
  type: TYPE_NORMAL
- en: cache_datesboolean, default True
  prefs: []
  type: TYPE_NORMAL
- en: If True, use a cache of unique, converted dates to apply the datetime conversion.
    May produce significant speed-up when parsing duplicate date strings, especially
    ones with timezone offsets.
  prefs: []
  type: TYPE_NORMAL
- en: Iteration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: iteratorboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.
  prefs: []
  type: TYPE_NORMAL
- en: chunksizeint, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration. See [iterating and chunking](#io-chunking)
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Quoting, compression, and file format
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: compression{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`,
    `dict`}, default `'infer'`
  prefs: []
  type: TYPE_NORMAL
- en: 'For on-the-fly decompression of on-disk data. If ‘infer’, then use gzip, bz2,
    zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in ‘.gz’, ‘.bz2’,
    ‘.zip’, ‘.xz’, ‘.zst’, respectively, and no decompression otherwise. If using
    ‘zip’, the ZIP file must contain only one data file to be read in. Set to `None`
    for no decompression. Can also be a dict with key `''method''` set to one of {`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd''`} and other key-value pairs are forwarded to
    `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`.
    As an example, the following could be passed for faster compression and to create
    a reproducible gzip archive: `compression={''method'': ''gzip'', ''compresslevel'':
    1, ''mtime'': 1}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’
    to `gzip.open`.'
  prefs: []
  type: TYPE_NORMAL
- en: thousandsstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Thousands separator.
  prefs: []
  type: TYPE_NORMAL
- en: decimalstr, default `'.'`
  prefs: []
  type: TYPE_NORMAL
- en: Character to recognize as decimal point. E.g. use `','` for European data.
  prefs: []
  type: TYPE_NORMAL
- en: float_precisionstring, default None
  prefs: []
  type: TYPE_NORMAL
- en: Specifies which converter the C engine should use for floating-point values.
    The options are `None` for the ordinary converter, `high` for the high-precision
    converter, and `round_trip` for the round-trip converter.
  prefs: []
  type: TYPE_NORMAL
- en: lineterminatorstr (length 1), default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Character to break file into lines. Only valid with C parser.
  prefs: []
  type: TYPE_NORMAL
- en: quotecharstr (length 1)
  prefs: []
  type: TYPE_NORMAL
- en: The character used to denote the start and end of a quoted item. Quoted items
    can include the delimiter and it will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: quotingint or `csv.QUOTE_*` instance, default `0`
  prefs: []
  type: TYPE_NORMAL
- en: Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL`
    (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).
  prefs: []
  type: TYPE_NORMAL
- en: doublequoteboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether
    or not to interpret two consecutive `quotechar` elements **inside** a field as
    a single `quotechar` element.
  prefs: []
  type: TYPE_NORMAL
- en: escapecharstr (length 1), default `None`
  prefs: []
  type: TYPE_NORMAL
- en: One-character string used to escape delimiter when quoting is `QUOTE_NONE`.
  prefs: []
  type: TYPE_NORMAL
- en: commentstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Indicates remainder of line should not be parsed. If found at the beginning
    of a line, the line will be ignored altogether. This parameter must be a single
    character. Like empty lines (as long as `skip_blank_lines=True`), fully commented
    lines are ignored by the parameter `header` but not by `skiprows`. For example,
    if `comment='#'`, parsing ‘#empty\na,b,c\n1,2,3’ with `header=0` will result in
    ‘a,b,c’ being treated as the header.
  prefs: []
  type: TYPE_NORMAL
- en: encodingstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python
    standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  prefs: []
  type: TYPE_NORMAL
- en: dialectstr or [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") instance, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'If provided, this parameter will override values (default or not) for the following
    parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`,
    and `quoting`. If it is necessary to override values, a ParserWarning will be
    issued. See [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") documentation for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: on_bad_lines(‘error’, ‘warn’, ‘skip’), default ‘error’
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :'
  prefs: []
  type: TYPE_NORMAL
- en: ‘error’, raise an ParserError when a bad line is encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘warn’, print a warning when a bad line is encountered and skip that line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘skip’, skip bad lines without raising or warning when they are encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: '### Specifying column data types'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can indicate the data type for the whole `DataFrame` or individual columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE308]'
  prefs: []
  type: TYPE_PRE
- en: Fortunately, pandas offers more than one way to ensure that your column(s) contain
    only one `dtype`. If you’re unfamiliar with these concepts, you can see [here](basics.html#basics-dtypes)
    to learn more about dtypes, and [here](basics.html#basics-object-conversion) to
    learn more about `object` conversion in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, you can use the `converters` argument of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE309]'
  prefs: []
  type: TYPE_PRE
- en: Or you can use the [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") function to coerce the dtypes after reading in the data,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE310]'
  prefs: []
  type: TYPE_PRE
- en: which will convert all valid parsing to floats, leaving the invalid parsing
    as `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, how you deal with reading in columns containing mixed dtypes depends
    on your specific needs. In the case above, if you wanted to `NaN` out the data
    anomalies, then [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") is probably your best option. However, if you wanted for
    all the data to be coerced, no matter the type, then using the `converters` argument
    of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    would certainly be worth trying.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, reading in abnormal data with columns containing mixed dtypes
    will result in an inconsistent dataset. If you rely on pandas to infer the dtypes
    of your columns, the parsing engine will go and infer the dtypes for different
    chunks of the data, rather than the whole dataset at once. Consequently, you can
    end up with column(s) with mixed dtypes. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE311]'
  prefs: []
  type: TYPE_PRE
- en: will result with `mixed_df` containing an `int` dtype for certain chunks of
    the column, and `str` for others due to the mixed dtypes from the data that was
    read in. It is important to note that the overall column will be marked with a
    `dtype` of `object`, which is used for columns with mixed dtypes.
  prefs: []
  type: TYPE_NORMAL
- en: Setting `dtype_backend="numpy_nullable"` will result in nullable dtypes for
    every column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE312]  ### Specifying categorical dtype'
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` columns can be parsed directly by specifying `dtype=''category''`
    or `dtype=CategoricalDtype(categories, ordered)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE313]'
  prefs: []
  type: TYPE_PRE
- en: 'Individual columns can be parsed as a `Categorical` using a dict specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE314]'
  prefs: []
  type: TYPE_PRE
- en: Specifying `dtype='category'` will result in an unordered `Categorical` whose
    `categories` are the unique values observed in the data. For more control on the
    categories and order, create a `CategoricalDtype` ahead of time, and pass that
    for that column’s `dtype`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE315]'
  prefs: []
  type: TYPE_PRE
- en: When using `dtype=CategoricalDtype`, “unexpected” values outside of `dtype.categories`
    are treated as missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE316]'
  prefs: []
  type: TYPE_PRE
- en: This matches the behavior of `Categorical.set_categories()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `dtype='category'`, the resulting categories will always be parsed as strings
    (object dtype). If the categories are numeric they can be converted using the
    [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric "pandas.to_numeric")
    function, or as appropriate, another converter such as [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime").
  prefs: []
  type: TYPE_NORMAL
- en: When `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all numeric,
    all datetimes, etc.), the conversion is done automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE317]'
  prefs: []
  type: TYPE_PRE
- en: Naming and using columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Handling column names'
  prefs: []
  type: TYPE_NORMAL
- en: 'A file may or may not have a header row. pandas assumes the first row should
    be used as the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE318]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying the `names` argument in conjunction with `header` you can indicate
    other names to use and whether or not to throw away the header row (if any):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE319]'
  prefs: []
  type: TYPE_PRE
- en: 'If the header is in a row other than the first, pass the row number to `header`.
    This will skip the preceding rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE320]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Default behavior is to infer the column names: if no names are passed the behavior
    is identical to `header=0` and column names are inferred from the first non-blank
    line of the file, if column names are passed explicitly then the behavior is identical
    to `header=None`.  ### Duplicate names parsing'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the file or header contains duplicate names, pandas will by default distinguish
    between them so as to prevent overwriting data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE321]'
  prefs: []
  type: TYPE_PRE
- en: There is no more duplicate data because duplicate columns ‘X’, …, ‘X’ become
    ‘X’, ‘X.1’, …, ‘X.N’.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Filtering columns (`usecols`)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `usecols` argument allows you to select any subset of the columns in a
    file, either using the column names, position numbers or a callable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE322]'
  prefs: []
  type: TYPE_PRE
- en: 'The `usecols` argument can also be used to specify which columns not to use
    in the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE323]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the callable is specifying that we exclude the “a” and “c” columns
    from the output.
  prefs: []
  type: TYPE_NORMAL
- en: Comments and empty lines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Ignoring line comments and empty lines'
  prefs: []
  type: TYPE_NORMAL
- en: If the `comment` parameter is specified, then completely commented lines will
    be ignored. By default, completely blank lines will be ignored as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE324]'
  prefs: []
  type: TYPE_PRE
- en: 'If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE325]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'The presence of ignored lines might create ambiguities involving line numbers;
    the parameter `header` uses row numbers (ignoring commented/empty lines), while
    `skiprows` uses line numbers (including commented/empty lines):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE326]'
  prefs: []
  type: TYPE_PRE
- en: 'If both `header` and `skiprows` are specified, `header` will be relative to
    the end of `skiprows`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE327]  #### Comments'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes comments or meta data may be included in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE328]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the parser includes the comments in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE329]'
  prefs: []
  type: TYPE_PRE
- en: 'We can suppress the comments using the `comment` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE330]  ### Dealing with Unicode data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `encoding` argument should be used for encoded unicode data, which will
    result in byte strings being decoded to unicode in the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE331]'
  prefs: []
  type: TYPE_PRE
- en: 'Some formats which encode all characters as multiple bytes, like UTF-16, won’t
    parse correctly at all without specifying the encoding. [Full list of Python standard
    encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).  ###
    Index columns and trailing delimiters'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a file has one more column of data than the number of column names, the
    first column will be used as the `DataFrame`’s row names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE332]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE333]'
  prefs: []
  type: TYPE_PRE
- en: Ordinarily, you can achieve this behavior using the `index_col` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some exception cases when a file has been prepared with delimiters
    at the end of each data line, confusing the parser. To explicitly disable the
    index column inference and discard the last column, pass `index_col=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE334]'
  prefs: []
  type: TYPE_PRE
- en: If a subset of data is being parsed using the `usecols` option, the `index_col`
    specification is based on that subset, not the original data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE335]  ### Date Handling'
  prefs: []
  type: TYPE_NORMAL
- en: Specifying date columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To better facilitate working with datetime data, [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") uses the keyword arguments `parse_dates` and `date_format`
    to allow users to specify a variety of columns and date/time formats to turn the
    input text data into `datetime` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest case is to just pass in `parse_dates=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE336]'
  prefs: []
  type: TYPE_PRE
- en: It is often the case that we may want to store date and time data separately,
    or store various date fields separately. the `parse_dates` keyword can be used
    to specify a combination of columns to parse the dates and/or times from.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a list of column lists to `parse_dates`, the resulting date
    columns will be prepended to the output (so as to not affect the existing column
    order) and the new column names will be the concatenation of the component column
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE337]'
  prefs: []
  type: TYPE_PRE
- en: 'By default the parser removes the component date columns, but you can choose
    to retain them via the `keep_date_col` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE338]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you wish to combine multiple columns into a single date column,
    a nested list must be used. In other words, `parse_dates=[1, 2]` indicates that
    the second and third columns should each be parsed as separate date columns while
    `parse_dates=[[1, 2]]` means the two columns should be parsed into a single column.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use a dict to specify custom name columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE339]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to remember that if multiple text columns are to be parsed
    into a single date column, then a new column is prepended to the data. The `index_col`
    specification is based off of this new set of columns rather than the original
    data columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE340]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a column or index contains an unparsable date, the entire column or index
    will be returned unaltered as an object data type. For non-standard datetime parsing,
    use [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") after `pd.read_csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g
    “2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your
    data to store datetimes in this format, load times will be significantly faster,
    ~20x has been observed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.2.0: Combining date columns inside read_csv is deprecated.
    Use `pd.to_datetime` on the relevant result columns instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Date parsing functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, the parser allows you to specify a custom `date_format`. Performance-wise,
    you should try these methods of parsing dates in order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or
    `date_format={column_name: "%d/%m/%Y"}`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you different formats for different columns, or want to pass any extra options
    (such as `utc`) to `to_datetime`, then you should read in your data as `object`
    dtype, and then use `to_datetime`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### Parsing a CSV with mixed timezones'
  prefs: []
  type: TYPE_NORMAL
- en: pandas cannot natively represent a column or index with mixed timezones. If
    your CSV file contains columns with a mixture of timezones, the default result
    will be an object-dtype column with strings, even with `parse_dates`. To parse
    the mixed-timezone values as a datetime column, read in as `object` dtype and
    then call [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") with `utc=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE341]  #### Inferring datetime format'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of datetime strings that can be guessed (all representing
    December 30th, 2011 at 00:00:00):'
  prefs: []
  type: TYPE_NORMAL
- en: “20111230”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “2011/12/30”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “20111230 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “12/30/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/Dec/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/December/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`,
    it will guess “01/12/2011” to be December 1st. With `dayfirst=False` (default)
    it will guess “01/12/2011” to be January 12th.
  prefs: []
  type: TYPE_NORMAL
- en: If you try to parse a column of date strings, pandas will attempt to guess the
    format from the first non-NaN element, and will then parse the rest of the column
    with that format. If pandas fails to guess the format (for example if your first
    string is `'01 December US/Pacific 2000'`), then a warning will be raised and
    each row will be parsed individually by `dateutil.parser.parse`. The safest way
    to parse dates is to explicitly set `format=`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE342]'
  prefs: []
  type: TYPE_PRE
- en: In the case that you have mixed datetime formats within the same column, you
    can pass `format='mixed'`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE343]'
  prefs: []
  type: TYPE_PRE
- en: 'or, if your datetime formats are all ISO8601 (possibly not identically-formatted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE344]'
  prefs: []
  type: TYPE_PRE
- en: International date formats
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While US date formats tend to be MM/DD/YYYY, many international formats use
    DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE345]'
  prefs: []
  type: TYPE_PRE
- en: Writing CSVs to binary file objects
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  prefs: []
  type: TYPE_NORMAL
- en: '`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary
    mode. In most cases, it is not necessary to specify `mode` as Pandas will auto-detect
    whether the file object is opened in text or binary mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE346]  ### Specifying method for floating-point conversion'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `float_precision` can be specified in order to use a specific
    floating-point converter during parsing with the C engine. The options are the
    ordinary converter, the high-precision converter, and the round-trip converter
    (which is guaranteed to round-trip values after writing to a file). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE347]  ### Thousand separators'
  prefs: []
  type: TYPE_NORMAL
- en: 'For large numbers that have been written with a thousands separator, you can
    set the `thousands` keyword to a string of length 1 so that integers will be parsed
    correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, numbers with a thousands separator will be parsed as strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE348]'
  prefs: []
  type: TYPE_PRE
- en: 'The `thousands` keyword allows integers to be parsed correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE349]  ### NA values'
  prefs: []
  type: TYPE_NORMAL
- en: To control which values are parsed as missing values (which are signified by
    `NaN`), specify a string in `na_values`. If you specify a list of strings, then
    all values in it are considered to be missing values. If you specify a number
    (a `float`, like `5.0` or an `integer` like `5`), the corresponding equivalent
    values will also imply a missing value (in this case effectively `[5.0, 5]` are
    recognized as `NaN`).
  prefs: []
  type: TYPE_NORMAL
- en: To completely override the default values that are recognized as missing, specify
    `keep_default_na=False`.
  prefs: []
  type: TYPE_NORMAL
- en: The default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',
    '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN',
    '-NaN', 'nan', '-nan', 'None', '']`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE350]'
  prefs: []
  type: TYPE_PRE
- en: In the example above `5` and `5.0` will be recognized as `NaN`, in addition
    to the defaults. A string will first be interpreted as a numerical `5`, then as
    a `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE351]'
  prefs: []
  type: TYPE_PRE
- en: Above, only an empty field will be recognized as `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE352]'
  prefs: []
  type: TYPE_PRE
- en: Above, both `NA` and `0` as strings are `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE353]'
  prefs: []
  type: TYPE_PRE
- en: 'The default values, in addition to the string `"Nope"` are recognized as `NaN`.  ###
    Infinity'
  prefs: []
  type: TYPE_NORMAL
- en: '`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf`
    as `-np.inf` (negative infinity). These will ignore the case of the value, meaning
    `Inf`, will also be parsed as `np.inf`.  ### Boolean values'
  prefs: []
  type: TYPE_NORMAL
- en: 'The common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as
    boolean. Occasionally you might want to recognize other values as being boolean.
    To do this, use the `true_values` and `false_values` options as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE354]  ### Handling “bad” lines'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some files may have malformed lines with too few fields or too many. Lines
    with too few fields will have NA values filled in the trailing fields. Lines with
    too many fields will raise an error by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE355]'
  prefs: []
  type: TYPE_PRE
- en: 'You can elect to skip bad lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE356]'
  prefs: []
  type: TYPE_PRE
- en: New in version 1.4.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Or pass a callable function to handle the bad line if `engine="python"`. The
    bad line will be a list of strings that was split by the `sep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE357]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The callable function will handle only a line with too many fields. Bad lines
    caused by other errors will be silently skipped.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE358]'
  prefs: []
  type: TYPE_PRE
- en: The line was not processed in this case, as a “bad line” here is caused by an
    escape character.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `usecols` parameter to eliminate extraneous column data
    that appear in some lines but not others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE359]'
  prefs: []
  type: TYPE_PRE
- en: In case you want to keep all data including the lines with too many fields,
    you can specify a sufficient number of `names`. This ensures that lines with not
    enough fields are filled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE360]  ### Dialect'
  prefs: []
  type: TYPE_NORMAL
- en: The `dialect` keyword gives greater flexibility in specifying the file format.
    By default it uses the Excel dialect but you can specify either the dialect name
    or a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect "(in
    Python v3.12)") instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you had data with unenclosed quotes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE361]'
  prefs: []
  type: TYPE_PRE
- en: By default, `read_csv` uses the Excel dialect and treats the double quote as
    the quote character, which causes it to fail when it finds a newline before it
    finds the closing double quote.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get around this using `dialect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE362]'
  prefs: []
  type: TYPE_PRE
- en: 'All of the dialect options can be specified separately by keyword arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE363]'
  prefs: []
  type: TYPE_PRE
- en: 'Another common dialect option is `skipinitialspace`, to skip any whitespace
    after a delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE364]'
  prefs: []
  type: TYPE_PRE
- en: 'The parsers make every attempt to “do the right thing” and not be fragile.
    Type inference is a pretty big deal. If a column can be coerced to integer dtype
    without altering the contents, the parser will do so. Any non-numeric columns
    will come through as object dtype as with the rest of pandas objects.  ### Quoting
    and Escape Characters'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quotes (and other escape characters) in embedded fields can be handled in any
    number of ways. One way is to use backslashes; to properly parse this data, you
    should pass the `escapechar` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE365]  ### Files with fixed width columns'
  prefs: []
  type: TYPE_NORMAL
- en: 'While [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") reads delimited data, the [`read_fwf()`](../reference/api/pandas.read_fwf.html#pandas.read_fwf
    "pandas.read_fwf") function works with data files that have known and fixed column
    widths. The function parameters to `read_fwf` are largely the same as `read_csv`
    with two extra parameters, and a different usage of the `delimiter` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`colspecs`: A list of pairs (tuples) giving the extents of the fixed-width
    fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’
    can be used to instruct the parser to try detecting the column specifications
    from the first 100 rows of the data. Default behavior, if not specified, is to
    infer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`widths`: A list of field widths which can be used instead of ‘colspecs’ if
    the intervals are contiguous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delimiter`: Characters to consider as filler characters in the fixed-width
    file. Can be used to specify the filler character of the fields if it is not spaces
    (e.g., ‘~’).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider a typical fixed-width data file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE366]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to parse this file into a `DataFrame`, we simply need to supply the
    column specifications to the `read_fwf` function along with the file name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE367]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how the parser automatically picks column names X.<column number> when
    `header=None` argument is specified. Alternatively, you can supply just the column
    widths for contiguous columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE368]'
  prefs: []
  type: TYPE_PRE
- en: The parser will take care of extra white spaces around the columns so it’s ok
    to have extra separation between the columns in the file.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `read_fwf` will try to infer the file’s `colspecs` by using the
    first 100 rows of the file. It can do it only in cases when the columns are aligned
    and correctly separated by the provided `delimiter` (default delimiter is whitespace).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE369]'
  prefs: []
  type: TYPE_PRE
- en: '`read_fwf` supports the `dtype` parameter for specifying the types of parsed
    columns to be different from the inferred type.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE370]'
  prefs: []
  type: TYPE_PRE
- en: Indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Files with an “implicit” index column
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a file with one less entry in the header than the number of data column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE371]'
  prefs: []
  type: TYPE_PRE
- en: 'In this special case, `read_csv` assumes that the first column is to be used
    as the index of the `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE372]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the dates weren’t automatically parsed. In that case you would need
    to do as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE373]'
  prefs: []
  type: TYPE_PRE
- en: Reading an index with a `MultiIndex`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you have data indexed by two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE374]'
  prefs: []
  type: TYPE_PRE
- en: 'The `index_col` argument to `read_csv` can take a list of column numbers to
    turn multiple columns into a `MultiIndex` for the index of the returned object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE375]'
  prefs: []
  type: TYPE_PRE
- en: '#### Reading columns with a `MultiIndex`'
  prefs: []
  type: TYPE_NORMAL
- en: By specifying list of row locations for the `header` argument, you can read
    in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the
    intervening rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE376]'
  prefs: []
  type: TYPE_PRE
- en: '`read_csv` is also able to interpret a more common format of multi-columns
    indices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE377]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If an `index_col` is not specified (e.g. you don’t have an index, or wrote
    it with `df.to_csv(..., index=False)`, then any `names` on the columns index will
    be *lost*.  ### Automatically “sniffing” the delimiter'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_csv` is capable of inferring delimited (not necessarily comma-separated)
    files, as pandas uses the [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)") class of the csv module. For this, you have to specify `sep=None`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE378]  ### Reading multiple files to create a single DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s best to use [`concat()`](../reference/api/pandas.concat.html#pandas.concat
    "pandas.concat") to combine multiple files. See the [cookbook](cookbook.html#cookbook-csv-multiple-files)
    for an example.  ### Iterating through files chunk by chunk'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you wish to iterate through a (potentially very large) file lazily
    rather than reading the entire file into memory, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE379]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying a `chunksize` to `read_csv`, the return value will be an iterable
    object of type `TextFileReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE380]'
  prefs: []
  type: TYPE_PRE
- en: 'Changed in version 1.2: `read_csv/json/sas` return a context-manager when iterating
    through a file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifying `iterator=True` will also return the `TextFileReader` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE381]'
  prefs: []
  type: TYPE_PRE
- en: Specifying the parser engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pandas currently supports three engines, the C engine, the python engine, and
    an experimental pyarrow engine (requires the `pyarrow` package). In general, the
    pyarrow engine is fastest on larger workloads and is equivalent in speed to the
    C engine on most other workloads. The python engine tends to be slower than the
    pyarrow and C engines on most workloads. However, the pyarrow engine is much less
    robust than the C engine, which lacks a few features compared to the Python engine.
  prefs: []
  type: TYPE_NORMAL
- en: Where possible, pandas uses the C parser (specified as `engine='c'`), but it
    may fall back to Python if C-unsupported options are specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, options unsupported by the C and pyarrow engines include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sep` other than a single character (e.g. regex separators)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipfooter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep=None` with `delim_whitespace=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying any of the above options will produce a `ParserWarning` unless the
    python engine is selected explicitly using `engine='python'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Options that are unsupported by the pyarrow engine which are not covered by
    the list above include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`float_precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nrows`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thousands`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_map`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dialect`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`on_bad_lines`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delim_whitespace`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`converters`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decimal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iterator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dayfirst`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`infer_datetime_format`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipinitialspace`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_memory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying these options with `engine='pyarrow'` will raise a `ValueError`.
  prefs: []
  type: TYPE_NORMAL
- en: '### Reading/writing remote files'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pass in a URL to read or write remote files to many of pandas’ IO functions
    - the following example shows reading a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE382]'
  prefs: []
  type: TYPE_PRE
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'A custom header can be sent alongside HTTP(s) requests by passing a dictionary
    of header key value mappings to the `storage_options` keyword argument as shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE383]'
  prefs: []
  type: TYPE_PRE
- en: 'All URLs which are not local files or HTTP(s) are handled by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/),
    if installed, and its various filesystem implementations (including Amazon S3,
    Google Cloud, SSH, FTP, webHDFS…). Some of these implementations will require
    additional packages to be installed, for example S3 URLs require the [s3fs](https://pypi.org/project/s3fs/)
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE384]'
  prefs: []
  type: TYPE_PRE
- en: When dealing with remote storage systems, you might need extra configuration
    with environment variables or config files in special locations. For example,
    to access data in your S3 bucket, you will need to define credentials in one of
    the several ways listed in the [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/#credentials).
    The same is true for several of the storage backends, and you should follow the
    links at [fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations)
    for implementations built into `fsspec` and [fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)
    for those not included in the main `fsspec` distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also pass parameters directly to the backend driver. Since `fsspec`
    does not utilize the `AWS_S3_HOST` environment variable, we can directly define
    a dictionary containing the endpoint_url and pass the object into the storage
    option parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE385]'
  prefs: []
  type: TYPE_PRE
- en: More sample configurations and documentation can be found at [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage).
  prefs: []
  type: TYPE_NORMAL
- en: If you do *not* have S3 credentials, you can still access public data by specifying
    an anonymous connection, such as
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE386]'
  prefs: []
  type: TYPE_PRE
- en: '`fsspec` also allows complex URLs, for accessing data in compressed archives,
    local caching of files, and more. To locally cache the above example, you would
    modify the call to'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE387]'
  prefs: []
  type: TYPE_PRE
- en: where we specify that the “anon” parameter is meant for the “s3” part of the
    implementation, not to the caching implementation. Note that this caches to a
    temporary directory for the duration of the session only, but you can also specify
    a permanent store.
  prefs: []
  type: TYPE_NORMAL
- en: Writing out data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Writing to CSV format'
  prefs: []
  type: TYPE_NORMAL
- en: The `Series` and `DataFrame` objects have an instance method `to_csv` which
    allows storing the contents of the object as a comma-separated-values file. The
    function takes a number of arguments. Only the first is required.
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf`: A string path to the file to write or a file object. If a file
    object it must be opened with `newline=''''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep` : Field delimiter for the output file (default “,”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep`: A string representation of a missing value (default ‘’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format`: Format string for floating point numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: Columns to write (default None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header`: Whether to write out the column names (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index`: whether to write row (index) names (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_label`: Column label(s) for index column(s) if desired. If None (default),
    and `header` and `index` are True, then the index names are used. (A sequence
    should be given if the `DataFrame` uses MultiIndex).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : Python write mode, default ‘w’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding`: a string representing the encoding to use if the contents are non-ASCII,
    for Python versions prior to 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`: Character sequence denoting line end (default `os.linesep`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).
    Note that if you have set a `float_format` then floats are converted to strings
    and csv.QUOTE_NONNUMERIC will treat them as non-numeric'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quotechar`: Character used to quote fields (default ‘”’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doublequote`: Control quoting of `quotechar` in fields (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`escapechar`: Character used to escape `sep` and `quotechar` when appropriate
    (default None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`: Number of rows to write at a time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_format`: Format string for datetime objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a formatted string
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `DataFrame` object has an instance method `to_string` which allows control
    over the string representation of the object. All arguments are optional:'
  prefs: []
  type: TYPE_NORMAL
- en: '`buf` default None, for example a StringIO object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` default None, which columns to write'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col_space` default None, minimum width of each column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep` default `NaN`, representation of NA value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formatters` default None, a dictionary (by column) of functions each of which
    takes a single argument and returns a formatted string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format` default None, a function which takes a single (float) argument
    and returns a formatted string; to be applied to floats in the `DataFrame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparsify` default True, set to False for a `DataFrame` with a hierarchical
    index to print every MultiIndex key at each row.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_names` default True, will print the names of the indices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` default True, will print the index (ie, row labels)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header` default True, will print the column labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`justify` default `left`, will print column headers left- or right-justified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Series` object also has a `to_string` method, but with only the `buf`,
    `na_rep`, `float_format` arguments. There is also a `length` argument which, if
    set to `True`, will additionally output the length of the Series.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    accepts the following common arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: filepath_or_buffervarious
  prefs: []
  type: TYPE_NORMAL
- en: Either a path to a file (a [`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)"), or `py:py._path.local.LocalPath`), URL (including http,
    ftp, and S3 locations), or any object with a `read()` method (such as an open
    file or [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO "(in
    Python v3.12)")).
  prefs: []
  type: TYPE_NORMAL
- en: sepstr, defaults to `','` for [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), `\t` for [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
  prefs: []
  type: TYPE_NORMAL
- en: 'Delimiter to use. If sep is `None`, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will be used
    and automatically detect the separator by Python’s builtin sniffer tool, [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)"). In addition, separators longer than 1 character and different
    from `''\s+''` will be interpreted as regular expressions and will also force
    the use of the Python parsing engine. Note that regex delimiters are prone to
    ignoring quoted data. Regex example: `''\\r\\t''`.'
  prefs: []
  type: TYPE_NORMAL
- en: delimiterstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Alternative argument name for sep.
  prefs: []
  type: TYPE_NORMAL
- en: delim_whitespaceboolean, default False
  prefs: []
  type: TYPE_NORMAL
- en: Specifies whether or not whitespace (e.g. `' '` or `'\t'`) will be used as the
    delimiter. Equivalent to setting `sep='\s+'`. If this option is set to `True`,
    nothing should be passed in for the `delimiter` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Column and index locations and names
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: headerint or list of ints, default `'infer'`
  prefs: []
  type: TYPE_NORMAL
- en: 'Row number(s) to use as the column names, and the start of the data. Default
    behavior is to infer the column names: if no names are passed the behavior is
    identical to `header=0` and column names are inferred from the first line of the
    file, if column names are passed explicitly then the behavior is identical to
    `header=None`. Explicitly pass `header=0` to be able to replace existing names.'
  prefs: []
  type: TYPE_NORMAL
- en: The header can be a list of ints that specify row locations for a MultiIndex
    on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this parameter ignores
    commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes
    the first line of data rather than the first line of the file.
  prefs: []
  type: TYPE_NORMAL
- en: namesarray-like, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: List of column names to use. If file contains no header row, then you should
    explicitly pass `header=None`. Duplicates in this list are not allowed.
  prefs: []
  type: TYPE_NORMAL
- en: index_colint, str, sequence of int / str, or False, optional, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Column(s) to use as the row labels of the `DataFrame`, either given as string
    name or column index. If a sequence of int / str is given, a MultiIndex is used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`index_col=False` can be used to force pandas to *not* use the first column
    as the index, e.g. when you have a malformed file with delimiters at the end of
    each line.'
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `None` instructs pandas to guess. If the number of fields
    in the column header row is equal to the number of fields in the body of the data
    file, then a default index is used. If it is larger, then the first columns are
    used as index so that the remaining number of fields in the body are equal to
    the number of fields in the header.
  prefs: []
  type: TYPE_NORMAL
- en: The first row after the header is used to determine the number of columns, which
    will go into the index. If the subsequent rows contain less columns than the first
    row, they are filled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: This can be avoided through `usecols`. This ensures that the columns are taken
    as is and the trailing data are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: usecolslist-like or callable, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Return a subset of the columns. If list-like, all elements must either be positional
    (i.e. integer indices into the document columns) or strings that correspond to
    column names provided either by the user in `names` or inferred from the document
    header row(s). If `names` are given, the document header row(s) are not taken
    into account. For example, a valid list-like `usecols` parameter would be `[0,
    1, 2]` or `['foo', 'bar', 'baz']`.
  prefs: []
  type: TYPE_NORMAL
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate
    a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo',
    'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the column names,
    returning names where the callable function evaluates to True:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE388]'
  prefs: []
  type: TYPE_PRE
- en: Using this parameter results in much faster parsing time and lower memory usage
    when using the c engine. The Python engine loads the data first before deciding
    which columns to drop.
  prefs: []
  type: TYPE_NORMAL
- en: General parsing configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: dtypeType name or dict of column -> type, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'Data type for data or columns. E.g. `{''a'': np.float64, ''b'': np.int32, ''c'':
    ''Int64''}` Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype. If converters are specified, they will be
    applied INSTEAD of dtype conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 1.5.0: Support for defaultdict was added. Specify a defaultdict
    as input where the default determines the dtype of the columns which are not explicitly
    listed.'
  prefs: []
  type: TYPE_NORMAL
- en: dtype_backend{“numpy_nullable”, “pyarrow”}, defaults to NumPy backed DataFrames
  prefs: []
  type: TYPE_NORMAL
- en: Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
    nullable dtypes are used for all dtypes that have a nullable implementation when
    “numpy_nullable” is set, pyarrow is used for all dtypes if “pyarrow” is set.
  prefs: []
  type: TYPE_NORMAL
- en: The dtype_backends are still experimential.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: engine{`'c'`, `'python'`, `'pyarrow'`}
  prefs: []
  type: TYPE_NORMAL
- en: Parser engine to use. The C and pyarrow engines are faster, while the python
    engine is currently more feature-complete. Multithreading is currently only supported
    by the pyarrow engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 1.4.0: The “pyarrow” engine was added as an *experimental* engine,
    and some features are unsupported, or may not work correctly, with this engine.'
  prefs: []
  type: TYPE_NORMAL
- en: convertersdict, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.
  prefs: []
  type: TYPE_NORMAL
- en: true_valueslist, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Values to consider as `True`.
  prefs: []
  type: TYPE_NORMAL
- en: false_valueslist, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Values to consider as `False`.
  prefs: []
  type: TYPE_NORMAL
- en: skipinitialspaceboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Skip spaces after delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: skiprowslist-like or integer, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
    of the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE389]'
  prefs: []
  type: TYPE_PRE
- en: skipfooterint, default `0`
  prefs: []
  type: TYPE_NORMAL
- en: Number of lines at bottom of file to skip (unsupported with engine=’c’).
  prefs: []
  type: TYPE_NORMAL
- en: nrowsint, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Number of rows of file to read. Useful for reading pieces of large files.
  prefs: []
  type: TYPE_NORMAL
- en: low_memoryboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: Internally process the file in chunks, resulting in lower memory use while parsing,
    but possibly mixed type inference. To ensure no mixed types either set `False`,
    or specify the type with the `dtype` parameter. Note that the entire file is read
    into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter
    to return the data in chunks. (Only valid with C parser)
  prefs: []
  type: TYPE_NORMAL
- en: memory_mapboolean, default False
  prefs: []
  type: TYPE_NORMAL
- en: If a filepath is provided for `filepath_or_buffer`, map the file object directly
    onto memory and access the data directly from there. Using this option can improve
    performance because there is no longer any I/O overhead.
  prefs: []
  type: TYPE_NORMAL
- en: NA and missing data handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: na_valuesscalar, str, list-like, or dict, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Additional strings to recognize as NA/NaN. If dict passed, specific per-column
    NA values. See [na values const](#io-navaluesconst) below for a list of the values
    interpreted as NaN by default.
  prefs: []
  type: TYPE_NORMAL
- en: keep_default_naboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether or not to include the default NaN values when parsing the data. Depending
    on whether `na_values` is passed in, the behavior is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is
    appended to the default NaN values used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are not specified, only the
    default NaN values are used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are specified, only the NaN
    values specified `na_values` are used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are not specified, no strings
    will be parsed as NaN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if `na_filter` is passed in as `False`, the `keep_default_na` and
    `na_values` parameters will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: na_filterboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: Detect missing value markers (empty strings and the value of na_values). In
    data without any NAs, passing `na_filter=False` can improve the performance of
    reading a large file.
  prefs: []
  type: TYPE_NORMAL
- en: verboseboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Indicate number of NA values placed in non-numeric columns.
  prefs: []
  type: TYPE_NORMAL
- en: skip_blank_linesboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: If `True`, skip over blank lines rather than interpreting as NaN values.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Datetime handling'
  prefs: []
  type: TYPE_NORMAL
- en: parse_datesboolean or list of ints or names or list of lists or dict, default
    `False`.
  prefs: []
  type: TYPE_NORMAL
- en: If `True` -> try parsing the index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `{''foo'': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A fast-path exists for iso8601-formatted dates.
  prefs: []
  type: TYPE_NORMAL
- en: infer_datetime_formatboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates is enabled for a column, attempt to infer the datetime
    format to speed up the processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: A strict version of this argument is now the
    default, passing it has no effect.'
  prefs: []
  type: TYPE_NORMAL
- en: keep_date_colboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates specifies combining multiple columns then keep the
    original columns.
  prefs: []
  type: TYPE_NORMAL
- en: date_parserfunction, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'Function to use for converting a sequence of string columns to an array of
    datetime instances. The default uses `dateutil.parser.parser` to do the conversion.
    pandas will try to call date_parser in three different ways, advancing to the
    next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates)
    as arguments; 2) concatenate (row-wise) the string values from the columns defined
    by parse_dates into a single array and pass that; and 3) call date_parser once
    for each row using one or more strings (corresponding to the columns defined by
    parse_dates) as arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: Use `date_format` instead, or read in as `object`
    and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.'
  prefs: []
  type: TYPE_NORMAL
- en: date_formatstr or dict of column -> format, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: If used in conjunction with `parse_dates`, will parse dates according to this
    format. For anything more complex, please read in as `object` and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: dayfirstboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: DD/MM format dates, international and European format.
  prefs: []
  type: TYPE_NORMAL
- en: cache_datesboolean, default True
  prefs: []
  type: TYPE_NORMAL
- en: If True, use a cache of unique, converted dates to apply the datetime conversion.
    May produce significant speed-up when parsing duplicate date strings, especially
    ones with timezone offsets.
  prefs: []
  type: TYPE_NORMAL
- en: Iteration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: iteratorboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.
  prefs: []
  type: TYPE_NORMAL
- en: chunksizeint, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration. See [iterating and chunking](#io-chunking)
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Quoting, compression, and file format
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: compression{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`,
    `dict`}, default `'infer'`
  prefs: []
  type: TYPE_NORMAL
- en: 'For on-the-fly decompression of on-disk data. If ‘infer’, then use gzip, bz2,
    zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in ‘.gz’, ‘.bz2’,
    ‘.zip’, ‘.xz’, ‘.zst’, respectively, and no decompression otherwise. If using
    ‘zip’, the ZIP file must contain only one data file to be read in. Set to `None`
    for no decompression. Can also be a dict with key `''method''` set to one of {`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd''`} and other key-value pairs are forwarded to
    `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`.
    As an example, the following could be passed for faster compression and to create
    a reproducible gzip archive: `compression={''method'': ''gzip'', ''compresslevel'':
    1, ''mtime'': 1}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’
    to `gzip.open`.'
  prefs: []
  type: TYPE_NORMAL
- en: thousandsstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Thousands separator.
  prefs: []
  type: TYPE_NORMAL
- en: decimalstr, default `'.'`
  prefs: []
  type: TYPE_NORMAL
- en: Character to recognize as decimal point. E.g. use `','` for European data.
  prefs: []
  type: TYPE_NORMAL
- en: float_precisionstring, default None
  prefs: []
  type: TYPE_NORMAL
- en: Specifies which converter the C engine should use for floating-point values.
    The options are `None` for the ordinary converter, `high` for the high-precision
    converter, and `round_trip` for the round-trip converter.
  prefs: []
  type: TYPE_NORMAL
- en: lineterminatorstr (length 1), default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Character to break file into lines. Only valid with C parser.
  prefs: []
  type: TYPE_NORMAL
- en: quotecharstr (length 1)
  prefs: []
  type: TYPE_NORMAL
- en: The character used to denote the start and end of a quoted item. Quoted items
    can include the delimiter and it will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: quotingint or `csv.QUOTE_*` instance, default `0`
  prefs: []
  type: TYPE_NORMAL
- en: Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL`
    (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).
  prefs: []
  type: TYPE_NORMAL
- en: doublequoteboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether
    or not to interpret two consecutive `quotechar` elements **inside** a field as
    a single `quotechar` element.
  prefs: []
  type: TYPE_NORMAL
- en: escapecharstr (length 1), default `None`
  prefs: []
  type: TYPE_NORMAL
- en: One-character string used to escape delimiter when quoting is `QUOTE_NONE`.
  prefs: []
  type: TYPE_NORMAL
- en: commentstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Indicates remainder of line should not be parsed. If found at the beginning
    of a line, the line will be ignored altogether. This parameter must be a single
    character. Like empty lines (as long as `skip_blank_lines=True`), fully commented
    lines are ignored by the parameter `header` but not by `skiprows`. For example,
    if `comment='#'`, parsing ‘#empty\na,b,c\n1,2,3’ with `header=0` will result in
    ‘a,b,c’ being treated as the header.
  prefs: []
  type: TYPE_NORMAL
- en: encodingstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python
    standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  prefs: []
  type: TYPE_NORMAL
- en: dialectstr or [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") instance, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'If provided, this parameter will override values (default or not) for the following
    parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`,
    and `quoting`. If it is necessary to override values, a ParserWarning will be
    issued. See [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") documentation for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: on_bad_lines(‘error’, ‘warn’, ‘skip’), default ‘error’
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :'
  prefs: []
  type: TYPE_NORMAL
- en: ‘error’, raise an ParserError when a bad line is encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘warn’, print a warning when a bad line is encountered and skip that line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘skip’, skip bad lines without raising or warning when they are encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: Basic
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: filepath_or_buffervarious
  prefs: []
  type: TYPE_NORMAL
- en: Either a path to a file (a [`str`](https://docs.python.org/3/library/stdtypes.html#str
    "(in Python v3.12)"), [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path
    "(in Python v3.12)"), or `py:py._path.local.LocalPath`), URL (including http,
    ftp, and S3 locations), or any object with a `read()` method (such as an open
    file or [`StringIO`](https://docs.python.org/3/library/io.html#io.StringIO "(in
    Python v3.12)")).
  prefs: []
  type: TYPE_NORMAL
- en: sepstr, defaults to `','` for [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"), `\t` for [`read_table()`](../reference/api/pandas.read_table.html#pandas.read_table
    "pandas.read_table")
  prefs: []
  type: TYPE_NORMAL
- en: 'Delimiter to use. If sep is `None`, the C engine cannot automatically detect
    the separator, but the Python parsing engine can, meaning the latter will be used
    and automatically detect the separator by Python’s builtin sniffer tool, [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)"). In addition, separators longer than 1 character and different
    from `''\s+''` will be interpreted as regular expressions and will also force
    the use of the Python parsing engine. Note that regex delimiters are prone to
    ignoring quoted data. Regex example: `''\\r\\t''`.'
  prefs: []
  type: TYPE_NORMAL
- en: delimiterstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Alternative argument name for sep.
  prefs: []
  type: TYPE_NORMAL
- en: delim_whitespaceboolean, default False
  prefs: []
  type: TYPE_NORMAL
- en: Specifies whether or not whitespace (e.g. `' '` or `'\t'`) will be used as the
    delimiter. Equivalent to setting `sep='\s+'`. If this option is set to `True`,
    nothing should be passed in for the `delimiter` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Column and index locations and names
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: headerint or list of ints, default `'infer'`
  prefs: []
  type: TYPE_NORMAL
- en: 'Row number(s) to use as the column names, and the start of the data. Default
    behavior is to infer the column names: if no names are passed the behavior is
    identical to `header=0` and column names are inferred from the first line of the
    file, if column names are passed explicitly then the behavior is identical to
    `header=None`. Explicitly pass `header=0` to be able to replace existing names.'
  prefs: []
  type: TYPE_NORMAL
- en: The header can be a list of ints that specify row locations for a MultiIndex
    on the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be
    skipped (e.g. 2 in this example is skipped). Note that this parameter ignores
    commented lines and empty lines if `skip_blank_lines=True`, so header=0 denotes
    the first line of data rather than the first line of the file.
  prefs: []
  type: TYPE_NORMAL
- en: namesarray-like, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: List of column names to use. If file contains no header row, then you should
    explicitly pass `header=None`. Duplicates in this list are not allowed.
  prefs: []
  type: TYPE_NORMAL
- en: index_colint, str, sequence of int / str, or False, optional, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Column(s) to use as the row labels of the `DataFrame`, either given as string
    name or column index. If a sequence of int / str is given, a MultiIndex is used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`index_col=False` can be used to force pandas to *not* use the first column
    as the index, e.g. when you have a malformed file with delimiters at the end of
    each line.'
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `None` instructs pandas to guess. If the number of fields
    in the column header row is equal to the number of fields in the body of the data
    file, then a default index is used. If it is larger, then the first columns are
    used as index so that the remaining number of fields in the body are equal to
    the number of fields in the header.
  prefs: []
  type: TYPE_NORMAL
- en: The first row after the header is used to determine the number of columns, which
    will go into the index. If the subsequent rows contain less columns than the first
    row, they are filled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: This can be avoided through `usecols`. This ensures that the columns are taken
    as is and the trailing data are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: usecolslist-like or callable, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Return a subset of the columns. If list-like, all elements must either be positional
    (i.e. integer indices into the document columns) or strings that correspond to
    column names provided either by the user in `names` or inferred from the document
    header row(s). If `names` are given, the document header row(s) are not taken
    into account. For example, a valid list-like `usecols` parameter would be `[0,
    1, 2]` or `['foo', 'bar', 'baz']`.
  prefs: []
  type: TYPE_NORMAL
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To instantiate
    a DataFrame from `data` with element order preserved use `pd.read_csv(data, usecols=['foo',
    'bar'])[['foo', 'bar']]` for columns in `['foo', 'bar']` order or `pd.read_csv(data,
    usecols=['foo', 'bar'])[['bar', 'foo']]` for `['bar', 'foo']` order.
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the column names,
    returning names where the callable function evaluates to True:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE390]'
  prefs: []
  type: TYPE_PRE
- en: Using this parameter results in much faster parsing time and lower memory usage
    when using the c engine. The Python engine loads the data first before deciding
    which columns to drop.
  prefs: []
  type: TYPE_NORMAL
- en: General parsing configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: dtypeType name or dict of column -> type, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'Data type for data or columns. E.g. `{''a'': np.float64, ''b'': np.int32, ''c'':
    ''Int64''}` Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype. If converters are specified, they will be
    applied INSTEAD of dtype conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 1.5.0: Support for defaultdict was added. Specify a defaultdict
    as input where the default determines the dtype of the columns which are not explicitly
    listed.'
  prefs: []
  type: TYPE_NORMAL
- en: dtype_backend{“numpy_nullable”, “pyarrow”}, defaults to NumPy backed DataFrames
  prefs: []
  type: TYPE_NORMAL
- en: Which dtype_backend to use, e.g. whether a DataFrame should have NumPy arrays,
    nullable dtypes are used for all dtypes that have a nullable implementation when
    “numpy_nullable” is set, pyarrow is used for all dtypes if “pyarrow” is set.
  prefs: []
  type: TYPE_NORMAL
- en: The dtype_backends are still experimential.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: engine{`'c'`, `'python'`, `'pyarrow'`}
  prefs: []
  type: TYPE_NORMAL
- en: Parser engine to use. The C and pyarrow engines are faster, while the python
    engine is currently more feature-complete. Multithreading is currently only supported
    by the pyarrow engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 1.4.0: The “pyarrow” engine was added as an *experimental* engine,
    and some features are unsupported, or may not work correctly, with this engine.'
  prefs: []
  type: TYPE_NORMAL
- en: convertersdict, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.
  prefs: []
  type: TYPE_NORMAL
- en: true_valueslist, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Values to consider as `True`.
  prefs: []
  type: TYPE_NORMAL
- en: false_valueslist, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Values to consider as `False`.
  prefs: []
  type: TYPE_NORMAL
- en: skipinitialspaceboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Skip spaces after delimiter.
  prefs: []
  type: TYPE_NORMAL
- en: skiprowslist-like or integer, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Line numbers to skip (0-indexed) or number of lines to skip (int) at the start
    of the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If callable, the callable function will be evaluated against the row indices,
    returning True if the row should be skipped and False otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE391]'
  prefs: []
  type: TYPE_PRE
- en: skipfooterint, default `0`
  prefs: []
  type: TYPE_NORMAL
- en: Number of lines at bottom of file to skip (unsupported with engine=’c’).
  prefs: []
  type: TYPE_NORMAL
- en: nrowsint, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Number of rows of file to read. Useful for reading pieces of large files.
  prefs: []
  type: TYPE_NORMAL
- en: low_memoryboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: Internally process the file in chunks, resulting in lower memory use while parsing,
    but possibly mixed type inference. To ensure no mixed types either set `False`,
    or specify the type with the `dtype` parameter. Note that the entire file is read
    into a single `DataFrame` regardless, use the `chunksize` or `iterator` parameter
    to return the data in chunks. (Only valid with C parser)
  prefs: []
  type: TYPE_NORMAL
- en: memory_mapboolean, default False
  prefs: []
  type: TYPE_NORMAL
- en: If a filepath is provided for `filepath_or_buffer`, map the file object directly
    onto memory and access the data directly from there. Using this option can improve
    performance because there is no longer any I/O overhead.
  prefs: []
  type: TYPE_NORMAL
- en: NA and missing data handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: na_valuesscalar, str, list-like, or dict, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Additional strings to recognize as NA/NaN. If dict passed, specific per-column
    NA values. See [na values const](#io-navaluesconst) below for a list of the values
    interpreted as NaN by default.
  prefs: []
  type: TYPE_NORMAL
- en: keep_default_naboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether or not to include the default NaN values when parsing the data. Depending
    on whether `na_values` is passed in, the behavior is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are specified, `na_values` is
    appended to the default NaN values used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `True`, and `na_values` are not specified, only the
    default NaN values are used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are specified, only the NaN
    values specified `na_values` are used for parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `keep_default_na` is `False`, and `na_values` are not specified, no strings
    will be parsed as NaN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if `na_filter` is passed in as `False`, the `keep_default_na` and
    `na_values` parameters will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: na_filterboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: Detect missing value markers (empty strings and the value of na_values). In
    data without any NAs, passing `na_filter=False` can improve the performance of
    reading a large file.
  prefs: []
  type: TYPE_NORMAL
- en: verboseboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Indicate number of NA values placed in non-numeric columns.
  prefs: []
  type: TYPE_NORMAL
- en: skip_blank_linesboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: If `True`, skip over blank lines rather than interpreting as NaN values.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Datetime handling'
  prefs: []
  type: TYPE_NORMAL
- en: parse_datesboolean or list of ints or names or list of lists or dict, default
    `False`.
  prefs: []
  type: TYPE_NORMAL
- en: If `True` -> try parsing the index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `{''foo'': [1, 3]}` -> parse columns 1, 3 as date and call result ‘foo’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A fast-path exists for iso8601-formatted dates.
  prefs: []
  type: TYPE_NORMAL
- en: infer_datetime_formatboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates is enabled for a column, attempt to infer the datetime
    format to speed up the processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: A strict version of this argument is now the
    default, passing it has no effect.'
  prefs: []
  type: TYPE_NORMAL
- en: keep_date_colboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: If `True` and parse_dates specifies combining multiple columns then keep the
    original columns.
  prefs: []
  type: TYPE_NORMAL
- en: date_parserfunction, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'Function to use for converting a sequence of string columns to an array of
    datetime instances. The default uses `dateutil.parser.parser` to do the conversion.
    pandas will try to call date_parser in three different ways, advancing to the
    next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates)
    as arguments; 2) concatenate (row-wise) the string values from the columns defined
    by parse_dates into a single array and pass that; and 3) call date_parser once
    for each row using one or more strings (corresponding to the columns defined by
    parse_dates) as arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.0.0: Use `date_format` instead, or read in as `object`
    and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.'
  prefs: []
  type: TYPE_NORMAL
- en: date_formatstr or dict of column -> format, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: If used in conjunction with `parse_dates`, will parse dates according to this
    format. For anything more complex, please read in as `object` and then apply [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") as-needed.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 2.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: dayfirstboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: DD/MM format dates, international and European format.
  prefs: []
  type: TYPE_NORMAL
- en: cache_datesboolean, default True
  prefs: []
  type: TYPE_NORMAL
- en: If True, use a cache of unique, converted dates to apply the datetime conversion.
    May produce significant speed-up when parsing duplicate date strings, especially
    ones with timezone offsets.
  prefs: []
  type: TYPE_NORMAL
- en: Iteration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: iteratorboolean, default `False`
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration or getting chunks with `get_chunk()`.
  prefs: []
  type: TYPE_NORMAL
- en: chunksizeint, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Return `TextFileReader` object for iteration. See [iterating and chunking](#io-chunking)
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Quoting, compression, and file format
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: compression{`'infer'`, `'gzip'`, `'bz2'`, `'zip'`, `'xz'`, `'zstd'`, `None`,
    `dict`}, default `'infer'`
  prefs: []
  type: TYPE_NORMAL
- en: 'For on-the-fly decompression of on-disk data. If ‘infer’, then use gzip, bz2,
    zip, xz, or zstandard if `filepath_or_buffer` is path-like ending in ‘.gz’, ‘.bz2’,
    ‘.zip’, ‘.xz’, ‘.zst’, respectively, and no decompression otherwise. If using
    ‘zip’, the ZIP file must contain only one data file to be read in. Set to `None`
    for no decompression. Can also be a dict with key `''method''` set to one of {`''zip''`,
    `''gzip''`, `''bz2''`, `''zstd''`} and other key-value pairs are forwarded to
    `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or `zstandard.ZstdDecompressor`.
    As an example, the following could be passed for faster compression and to create
    a reproducible gzip archive: `compression={''method'': ''gzip'', ''compresslevel'':
    1, ''mtime'': 1}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Changed in version 1.2.0: Previous versions forwarded dict entries for ‘gzip’
    to `gzip.open`.'
  prefs: []
  type: TYPE_NORMAL
- en: thousandsstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Thousands separator.
  prefs: []
  type: TYPE_NORMAL
- en: decimalstr, default `'.'`
  prefs: []
  type: TYPE_NORMAL
- en: Character to recognize as decimal point. E.g. use `','` for European data.
  prefs: []
  type: TYPE_NORMAL
- en: float_precisionstring, default None
  prefs: []
  type: TYPE_NORMAL
- en: Specifies which converter the C engine should use for floating-point values.
    The options are `None` for the ordinary converter, `high` for the high-precision
    converter, and `round_trip` for the round-trip converter.
  prefs: []
  type: TYPE_NORMAL
- en: lineterminatorstr (length 1), default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Character to break file into lines. Only valid with C parser.
  prefs: []
  type: TYPE_NORMAL
- en: quotecharstr (length 1)
  prefs: []
  type: TYPE_NORMAL
- en: The character used to denote the start and end of a quoted item. Quoted items
    can include the delimiter and it will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: quotingint or `csv.QUOTE_*` instance, default `0`
  prefs: []
  type: TYPE_NORMAL
- en: Control field quoting behavior per `csv.QUOTE_*` constants. Use one of `QUOTE_MINIMAL`
    (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE` (3).
  prefs: []
  type: TYPE_NORMAL
- en: doublequoteboolean, default `True`
  prefs: []
  type: TYPE_NORMAL
- en: When `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate whether
    or not to interpret two consecutive `quotechar` elements **inside** a field as
    a single `quotechar` element.
  prefs: []
  type: TYPE_NORMAL
- en: escapecharstr (length 1), default `None`
  prefs: []
  type: TYPE_NORMAL
- en: One-character string used to escape delimiter when quoting is `QUOTE_NONE`.
  prefs: []
  type: TYPE_NORMAL
- en: commentstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Indicates remainder of line should not be parsed. If found at the beginning
    of a line, the line will be ignored altogether. This parameter must be a single
    character. Like empty lines (as long as `skip_blank_lines=True`), fully commented
    lines are ignored by the parameter `header` but not by `skiprows`. For example,
    if `comment='#'`, parsing ‘#empty\na,b,c\n1,2,3’ with `header=0` will result in
    ‘a,b,c’ being treated as the header.
  prefs: []
  type: TYPE_NORMAL
- en: encodingstr, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: Encoding to use for UTF when reading/writing (e.g. `'utf-8'`). [List of Python
    standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  prefs: []
  type: TYPE_NORMAL
- en: dialectstr or [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") instance, default `None`
  prefs: []
  type: TYPE_NORMAL
- en: 'If provided, this parameter will override values (default or not) for the following
    parameters: `delimiter`, `doublequote`, `escapechar`, `skipinitialspace`, `quotechar`,
    and `quoting`. If it is necessary to override values, a ParserWarning will be
    issued. See [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect
    "(in Python v3.12)") documentation for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Error handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: on_bad_lines(‘error’, ‘warn’, ‘skip’), default ‘error’
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifies what to do upon encountering a bad line (a line with too many fields).
    Allowed values are :'
  prefs: []
  type: TYPE_NORMAL
- en: ‘error’, raise an ParserError when a bad line is encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘warn’, print a warning when a bad line is encountered and skip that line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‘skip’, skip bad lines without raising or warning when they are encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: '### Specifying column data types'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can indicate the data type for the whole `DataFrame` or individual columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE392]'
  prefs: []
  type: TYPE_PRE
- en: Fortunately, pandas offers more than one way to ensure that your column(s) contain
    only one `dtype`. If you’re unfamiliar with these concepts, you can see [here](basics.html#basics-dtypes)
    to learn more about dtypes, and [here](basics.html#basics-object-conversion) to
    learn more about `object` conversion in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, you can use the `converters` argument of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE393]'
  prefs: []
  type: TYPE_PRE
- en: Or you can use the [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") function to coerce the dtypes after reading in the data,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE394]'
  prefs: []
  type: TYPE_PRE
- en: which will convert all valid parsing to floats, leaving the invalid parsing
    as `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, how you deal with reading in columns containing mixed dtypes depends
    on your specific needs. In the case above, if you wanted to `NaN` out the data
    anomalies, then [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric
    "pandas.to_numeric") is probably your best option. However, if you wanted for
    all the data to be coerced, no matter the type, then using the `converters` argument
    of [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv "pandas.read_csv")
    would certainly be worth trying.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, reading in abnormal data with columns containing mixed dtypes
    will result in an inconsistent dataset. If you rely on pandas to infer the dtypes
    of your columns, the parsing engine will go and infer the dtypes for different
    chunks of the data, rather than the whole dataset at once. Consequently, you can
    end up with column(s) with mixed dtypes. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE395]'
  prefs: []
  type: TYPE_PRE
- en: will result with `mixed_df` containing an `int` dtype for certain chunks of
    the column, and `str` for others due to the mixed dtypes from the data that was
    read in. It is important to note that the overall column will be marked with a
    `dtype` of `object`, which is used for columns with mixed dtypes.
  prefs: []
  type: TYPE_NORMAL
- en: Setting `dtype_backend="numpy_nullable"` will result in nullable dtypes for
    every column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE396]'
  prefs: []
  type: TYPE_PRE
- en: '### Specifying categorical dtype'
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` columns can be parsed directly by specifying `dtype=''category''`
    or `dtype=CategoricalDtype(categories, ordered)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE397]'
  prefs: []
  type: TYPE_PRE
- en: 'Individual columns can be parsed as a `Categorical` using a dict specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE398]'
  prefs: []
  type: TYPE_PRE
- en: Specifying `dtype='category'` will result in an unordered `Categorical` whose
    `categories` are the unique values observed in the data. For more control on the
    categories and order, create a `CategoricalDtype` ahead of time, and pass that
    for that column’s `dtype`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE399]'
  prefs: []
  type: TYPE_PRE
- en: When using `dtype=CategoricalDtype`, “unexpected” values outside of `dtype.categories`
    are treated as missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE400]'
  prefs: []
  type: TYPE_PRE
- en: This matches the behavior of `Categorical.set_categories()`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `dtype='category'`, the resulting categories will always be parsed as strings
    (object dtype). If the categories are numeric they can be converted using the
    [`to_numeric()`](../reference/api/pandas.to_numeric.html#pandas.to_numeric "pandas.to_numeric")
    function, or as appropriate, another converter such as [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime").
  prefs: []
  type: TYPE_NORMAL
- en: When `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all numeric,
    all datetimes, etc.), the conversion is done automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE401]'
  prefs: []
  type: TYPE_PRE
- en: Naming and using columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Handling column names'
  prefs: []
  type: TYPE_NORMAL
- en: 'A file may or may not have a header row. pandas assumes the first row should
    be used as the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE402]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying the `names` argument in conjunction with `header` you can indicate
    other names to use and whether or not to throw away the header row (if any):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE403]'
  prefs: []
  type: TYPE_PRE
- en: 'If the header is in a row other than the first, pass the row number to `header`.
    This will skip the preceding rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE404]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Default behavior is to infer the column names: if no names are passed the behavior
    is identical to `header=0` and column names are inferred from the first non-blank
    line of the file, if column names are passed explicitly then the behavior is identical
    to `header=None`.  #### Handling column names'
  prefs: []
  type: TYPE_NORMAL
- en: 'A file may or may not have a header row. pandas assumes the first row should
    be used as the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE405]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying the `names` argument in conjunction with `header` you can indicate
    other names to use and whether or not to throw away the header row (if any):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE406]'
  prefs: []
  type: TYPE_PRE
- en: 'If the header is in a row other than the first, pass the row number to `header`.
    This will skip the preceding rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE407]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Default behavior is to infer the column names: if no names are passed the behavior
    is identical to `header=0` and column names are inferred from the first non-blank
    line of the file, if column names are passed explicitly then the behavior is identical
    to `header=None`.'
  prefs: []
  type: TYPE_NORMAL
- en: '### Duplicate names parsing'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the file or header contains duplicate names, pandas will by default distinguish
    between them so as to prevent overwriting data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE408]'
  prefs: []
  type: TYPE_PRE
- en: There is no more duplicate data because duplicate columns ‘X’, …, ‘X’ become
    ‘X’, ‘X.1’, …, ‘X.N’.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Filtering columns (`usecols`)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `usecols` argument allows you to select any subset of the columns in a
    file, either using the column names, position numbers or a callable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE409]'
  prefs: []
  type: TYPE_PRE
- en: 'The `usecols` argument can also be used to specify which columns not to use
    in the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE410]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the callable is specifying that we exclude the “a” and “c” columns
    from the output.  #### Filtering columns (`usecols`)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `usecols` argument allows you to select any subset of the columns in a
    file, either using the column names, position numbers or a callable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE411]'
  prefs: []
  type: TYPE_PRE
- en: 'The `usecols` argument can also be used to specify which columns not to use
    in the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE412]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the callable is specifying that we exclude the “a” and “c” columns
    from the output.
  prefs: []
  type: TYPE_NORMAL
- en: Comments and empty lines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Ignoring line comments and empty lines'
  prefs: []
  type: TYPE_NORMAL
- en: If the `comment` parameter is specified, then completely commented lines will
    be ignored. By default, completely blank lines will be ignored as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE413]'
  prefs: []
  type: TYPE_PRE
- en: 'If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE414]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'The presence of ignored lines might create ambiguities involving line numbers;
    the parameter `header` uses row numbers (ignoring commented/empty lines), while
    `skiprows` uses line numbers (including commented/empty lines):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE415]'
  prefs: []
  type: TYPE_PRE
- en: 'If both `header` and `skiprows` are specified, `header` will be relative to
    the end of `skiprows`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE416]  #### Comments'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes comments or meta data may be included in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE417]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the parser includes the comments in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE418]'
  prefs: []
  type: TYPE_PRE
- en: 'We can suppress the comments using the `comment` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE419]  #### Ignoring line comments and empty lines'
  prefs: []
  type: TYPE_NORMAL
- en: If the `comment` parameter is specified, then completely commented lines will
    be ignored. By default, completely blank lines will be ignored as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE420]'
  prefs: []
  type: TYPE_PRE
- en: 'If `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE421]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'The presence of ignored lines might create ambiguities involving line numbers;
    the parameter `header` uses row numbers (ignoring commented/empty lines), while
    `skiprows` uses line numbers (including commented/empty lines):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE422]'
  prefs: []
  type: TYPE_PRE
- en: 'If both `header` and `skiprows` are specified, `header` will be relative to
    the end of `skiprows`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE423]'
  prefs: []
  type: TYPE_PRE
- en: '#### Comments'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes comments or meta data may be included in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE424]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the parser includes the comments in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE425]'
  prefs: []
  type: TYPE_PRE
- en: 'We can suppress the comments using the `comment` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE426]'
  prefs: []
  type: TYPE_PRE
- en: '### Dealing with Unicode data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `encoding` argument should be used for encoded unicode data, which will
    result in byte strings being decoded to unicode in the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE427]'
  prefs: []
  type: TYPE_PRE
- en: Some formats which encode all characters as multiple bytes, like UTF-16, won’t
    parse correctly at all without specifying the encoding. [Full list of Python standard
    encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).
  prefs: []
  type: TYPE_NORMAL
- en: '### Index columns and trailing delimiters'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a file has one more column of data than the number of column names, the
    first column will be used as the `DataFrame`’s row names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE428]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE429]'
  prefs: []
  type: TYPE_PRE
- en: Ordinarily, you can achieve this behavior using the `index_col` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some exception cases when a file has been prepared with delimiters
    at the end of each data line, confusing the parser. To explicitly disable the
    index column inference and discard the last column, pass `index_col=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE430]'
  prefs: []
  type: TYPE_PRE
- en: If a subset of data is being parsed using the `usecols` option, the `index_col`
    specification is based on that subset, not the original data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE431]'
  prefs: []
  type: TYPE_PRE
- en: '### Date Handling'
  prefs: []
  type: TYPE_NORMAL
- en: Specifying date columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To better facilitate working with datetime data, [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") uses the keyword arguments `parse_dates` and `date_format`
    to allow users to specify a variety of columns and date/time formats to turn the
    input text data into `datetime` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest case is to just pass in `parse_dates=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE432]'
  prefs: []
  type: TYPE_PRE
- en: It is often the case that we may want to store date and time data separately,
    or store various date fields separately. the `parse_dates` keyword can be used
    to specify a combination of columns to parse the dates and/or times from.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a list of column lists to `parse_dates`, the resulting date
    columns will be prepended to the output (so as to not affect the existing column
    order) and the new column names will be the concatenation of the component column
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE433]'
  prefs: []
  type: TYPE_PRE
- en: 'By default the parser removes the component date columns, but you can choose
    to retain them via the `keep_date_col` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE434]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you wish to combine multiple columns into a single date column,
    a nested list must be used. In other words, `parse_dates=[1, 2]` indicates that
    the second and third columns should each be parsed as separate date columns while
    `parse_dates=[[1, 2]]` means the two columns should be parsed into a single column.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use a dict to specify custom name columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE435]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to remember that if multiple text columns are to be parsed
    into a single date column, then a new column is prepended to the data. The `index_col`
    specification is based off of this new set of columns rather than the original
    data columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE436]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a column or index contains an unparsable date, the entire column or index
    will be returned unaltered as an object data type. For non-standard datetime parsing,
    use [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") after `pd.read_csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g
    “2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your
    data to store datetimes in this format, load times will be significantly faster,
    ~20x has been observed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.2.0: Combining date columns inside read_csv is deprecated.
    Use `pd.to_datetime` on the relevant result columns instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Date parsing functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, the parser allows you to specify a custom `date_format`. Performance-wise,
    you should try these methods of parsing dates in order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or
    `date_format={column_name: "%d/%m/%Y"}`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you different formats for different columns, or want to pass any extra options
    (such as `utc`) to `to_datetime`, then you should read in your data as `object`
    dtype, and then use `to_datetime`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### Parsing a CSV with mixed timezones'
  prefs: []
  type: TYPE_NORMAL
- en: pandas cannot natively represent a column or index with mixed timezones. If
    your CSV file contains columns with a mixture of timezones, the default result
    will be an object-dtype column with strings, even with `parse_dates`. To parse
    the mixed-timezone values as a datetime column, read in as `object` dtype and
    then call [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") with `utc=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE437]  #### Inferring datetime format'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of datetime strings that can be guessed (all representing
    December 30th, 2011 at 00:00:00):'
  prefs: []
  type: TYPE_NORMAL
- en: “20111230”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “2011/12/30”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “20111230 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “12/30/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/Dec/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/December/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`,
    it will guess “01/12/2011” to be December 1st. With `dayfirst=False` (default)
    it will guess “01/12/2011” to be January 12th.
  prefs: []
  type: TYPE_NORMAL
- en: If you try to parse a column of date strings, pandas will attempt to guess the
    format from the first non-NaN element, and will then parse the rest of the column
    with that format. If pandas fails to guess the format (for example if your first
    string is `'01 December US/Pacific 2000'`), then a warning will be raised and
    each row will be parsed individually by `dateutil.parser.parse`. The safest way
    to parse dates is to explicitly set `format=`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE438]'
  prefs: []
  type: TYPE_PRE
- en: In the case that you have mixed datetime formats within the same column, you
    can pass `format='mixed'`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE439]'
  prefs: []
  type: TYPE_PRE
- en: 'or, if your datetime formats are all ISO8601 (possibly not identically-formatted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE440]'
  prefs: []
  type: TYPE_PRE
- en: International date formats
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While US date formats tend to be MM/DD/YYYY, many international formats use
    DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE441]'
  prefs: []
  type: TYPE_PRE
- en: Writing CSVs to binary file objects
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  prefs: []
  type: TYPE_NORMAL
- en: '`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary
    mode. In most cases, it is not necessary to specify `mode` as Pandas will auto-detect
    whether the file object is opened in text or binary mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE442]'
  prefs: []
  type: TYPE_PRE
- en: Specifying date columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To better facilitate working with datetime data, [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") uses the keyword arguments `parse_dates` and `date_format`
    to allow users to specify a variety of columns and date/time formats to turn the
    input text data into `datetime` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest case is to just pass in `parse_dates=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE443]'
  prefs: []
  type: TYPE_PRE
- en: It is often the case that we may want to store date and time data separately,
    or store various date fields separately. the `parse_dates` keyword can be used
    to specify a combination of columns to parse the dates and/or times from.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a list of column lists to `parse_dates`, the resulting date
    columns will be prepended to the output (so as to not affect the existing column
    order) and the new column names will be the concatenation of the component column
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE444]'
  prefs: []
  type: TYPE_PRE
- en: 'By default the parser removes the component date columns, but you can choose
    to retain them via the `keep_date_col` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE445]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you wish to combine multiple columns into a single date column,
    a nested list must be used. In other words, `parse_dates=[1, 2]` indicates that
    the second and third columns should each be parsed as separate date columns while
    `parse_dates=[[1, 2]]` means the two columns should be parsed into a single column.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use a dict to specify custom name columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE446]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to remember that if multiple text columns are to be parsed
    into a single date column, then a new column is prepended to the data. The `index_col`
    specification is based off of this new set of columns rather than the original
    data columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE447]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a column or index contains an unparsable date, the entire column or index
    will be returned unaltered as an object data type. For non-standard datetime parsing,
    use [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") after `pd.read_csv`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: read_csv has a fast_path for parsing datetime strings in iso8601 format, e.g
    “2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your
    data to store datetimes in this format, load times will be significantly faster,
    ~20x has been observed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deprecated since version 2.2.0: Combining date columns inside read_csv is deprecated.
    Use `pd.to_datetime` on the relevant result columns instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Date parsing functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, the parser allows you to specify a custom `date_format`. Performance-wise,
    you should try these methods of parsing dates in order:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know the format, use `date_format`, e.g.: `date_format="%d/%m/%Y"` or
    `date_format={column_name: "%d/%m/%Y"}`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you different formats for different columns, or want to pass any extra options
    (such as `utc`) to `to_datetime`, then you should read in your data as `object`
    dtype, and then use `to_datetime`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '#### Parsing a CSV with mixed timezones'
  prefs: []
  type: TYPE_NORMAL
- en: pandas cannot natively represent a column or index with mixed timezones. If
    your CSV file contains columns with a mixture of timezones, the default result
    will be an object-dtype column with strings, even with `parse_dates`. To parse
    the mixed-timezone values as a datetime column, read in as `object` dtype and
    then call [`to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime") with `utc=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE448]'
  prefs: []
  type: TYPE_PRE
- en: '#### Inferring datetime format'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of datetime strings that can be guessed (all representing
    December 30th, 2011 at 00:00:00):'
  prefs: []
  type: TYPE_NORMAL
- en: “20111230”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “2011/12/30”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “20111230 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “12/30/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/Dec/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “30/December/2011 00:00:00”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that format inference is sensitive to `dayfirst`. With `dayfirst=True`,
    it will guess “01/12/2011” to be December 1st. With `dayfirst=False` (default)
    it will guess “01/12/2011” to be January 12th.
  prefs: []
  type: TYPE_NORMAL
- en: If you try to parse a column of date strings, pandas will attempt to guess the
    format from the first non-NaN element, and will then parse the rest of the column
    with that format. If pandas fails to guess the format (for example if your first
    string is `'01 December US/Pacific 2000'`), then a warning will be raised and
    each row will be parsed individually by `dateutil.parser.parse`. The safest way
    to parse dates is to explicitly set `format=`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE449]'
  prefs: []
  type: TYPE_PRE
- en: In the case that you have mixed datetime formats within the same column, you
    can pass `format='mixed'`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE450]'
  prefs: []
  type: TYPE_PRE
- en: 'or, if your datetime formats are all ISO8601 (possibly not identically-formatted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE451]'
  prefs: []
  type: TYPE_PRE
- en: International date formats
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While US date formats tend to be MM/DD/YYYY, many international formats use
    DD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE452]'
  prefs: []
  type: TYPE_PRE
- en: Writing CSVs to binary file objects
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  prefs: []
  type: TYPE_NORMAL
- en: '`df.to_csv(..., mode="wb")` allows writing a CSV to a file object opened binary
    mode. In most cases, it is not necessary to specify `mode` as Pandas will auto-detect
    whether the file object is opened in text or binary mode.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE453]'
  prefs: []
  type: TYPE_PRE
- en: '### Specifying method for floating-point conversion'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `float_precision` can be specified in order to use a specific
    floating-point converter during parsing with the C engine. The options are the
    ordinary converter, the high-precision converter, and the round-trip converter
    (which is guaranteed to round-trip values after writing to a file). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE454]'
  prefs: []
  type: TYPE_PRE
- en: '### Thousand separators'
  prefs: []
  type: TYPE_NORMAL
- en: 'For large numbers that have been written with a thousands separator, you can
    set the `thousands` keyword to a string of length 1 so that integers will be parsed
    correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, numbers with a thousands separator will be parsed as strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE455]'
  prefs: []
  type: TYPE_PRE
- en: 'The `thousands` keyword allows integers to be parsed correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE456]'
  prefs: []
  type: TYPE_PRE
- en: '### NA values'
  prefs: []
  type: TYPE_NORMAL
- en: To control which values are parsed as missing values (which are signified by
    `NaN`), specify a string in `na_values`. If you specify a list of strings, then
    all values in it are considered to be missing values. If you specify a number
    (a `float`, like `5.0` or an `integer` like `5`), the corresponding equivalent
    values will also imply a missing value (in this case effectively `[5.0, 5]` are
    recognized as `NaN`).
  prefs: []
  type: TYPE_NORMAL
- en: To completely override the default values that are recognized as missing, specify
    `keep_default_na=False`.
  prefs: []
  type: TYPE_NORMAL
- en: The default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',
    '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN',
    '-NaN', 'nan', '-nan', 'None', '']`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE457]'
  prefs: []
  type: TYPE_PRE
- en: In the example above `5` and `5.0` will be recognized as `NaN`, in addition
    to the defaults. A string will first be interpreted as a numerical `5`, then as
    a `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE458]'
  prefs: []
  type: TYPE_PRE
- en: Above, only an empty field will be recognized as `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE459]'
  prefs: []
  type: TYPE_PRE
- en: Above, both `NA` and `0` as strings are `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE460]'
  prefs: []
  type: TYPE_PRE
- en: The default values, in addition to the string `"Nope"` are recognized as `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '### Infinity'
  prefs: []
  type: TYPE_NORMAL
- en: '`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf`
    as `-np.inf` (negative infinity). These will ignore the case of the value, meaning
    `Inf`, will also be parsed as `np.inf`.'
  prefs: []
  type: TYPE_NORMAL
- en: '### Boolean values'
  prefs: []
  type: TYPE_NORMAL
- en: 'The common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as
    boolean. Occasionally you might want to recognize other values as being boolean.
    To do this, use the `true_values` and `false_values` options as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE461]'
  prefs: []
  type: TYPE_PRE
- en: '### Handling “bad” lines'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some files may have malformed lines with too few fields or too many. Lines
    with too few fields will have NA values filled in the trailing fields. Lines with
    too many fields will raise an error by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE462]'
  prefs: []
  type: TYPE_PRE
- en: 'You can elect to skip bad lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE463]'
  prefs: []
  type: TYPE_PRE
- en: New in version 1.4.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Or pass a callable function to handle the bad line if `engine="python"`. The
    bad line will be a list of strings that was split by the `sep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE464]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The callable function will handle only a line with too many fields. Bad lines
    caused by other errors will be silently skipped.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE465]'
  prefs: []
  type: TYPE_PRE
- en: The line was not processed in this case, as a “bad line” here is caused by an
    escape character.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also use the `usecols` parameter to eliminate extraneous column data
    that appear in some lines but not others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE466]'
  prefs: []
  type: TYPE_PRE
- en: In case you want to keep all data including the lines with too many fields,
    you can specify a sufficient number of `names`. This ensures that lines with not
    enough fields are filled with `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE467]'
  prefs: []
  type: TYPE_PRE
- en: '### Dialect'
  prefs: []
  type: TYPE_NORMAL
- en: The `dialect` keyword gives greater flexibility in specifying the file format.
    By default it uses the Excel dialect but you can specify either the dialect name
    or a [`csv.Dialect`](https://docs.python.org/3/library/csv.html#csv.Dialect "(in
    Python v3.12)") instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you had data with unenclosed quotes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE468]'
  prefs: []
  type: TYPE_PRE
- en: By default, `read_csv` uses the Excel dialect and treats the double quote as
    the quote character, which causes it to fail when it finds a newline before it
    finds the closing double quote.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get around this using `dialect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE469]'
  prefs: []
  type: TYPE_PRE
- en: 'All of the dialect options can be specified separately by keyword arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE470]'
  prefs: []
  type: TYPE_PRE
- en: 'Another common dialect option is `skipinitialspace`, to skip any whitespace
    after a delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE471]'
  prefs: []
  type: TYPE_PRE
- en: The parsers make every attempt to “do the right thing” and not be fragile. Type
    inference is a pretty big deal. If a column can be coerced to integer dtype without
    altering the contents, the parser will do so. Any non-numeric columns will come
    through as object dtype as with the rest of pandas objects.
  prefs: []
  type: TYPE_NORMAL
- en: '### Quoting and Escape Characters'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quotes (and other escape characters) in embedded fields can be handled in any
    number of ways. One way is to use backslashes; to properly parse this data, you
    should pass the `escapechar` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE472]'
  prefs: []
  type: TYPE_PRE
- en: '### Files with fixed width columns'
  prefs: []
  type: TYPE_NORMAL
- en: 'While [`read_csv()`](../reference/api/pandas.read_csv.html#pandas.read_csv
    "pandas.read_csv") reads delimited data, the [`read_fwf()`](../reference/api/pandas.read_fwf.html#pandas.read_fwf
    "pandas.read_fwf") function works with data files that have known and fixed column
    widths. The function parameters to `read_fwf` are largely the same as `read_csv`
    with two extra parameters, and a different usage of the `delimiter` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`colspecs`: A list of pairs (tuples) giving the extents of the fixed-width
    fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’
    can be used to instruct the parser to try detecting the column specifications
    from the first 100 rows of the data. Default behavior, if not specified, is to
    infer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`widths`: A list of field widths which can be used instead of ‘colspecs’ if
    the intervals are contiguous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delimiter`: Characters to consider as filler characters in the fixed-width
    file. Can be used to specify the filler character of the fields if it is not spaces
    (e.g., ‘~’).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider a typical fixed-width data file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE473]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to parse this file into a `DataFrame`, we simply need to supply the
    column specifications to the `read_fwf` function along with the file name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE474]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how the parser automatically picks column names X.<column number> when
    `header=None` argument is specified. Alternatively, you can supply just the column
    widths for contiguous columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE475]'
  prefs: []
  type: TYPE_PRE
- en: The parser will take care of extra white spaces around the columns so it’s ok
    to have extra separation between the columns in the file.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `read_fwf` will try to infer the file’s `colspecs` by using the
    first 100 rows of the file. It can do it only in cases when the columns are aligned
    and correctly separated by the provided `delimiter` (default delimiter is whitespace).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE476]'
  prefs: []
  type: TYPE_PRE
- en: '`read_fwf` supports the `dtype` parameter for specifying the types of parsed
    columns to be different from the inferred type.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE477]'
  prefs: []
  type: TYPE_PRE
- en: Indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Files with an “implicit” index column
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a file with one less entry in the header than the number of data column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE478]'
  prefs: []
  type: TYPE_PRE
- en: 'In this special case, `read_csv` assumes that the first column is to be used
    as the index of the `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE479]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the dates weren’t automatically parsed. In that case you would need
    to do as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE480]'
  prefs: []
  type: TYPE_PRE
- en: Reading an index with a `MultiIndex`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you have data indexed by two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE481]'
  prefs: []
  type: TYPE_PRE
- en: 'The `index_col` argument to `read_csv` can take a list of column numbers to
    turn multiple columns into a `MultiIndex` for the index of the returned object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE482]'
  prefs: []
  type: TYPE_PRE
- en: '#### Reading columns with a `MultiIndex`'
  prefs: []
  type: TYPE_NORMAL
- en: By specifying list of row locations for the `header` argument, you can read
    in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the
    intervening rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE483]'
  prefs: []
  type: TYPE_PRE
- en: '`read_csv` is also able to interpret a more common format of multi-columns
    indices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE484]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If an `index_col` is not specified (e.g. you don’t have an index, or wrote it
    with `df.to_csv(..., index=False)`, then any `names` on the columns index will
    be *lost*.
  prefs: []
  type: TYPE_NORMAL
- en: Files with an “implicit” index column
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a file with one less entry in the header than the number of data column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE485]'
  prefs: []
  type: TYPE_PRE
- en: 'In this special case, `read_csv` assumes that the first column is to be used
    as the index of the `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE486]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the dates weren’t automatically parsed. In that case you would need
    to do as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE487]'
  prefs: []
  type: TYPE_PRE
- en: Reading an index with a `MultiIndex`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you have data indexed by two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE488]'
  prefs: []
  type: TYPE_PRE
- en: 'The `index_col` argument to `read_csv` can take a list of column numbers to
    turn multiple columns into a `MultiIndex` for the index of the returned object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE489]'
  prefs: []
  type: TYPE_PRE
- en: '#### Reading columns with a `MultiIndex`'
  prefs: []
  type: TYPE_NORMAL
- en: By specifying list of row locations for the `header` argument, you can read
    in a `MultiIndex` for the columns. Specifying non-consecutive rows will skip the
    intervening rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE490]'
  prefs: []
  type: TYPE_PRE
- en: '`read_csv` is also able to interpret a more common format of multi-columns
    indices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE491]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If an `index_col` is not specified (e.g. you don’t have an index, or wrote it
    with `df.to_csv(..., index=False)`, then any `names` on the columns index will
    be *lost*.
  prefs: []
  type: TYPE_NORMAL
- en: '### Automatically “sniffing” the delimiter'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_csv` is capable of inferring delimited (not necessarily comma-separated)
    files, as pandas uses the [`csv.Sniffer`](https://docs.python.org/3/library/csv.html#csv.Sniffer
    "(in Python v3.12)") class of the csv module. For this, you have to specify `sep=None`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE492]'
  prefs: []
  type: TYPE_PRE
- en: '### Reading multiple files to create a single DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: It’s best to use [`concat()`](../reference/api/pandas.concat.html#pandas.concat
    "pandas.concat") to combine multiple files. See the [cookbook](cookbook.html#cookbook-csv-multiple-files)
    for an example.
  prefs: []
  type: TYPE_NORMAL
- en: '### Iterating through files chunk by chunk'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you wish to iterate through a (potentially very large) file lazily
    rather than reading the entire file into memory, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE493]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying a `chunksize` to `read_csv`, the return value will be an iterable
    object of type `TextFileReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE494]'
  prefs: []
  type: TYPE_PRE
- en: 'Changed in version 1.2: `read_csv/json/sas` return a context-manager when iterating
    through a file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifying `iterator=True` will also return the `TextFileReader` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE495]'
  prefs: []
  type: TYPE_PRE
- en: Specifying the parser engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pandas currently supports three engines, the C engine, the python engine, and
    an experimental pyarrow engine (requires the `pyarrow` package). In general, the
    pyarrow engine is fastest on larger workloads and is equivalent in speed to the
    C engine on most other workloads. The python engine tends to be slower than the
    pyarrow and C engines on most workloads. However, the pyarrow engine is much less
    robust than the C engine, which lacks a few features compared to the Python engine.
  prefs: []
  type: TYPE_NORMAL
- en: Where possible, pandas uses the C parser (specified as `engine='c'`), but it
    may fall back to Python if C-unsupported options are specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, options unsupported by the C and pyarrow engines include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sep` other than a single character (e.g. regex separators)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipfooter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep=None` with `delim_whitespace=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying any of the above options will produce a `ParserWarning` unless the
    python engine is selected explicitly using `engine='python'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Options that are unsupported by the pyarrow engine which are not covered by
    the list above include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`float_precision`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nrows`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thousands`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory_map`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dialect`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`on_bad_lines`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delim_whitespace`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`converters`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decimal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iterator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dayfirst`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`infer_datetime_format`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipinitialspace`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_memory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying these options with `engine='pyarrow'` will raise a `ValueError`.
  prefs: []
  type: TYPE_NORMAL
- en: '### Reading/writing remote files'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pass in a URL to read or write remote files to many of pandas’ IO functions
    - the following example shows reading a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE496]'
  prefs: []
  type: TYPE_PRE
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'A custom header can be sent alongside HTTP(s) requests by passing a dictionary
    of header key value mappings to the `storage_options` keyword argument as shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE497]'
  prefs: []
  type: TYPE_PRE
- en: 'All URLs which are not local files or HTTP(s) are handled by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/),
    if installed, and its various filesystem implementations (including Amazon S3,
    Google Cloud, SSH, FTP, webHDFS…). Some of these implementations will require
    additional packages to be installed, for example S3 URLs require the [s3fs](https://pypi.org/project/s3fs/)
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE498]'
  prefs: []
  type: TYPE_PRE
- en: When dealing with remote storage systems, you might need extra configuration
    with environment variables or config files in special locations. For example,
    to access data in your S3 bucket, you will need to define credentials in one of
    the several ways listed in the [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/#credentials).
    The same is true for several of the storage backends, and you should follow the
    links at [fsimpl1](https://filesystem-spec.readthedocs.io/en/latest/api.html#built-in-implementations)
    for implementations built into `fsspec` and [fsimpl2](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)
    for those not included in the main `fsspec` distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also pass parameters directly to the backend driver. Since `fsspec`
    does not utilize the `AWS_S3_HOST` environment variable, we can directly define
    a dictionary containing the endpoint_url and pass the object into the storage
    option parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE499]'
  prefs: []
  type: TYPE_PRE
- en: More sample configurations and documentation can be found at [S3Fs documentation](https://s3fs.readthedocs.io/en/latest/index.html?highlight=host#s3-compatible-storage).
  prefs: []
  type: TYPE_NORMAL
- en: If you do *not* have S3 credentials, you can still access public data by specifying
    an anonymous connection, such as
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.2.0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE500]'
  prefs: []
  type: TYPE_PRE
- en: '`fsspec` also allows complex URLs, for accessing data in compressed archives,
    local caching of files, and more. To locally cache the above example, you would
    modify the call to'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE501]'
  prefs: []
  type: TYPE_PRE
- en: where we specify that the “anon” parameter is meant for the “s3” part of the
    implementation, not to the caching implementation. Note that this caches to a
    temporary directory for the duration of the session only, but you can also specify
    a permanent store.
  prefs: []
  type: TYPE_NORMAL
- en: Writing out data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '#### Writing to CSV format'
  prefs: []
  type: TYPE_NORMAL
- en: The `Series` and `DataFrame` objects have an instance method `to_csv` which
    allows storing the contents of the object as a comma-separated-values file. The
    function takes a number of arguments. Only the first is required.
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf`: A string path to the file to write or a file object. If a file
    object it must be opened with `newline=''''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep` : Field delimiter for the output file (default “,”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep`: A string representation of a missing value (default ‘’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format`: Format string for floating point numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: Columns to write (default None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header`: Whether to write out the column names (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index`: whether to write row (index) names (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_label`: Column label(s) for index column(s) if desired. If None (default),
    and `header` and `index` are True, then the index names are used. (A sequence
    should be given if the `DataFrame` uses MultiIndex).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : Python write mode, default ‘w’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding`: a string representing the encoding to use if the contents are non-ASCII,
    for Python versions prior to 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`: Character sequence denoting line end (default `os.linesep`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).
    Note that if you have set a `float_format` then floats are converted to strings
    and csv.QUOTE_NONNUMERIC will treat them as non-numeric'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quotechar`: Character used to quote fields (default ‘”’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doublequote`: Control quoting of `quotechar` in fields (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`escapechar`: Character used to escape `sep` and `quotechar` when appropriate
    (default None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`: Number of rows to write at a time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_format`: Format string for datetime objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a formatted string
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `DataFrame` object has an instance method `to_string` which allows control
    over the string representation of the object. All arguments are optional:'
  prefs: []
  type: TYPE_NORMAL
- en: '`buf` default None, for example a StringIO object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` default None, which columns to write'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col_space` default None, minimum width of each column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep` default `NaN`, representation of NA value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formatters` default None, a dictionary (by column) of functions each of which
    takes a single argument and returns a formatted string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format` default None, a function which takes a single (float) argument
    and returns a formatted string; to be applied to floats in the `DataFrame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparsify` default True, set to False for a `DataFrame` with a hierarchical
    index to print every MultiIndex key at each row.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_names` default True, will print the names of the indices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` default True, will print the index (ie, row labels)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header` default True, will print the column labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`justify` default `left`, will print column headers left- or right-justified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Series` object also has a `to_string` method, but with only the `buf`,
    `na_rep`, `float_format` arguments. There is also a `length` argument which, if
    set to `True`, will additionally output the length of the Series.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Writing to CSV format'
  prefs: []
  type: TYPE_NORMAL
- en: The `Series` and `DataFrame` objects have an instance method `to_csv` which
    allows storing the contents of the object as a comma-separated-values file. The
    function takes a number of arguments. Only the first is required.
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf`: A string path to the file to write or a file object. If a file
    object it must be opened with `newline=''''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep` : Field delimiter for the output file (default “,”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep`: A string representation of a missing value (default ‘’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format`: Format string for floating point numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns`: Columns to write (default None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header`: Whether to write out the column names (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index`: whether to write row (index) names (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_label`: Column label(s) for index column(s) if desired. If None (default),
    and `header` and `index` are True, then the index names are used. (A sequence
    should be given if the `DataFrame` uses MultiIndex).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : Python write mode, default ‘w’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding`: a string representing the encoding to use if the contents are non-ASCII,
    for Python versions prior to 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineterminator`: Character sequence denoting line end (default `os.linesep`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).
    Note that if you have set a `float_format` then floats are converted to strings
    and csv.QUOTE_NONNUMERIC will treat them as non-numeric'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quotechar`: Character used to quote fields (default ‘”’)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doublequote`: Control quoting of `quotechar` in fields (default True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`escapechar`: Character used to escape `sep` and `quotechar` when appropriate
    (default None)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize`: Number of rows to write at a time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_format`: Format string for datetime objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a formatted string
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `DataFrame` object has an instance method `to_string` which allows control
    over the string representation of the object. All arguments are optional:'
  prefs: []
  type: TYPE_NORMAL
- en: '`buf` default None, for example a StringIO object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`columns` default None, which columns to write'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col_space` default None, minimum width of each column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na_rep` default `NaN`, representation of NA value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`formatters` default None, a dictionary (by column) of functions each of which
    takes a single argument and returns a formatted string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_format` default None, a function which takes a single (float) argument
    and returns a formatted string; to be applied to floats in the `DataFrame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparsify` default True, set to False for a `DataFrame` with a hierarchical
    index to print every MultiIndex key at each row.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_names` default True, will print the names of the indices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` default True, will print the index (ie, row labels)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`header` default True, will print the column labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`justify` default `left`, will print column headers left- or right-justified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Series` object also has a `to_string` method, but with only the `buf`,
    `na_rep`, `float_format` arguments. There is also a `length` argument which, if
    set to `True`, will additionally output the length of the Series.
  prefs: []
  type: TYPE_NORMAL
- en: '## JSON'
  prefs: []
  type: TYPE_NORMAL
- en: Read and write `JSON` format files and strings.
  prefs: []
  type: TYPE_NORMAL
- en: '### Writing JSON'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Series` or `DataFrame` can be converted to a valid JSON string. Use `to_json`
    with optional parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` : the pathname or buffer to write the output. This can be `None`
    in which case a JSON string is returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Series`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`date_format` : string, type of date conversion, ‘epoch’ for timestamp, ‘iso’
    for ISO8601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`double_precision` : The number of decimal places to use when encoding floating
    point values, default 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_ascii` : force encoded string to be ASCII, default True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : The time unit to encode to, governs timestamp and ISO8601 precision.
    One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds
    respectively. Default ‘ms’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_handler` : The handler to call if an object cannot otherwise be converted
    to a suitable format for JSON. Takes a single argument, which is the object to
    convert, and returns a serializable object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : If `records` orient, then will write each record per line as json.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : string, writer mode when writing to path. ‘w’ for write, ‘a’ for append.
    Default ‘w’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note `NaN`’s, `NaT`’s and `None` will be converted to `null` and `datetime`
    objects will be converted based on the `date_format` and `date_unit` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE502]'
  prefs: []
  type: TYPE_PRE
- en: Orient options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a number of different options for the format of the resulting JSON
    file / string. Consider the following `DataFrame` and `Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE503]'
  prefs: []
  type: TYPE_PRE
- en: '**Column oriented** (the default for `DataFrame`) serializes the data as nested
    JSON objects with column labels acting as the primary index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE504]'
  prefs: []
  type: TYPE_PRE
- en: '**Index oriented** (the default for `Series`) similar to column oriented but
    the index labels are now primary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE505]'
  prefs: []
  type: TYPE_PRE
- en: '**Record oriented** serializes the data to a JSON array of column -> value
    records, index labels are not included. This is useful for passing `DataFrame`
    data to plotting libraries, for example the JavaScript library `d3.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE506]'
  prefs: []
  type: TYPE_PRE
- en: '**Value oriented** is a bare-bones option which serializes to nested JSON arrays
    of values only, column and index labels are not included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE507]'
  prefs: []
  type: TYPE_PRE
- en: '**Split oriented** serializes to a JSON object containing separate entries
    for values, index and columns. Name is also included for `Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE508]'
  prefs: []
  type: TYPE_PRE
- en: '**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/),
    allowing for the preservation of metadata including but not limited to dtypes
    and index names.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Any orient option that encodes to a JSON object will not preserve the ordering
    of index and column labels during round-trip serialization. If you wish to preserve
    label ordering use the `split` option as it uses ordered containers.
  prefs: []
  type: TYPE_NORMAL
- en: Date handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Writing in ISO date format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE509]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing in ISO date format, with microseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE510]'
  prefs: []
  type: TYPE_PRE
- en: 'Epoch timestamps, in seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE511]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing to a file, with a date index and a date column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE512]'
  prefs: []
  type: TYPE_PRE
- en: Fallback behavior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the JSON serializer cannot handle the container contents directly it will
    fall back in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,
    if provided, will be called for each value, otherwise an exception is raised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'if an object is unsupported it will attempt the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check if the object has defined a `toDict` method and call it. A `toDict` method
    should return a `dict` which will then be JSON serialized.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: invoke the `default_handler` if one was provided.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: convert the object to a `dict` by traversing its contents. However this will
    often fail with an `OverflowError` or give unexpected results.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general the best approach for unsupported objects or dtypes is to provide
    a `default_handler`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE513]'
  prefs: []
  type: TYPE_PRE
- en: 'can be dealt with by specifying a simple `default_handler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE514]  ### Reading JSON'
  prefs: []
  type: TYPE_NORMAL
- en: Reading a JSON string to pandas object can take a number of parameters. The
    parser will try to parse a `DataFrame` if `typ` is not supplied or is `None`.
    To explicitly force `Series` parsing, pass `typ=series`
  prefs: []
  type: TYPE_NORMAL
- en: '`filepath_or_buffer` : a **VALID** JSON string or file handle / StringIO. The
    string could be a URL. Valid URL schemes include http, ftp, S3, and file. For
    file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typ` : type of object to recover (series or frame), default ‘frame’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Series :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`dtype` : if True, infer dtypes, if a dict of column to dtype, then use those,
    if `False`, then don’t infer dtypes at all, default is True, apply only to the
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_axes` : boolean, try to convert the axes to the proper dtypes, default
    is `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_dates` : a list of columns to parse for dates; If `True`, then try
    to parse date-like columns, default is `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_default_dates` : boolean, default `True`. If parsing dates, then parse
    the default date-like columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`precise_float` : boolean, default `False`. Set to enable usage of higher precision
    (strtod) function when decoding string to double values. Default (`False`) is
    to use fast but less precise builtin functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : string, the timestamp unit to detect if converting dates. Default
    None. By default the timestamp precision will be detected, if this is not desired
    then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force timestamp precision to seconds,
    milliseconds, microseconds or nanoseconds respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : reads file as one json object per line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding` : The encoding to use to decode py3 bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize` : when used in combination with `lines=True`, return a `pandas.api.typing.JsonReader`
    which reads in `chunksize` lines per iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`engine`: Either `"ujson"`, the built-in JSON parser, or `"pyarrow"` which
    dispatches to pyarrow’s `pyarrow.json.read_json`. The `"pyarrow"` is only available
    when `lines=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parser will raise one of `ValueError/TypeError/AssertionError` if the JSON
    is not parseable.
  prefs: []
  type: TYPE_NORMAL
- en: If a non-default `orient` was used when encoding to JSON be sure to pass the
    same option here so that decoding produces sensible results, see [Orient Options](#orient-options)
    for an overview.
  prefs: []
  type: TYPE_NORMAL
- en: Data conversion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will
    try to parse the axes, and all of the data into appropriate types, including dates.
    If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes`
    should only be set to `False` if you need to preserve string-like numbers (e.g.
    ‘1’, ‘2’) in an axes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Large integer values may be converted to dates if `convert_dates=True` and
    the data and / or column labels appear ‘date-like’. The exact threshold depends
    on the `date_unit` specified. ‘date-like’ means that the column label meets one
    of the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: it ends with `'_at'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it ends with `'_time'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it begins with `'timestamp'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'modified'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'date'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'When reading JSON data, automatic coercing into dtypes has some quirks:'
  prefs: []
  type: TYPE_NORMAL
- en: an index can be reconstructed in a different order from serialization, that
    is, the returned order is not guaranteed to be the same as before serialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a column that was `float` data will be converted to `integer` if it can be done
    safely, e.g. a column of `1.`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bool columns will be converted to `integer` on reconstruction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus there are times where you may want to specify specific dtypes via the `dtype`
    keyword argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading from a JSON string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE515]'
  prefs: []
  type: TYPE_PRE
- en: 'Reading from a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE516]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t convert any data (but still convert axes and dates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE517]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify dtypes for conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE518]'
  prefs: []
  type: TYPE_PRE
- en: 'Preserve string indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE519]'
  prefs: []
  type: TYPE_PRE
- en: 'Dates written in nanoseconds need to be read back in nanoseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE520]'
  prefs: []
  type: TYPE_PRE
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE521]  ### Normalization'
  prefs: []
  type: TYPE_NORMAL
- en: pandas provides a utility function to take a dict or list of dicts and *normalize*
    this semi-structured data into a flat table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE522]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE523]'
  prefs: []
  type: TYPE_PRE
- en: The max_level parameter provides more control over which level to end normalization.
    With max_level=1 the following snippet normalizes until 1st nesting level of the
    provided dict.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE524]  ### Line delimited json'
  prefs: []
  type: TYPE_NORMAL
- en: pandas is able to read and write line-delimited json files that are common in
    data processing pipelines using Hadoop or Spark.
  prefs: []
  type: TYPE_NORMAL
- en: For line-delimited json files, pandas can also return an iterator which reads
    in `chunksize` lines at a time. This can be useful for large files or to read
    from a stream.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE525]'
  prefs: []
  type: TYPE_PRE
- en: Line-limited json can also be read using the pyarrow reader by specifying `engine="pyarrow"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE526]'
  prefs: []
  type: TYPE_PRE
- en: 'New in version 2.0.0.  ### Table schema'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table Schema](https://specs.frictionlessdata.io/table-schema/) is a spec for
    describing tabular datasets as a JSON object. The JSON includes information on
    the field names, types, and other attributes. You can use the orient `table` to
    build a JSON string with two fields, `schema` and `data`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE527]'
  prefs: []
  type: TYPE_PRE
- en: The `schema` field contains the `fields` key, which itself contains a list of
    column name to type pairs, including the `Index` or `MultiIndex` (see below for
    a list of types). The `schema` field also contains a `primaryKey` field if the
    (Multi)index is unique.
  prefs: []
  type: TYPE_NORMAL
- en: The second field, `data`, contains the serialized data with the `records` orient.
    The index is included, and any datetimes are ISO 8601 formatted, as required by
    the Table Schema spec.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of types supported are described in the Table Schema spec. This
    table shows the mapping from pandas types:'
  prefs: []
  type: TYPE_NORMAL
- en: '| pandas type | Table Schema type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| int64 | integer |'
  prefs: []
  type: TYPE_TB
- en: '| float64 | number |'
  prefs: []
  type: TYPE_TB
- en: '| bool | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | datetime |'
  prefs: []
  type: TYPE_TB
- en: '| timedelta64[ns] | duration |'
  prefs: []
  type: TYPE_TB
- en: '| categorical | any |'
  prefs: []
  type: TYPE_TB
- en: '| object | str |'
  prefs: []
  type: TYPE_TB
- en: 'A few notes on the generated table schema:'
  prefs: []
  type: TYPE_NORMAL
- en: The `schema` object contains a `pandas_version` field. This contains the version
    of pandas’ dialect of the schema, and will be incremented with each revision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All dates are converted to UTC when serializing. Even timezone naive values,
    which are treated as UTC with an offset of 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE528]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: datetimes with a timezone (before serializing), include an additional field
    `tz` with the time zone name (e.g. `'US/Central'`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE529]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Periods are converted to timestamps before serialization, and so have the same
    behavior of being converted to UTC. In addition, periods will contain and additional
    field `freq` with the period’s frequency, e.g. `'A-DEC'`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE530]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Categoricals use the `any` type and an `enum` constraint listing the set of
    possible values. Additionally, an `ordered` field is included:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE531]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A `primaryKey` field, containing an array of labels, is included *if the index
    is unique*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE532]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `primaryKey` behavior is the same with MultiIndexes, but in this case the
    `primaryKey` is an array:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE533]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The default naming roughly follows these rules:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For series, the `object.name` is used. If that’s none, then the name is `values`
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `DataFrames`, the stringified version of the column name is used
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `Index` (not `MultiIndex`), `index.name` is used, with a fallback to `index`
    if that is None.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `MultiIndex`, `mi.names` is used. If any level has no name, then `level_<i>`
    is used.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_json` also accepts `orient=''table''` as an argument. This allows for
    the preservation of metadata such as dtypes and index names in a round-trippable
    manner.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE534]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the literal string ‘index’ as the name of an [`Index`](../reference/api/pandas.Index.html#pandas.Index
    "pandas.Index") is not round-trippable, nor are any names beginning with `'level_'`
    within a [`MultiIndex`](../reference/api/pandas.MultiIndex.html#pandas.MultiIndex
    "pandas.MultiIndex"). These are used by default in [`DataFrame.to_json()`](../reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json
    "pandas.DataFrame.to_json") to indicate missing values and the subsequent read
    cannot distinguish the intent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE535]'
  prefs: []
  type: TYPE_PRE
- en: When using `orient='table'` along with user-defined `ExtensionArray`, the generated
    schema will contain an additional `extDtype` key in the respective `fields` element.
    This extra key is not standard but does enable JSON roundtrips for extension types
    (e.g. `read_json(df.to_json(orient="table"), orient="table")`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `extDtype` key carries the name of the extension, if you have properly
    registered the `ExtensionDtype`, pandas will use said name to perform a lookup
    into the registry and re-convert the serialized data into your custom dtype.  ###
    Writing JSON'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Series` or `DataFrame` can be converted to a valid JSON string. Use `to_json`
    with optional parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`path_or_buf` : the pathname or buffer to write the output. This can be `None`
    in which case a JSON string is returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Series`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`date_format` : string, type of date conversion, ‘epoch’ for timestamp, ‘iso’
    for ISO8601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`double_precision` : The number of decimal places to use when encoding floating
    point values, default 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_ascii` : force encoded string to be ASCII, default True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : The time unit to encode to, governs timestamp and ISO8601 precision.
    One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds
    respectively. Default ‘ms’.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default_handler` : The handler to call if an object cannot otherwise be converted
    to a suitable format for JSON. Takes a single argument, which is the object to
    convert, and returns a serializable object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : If `records` orient, then will write each record per line as json.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode` : string, writer mode when writing to path. ‘w’ for write, ‘a’ for append.
    Default ‘w’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note `NaN`’s, `NaT`’s and `None` will be converted to `null` and `datetime`
    objects will be converted based on the `date_format` and `date_unit` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE536]'
  prefs: []
  type: TYPE_PRE
- en: Orient options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a number of different options for the format of the resulting JSON
    file / string. Consider the following `DataFrame` and `Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE537]'
  prefs: []
  type: TYPE_PRE
- en: '**Column oriented** (the default for `DataFrame`) serializes the data as nested
    JSON objects with column labels acting as the primary index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE538]'
  prefs: []
  type: TYPE_PRE
- en: '**Index oriented** (the default for `Series`) similar to column oriented but
    the index labels are now primary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE539]'
  prefs: []
  type: TYPE_PRE
- en: '**Record oriented** serializes the data to a JSON array of column -> value
    records, index labels are not included. This is useful for passing `DataFrame`
    data to plotting libraries, for example the JavaScript library `d3.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE540]'
  prefs: []
  type: TYPE_PRE
- en: '**Value oriented** is a bare-bones option which serializes to nested JSON arrays
    of values only, column and index labels are not included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE541]'
  prefs: []
  type: TYPE_PRE
- en: '**Split oriented** serializes to a JSON object containing separate entries
    for values, index and columns. Name is also included for `Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE542]'
  prefs: []
  type: TYPE_PRE
- en: '**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/),
    allowing for the preservation of metadata including but not limited to dtypes
    and index names.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Any orient option that encodes to a JSON object will not preserve the ordering
    of index and column labels during round-trip serialization. If you wish to preserve
    label ordering use the `split` option as it uses ordered containers.
  prefs: []
  type: TYPE_NORMAL
- en: Date handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Writing in ISO date format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE543]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing in ISO date format, with microseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE544]'
  prefs: []
  type: TYPE_PRE
- en: 'Epoch timestamps, in seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE545]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing to a file, with a date index and a date column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE546]'
  prefs: []
  type: TYPE_PRE
- en: Fallback behavior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the JSON serializer cannot handle the container contents directly it will
    fall back in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,
    if provided, will be called for each value, otherwise an exception is raised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'if an object is unsupported it will attempt the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check if the object has defined a `toDict` method and call it. A `toDict` method
    should return a `dict` which will then be JSON serialized.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: invoke the `default_handler` if one was provided.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: convert the object to a `dict` by traversing its contents. However this will
    often fail with an `OverflowError` or give unexpected results.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general the best approach for unsupported objects or dtypes is to provide
    a `default_handler`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE547]'
  prefs: []
  type: TYPE_PRE
- en: 'can be dealt with by specifying a simple `default_handler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE548]'
  prefs: []
  type: TYPE_PRE
- en: Orient options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a number of different options for the format of the resulting JSON
    file / string. Consider the following `DataFrame` and `Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE549]'
  prefs: []
  type: TYPE_PRE
- en: '**Column oriented** (the default for `DataFrame`) serializes the data as nested
    JSON objects with column labels acting as the primary index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE550]'
  prefs: []
  type: TYPE_PRE
- en: '**Index oriented** (the default for `Series`) similar to column oriented but
    the index labels are now primary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE551]'
  prefs: []
  type: TYPE_PRE
- en: '**Record oriented** serializes the data to a JSON array of column -> value
    records, index labels are not included. This is useful for passing `DataFrame`
    data to plotting libraries, for example the JavaScript library `d3.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE552]'
  prefs: []
  type: TYPE_PRE
- en: '**Value oriented** is a bare-bones option which serializes to nested JSON arrays
    of values only, column and index labels are not included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE553]'
  prefs: []
  type: TYPE_PRE
- en: '**Split oriented** serializes to a JSON object containing separate entries
    for values, index and columns. Name is also included for `Series`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE554]'
  prefs: []
  type: TYPE_PRE
- en: '**Table oriented** serializes to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/),
    allowing for the preservation of metadata including but not limited to dtypes
    and index names.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Any orient option that encodes to a JSON object will not preserve the ordering
    of index and column labels during round-trip serialization. If you wish to preserve
    label ordering use the `split` option as it uses ordered containers.
  prefs: []
  type: TYPE_NORMAL
- en: Date handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Writing in ISO date format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE555]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing in ISO date format, with microseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE556]'
  prefs: []
  type: TYPE_PRE
- en: 'Epoch timestamps, in seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE557]'
  prefs: []
  type: TYPE_PRE
- en: 'Writing to a file, with a date index and a date column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE558]'
  prefs: []
  type: TYPE_PRE
- en: Fallback behavior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the JSON serializer cannot handle the container contents directly it will
    fall back in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: if the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,
    if provided, will be called for each value, otherwise an exception is raised.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'if an object is unsupported it will attempt the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: check if the object has defined a `toDict` method and call it. A `toDict` method
    should return a `dict` which will then be JSON serialized.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: invoke the `default_handler` if one was provided.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: convert the object to a `dict` by traversing its contents. However this will
    often fail with an `OverflowError` or give unexpected results.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general the best approach for unsupported objects or dtypes is to provide
    a `default_handler`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE559]'
  prefs: []
  type: TYPE_PRE
- en: 'can be dealt with by specifying a simple `default_handler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE560]'
  prefs: []
  type: TYPE_PRE
- en: '### Reading JSON'
  prefs: []
  type: TYPE_NORMAL
- en: Reading a JSON string to pandas object can take a number of parameters. The
    parser will try to parse a `DataFrame` if `typ` is not supplied or is `None`.
    To explicitly force `Series` parsing, pass `typ=series`
  prefs: []
  type: TYPE_NORMAL
- en: '`filepath_or_buffer` : a **VALID** JSON string or file handle / StringIO. The
    string could be a URL. Valid URL schemes include http, ftp, S3, and file. For
    file URLs, a host is expected. For instance, a local file could be file ://localhost/path/to/table.json'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typ` : type of object to recover (series or frame), default ‘frame’'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orient` :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Series :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `index`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: default is `columns`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: allowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The format of the JSON string
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| `split` | dict like {index -> [index], columns -> [columns], data -> [values]}
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `records` | list like [{column -> value}, … , {column -> value}] |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `index` | dict like {index -> {column -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `columns` | dict like {column -> {index -> value}} |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `values` | just the values array |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| `table` | adhering to the JSON [Table Schema](https://specs.frictionlessdata.io/table-schema/)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '`dtype` : if True, infer dtypes, if a dict of column to dtype, then use those,
    if `False`, then don’t infer dtypes at all, default is True, apply only to the
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_axes` : boolean, try to convert the axes to the proper dtypes, default
    is `True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convert_dates` : a list of columns to parse for dates; If `True`, then try
    to parse date-like columns, default is `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_default_dates` : boolean, default `True`. If parsing dates, then parse
    the default date-like columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`precise_float` : boolean, default `False`. Set to enable usage of higher precision
    (strtod) function when decoding string to double values. Default (`False`) is
    to use fast but less precise builtin functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date_unit` : string, the timestamp unit to detect if converting dates. Default
    None. By default the timestamp precision will be detected, if this is not desired
    then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force timestamp precision to seconds,
    milliseconds, microseconds or nanoseconds respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lines` : reads file as one json object per line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding` : The encoding to use to decode py3 bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunksize` : when used in combination with `lines=True`, return a `pandas.api.typing.JsonReader`
    which reads in `chunksize` lines per iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`engine`: Either `"ujson"`, the built-in JSON parser, or `"pyarrow"` which
    dispatches to pyarrow’s `pyarrow.json.read_json`. The `"pyarrow"` is only available
    when `lines=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parser will raise one of `ValueError/TypeError/AssertionError` if the JSON
    is not parseable.
  prefs: []
  type: TYPE_NORMAL
- en: If a non-default `orient` was used when encoding to JSON be sure to pass the
    same option here so that decoding produces sensible results, see [Orient Options](#orient-options)
    for an overview.
  prefs: []
  type: TYPE_NORMAL
- en: Data conversion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will
    try to parse the axes, and all of the data into appropriate types, including dates.
    If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes`
    should only be set to `False` if you need to preserve string-like numbers (e.g.
    ‘1’, ‘2’) in an axes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Large integer values may be converted to dates if `convert_dates=True` and
    the data and / or column labels appear ‘date-like’. The exact threshold depends
    on the `date_unit` specified. ‘date-like’ means that the column label meets one
    of the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: it ends with `'_at'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it ends with `'_time'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it begins with `'timestamp'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'modified'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'date'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'When reading JSON data, automatic coercing into dtypes has some quirks:'
  prefs: []
  type: TYPE_NORMAL
- en: an index can be reconstructed in a different order from serialization, that
    is, the returned order is not guaranteed to be the same as before serialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a column that was `float` data will be converted to `integer` if it can be done
    safely, e.g. a column of `1.`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bool columns will be converted to `integer` on reconstruction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus there are times where you may want to specify specific dtypes via the `dtype`
    keyword argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading from a JSON string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE561]'
  prefs: []
  type: TYPE_PRE
- en: 'Reading from a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE562]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t convert any data (but still convert axes and dates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE563]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify dtypes for conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE564]'
  prefs: []
  type: TYPE_PRE
- en: 'Preserve string indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE565]'
  prefs: []
  type: TYPE_PRE
- en: 'Dates written in nanoseconds need to be read back in nanoseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE566]'
  prefs: []
  type: TYPE_PRE
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE567]'
  prefs: []
  type: TYPE_PRE
- en: Data conversion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The default of `convert_axes=True`, `dtype=True`, and `convert_dates=True` will
    try to parse the axes, and all of the data into appropriate types, including dates.
    If you need to override specific dtypes, pass a dict to `dtype`. `convert_axes`
    should only be set to `False` if you need to preserve string-like numbers (e.g.
    ‘1’, ‘2’) in an axes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Large integer values may be converted to dates if `convert_dates=True` and
    the data and / or column labels appear ‘date-like’. The exact threshold depends
    on the `date_unit` specified. ‘date-like’ means that the column label meets one
    of the following criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: it ends with `'_at'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it ends with `'_time'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it begins with `'timestamp'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'modified'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is `'date'`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'When reading JSON data, automatic coercing into dtypes has some quirks:'
  prefs: []
  type: TYPE_NORMAL
- en: an index can be reconstructed in a different order from serialization, that
    is, the returned order is not guaranteed to be the same as before serialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a column that was `float` data will be converted to `integer` if it can be done
    safely, e.g. a column of `1.`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bool columns will be converted to `integer` on reconstruction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus there are times where you may want to specify specific dtypes via the `dtype`
    keyword argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading from a JSON string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE568]'
  prefs: []
  type: TYPE_PRE
- en: 'Reading from a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE569]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t convert any data (but still convert axes and dates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE570]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify dtypes for conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE571]'
  prefs: []
  type: TYPE_PRE
- en: 'Preserve string indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE572]'
  prefs: []
  type: TYPE_PRE
- en: 'Dates written in nanoseconds need to be read back in nanoseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE573]'
  prefs: []
  type: TYPE_PRE
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE574]'
  prefs: []
  type: TYPE_PRE
- en: '### Normalization'
  prefs: []
  type: TYPE_NORMAL
- en: pandas provides a utility function to take a dict or list of dicts and *normalize*
    this semi-structured data into a flat table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE575]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE576]'
  prefs: []
  type: TYPE_PRE
- en: The max_level parameter provides more control over which level to end normalization.
    With max_level=1 the following snippet normalizes until 1st nesting level of the
    provided dict.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE577]'
  prefs: []
  type: TYPE_PRE
- en: '### Line delimited json'
  prefs: []
  type: TYPE_NORMAL
- en: pandas is able to read and write line-delimited json files that are common in
    data processing pipelines using Hadoop or Spark.
  prefs: []
  type: TYPE_NORMAL
- en: For line-delimited json files, pandas can also return an iterator which reads
    in `chunksize` lines at a time. This can be useful for large files or to read
    from a stream.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE578]'
  prefs: []
  type: TYPE_PRE
- en: Line-limited json can also be read using the pyarrow reader by specifying `engine="pyarrow"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE579]'
  prefs: []
  type: TYPE_PRE
- en: New in version 2.0.0.
  prefs: []
  type: TYPE_NORMAL
- en: '### Table schema'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table Schema](https://specs.frictionlessdata.io/table-schema/) is a spec for
    describing tabular datasets as a JSON object. The JSON includes information on
    the field names, types, and other attributes. You can use the orient `table` to
    build a JSON string with two fields, `schema` and `data`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE580]'
  prefs: []
  type: TYPE_PRE
- en: The `schema` field contains the `fields` key, which itself contains a list of
    column name to type pairs, including the `Index` or `MultiIndex` (see below for
    a list of types). The `schema` field also contains a `primaryKey` field if the
    (Multi)index is unique.
  prefs: []
  type: TYPE_NORMAL
- en: The second field, `data`, contains the serialized data with the `records` orient.
    The index is included, and any datetimes are ISO 8601 formatted, as required by
    the Table Schema spec.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of types supported are described in the Table Schema spec. This
    table shows the mapping from pandas types:'
  prefs: []
  type: TYPE_NORMAL
- en: '| pandas type | Table Schema type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| int64 | integer |'
  prefs: []
  type: TYPE_TB
- en: '| float64 | number |'
  prefs: []
  type: TYPE_TB
- en: '| bool | boolean |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | datetime |'
  prefs: []
  type: TYPE_TB
- en: '| timedelta64[ns] | duration |'
  prefs: []
  type: TYPE_TB
- en: '| categorical | any |'
  prefs: []
  type: TYPE_TB
- en: '| object | str |'
  prefs: []
  type: TYPE_TB
- en: 'A few notes on the generated table schema:'
  prefs: []
  type: TYPE_NORMAL
- en: The `schema` object contains a `pandas_version` field. This contains the version
    of pandas’ dialect of the schema, and will be incremented with each revision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All dates are converted to UTC when serializing. Even timezone naive values,
    which are treated as UTC with an offset of 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE581]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: datetimes with a timezone (before serializing), include an additional field
    `tz` with the time zone name (e.g. `'US/Central'`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE582]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Periods are converted to timestamps before serialization, and so have the same
    behavior of being converted to UTC. In addition, periods will contain and additional
    field `freq` with the period’s frequency, e.g. `'A-DEC'`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE583]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Categoricals use the `any` type and an `enum` constraint listing the set of
    possible values. Additionally, an `ordered` field is included:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE584]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A `primaryKey` field, containing an array of labels, is included *if the index
    is unique*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE585]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `primaryKey` behavior is the same with MultiIndexes, but in this case the
    `primaryKey` is an array:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE586]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The default naming roughly follows these rules:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For series, the `object.name` is used. If that’s none, then the name is `values`
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `DataFrames`, the stringified version of the column name is used
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `Index` (not `MultiIndex`), `index.name` is used, with a fallback to `index`
    if that is None.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: For `MultiIndex`, `mi.names` is used. If any level has no name, then `level_<i>`
    is used.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_json` also accepts `orient=''table''` as an argument. This allows for
    the preservation of metadata such as dtypes and index names in a round-trippable
    manner.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE587]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the literal string ‘index’ as the name of an [`Index`](../reference/api/pandas.Index.html#pandas.Index
    "pandas.Index") is not round-trippable, nor are any names beginning with `'level_'`
    within a [`MultiIndex`](../reference/api/pandas.MultiIndex.html#pandas.MultiIndex
    "pandas.MultiIndex"). These are used by default in [`DataFrame.to_json()`](../reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json
    "pandas.DataFrame.to_json") to indicate missing values and the subsequent read
    cannot distinguish the intent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE588]'
  prefs: []
  type: TYPE_PRE
- en: When using `orient='table'` along with user-defined `ExtensionArray`, the generated
    schema will contain an additional `extDtype` key in the respective `fields` element.
    This extra key is not standard but does enable JSON roundtrips for extension types
    (e.g. `read_json(df.to_json(orient="table"), orient="table")`).
  prefs: []
  type: TYPE_NORMAL
- en: The `extDtype` key carries the name of the extension, if you have properly registered
    the `ExtensionDtype`, pandas will use said name to perform a lookup into the registry
    and re-convert the serialized data into your custom dtype.
  prefs: []
  type: TYPE_NORMAL
- en: HTML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### Reading HTML content'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: We **highly encourage** you to read the [HTML Table Parsing gotchas](#io-html-gotchas)
    below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_html()` function can accept an HTML string/file/URL and
    will parse HTML tables into list of pandas `DataFrames`. Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`read_html` returns a `list` of `DataFrame` objects, even if there is only
    a single table contained in the HTML content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL with no options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE589]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The data from the above URL changes every Monday so the resulting data above
    may be slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL while passing headers alongside the HTTP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE590]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We see above that the headers we passed are reflected in the HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read in the content of the file from the above URL and pass it to `read_html`
    as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE591]'
  prefs: []
  type: TYPE_PRE
- en: 'You can even pass in an instance of `StringIO` if you so desire:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE592]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The following examples are not run by the IPython evaluator due to the fact
    that having so many network-accessing functions slows down the documentation build.
    If you spot an error or an example that doesn’t run, please do not hesitate to
    report it over on [pandas GitHub issues page](https://github.com/pandas-dev/pandas/issues).
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL and match a table that contains specific text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE593]'
  prefs: []
  type: TYPE_PRE
- en: Specify a header row (by default `<th>` or `<td>` elements located within a
    `<thead>` are used to form the column index, if multiple rows are contained within
    `<thead>` then a MultiIndex is created); if specified, the header row is taken
    from the data minus the parsed header elements (`<th>` elements).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE594]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify an index column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE595]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a number of rows to skip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE596]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a number of rows to skip using a list (`range` works as well):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE597]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify an HTML attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE598]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify values that should be converted to NaN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE599]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify whether to keep the default set of NaN values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE600]'
  prefs: []
  type: TYPE_PRE
- en: Specify converters for columns. This is useful for numerical text data that
    has leading zeros. By default columns that are numerical are cast to numeric types
    and the leading zeros are lost. To avoid this, we can convert these columns to
    strings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE601]'
  prefs: []
  type: TYPE_PRE
- en: 'Use some combination of the above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE602]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in pandas `to_html` output (with some loss of floating point precision):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE603]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lxml` backend will raise an error on a failed parse if that is the only
    parser you provide. If you only have a single parser you can provide just a string,
    but it is considered good practice to pass a list with one string if, for example,
    the function expects a sequence of strings. You may use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE604]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you could pass `flavor=''lxml''` without a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE605]'
  prefs: []
  type: TYPE_PRE
- en: However, if you have bs4 and html5lib installed and pass `None` or `['lxml',
    'bs4']` then the parse will most likely succeed. Note that *as soon as a parse
    succeeds, the function will return*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE606]'
  prefs: []
  type: TYPE_PRE
- en: Links can be extracted from cells along with the text using `extract_links="all"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE607]'
  prefs: []
  type: TYPE_PRE
- en: 'New in version 1.5.0.  ### Writing to HTML files'
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_html` which renders the contents
    of the `DataFrame` as an HTML table. The function arguments are as in the method
    `to_string` described above.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Not all of the possible options for `DataFrame.to_html` are shown here for brevity’s
    sake. See `DataFrame.to_html()` for the full set of options.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In an HTML-rendering supported environment like a Jupyter Notebook, `display(HTML(...))``
    will render the raw HTML into the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE608]'
  prefs: []
  type: TYPE_PRE
- en: 'The `columns` argument will limit the columns shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE609]'
  prefs: []
  type: TYPE_PRE
- en: '`float_format` takes a Python callable to control the precision of floating
    point values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE610]'
  prefs: []
  type: TYPE_PRE
- en: '`bold_rows` will make the row labels bold by default, but you can turn that
    off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE611]'
  prefs: []
  type: TYPE_PRE
- en: The `classes` argument provides the ability to give the resulting HTML table
    CSS classes. Note that these classes are *appended* to the existing `'dataframe'`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE612]'
  prefs: []
  type: TYPE_PRE
- en: The `render_links` argument provides the ability to add hyperlinks to cells
    that contain URLs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE613]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the `escape` argument allows you to control whether the “<”, “>” and
    “&” characters escaped in the resulting HTML (by default it is `True`). So to
    get the HTML without escaped characters pass `escape=False`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE614]'
  prefs: []
  type: TYPE_PRE
- en: 'Escaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE615]'
  prefs: []
  type: TYPE_PRE
- en: 'Not escaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE616]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Some browsers may not show a difference in the rendering of the previous two
    HTML tables.  ### HTML Table Parsing Gotchas'
  prefs: []
  type: TYPE_NORMAL
- en: There are some versioning issues surrounding the libraries that are used to
    parse HTML tables in the top-level pandas io function `read_html`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Issues with** [**lxml**](https://lxml.de)'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) is very fast.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) requires Cython to install correctly.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) does *not* make any guarantees about the results
    of its parse *unless* it is given [**strictly valid markup**](https://validator.w3.org/docs/help.html#validation_basics).'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: In light of the above, we have chosen to allow you, the user, to use the [**lxml**](https://lxml.de)
    backend, but **this backend will use** [**html5lib**](https://github.com/html5lib/html5lib-python)
    if [**lxml**](https://lxml.de) fails to parse
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: It is therefore *highly recommended* that you install both [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    and [**html5lib**](https://github.com/html5lib/html5lib-python), so that you will
    still get a valid result (provided everything else is valid) even if [**lxml**](https://lxml.de)
    fails.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**lxml**](https://lxml.de) **as a backend**'
  prefs: []
  type: TYPE_NORMAL
- en: The above issues hold here as well since [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    is essentially just a wrapper around a parser backend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**html5lib**](https://github.com/html5lib/html5lib-python) **as a backend**'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is far more lenient
    than [**lxml**](https://lxml.de) and consequently deals with *real-life markup*
    in a much saner way rather than just, e.g., dropping an element without notifying
    you.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) *generates valid
    HTML5 markup from invalid markup automatically*. This is extremely important for
    parsing HTML tables, since it guarantees a valid document. However, that does
    NOT mean that it is “correct”, since the process of fixing markup does not have
    a single definition.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is pure Python
    and requires no additional build steps beyond its own installation.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The biggest drawback to using [**html5lib**](https://github.com/html5lib/html5lib-python)
    is that it is slow as molasses. However consider the fact that many tables on
    the web are not big enough for the parsing algorithm runtime to matter. It is
    more likely that the bottleneck will be in the process of reading the raw text
    from the URL over the web, i.e., IO (input-output). For very large tables, this
    might not be true.  ### Reading HTML content'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: We **highly encourage** you to read the [HTML Table Parsing gotchas](#io-html-gotchas)
    below regarding the issues surrounding the BeautifulSoup4/html5lib/lxml parsers.
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_html()` function can accept an HTML string/file/URL and
    will parse HTML tables into list of pandas `DataFrames`. Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`read_html` returns a `list` of `DataFrame` objects, even if there is only
    a single table contained in the HTML content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL with no options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE617]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The data from the above URL changes every Monday so the resulting data above
    may be slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL while passing headers alongside the HTTP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE618]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We see above that the headers we passed are reflected in the HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read in the content of the file from the above URL and pass it to `read_html`
    as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE619]'
  prefs: []
  type: TYPE_PRE
- en: 'You can even pass in an instance of `StringIO` if you so desire:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE620]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The following examples are not run by the IPython evaluator due to the fact
    that having so many network-accessing functions slows down the documentation build.
    If you spot an error or an example that doesn’t run, please do not hesitate to
    report it over on [pandas GitHub issues page](https://github.com/pandas-dev/pandas/issues).
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a URL and match a table that contains specific text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE621]'
  prefs: []
  type: TYPE_PRE
- en: Specify a header row (by default `<th>` or `<td>` elements located within a
    `<thead>` are used to form the column index, if multiple rows are contained within
    `<thead>` then a MultiIndex is created); if specified, the header row is taken
    from the data minus the parsed header elements (`<th>` elements).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE622]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify an index column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE623]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a number of rows to skip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE624]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a number of rows to skip using a list (`range` works as well):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE625]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify an HTML attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE626]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify values that should be converted to NaN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE627]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify whether to keep the default set of NaN values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE628]'
  prefs: []
  type: TYPE_PRE
- en: Specify converters for columns. This is useful for numerical text data that
    has leading zeros. By default columns that are numerical are cast to numeric types
    and the leading zeros are lost. To avoid this, we can convert these columns to
    strings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE629]'
  prefs: []
  type: TYPE_PRE
- en: 'Use some combination of the above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE630]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in pandas `to_html` output (with some loss of floating point precision):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE631]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lxml` backend will raise an error on a failed parse if that is the only
    parser you provide. If you only have a single parser you can provide just a string,
    but it is considered good practice to pass a list with one string if, for example,
    the function expects a sequence of strings. You may use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE632]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you could pass `flavor=''lxml''` without a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE633]'
  prefs: []
  type: TYPE_PRE
- en: However, if you have bs4 and html5lib installed and pass `None` or `['lxml',
    'bs4']` then the parse will most likely succeed. Note that *as soon as a parse
    succeeds, the function will return*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE634]'
  prefs: []
  type: TYPE_PRE
- en: Links can be extracted from cells along with the text using `extract_links="all"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE635]'
  prefs: []
  type: TYPE_PRE
- en: New in version 1.5.0.
  prefs: []
  type: TYPE_NORMAL
- en: '### Writing to HTML files'
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_html` which renders the contents
    of the `DataFrame` as an HTML table. The function arguments are as in the method
    `to_string` described above.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Not all of the possible options for `DataFrame.to_html` are shown here for brevity’s
    sake. See `DataFrame.to_html()` for the full set of options.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In an HTML-rendering supported environment like a Jupyter Notebook, `display(HTML(...))``
    will render the raw HTML into the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE636]'
  prefs: []
  type: TYPE_PRE
- en: 'The `columns` argument will limit the columns shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE637]'
  prefs: []
  type: TYPE_PRE
- en: '`float_format` takes a Python callable to control the precision of floating
    point values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE638]'
  prefs: []
  type: TYPE_PRE
- en: '`bold_rows` will make the row labels bold by default, but you can turn that
    off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE639]'
  prefs: []
  type: TYPE_PRE
- en: The `classes` argument provides the ability to give the resulting HTML table
    CSS classes. Note that these classes are *appended* to the existing `'dataframe'`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE640]'
  prefs: []
  type: TYPE_PRE
- en: The `render_links` argument provides the ability to add hyperlinks to cells
    that contain URLs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE641]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the `escape` argument allows you to control whether the “<”, “>” and
    “&” characters escaped in the resulting HTML (by default it is `True`). So to
    get the HTML without escaped characters pass `escape=False`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE642]'
  prefs: []
  type: TYPE_PRE
- en: 'Escaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE643]'
  prefs: []
  type: TYPE_PRE
- en: 'Not escaped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE644]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Some browsers may not show a difference in the rendering of the previous two
    HTML tables.
  prefs: []
  type: TYPE_NORMAL
- en: '### HTML Table Parsing Gotchas'
  prefs: []
  type: TYPE_NORMAL
- en: There are some versioning issues surrounding the libraries that are used to
    parse HTML tables in the top-level pandas io function `read_html`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Issues with** [**lxml**](https://lxml.de)'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) is very fast.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) requires Cython to install correctly.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**lxml**](https://lxml.de) does *not* make any guarantees about the results
    of its parse *unless* it is given [**strictly valid markup**](https://validator.w3.org/docs/help.html#validation_basics).'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: In light of the above, we have chosen to allow you, the user, to use the [**lxml**](https://lxml.de)
    backend, but **this backend will use** [**html5lib**](https://github.com/html5lib/html5lib-python)
    if [**lxml**](https://lxml.de) fails to parse
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: It is therefore *highly recommended* that you install both [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    and [**html5lib**](https://github.com/html5lib/html5lib-python), so that you will
    still get a valid result (provided everything else is valid) even if [**lxml**](https://lxml.de)
    fails.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**lxml**](https://lxml.de) **as a backend**'
  prefs: []
  type: TYPE_NORMAL
- en: The above issues hold here as well since [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    is essentially just a wrapper around a parser backend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Issues with** [**BeautifulSoup4**](https://www.crummy.com/software/BeautifulSoup)
    **using** [**html5lib**](https://github.com/html5lib/html5lib-python) **as a backend**'
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is far more lenient
    than [**lxml**](https://lxml.de) and consequently deals with *real-life markup*
    in a much saner way rather than just, e.g., dropping an element without notifying
    you.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) *generates valid
    HTML5 markup from invalid markup automatically*. This is extremely important for
    parsing HTML tables, since it guarantees a valid document. However, that does
    NOT mean that it is “correct”, since the process of fixing markup does not have
    a single definition.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**html5lib**](https://github.com/html5lib/html5lib-python) is pure Python
    and requires no additional build steps beyond its own installation.'
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The biggest drawback to using [**html5lib**](https://github.com/html5lib/html5lib-python)
    is that it is slow as molasses. However consider the fact that many tables on
    the web are not big enough for the parsing algorithm runtime to matter. It is
    more likely that the bottleneck will be in the process of reading the raw text
    from the URL over the web, i.e., IO (input-output). For very large tables, this
    might not be true.
  prefs:
  - PREF_IND
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: '## LaTeX'
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: Currently there are no methods to read from LaTeX, only output methods.
  prefs: []
  type: TYPE_NORMAL
- en: Writing to LaTeX files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame *and* Styler objects currently have a `to_latex` method. We recommend
    using the [Styler.to_latex()](../reference/api/pandas.io.formats.style.Styler.to_latex.html)
    method over [DataFrame.to_latex()](../reference/api/pandas.DataFrame.to_latex.html)
    due to the former’s greater flexibility with conditional styling, and the latter’s
    possible future deprecation.
  prefs: []
  type: TYPE_NORMAL
- en: Review the documentation for [Styler.to_latex](../reference/api/pandas.io.formats.style.Styler.to_latex.html),
    which gives examples of conditional styling and explains the operation of its
    keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: For simple application the following pattern is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE645]'
  prefs: []
  type: TYPE_PRE
- en: To format values before output, chain the [Styler.format](../reference/api/pandas.io.formats.style.Styler.format.html)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE646]'
  prefs: []
  type: TYPE_PRE
- en: Writing to LaTeX files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame *and* Styler objects currently have a `to_latex` method. We recommend
    using the [Styler.to_latex()](../reference/api/pandas.io.formats.style.Styler.to_latex.html)
    method over [DataFrame.to_latex()](../reference/api/pandas.DataFrame.to_latex.html)
    due to the former’s greater flexibility with conditional styling, and the latter’s
    possible future deprecation.
  prefs: []
  type: TYPE_NORMAL
- en: Review the documentation for [Styler.to_latex](../reference/api/pandas.io.formats.style.Styler.to_latex.html),
    which gives examples of conditional styling and explains the operation of its
    keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: For simple application the following pattern is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE647]'
  prefs: []
  type: TYPE_PRE
- en: To format values before output, chain the [Styler.format](../reference/api/pandas.io.formats.style.Styler.format.html)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE648]'
  prefs: []
  type: TYPE_PRE
- en: XML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### Reading XML'
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_xml()` function can accept an XML string/file/URL and will
    parse nodes and attributes into a pandas `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since there is no standard XML structure where design types can vary in many
    ways, `read_xml` works best with flatter, shallow versions. If an XML document
    is deeply nested, use the `stylesheet` feature to transform XML into a flatter
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read an XML string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE649]'
  prefs: []
  type: TYPE_PRE
- en: 'Read a URL with no options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE650]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the content of the “books.xml” file and pass it to `read_xml` as a
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE651]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the content of the “books.xml” as instance of `StringIO` or `BytesIO`
    and pass it to `read_xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE652]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE653]'
  prefs: []
  type: TYPE_PRE
- en: 'Even read XML from AWS S3 buckets such as NIH NCBI PMC Article Datasets providing
    Biomedical and Life Science Jorurnals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE654]'
  prefs: []
  type: TYPE_PRE
- en: 'With [lxml](https://lxml.de) as default `parser`, you access the full-featured
    XML library that extends Python’s ElementTree API. One powerful tool is ability
    to query nodes selectively or conditionally with more expressive XPath:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE655]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify only elements or only attributes to parse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE656]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE657]'
  prefs: []
  type: TYPE_PRE
- en: XML documents can have namespaces with prefixes and default namespaces without
    prefixes both of which are denoted with a special attribute `xmlns`. In order
    to parse by node under a namespace context, `xpath` must reference a prefix.
  prefs: []
  type: TYPE_NORMAL
- en: For example, below XML contains a namespace with prefix, `doc`, and URI at `https://example.com`.
    In order to parse `doc:row` nodes, `namespaces` must be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE658]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, an XML document can have a default namespace without prefix. Failing
    to assign a temporary prefix will return no nodes and raise a `ValueError`. But
    assigning *any* temporary name to correct URI allows parsing by nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE659]'
  prefs: []
  type: TYPE_PRE
- en: However, if XPath does not reference node names such as default, `/*`, then
    `namespaces` is not required.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since `xpath` identifies the parent of content to be parsed, only immediate
    desendants which include child nodes or current attributes are parsed. Therefore,
    `read_xml` will not parse the text of grandchildren or other descendants and will
    not parse attributes of any descendant. To retrieve lower level content, adjust
    xpath to lower level. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE660]'
  prefs: []
  type: TYPE_PRE
- en: shows the attribute `sides` on `shape` element was not parsed as expected since
    this attribute resides on the child of `row` element and not `row` element itself.
    In other words, `sides` attribute is a grandchild level descendant of `row` element.
    However, the `xpath` targets `row` element which covers only its children and
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: With [lxml](https://lxml.de) as parser, you can flatten nested XML documents
    with an XSLT script which also can be string/file/URL types. As background, [XSLT](https://www.w3.org/TR/xslt/)
    is a special-purpose language written in a special XML file that can transform
    original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using
    an XSLT processor.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider this somewhat nested structure of Chicago “L” Rides where
    station and rides elements encapsulate data in their own sections. With below
    XSLT, `lxml` can transform original nested document into a flatter output (as
    shown below for demonstration) for easier parse into `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE661]'
  prefs: []
  type: TYPE_PRE
- en: For very large XML files that can range in hundreds of megabytes to gigabytes,
    [`pandas.read_xml()`](../reference/api/pandas.read_xml.html#pandas.read_xml "pandas.read_xml")
    supports parsing such sizeable files using [lxml’s iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk)
    and [etree’s iterparse](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse)
    which are memory-efficient methods to iterate through an XML tree and extract
    specific elements and attributes. without holding entire tree in memory.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.5.0.
  prefs: []
  type: TYPE_NORMAL
- en: To use this feature, you must pass a physical XML file path into `read_xml`
    and use the `iterparse` argument. Files should not be compressed or point to online
    sources but stored on local disk. Also, `iterparse` should be a dictionary where
    the key is the repeating nodes in document (which become the rows) and the value
    is a list of any element or attribute that is a descendant (i.e., child, grandchild)
    of repeating node. Since XPath is not used in this method, descendants do not
    need to share same relationship with one another. Below shows example of reading
    in Wikipedia’s very large (12 GB+) latest article data dump.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE662]  ### Writing XML'
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_xml` which renders the contents
    of the `DataFrame` as an XML document.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This method does not support special properties of XML including DTD, CData,
    XSD schemas, processing instructions, comments, and others. Only namespaces at
    the root level is supported. However, `stylesheet` allows design changes after
    initial output.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write an XML without options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE663]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with new root and row name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE664]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an attribute-centric XML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE665]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a mix of elements and attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE666]'
  prefs: []
  type: TYPE_PRE
- en: 'Any `DataFrames` with hierarchical columns will be flattened for XML element
    names with levels delimited by underscores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE667]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with default namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE668]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with namespace prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE669]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML without declaration or pretty print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE670]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML and transform with stylesheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE671]'
  prefs: []
  type: TYPE_PRE
- en: XML Final Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All XML documents adhere to [W3C specifications](https://www.w3.org/TR/xml/).
    Both `etree` and `lxml` parsers will fail to parse any markup document that is
    not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document
    unless it follows XHTML specs. However, other popular markup types including KML,
    XAML, RSS, MusicML, MathML are compliant [XML schemas](https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For above reason, if your application builds XML prior to pandas operations,
    use appropriate DOM libraries like `etree` and `lxml` to build the necessary document
    and not by string concatenation or regex adjustments. Always remember XML is a
    *special* text file with markup rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With very large XML files (several hundred MBs to GBs), XPath and XSLT can become
    memory-intensive operations. Be sure to have enough available RAM for reading
    and writing to large XML files (roughly about 5 times the size of text).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because XSLT is a programming language, use it with caution since such scripts
    can pose a security risk in your environment and can run large or infinite recursive
    operations. Always test scripts on small fragments before full run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [etree](https://docs.python.org/3/library/xml.etree.elementtree.html) parser
    supports all functionality of both `read_xml` and `to_xml` except for complex
    XPath and any XSLT. Though limited in features, `etree` is still a reliable and
    capable parser and tree builder. Its performance may trail `lxml` to a certain
    degree for larger files but relatively unnoticeable on small to medium size files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### Reading XML'
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: The top-level `read_xml()` function can accept an XML string/file/URL and will
    parse nodes and attributes into a pandas `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since there is no standard XML structure where design types can vary in many
    ways, `read_xml` works best with flatter, shallow versions. If an XML document
    is deeply nested, use the `stylesheet` feature to transform XML into a flatter
    version.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read an XML string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE672]'
  prefs: []
  type: TYPE_PRE
- en: 'Read a URL with no options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE673]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the content of the “books.xml” file and pass it to `read_xml` as a
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE674]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the content of the “books.xml” as instance of `StringIO` or `BytesIO`
    and pass it to `read_xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE675]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE676]'
  prefs: []
  type: TYPE_PRE
- en: 'Even read XML from AWS S3 buckets such as NIH NCBI PMC Article Datasets providing
    Biomedical and Life Science Jorurnals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE677]'
  prefs: []
  type: TYPE_PRE
- en: 'With [lxml](https://lxml.de) as default `parser`, you access the full-featured
    XML library that extends Python’s ElementTree API. One powerful tool is ability
    to query nodes selectively or conditionally with more expressive XPath:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE678]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify only elements or only attributes to parse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE679]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE680]'
  prefs: []
  type: TYPE_PRE
- en: XML documents can have namespaces with prefixes and default namespaces without
    prefixes both of which are denoted with a special attribute `xmlns`. In order
    to parse by node under a namespace context, `xpath` must reference a prefix.
  prefs: []
  type: TYPE_NORMAL
- en: For example, below XML contains a namespace with prefix, `doc`, and URI at `https://example.com`.
    In order to parse `doc:row` nodes, `namespaces` must be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE681]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, an XML document can have a default namespace without prefix. Failing
    to assign a temporary prefix will return no nodes and raise a `ValueError`. But
    assigning *any* temporary name to correct URI allows parsing by nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE682]'
  prefs: []
  type: TYPE_PRE
- en: However, if XPath does not reference node names such as default, `/*`, then
    `namespaces` is not required.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since `xpath` identifies the parent of content to be parsed, only immediate
    desendants which include child nodes or current attributes are parsed. Therefore,
    `read_xml` will not parse the text of grandchildren or other descendants and will
    not parse attributes of any descendant. To retrieve lower level content, adjust
    xpath to lower level. For example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE683]'
  prefs: []
  type: TYPE_PRE
- en: shows the attribute `sides` on `shape` element was not parsed as expected since
    this attribute resides on the child of `row` element and not `row` element itself.
    In other words, `sides` attribute is a grandchild level descendant of `row` element.
    However, the `xpath` targets `row` element which covers only its children and
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: With [lxml](https://lxml.de) as parser, you can flatten nested XML documents
    with an XSLT script which also can be string/file/URL types. As background, [XSLT](https://www.w3.org/TR/xslt/)
    is a special-purpose language written in a special XML file that can transform
    original XML documents into other XML, HTML, even text (CSV, JSON, etc.) using
    an XSLT processor.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider this somewhat nested structure of Chicago “L” Rides where
    station and rides elements encapsulate data in their own sections. With below
    XSLT, `lxml` can transform original nested document into a flatter output (as
    shown below for demonstration) for easier parse into `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE684]'
  prefs: []
  type: TYPE_PRE
- en: For very large XML files that can range in hundreds of megabytes to gigabytes,
    [`pandas.read_xml()`](../reference/api/pandas.read_xml.html#pandas.read_xml "pandas.read_xml")
    supports parsing such sizeable files using [lxml’s iterparse](https://lxml.de/3.2/parsing.html#iterparse-and-iterwalk)
    and [etree’s iterparse](https://docs.python.org/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.iterparse)
    which are memory-efficient methods to iterate through an XML tree and extract
    specific elements and attributes. without holding entire tree in memory.
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.5.0.
  prefs: []
  type: TYPE_NORMAL
- en: To use this feature, you must pass a physical XML file path into `read_xml`
    and use the `iterparse` argument. Files should not be compressed or point to online
    sources but stored on local disk. Also, `iterparse` should be a dictionary where
    the key is the repeating nodes in document (which become the rows) and the value
    is a list of any element or attribute that is a descendant (i.e., child, grandchild)
    of repeating node. Since XPath is not used in this method, descendants do not
    need to share same relationship with one another. Below shows example of reading
    in Wikipedia’s very large (12 GB+) latest article data dump.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE685]'
  prefs: []
  type: TYPE_PRE
- en: '### Writing XML'
  prefs: []
  type: TYPE_NORMAL
- en: New in version 1.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` objects have an instance method `to_xml` which renders the contents
    of the `DataFrame` as an XML document.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This method does not support special properties of XML including DTD, CData,
    XSD schemas, processing instructions, comments, and others. Only namespaces at
    the root level is supported. However, `stylesheet` allows design changes after
    initial output.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write an XML without options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE686]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with new root and row name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE687]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an attribute-centric XML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE688]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a mix of elements and attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE689]'
  prefs: []
  type: TYPE_PRE
- en: 'Any `DataFrames` with hierarchical columns will be flattened for XML element
    names with levels delimited by underscores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE690]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with default namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE691]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML with namespace prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE692]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML without declaration or pretty print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE693]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an XML and transform with stylesheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE694]'
  prefs: []
  type: TYPE_PRE
- en: XML Final Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All XML documents adhere to [W3C specifications](https://www.w3.org/TR/xml/).
    Both `etree` and `lxml` parsers will fail to parse any markup document that is
    not well-formed or follows XML syntax rules. Do be aware HTML is not an XML document
    unless it follows XHTML specs. However, other popular markup types including KML,
    XAML, RSS, MusicML, MathML are compliant [XML schemas](https://en.wikipedia.org/wiki/List_of_types_of_XML_schemas).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For above reason, if your application builds XML prior to pandas operations,
    use appropriate DOM libraries like `etree` and `lxml` to build the necessary document
    and not by string concatenation or regex adjustments. Always remember XML is a
    *special* text file with markup rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With very large XML files (several hundred MBs to GBs), XPath and XSLT can become
    memory-intensive operations. Be sure to have enough available RAM for reading
    and writing to large XML files (roughly about 5 times the size of text).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because XSLT is a programming language, use it with caution since such scripts
    can pose a security risk in your environment and can run large or infinite recursive
    operations. Always test scripts on small fragments before full run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [etree](https://docs.python.org/3/library/xml.etree.elementtree.html) parser
    supports all functionality of both `read_xml` and `to_xml` except for complex
    XPath and any XSLT. Though limited in features, `etree` is still a reliable and
    capable parser and tree builder. Its performance may trail `lxml` to a certain
    degree for larger files but relatively unnoticeable on small to medium size files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '## Excel files'
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read Excel 2007+ (`.xlsx`) files using the `openpyxl`
    Python module. Excel 2003 (`.xls`) files can be read using `xlrd`. Binary Excel
    (`.xlsb`) files can be read using `pyxlsb`. All formats can be read using [calamine](#io-calamine)
    engine. The [`to_excel()`](../reference/api/pandas.DataFrame.to_excel.html#pandas.DataFrame.to_excel
    "pandas.DataFrame.to_excel") instance method is used for saving a `DataFrame`
    to Excel. Generally the semantics are similar to working with [csv](#io-read-csv-table)
    data. See the [cookbook](cookbook.html#cookbook-excel) for some advanced strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When `engine=None`, the following logic will be used to determine the engine:'
  prefs: []
  type: TYPE_NORMAL
- en: If `path_or_buffer` is an OpenDocument format (.odf, .ods, .odt), then [odf](https://pypi.org/project/odfpy/)
    will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise if `path_or_buffer` is an xls format, `xlrd` will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise if `path_or_buffer` is in xlsb format, `pyxlsb` will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise `openpyxl` will be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### Reading Excel files'
  prefs: []
  type: TYPE_NORMAL
- en: In the most basic use-case, `read_excel` takes a path to an Excel file, and
    the `sheet_name` indicating which sheet to parse.
  prefs: []
  type: TYPE_NORMAL
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.load_workbook()` to read
    in (`.xlsx`) and (`.xlsm`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlrd, pandas is using `xlrd.open_workbook()` to read in (`.xls`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine pyxlsb, pandas is using `pyxlsb.open_workbook()` to read in (`.xlsb`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.load()` to read in (`.ods`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine calamine, pandas is using `python_calamine.load_workbook()` to
    read in (`.xlsx`), (`.xlsm`), (`.xls`), (`.xlsb`), (`.ods`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE695]'
  prefs: []
  type: TYPE_PRE
- en: '#### `ExcelFile` class'
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate working with multiple sheets from the same file, the `ExcelFile`
    class can be used to wrap the file and can be passed into `read_excel` There will
    be a performance benefit for reading multiple sheets as the file is read into
    memory only once.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE696]'
  prefs: []
  type: TYPE_PRE
- en: The `ExcelFile` class can also be used as a context manager.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE697]'
  prefs: []
  type: TYPE_PRE
- en: The `sheet_names` property will generate a list of the sheet names in the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary use-case for an `ExcelFile` is parsing multiple sheets with different
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE698]'
  prefs: []
  type: TYPE_PRE
- en: Note that if the same parsing parameters are used for all sheets, a list of
    sheet names can simply be passed to `read_excel` with no loss in performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE699]'
  prefs: []
  type: TYPE_PRE
- en: '`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.
    This allows the user to control how the excel file is read. For example, sheets
    can be loaded on demand by calling `xlrd.open_workbook()` with `on_demand=True`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE700]  #### Specifying sheets'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: An ExcelFile’s attribute `sheet_names` provides access to a list of sheets.
  prefs: []
  type: TYPE_NORMAL
- en: The arguments `sheet_name` allows specifying the sheet or sheets to read.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default value for `sheet_name` is 0, indicating to read the first sheet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a string to refer to the name of a particular sheet in the workbook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass an integer to refer to the index of a sheet. Indices follow Python convention,
    beginning at 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a list of either strings or integers, to return a dictionary of specified
    sheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a `None` to return a dictionary of all available sheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE701]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the sheet index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE702]'
  prefs: []
  type: TYPE_PRE
- en: 'Using all default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE703]'
  prefs: []
  type: TYPE_PRE
- en: 'Using None to get all sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE704]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a list to get multiple sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE705]'
  prefs: []
  type: TYPE_PRE
- en: '`read_excel` can read more than one sheet, by setting `sheet_name` to either
    a list of sheet names, a list of sheet positions, or `None` to read all sheets.
    Sheets can be specified by sheet index or sheet name, using an integer or string,
    respectively.  #### Reading a `MultiIndex`'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_excel` can read a `MultiIndex` index, by passing a list of columns to
    `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If
    either the `index` or `columns` have serialized level names those will be read
    in as well by specifying the rows/columns that make up the levels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to read in a `MultiIndex` index without names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE706]'
  prefs: []
  type: TYPE_PRE
- en: If the index has level names, they will parsed as well, using the same parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE707]'
  prefs: []
  type: TYPE_PRE
- en: 'If the source file has both `MultiIndex` index and columns, lists specifying
    each should be passed to `index_col` and `header`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE708]'
  prefs: []
  type: TYPE_PRE
- en: Missing values in columns specified in `index_col` will be forward filled to
    allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward
    filling the missing values use `set_index` after reading the data instead of `index_col`.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing specific columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is often the case that users will insert columns to do temporary computations
    in Excel and you may not want to read in those columns. `read_excel` takes a `usecols`
    keyword to allow you to specify a subset of columns to parse.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a comma-delimited set of Excel columns and ranges as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE709]'
  prefs: []
  type: TYPE_PRE
- en: If `usecols` is a list of integers, then it is assumed to be the file column
    indices to be parsed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE710]'
  prefs: []
  type: TYPE_PRE
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `usecols` is a list of strings, it is assumed that each string corresponds
    to a column name provided either by the user in `names` or inferred from the document
    header row(s). Those strings define which columns will be parsed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE711]'
  prefs: []
  type: TYPE_PRE
- en: Element order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',
    'baz']`.
  prefs: []
  type: TYPE_NORMAL
- en: If `usecols` is callable, the callable function will be evaluated against the
    column names, returning names where the callable function evaluates to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE712]'
  prefs: []
  type: TYPE_PRE
- en: Parsing dates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Datetime-like values are normally automatically converted to the appropriate
    dtype when reading the excel file. But if you have a column of strings that *look*
    like dates (but are not actually formatted as dates in excel), you can use the
    `parse_dates` keyword to parse those strings to datetimes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE713]'
  prefs: []
  type: TYPE_PRE
- en: Cell converters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is possible to transform the contents of Excel cells via the `converters`
    option. For instance, to convert a column to boolean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE714]'
  prefs: []
  type: TYPE_PRE
- en: 'This options handles missing values and treats exceptions in the converters
    as missing data. Transformations are applied cell by cell rather than to the column
    as a whole, so the array dtype is not guaranteed. For instance, a column of integers
    with missing values cannot be transformed to an array with integer dtype, because
    NaN is strictly a float. You can manually mask missing data to recover integer
    dtype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE715]'
  prefs: []
  type: TYPE_PRE
- en: Dtype specifications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to converters, the type for an entire column can be specified
    using the `dtype` keyword, which takes a dictionary mapping column names to types.
    To interpret data with no type inference, use the type `str` or `object`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE716]  ### Writing Excel files'
  prefs: []
  type: TYPE_NORMAL
- en: Writing Excel files to disk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To write a `DataFrame` object to a sheet of an Excel file, you can use the
    `to_excel` instance method. The arguments are largely the same as `to_csv` described
    above, the first argument being the name of the excel file, and the optional second
    argument the name of the sheet to which the `DataFrame` should be written. For
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE717]'
  prefs: []
  type: TYPE_PRE
- en: Files with a `.xlsx` extension will be written using `xlsxwriter` (if available)
    or `openpyxl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataFrame` will be written in a way that tries to mimic the REPL output.
    The `index_label` will be placed in the second row instead of the first. You can
    place it in the first row by setting the `merge_cells` option in `to_excel()`
    to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE718]'
  prefs: []
  type: TYPE_PRE
- en: In order to write separate `DataFrames` to separate sheets in a single Excel
    file, one can pass an `ExcelWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE719]'
  prefs: []
  type: TYPE_PRE
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.Workbook()` to create a new
    sheet and `openpyxl.load_workbook()` to append data to an existing sheet. The
    openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlsxwriter, pandas is using `xlsxwriter.Workbook()` to write
    to (`.xlsx`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.OpenDocumentSpreadsheet()`
    to write to (`.ods`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Excel files to memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: pandas supports writing Excel files to buffer-like objects such as `StringIO`
    or `BytesIO` using `ExcelWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE720]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`engine` is optional but recommended. Setting the engine determines the version
    of workbook produced. Setting `engine=''xlrd''` will produce an Excel 2003-format
    workbook (xls). Using either `''openpyxl''` or `''xlsxwriter''` will produce an
    Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook
    is produced.  ### Excel writer engines'
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas chooses an Excel writer via two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: the `engine` keyword argument
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the filename extension (via the default specified in config options)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, pandas uses the [XlsxWriter](https://xlsxwriter.readthedocs.io)
    for `.xlsx`, [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsm`. If you
    have multiple engines installed, you can set the default engine through [setting
    the config options](options.html#options) `io.excel.xlsx.writer` and `io.excel.xls.writer`.
    pandas will fall back on [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsx`
    files if [Xlsxwriter](https://xlsxwriter.readthedocs.io) is not available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify which writer you want to use, you can pass an engine keyword argument
    to `to_excel` and to `ExcelWriter`. The built-in engines are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`openpyxl`: version 2.4 or higher is required'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xlsxwriter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE721]  ### Style and formatting'
  prefs: []
  type: TYPE_NORMAL
- en: The look and feel of Excel worksheets created from pandas can be modified using
    the following parameters on the `DataFrame`’s `to_excel` method.
  prefs: []
  type: TYPE_NORMAL
- en: '`float_format` : Format string for floating point numbers (default `None`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_panes` : A tuple of two integers representing the bottommost row and
    rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will
    freeze the first row and first column (default `None`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the [Xlsxwriter](https://xlsxwriter.readthedocs.io) engine provides many
    options for controlling the format of an Excel worksheet created with the `to_excel`
    method. Excellent examples can be found in the [Xlsxwriter](https://xlsxwriter.readthedocs.io)
    documentation here: [https://xlsxwriter.readthedocs.io/working_with_pandas.html](https://xlsxwriter.readthedocs.io/working_with_pandas.html)  ###
    Reading Excel files'
  prefs: []
  type: TYPE_NORMAL
- en: In the most basic use-case, `read_excel` takes a path to an Excel file, and
    the `sheet_name` indicating which sheet to parse.
  prefs: []
  type: TYPE_NORMAL
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.load_workbook()` to read
    in (`.xlsx`) and (`.xlsm`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlrd, pandas is using `xlrd.open_workbook()` to read in (`.xls`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine pyxlsb, pandas is using `pyxlsb.open_workbook()` to read in (`.xlsb`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.load()` to read in (`.ods`)
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine calamine, pandas is using `python_calamine.load_workbook()` to
    read in (`.xlsx`), (`.xlsm`), (`.xls`), (`.xlsb`), (`.ods`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE722]'
  prefs: []
  type: TYPE_PRE
- en: '#### `ExcelFile` class'
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate working with multiple sheets from the same file, the `ExcelFile`
    class can be used to wrap the file and can be passed into `read_excel` There will
    be a performance benefit for reading multiple sheets as the file is read into
    memory only once.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE723]'
  prefs: []
  type: TYPE_PRE
- en: The `ExcelFile` class can also be used as a context manager.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE724]'
  prefs: []
  type: TYPE_PRE
- en: The `sheet_names` property will generate a list of the sheet names in the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary use-case for an `ExcelFile` is parsing multiple sheets with different
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE725]'
  prefs: []
  type: TYPE_PRE
- en: Note that if the same parsing parameters are used for all sheets, a list of
    sheet names can simply be passed to `read_excel` with no loss in performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE726]'
  prefs: []
  type: TYPE_PRE
- en: '`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.
    This allows the user to control how the excel file is read. For example, sheets
    can be loaded on demand by calling `xlrd.open_workbook()` with `on_demand=True`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE727]  #### Specifying sheets'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: An ExcelFile’s attribute `sheet_names` provides access to a list of sheets.
  prefs: []
  type: TYPE_NORMAL
- en: The arguments `sheet_name` allows specifying the sheet or sheets to read.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default value for `sheet_name` is 0, indicating to read the first sheet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a string to refer to the name of a particular sheet in the workbook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass an integer to refer to the index of a sheet. Indices follow Python convention,
    beginning at 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a list of either strings or integers, to return a dictionary of specified
    sheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a `None` to return a dictionary of all available sheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE728]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the sheet index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE729]'
  prefs: []
  type: TYPE_PRE
- en: 'Using all default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE730]'
  prefs: []
  type: TYPE_PRE
- en: 'Using None to get all sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE731]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a list to get multiple sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE732]'
  prefs: []
  type: TYPE_PRE
- en: '`read_excel` can read more than one sheet, by setting `sheet_name` to either
    a list of sheet names, a list of sheet positions, or `None` to read all sheets.
    Sheets can be specified by sheet index or sheet name, using an integer or string,
    respectively.  #### Reading a `MultiIndex`'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_excel` can read a `MultiIndex` index, by passing a list of columns to
    `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If
    either the `index` or `columns` have serialized level names those will be read
    in as well by specifying the rows/columns that make up the levels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to read in a `MultiIndex` index without names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE733]'
  prefs: []
  type: TYPE_PRE
- en: If the index has level names, they will parsed as well, using the same parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE734]'
  prefs: []
  type: TYPE_PRE
- en: 'If the source file has both `MultiIndex` index and columns, lists specifying
    each should be passed to `index_col` and `header`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE735]'
  prefs: []
  type: TYPE_PRE
- en: Missing values in columns specified in `index_col` will be forward filled to
    allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward
    filling the missing values use `set_index` after reading the data instead of `index_col`.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing specific columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is often the case that users will insert columns to do temporary computations
    in Excel and you may not want to read in those columns. `read_excel` takes a `usecols`
    keyword to allow you to specify a subset of columns to parse.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a comma-delimited set of Excel columns and ranges as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE736]'
  prefs: []
  type: TYPE_PRE
- en: If `usecols` is a list of integers, then it is assumed to be the file column
    indices to be parsed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE737]'
  prefs: []
  type: TYPE_PRE
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `usecols` is a list of strings, it is assumed that each string corresponds
    to a column name provided either by the user in `names` or inferred from the document
    header row(s). Those strings define which columns will be parsed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE738]'
  prefs: []
  type: TYPE_PRE
- en: Element order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',
    'baz']`.
  prefs: []
  type: TYPE_NORMAL
- en: If `usecols` is callable, the callable function will be evaluated against the
    column names, returning names where the callable function evaluates to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE739]'
  prefs: []
  type: TYPE_PRE
- en: Parsing dates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Datetime-like values are normally automatically converted to the appropriate
    dtype when reading the excel file. But if you have a column of strings that *look*
    like dates (but are not actually formatted as dates in excel), you can use the
    `parse_dates` keyword to parse those strings to datetimes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE740]'
  prefs: []
  type: TYPE_PRE
- en: Cell converters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is possible to transform the contents of Excel cells via the `converters`
    option. For instance, to convert a column to boolean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE741]'
  prefs: []
  type: TYPE_PRE
- en: 'This options handles missing values and treats exceptions in the converters
    as missing data. Transformations are applied cell by cell rather than to the column
    as a whole, so the array dtype is not guaranteed. For instance, a column of integers
    with missing values cannot be transformed to an array with integer dtype, because
    NaN is strictly a float. You can manually mask missing data to recover integer
    dtype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE742]'
  prefs: []
  type: TYPE_PRE
- en: Dtype specifications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to converters, the type for an entire column can be specified
    using the `dtype` keyword, which takes a dictionary mapping column names to types.
    To interpret data with no type inference, use the type `str` or `object`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE743]'
  prefs: []
  type: TYPE_PRE
- en: '#### `ExcelFile` class'
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate working with multiple sheets from the same file, the `ExcelFile`
    class can be used to wrap the file and can be passed into `read_excel` There will
    be a performance benefit for reading multiple sheets as the file is read into
    memory only once.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE744]'
  prefs: []
  type: TYPE_PRE
- en: The `ExcelFile` class can also be used as a context manager.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE745]'
  prefs: []
  type: TYPE_PRE
- en: The `sheet_names` property will generate a list of the sheet names in the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary use-case for an `ExcelFile` is parsing multiple sheets with different
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE746]'
  prefs: []
  type: TYPE_PRE
- en: Note that if the same parsing parameters are used for all sheets, a list of
    sheet names can simply be passed to `read_excel` with no loss in performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE747]'
  prefs: []
  type: TYPE_PRE
- en: '`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.
    This allows the user to control how the excel file is read. For example, sheets
    can be loaded on demand by calling `xlrd.open_workbook()` with `on_demand=True`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE748]'
  prefs: []
  type: TYPE_PRE
- en: '#### Specifying sheets'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is `sheet_name`, not to be confused with `ExcelFile.sheet_names`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: An ExcelFile’s attribute `sheet_names` provides access to a list of sheets.
  prefs: []
  type: TYPE_NORMAL
- en: The arguments `sheet_name` allows specifying the sheet or sheets to read.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default value for `sheet_name` is 0, indicating to read the first sheet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a string to refer to the name of a particular sheet in the workbook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass an integer to refer to the index of a sheet. Indices follow Python convention,
    beginning at 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a list of either strings or integers, to return a dictionary of specified
    sheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass a `None` to return a dictionary of all available sheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE749]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the sheet index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE750]'
  prefs: []
  type: TYPE_PRE
- en: 'Using all default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE751]'
  prefs: []
  type: TYPE_PRE
- en: 'Using None to get all sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE752]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a list to get multiple sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE753]'
  prefs: []
  type: TYPE_PRE
- en: '`read_excel` can read more than one sheet, by setting `sheet_name` to either
    a list of sheet names, a list of sheet positions, or `None` to read all sheets.
    Sheets can be specified by sheet index or sheet name, using an integer or string,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Reading a `MultiIndex`'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_excel` can read a `MultiIndex` index, by passing a list of columns to
    `index_col` and a `MultiIndex` column by passing a list of rows to `header`. If
    either the `index` or `columns` have serialized level names those will be read
    in as well by specifying the rows/columns that make up the levels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to read in a `MultiIndex` index without names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE754]'
  prefs: []
  type: TYPE_PRE
- en: If the index has level names, they will parsed as well, using the same parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE755]'
  prefs: []
  type: TYPE_PRE
- en: 'If the source file has both `MultiIndex` index and columns, lists specifying
    each should be passed to `index_col` and `header`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE756]'
  prefs: []
  type: TYPE_PRE
- en: Missing values in columns specified in `index_col` will be forward filled to
    allow roundtripping with `to_excel` for `merged_cells=True`. To avoid forward
    filling the missing values use `set_index` after reading the data instead of `index_col`.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing specific columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is often the case that users will insert columns to do temporary computations
    in Excel and you may not want to read in those columns. `read_excel` takes a `usecols`
    keyword to allow you to specify a subset of columns to parse.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify a comma-delimited set of Excel columns and ranges as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE757]'
  prefs: []
  type: TYPE_PRE
- en: If `usecols` is a list of integers, then it is assumed to be the file column
    indices to be parsed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE758]'
  prefs: []
  type: TYPE_PRE
- en: Element order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `usecols` is a list of strings, it is assumed that each string corresponds
    to a column name provided either by the user in `names` or inferred from the document
    header row(s). Those strings define which columns will be parsed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE759]'
  prefs: []
  type: TYPE_PRE
- en: Element order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',
    'baz']`.
  prefs: []
  type: TYPE_NORMAL
- en: If `usecols` is callable, the callable function will be evaluated against the
    column names, returning names where the callable function evaluates to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE760]'
  prefs: []
  type: TYPE_PRE
- en: Parsing dates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Datetime-like values are normally automatically converted to the appropriate
    dtype when reading the excel file. But if you have a column of strings that *look*
    like dates (but are not actually formatted as dates in excel), you can use the
    `parse_dates` keyword to parse those strings to datetimes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE761]'
  prefs: []
  type: TYPE_PRE
- en: Cell converters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is possible to transform the contents of Excel cells via the `converters`
    option. For instance, to convert a column to boolean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE762]'
  prefs: []
  type: TYPE_PRE
- en: 'This options handles missing values and treats exceptions in the converters
    as missing data. Transformations are applied cell by cell rather than to the column
    as a whole, so the array dtype is not guaranteed. For instance, a column of integers
    with missing values cannot be transformed to an array with integer dtype, because
    NaN is strictly a float. You can manually mask missing data to recover integer
    dtype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE763]'
  prefs: []
  type: TYPE_PRE
- en: Dtype specifications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to converters, the type for an entire column can be specified
    using the `dtype` keyword, which takes a dictionary mapping column names to types.
    To interpret data with no type inference, use the type `str` or `object`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE764]'
  prefs: []
  type: TYPE_PRE
- en: '### Writing Excel files'
  prefs: []
  type: TYPE_NORMAL
- en: Writing Excel files to disk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To write a `DataFrame` object to a sheet of an Excel file, you can use the
    `to_excel` instance method. The arguments are largely the same as `to_csv` described
    above, the first argument being the name of the excel file, and the optional second
    argument the name of the sheet to which the `DataFrame` should be written. For
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE765]'
  prefs: []
  type: TYPE_PRE
- en: Files with a `.xlsx` extension will be written using `xlsxwriter` (if available)
    or `openpyxl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataFrame` will be written in a way that tries to mimic the REPL output.
    The `index_label` will be placed in the second row instead of the first. You can
    place it in the first row by setting the `merge_cells` option in `to_excel()`
    to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE766]'
  prefs: []
  type: TYPE_PRE
- en: In order to write separate `DataFrames` to separate sheets in a single Excel
    file, one can pass an `ExcelWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE767]'
  prefs: []
  type: TYPE_PRE
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.Workbook()` to create a new
    sheet and `openpyxl.load_workbook()` to append data to an existing sheet. The
    openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlsxwriter, pandas is using `xlsxwriter.Workbook()` to write
    to (`.xlsx`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.OpenDocumentSpreadsheet()`
    to write to (`.ods`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Excel files to memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: pandas supports writing Excel files to buffer-like objects such as `StringIO`
    or `BytesIO` using `ExcelWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE768]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`engine` is optional but recommended. Setting the engine determines the version
    of workbook produced. Setting `engine=''xlrd''` will produce an Excel 2003-format
    workbook (xls). Using either `''openpyxl''` or `''xlsxwriter''` will produce an
    Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook
    is produced.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing Excel files to disk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To write a `DataFrame` object to a sheet of an Excel file, you can use the
    `to_excel` instance method. The arguments are largely the same as `to_csv` described
    above, the first argument being the name of the excel file, and the optional second
    argument the name of the sheet to which the `DataFrame` should be written. For
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE769]'
  prefs: []
  type: TYPE_PRE
- en: Files with a `.xlsx` extension will be written using `xlsxwriter` (if available)
    or `openpyxl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DataFrame` will be written in a way that tries to mimic the REPL output.
    The `index_label` will be placed in the second row instead of the first. You can
    place it in the first row by setting the `merge_cells` option in `to_excel()`
    to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE770]'
  prefs: []
  type: TYPE_PRE
- en: In order to write separate `DataFrames` to separate sheets in a single Excel
    file, one can pass an `ExcelWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE771]'
  prefs: []
  type: TYPE_PRE
- en: When using the `engine_kwargs` parameter, pandas will pass these arguments to
    the engine. For this, it is important to know which function pandas is using internally.
  prefs: []
  type: TYPE_NORMAL
- en: For the engine openpyxl, pandas is using `openpyxl.Workbook()` to create a new
    sheet and `openpyxl.load_workbook()` to append data to an existing sheet. The
    openpyxl engine writes to (`.xlsx`) and (`.xlsm`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine xlsxwriter, pandas is using `xlsxwriter.Workbook()` to write
    to (`.xlsx`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the engine odf, pandas is using `odf.opendocument.OpenDocumentSpreadsheet()`
    to write to (`.ods`) files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Excel files to memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: pandas supports writing Excel files to buffer-like objects such as `StringIO`
    or `BytesIO` using `ExcelWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE772]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`engine` is optional but recommended. Setting the engine determines the version
    of workbook produced. Setting `engine=''xlrd''` will produce an Excel 2003-format
    workbook (xls). Using either `''openpyxl''` or `''xlsxwriter''` will produce an
    Excel 2007-format workbook (xlsx). If omitted, an Excel 2007-formatted workbook
    is produced.'
  prefs: []
  type: TYPE_NORMAL
- en: '### Excel writer engines'
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas chooses an Excel writer via two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: the `engine` keyword argument
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the filename extension (via the default specified in config options)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, pandas uses the [XlsxWriter](https://xlsxwriter.readthedocs.io)
    for `.xlsx`, [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsm`. If you
    have multiple engines installed, you can set the default engine through [setting
    the config options](options.html#options) `io.excel.xlsx.writer` and `io.excel.xls.writer`.
    pandas will fall back on [openpyxl](https://openpyxl.readthedocs.io/) for `.xlsx`
    files if [Xlsxwriter](https://xlsxwriter.readthedocs.io) is not available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify which writer you want to use, you can pass an engine keyword argument
    to `to_excel` and to `ExcelWriter`. The built-in engines are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`openpyxl`: version 2.4 or higher is required'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xlsxwriter`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE773]'
  prefs: []
  type: TYPE_PRE
- en: '### Style and formatting'
  prefs: []
  type: TYPE_NORMAL
- en: The look and feel of Excel worksheets created from pandas can be modified using
    the following parameters on the `DataFrame`’s `to_excel` method.
  prefs: []
  type: TYPE_NORMAL
- en: '`float_format` : Format string for floating point numbers (default `None`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`freeze_panes` : A tuple of two integers representing the bottommost row and
    rightmost column to freeze. Each of these parameters is one-based, so (1, 1) will
    freeze the first row and first column (default `None`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the [Xlsxwriter](https://xlsxwriter.readthedocs.io) engine provides many
    options for controlling the format of an Excel worksheet created with the `to_excel`
    method. Excellent examples can be found in the [Xlsxwriter](https://xlsxwriter.readthedocs.io)
    documentation here: [https://xlsxwriter.readthedocs.io/working_with_pandas.html](https://xlsxwriter.readthedocs.io/working_with_pandas.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '## OpenDocument Spreadsheets'
  prefs: []
  type: TYPE_NORMAL
- en: The io methods for [Excel files](#excel-files) also support reading and writing
    OpenDocument spreadsheets using the [odfpy](https://pypi.org/project/odfpy/) module.
    The semantics and features for reading and writing OpenDocument spreadsheets match
    what can be done for [Excel files](#excel-files) using `engine='odf'`. The optional
    dependency ‘odfpy’ needs to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read OpenDocument spreadsheets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE774]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, the `to_excel()` method can write OpenDocument spreadsheets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE775]'
  prefs: []
  type: TYPE_PRE
- en: '## Binary Excel (.xlsb) files'
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can also read binary Excel files using the `pyxlsb`
    module. The semantics and features for reading binary Excel files mostly match
    what can be done for [Excel files](#excel-files) using `engine='pyxlsb'`. `pyxlsb`
    does not recognize datetime types in files and will return floats instead (you
    can use [calamine](#io-calamine) if you need recognize datetime types).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE776]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Currently pandas only supports *reading* binary Excel files. Writing is not
    implemented.
  prefs: []
  type: TYPE_NORMAL
- en: '## Calamine (Excel and ODS files)'
  prefs: []
  type: TYPE_NORMAL
- en: The [`read_excel()`](../reference/api/pandas.read_excel.html#pandas.read_excel
    "pandas.read_excel") method can read Excel file (`.xlsx`, `.xlsm`, `.xls`, `.xlsb`)
    and OpenDocument spreadsheets (`.ods`) using the `python-calamine` module. This
    module is a binding for Rust library [calamine](https://crates.io/crates/calamine)
    and is faster than other engines in most cases. The optional dependency ‘python-calamine’
    needs to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE777]'
  prefs: []
  type: TYPE_PRE
- en: '## Clipboard'
  prefs: []
  type: TYPE_NORMAL
- en: 'A handy way to grab data is to use the `read_clipboard()` method, which takes
    the contents of the clipboard buffer and passes them to the `read_csv` method.
    For instance, you can copy the following text to the clipboard (CTRL-C on many
    operating systems):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE778]'
  prefs: []
  type: TYPE_PRE
- en: 'And then import the data directly to a `DataFrame` by calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE779]'
  prefs: []
  type: TYPE_PRE
- en: The `to_clipboard` method can be used to write the contents of a `DataFrame`
    to the clipboard. Following which you can paste the clipboard contents into other
    applications (CTRL-V on many operating systems). Here we illustrate writing a
    `DataFrame` into clipboard and reading it back.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE780]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we got the same content back, which we had earlier written to
    the clipboard.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to
    use these methods.
  prefs: []
  type: TYPE_NORMAL
- en: '## Pickling'
  prefs: []
  type: TYPE_NORMAL
- en: All pandas objects are equipped with `to_pickle` methods which use Python’s
    `cPickle` module to save data structures to disk using the pickle format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE781]'
  prefs: []
  type: TYPE_PRE
- en: 'The `read_pickle` function in the `pandas` namespace can be used to load any
    pickled pandas object (or any other pickled object) from file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE782]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Loading pickled data received from untrusted sources can be unsafe.
  prefs: []
  type: TYPE_NORMAL
- en: 'See: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle") is only guaranteed backwards compatible back to a few minor
    release.'
  prefs: []
  type: TYPE_NORMAL
- en: '### Compressed pickle files'
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle"), [`DataFrame.to_pickle()`](../reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle
    "pandas.DataFrame.to_pickle") and [`Series.to_pickle()`](../reference/api/pandas.Series.to_pickle.html#pandas.Series.to_pickle
    "pandas.Series.to_pickle") can read and write compressed pickle files. The compression
    types of `gzip`, `bz2`, `xz`, `zstd` are supported for reading and writing. The
    `zip` file format only supports reading and must contain only one data file to
    be read.'
  prefs: []
  type: TYPE_NORMAL
- en: The compression type can be an explicit parameter or be inferred from the file
    extension. If ‘infer’, then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename
    ends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The compression parameter can also be a `dict` in order to pass options to the
    compression protocol. It must have a `'method'` key set to the name of the compression
    protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`, `'xz'`, `'zstd'`}.
    All other key-value pairs are passed to the underlying compression library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE783]'
  prefs: []
  type: TYPE_PRE
- en: 'Using an explicit compression type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE784]'
  prefs: []
  type: TYPE_PRE
- en: 'Inferring compression type from the extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE785]'
  prefs: []
  type: TYPE_PRE
- en: 'The default is to ‘infer’:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE786]'
  prefs: []
  type: TYPE_PRE
- en: 'Passing options to the compression protocol in order to speed up compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE787]  ### Compressed pickle files'
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_pickle()`](../reference/api/pandas.read_pickle.html#pandas.read_pickle
    "pandas.read_pickle"), [`DataFrame.to_pickle()`](../reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle
    "pandas.DataFrame.to_pickle") and [`Series.to_pickle()`](../reference/api/pandas.Series.to_pickle.html#pandas.Series.to_pickle
    "pandas.Series.to_pickle") can read and write compressed pickle files. The compression
    types of `gzip`, `bz2`, `xz`, `zstd` are supported for reading and writing. The
    `zip` file format only supports reading and must contain only one data file to
    be read.'
  prefs: []
  type: TYPE_NORMAL
- en: The compression type can be an explicit parameter or be inferred from the file
    extension. If ‘infer’, then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename
    ends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The compression parameter can also be a `dict` in order to pass options to the
    compression protocol. It must have a `'method'` key set to the name of the compression
    protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`, `'xz'`, `'zstd'`}.
    All other key-value pairs are passed to the underlying compression library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE788]'
  prefs: []
  type: TYPE_PRE
- en: 'Using an explicit compression type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE789]'
  prefs: []
  type: TYPE_PRE
- en: 'Inferring compression type from the extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE790]'
  prefs: []
  type: TYPE_PRE
- en: 'The default is to ‘infer’:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE791]'
  prefs: []
  type: TYPE_PRE
- en: 'Passing options to the compression protocol in order to speed up compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE792]'
  prefs: []
  type: TYPE_PRE
- en: '## msgpack'
  prefs: []
  type: TYPE_NORMAL
- en: pandas support for `msgpack` has been removed in version 1.0.0\. It is recommended
    to use [pickle](#io-pickle) instead.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can also the Arrow IPC serialization format for on-the-wire
    transmission of pandas objects. For documentation on pyarrow, see [here](https://arrow.apache.org/docs/python/ipc.html).
  prefs: []
  type: TYPE_NORMAL
- en: '## HDF5 (PyTables)'
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is a dict-like object which reads and writes pandas using the high
    performance HDF5 format using the excellent [PyTables](https://www.pytables.org/)
    library. See the [cookbook](cookbook.html#cookbook-hdf) for some advanced strategies'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: pandas uses PyTables for reading and writing HDF5 files, which allows serializing
    object-dtype data with pickle. Loading pickled data received from untrusted sources
    can be unsafe.
  prefs: []
  type: TYPE_NORMAL
- en: 'See: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)
    for more.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE793]'
  prefs: []
  type: TYPE_PRE
- en: 'Objects can be written to the file just like adding key-value pairs to a dict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE794]'
  prefs: []
  type: TYPE_PRE
- en: 'In a current or later Python session, you can retrieve stored objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE795]'
  prefs: []
  type: TYPE_PRE
- en: 'Deletion of the object specified by the key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE796]'
  prefs: []
  type: TYPE_PRE
- en: 'Closing a Store and using a context manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE797]'
  prefs: []
  type: TYPE_PRE
- en: Read/write API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf`
    for writing, similar to how `read_csv` and `to_csv` work.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE798]'
  prefs: []
  type: TYPE_PRE
- en: HDFStore will by default not drop rows that are all missing. This behavior can
    be changed by setting `dropna=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE799]'
  prefs: []
  type: TYPE_PRE
- en: '### Fixed format'
  prefs: []
  type: TYPE_NORMAL
- en: The examples above show storing using `put`, which write the HDF5 to `PyTables`
    in a fixed array format, called the `fixed` format. These types of stores are
    **not** appendable once written (though you can simply remove them and rewrite).
    Nor are they **queryable**; they must be retrieved in their entirety. They also
    do not support dataframes with non-unique column names. The `fixed` format stores
    offer very fast writing and slightly faster reading than `table` stores. This
    format is specified by default when using `put` or `to_hdf` or by `format='fixed'`
    or `format='f'`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'A `fixed` format will raise a `TypeError` if you try to retrieve using a `where`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE800]  ### Table format'
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` supports another `PyTables` format on disk, the `table` format.
    Conceptually a `table` is shaped very much like a DataFrame, with rows and columns.
    A `table` may be appended to in the same or other sessions. In addition, delete
    and query type operations are supported. This format is specified by `format=''table''`
    or `format=''t''` to `append` or `put` or `to_hdf`.'
  prefs: []
  type: TYPE_NORMAL
- en: This format can be set as an option as well `pd.set_option('io.hdf.default_format','table')`
    to enable `put/append/to_hdf` to by default store in the `table` format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE801]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also create a `table` by passing `format=''table''` or `format=''t''`
    to a `put` operation.  ### Hierarchical keys'
  prefs: []
  type: TYPE_NORMAL
- en: Keys to a store can be specified as a string. These can be in a hierarchical
    path-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of
    sub-stores (or `Groups` in PyTables parlance). Keys can be specified without the
    leading ‘/’ and are **always** absolute (e.g. ‘foo’ refers to ‘/foo’). Removal
    operations can remove everything in the sub-store and **below**, so be *careful*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE802]'
  prefs: []
  type: TYPE_PRE
- en: You can walk through the group hierarchy using the `walk` method which will
    yield a tuple for each group key along with the relative keys of its contents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE803]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical keys cannot be retrieved as dotted (attribute) access as described
    above for items stored under the root node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE804]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE805]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, use explicit string based keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE806]  ### Storing types'
  prefs: []
  type: TYPE_NORMAL
- en: Storing mixed types in a table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing mixed-dtype data is supported. Strings are stored as a fixed-width using
    the maximum size of the appended column. Subsequent attempts at appending longer
    strings will raise a `ValueError`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Passing `min_itemsize={`values`: size}` as a parameter to append will set a
    larger minimum for the string columns. Storing `floats, strings, ints, bools,
    datetime64` are currently supported. For string columns, passing `nan_rep = ''nan''`
    to append will change the default nan representation on disk (which converts to/from
    `np.nan`), this defaults to `nan`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE807]'
  prefs: []
  type: TYPE_PRE
- en: Storing MultiIndex DataFrames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting
    from homogeneous index `DataFrames`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE808]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The `index` keyword is reserved and cannot be use as a level name.  ### Querying'
  prefs: []
  type: TYPE_NORMAL
- en: Querying a table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`select` and `delete` operations have an optional criterion that can be specified
    to select/delete only a subset of the data. This allows one to have a very large
    on-disk table and retrieve only a portion of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: A query is specified using the `Term` class under the hood, as a boolean expression.
  prefs: []
  type: TYPE_NORMAL
- en: '`index` and `columns` are supported indexers of `DataFrames`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `data_columns` are specified, these can be used as additional indexers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: level name in a MultiIndex, with default name `level_0`, `level_1`, … if not
    provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valid comparison operators are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`=, ==, !=, >, >=, <, <=`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid boolean expressions are combined with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`|` : or'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`&` : and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(` and `)` : for grouping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are similar to how boolean expressions are used in pandas for indexing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`=` will be automatically expanded to the comparison operator `==`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~` is the not operator, but can only be used in very limited circumstances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a list/tuple of expressions is passed they will be combined via `&`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are valid expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''index >= date''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns = [''A'', ''D'']"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns in [''A'', ''D'']"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns = A''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns == A''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"~(columns = [''A'', ''B''])"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''index > df.index[3] & string = "bar"''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"ts >= Timestamp(''2012-02-01'')"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"major_axis>=20130101"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `indexers` are on the left-hand side of the sub-expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '`columns`, `major_axis`, `ts`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-hand side of the sub-expression (after a comparison operator) can
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strings, e.g. `"bar"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: date-like, e.g. `20130101`, or `"20130101"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lists, e.g. `"['A', 'B']"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variables that are defined in the local names space, e.g. `date`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Passing a string to a query by interpolating it into the query expression is
    not recommended. Simply assign the string of interest to a variable and use that
    variable in an expression. For example, do this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE809]'
  prefs: []
  type: TYPE_PRE
- en: instead of this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE810]'
  prefs: []
  type: TYPE_PRE
- en: The latter will **not** work and will raise a `SyntaxError`.Note that there’s
    a single quote followed by a double quote in the `string` variable.
  prefs: []
  type: TYPE_NORMAL
- en: If you *must* interpolate, use the `'%r'` format specifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE811]'
  prefs: []
  type: TYPE_PRE
- en: which will quote `string`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE812]'
  prefs: []
  type: TYPE_PRE
- en: Use boolean expressions, with in-line function evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE813]'
  prefs: []
  type: TYPE_PRE
- en: Use inline column reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE814]'
  prefs: []
  type: TYPE_PRE
- en: 'The `columns` keyword can be supplied to select a list of columns to be returned,
    this is equivalent to passing a `''columns=list_of_columns_to_filter''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE815]'
  prefs: []
  type: TYPE_PRE
- en: '`start` and `stop` parameters can be specified to limit the total search space.
    These are in terms of the total number of rows in a table.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `ValueError` if the query expression has an unknown variable
    reference. Usually this means that you are trying to select on a column that is
    **not** a data_column.'
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `SyntaxError` if the query expression is not valid.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query timedelta64[ns]'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can store and query using the `timedelta64[ns]` type. Terms can be specified
    in the format: `<float>(<unit>)`, where float may be signed (and fractional),
    and unit can be `D,s,ms,us,ns` for the timedelta. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE816]  #### Query MultiIndex'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting from a `MultiIndex` can be achieved by using the name of the level.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE817]'
  prefs: []
  type: TYPE_PRE
- en: If the `MultiIndex` levels names are `None`, the levels are automatically made
    available via the `level_n` keyword with `n` the level of the `MultiIndex` you
    want to select from.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE818]'
  prefs: []
  type: TYPE_PRE
- en: Indexing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can create/modify an index for a table with `create_table_index` after data
    is already in the table (after and `append/put` operation). Creating a table index
    is **highly** encouraged. This will speed your queries a great deal when you use
    a `select` with the indexed dimension as the `where`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Indexes are automagically created on the indexables and any data columns you
    specify. This behavior can be turned off by passing `index=False` to `append`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE819]'
  prefs: []
  type: TYPE_PRE
- en: Oftentimes when appending large amounts of data to a store, it is useful to
    turn off index creation for each append, then recreate at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE820]'
  prefs: []
  type: TYPE_PRE
- en: Then create the index when finished appending.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE821]'
  prefs: []
  type: TYPE_PRE
- en: See [here](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)
    for how to create a completely-sorted-index (CSI) on an existing store.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query via data columns'
  prefs: []
  type: TYPE_NORMAL
- en: You can designate (and index) certain columns that you want to be able to perform
    queries (other than the `indexable` columns, which you can always query). For
    instance say you want to perform this common operation, on-disk, and return just
    the frame that matches this query. You can specify `data_columns = True` to force
    all columns to be `data_columns`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE822]'
  prefs: []
  type: TYPE_PRE
- en: There is some performance degradation by making lots of columns into `data columns`,
    so it is up to the user to designate these. In addition, you cannot change data
    columns (nor indexables) after the first append/put operation (Of course you can
    simply read in the data and create a new table!).
  prefs: []
  type: TYPE_NORMAL
- en: Iterator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and
    `select_as_multiple` to return an iterator on the results. The default is 50,000
    rows returned in a chunk.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE823]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the iterator with `read_hdf` which will open, then automatically
    close the store when finished iterating.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE824]'
  prefs: []
  type: TYPE_PRE
- en: Note, that the chunksize keyword applies to the **source** rows. So if you are
    doing a query, then the chunksize will subdivide the total rows in the table and
    the query applied, returning an iterator on potentially unequal sized chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a recipe for generating a query and using it to create equal sized return
    chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE825]'
  prefs: []
  type: TYPE_PRE
- en: Advanced queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Select a single column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE826]'
  prefs: []
  type: TYPE_PRE
- en: '##### Selecting coordinates'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE827]  ##### Selecting using a where mask'
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE828]'
  prefs: []
  type: TYPE_PRE
- en: Storer object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE829]'
  prefs: []
  type: TYPE_PRE
- en: Multiple table queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The methods `append_to_multiple` and `select_as_multiple` can perform appending/selecting
    from multiple tables at once. The idea is to have one table (call it the selector
    table) that you index most/all of the columns, and perform your queries. The other
    table(s) are data tables with an index matching the selector table’s index. You
    can then perform a very fast query on the selector table, yet get lots of data
    back. This method is similar to having a very wide table, but enables more efficient
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: The `append_to_multiple` method splits a given single DataFrame into multiple
    tables according to `d`, a dictionary that maps the table names to a list of ‘columns’
    you want in that table. If `None` is used in place of a list, that table will
    have the remaining unspecified columns of the given DataFrame. The argument `selector`
    defines which table is the selector table (which you can make queries from). The
    argument `dropna` will drop rows from the input `DataFrame` to ensure tables are
    synchronized. This means that if a row for one of the tables being written to
    is entirely `np.nan`, that row will be dropped from all tables.
  prefs: []
  type: TYPE_NORMAL
- en: If `dropna` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
    Remember that entirely `np.Nan` rows are not written to the HDFStore, so if you
    choose to call `dropna=False`, some tables may have more rows than others, and
    therefore `select_as_multiple` may not work or it may return unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE830]'
  prefs: []
  type: TYPE_PRE
- en: Delete from a table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can delete from a table selectively by specifying a `where`. In deleting
    rows, it is important to understand the `PyTables` deletes rows by erasing the
    rows, then **moving** the following data. Thus deleting can potentially be a very
    expensive operation depending on the orientation of your data. To get optimal
    performance, it’s worthwhile to have the dimension you are deleting be the first
    of the `indexables`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is ordered (on the disk) in terms of the `indexables`. Here’s a simple
    use case. You store panel-type data, with dates in the `major_axis` and ids in
    the `minor_axis`. The data is then interleaved like this:'
  prefs: []
  type: TYPE_NORMAL
- en: date_1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_2
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: date_2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be clear that a delete operation on the `major_axis` will be fairly
    quick, as one chunk is removed, then the following data moved. On the other hand
    a delete operation on the `minor_axis` will be very expensive. In this case it
    would almost certainly be faster to rewrite the table using a `where` that selects
    all but the missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files automatically.
    Thus, repeatedly deleting (or removing nodes) and adding again, **WILL TEND TO
    INCREASE THE FILE SIZE**.
  prefs: []
  type: TYPE_NORMAL
- en: To *repack and clean* the file, use [ptrepack](#io-hdf5-ptrepack).
  prefs: []
  type: TYPE_NORMAL
- en: '### Notes & caveats'
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`PyTables` allows the stored data to be compressed. This applies to all kinds
    of stores, not just tables. Two parameters are used to control compression: `complevel`
    and `complib`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`complevel` specifies if and how hard data is to be compressed. `complevel=0`
    and `complevel=None` disables compression and `0<complevel<10` enables compression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`complib` specifies which compression library to use. If nothing is specified
    the default library `zlib` is used. A compression library usually optimizes for
    either good compression rates or speed and the results will depend on the type
    of data. Which type of compression to choose depends on your specific needs and
    data. The list of supported compression libraries:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[zlib](https://zlib.net/): The default compression library. A classic in terms
    of compression, achieves good compression rates but is somewhat slow.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lzo](https://www.oberhumer.com/opensource/lzo/): Fast compression and decompression.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bzip2](https://sourceware.org/bzip2/): Good compression rates.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc](https://www.blosc.org/): Fast compression and decompression.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for alternative blosc compressors:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[blosc:blosclz](https://www.blosc.org/) This is the default compressor for
    `blosc`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html): A compact, very
    popular and fast compressor.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html): A tweaked version
    of LZ4, produces better compression ratios at the expense of speed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:snappy](https://google.github.io/snappy/): A popular compressor used
    in many places.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zlib](https://zlib.net/): A classic; somewhat slower than the previous
    ones, but achieving better compression ratios.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zstd](https://facebook.github.io/zstd/): An extremely well balanced
    codec; it provides the best compression ratios among the others above, and at
    reasonably fast speed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `complib` is defined as something other than the listed libraries a `ValueError`
    exception is issued.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the library specified with the `complib` option is missing on your platform,
    compression defaults to `zlib` without further ado.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable compression for all objects within the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE831]'
  prefs: []
  type: TYPE_PRE
- en: 'Or on-the-fly compression (this only applies to tables) in stores where compression
    is not enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE832]'
  prefs: []
  type: TYPE_PRE
- en: '#### ptrepack'
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` offers better write performance when tables are compressed after
    they are written, as opposed to turning on compression at the very beginning.
    You can use the supplied `PyTables` utility `ptrepack`. In addition, `ptrepack`
    can change compression levels after the fact.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE833]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow you to
    reuse previously deleted space. Alternatively, one can simply remove the file
    and write again, or use the `copy` method.  #### Caveats'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is **not-threadsafe for writing**. The underlying `PyTables` only
    supports concurrent reads (via threading or processes). If you need reading and
    writing *at the same time*, you need to serialize these operations in a single
    thread in a single process. You will corrupt your data otherwise. See the ([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397)) for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: If you use locks to manage write access between multiple processes, you may
    want to use [`fsync()`](https://docs.python.org/3/library/os.html#os.fsync "(in
    Python v3.12)") before releasing write locks. For convenience you can use `store.flush(fsync=True)`
    to do this for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a `table` is created columns (DataFrame) are fixed; only exactly the same
    columns can be appended
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not necessarily
    equal across timezone versions. So if data is localized to a specific timezone
    in the HDFStore using one version of a timezone library and that data is updated
    with another version, the data will be converted to UTC since these timezones
    are not considered equal. Either use the same version of timezone library or use
    `tz_convert` with the updated timezone definition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` will show a `NaturalNameWarning` if a column name cannot be used
    as an attribute selector. *Natural* identifiers contain only letters, numbers,
    and underscores, and may not begin with a number. Other identifiers cannot be
    used in a `where` clause and are generally a bad idea.  ### DataTypes'
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` will map an object dtype to the `PyTables` underlying dtype. This
    means the following types are known to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Represents missing values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| floating : `float64, float32, float16` | `np.nan` |'
  prefs: []
  type: TYPE_TB
- en: '| integer : `int64, int32, int8, uint64,uint32, uint8` |  |'
  prefs: []
  type: TYPE_TB
- en: '| boolean |  |'
  prefs: []
  type: TYPE_TB
- en: '| `datetime64[ns]` | `NaT` |'
  prefs: []
  type: TYPE_TB
- en: '| `timedelta64[ns]` | `NaT` |'
  prefs: []
  type: TYPE_TB
- en: '| categorical : see the section below |  |'
  prefs: []
  type: TYPE_TB
- en: '| object : `strings` | `np.nan` |'
  prefs: []
  type: TYPE_TB
- en: '`unicode` columns are not supported, and **WILL FAIL**.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  prefs: []
  type: TYPE_NORMAL
- en: You can write data that contains `category` dtypes to a `HDFStore`. Queries
    work the same as if it was an object array. However, the `category` dtyped data
    is stored in a more efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE834]'
  prefs: []
  type: TYPE_PRE
- en: String columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**min_itemsize**'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation of `HDFStore` uses a fixed column width (itemsize)
    for string columns. A string column itemsize is calculated as the maximum of the
    length of data (for that column) that is passed to the `HDFStore`, **in the first
    append**. Subsequent appends, may introduce a string for a column **larger** than
    the column can hold, an Exception will be raised (otherwise you could have a silent
    truncation of these columns, leading to loss of information). In the future we
    may relax this and allow a user-specified truncation to occur.
  prefs: []
  type: TYPE_NORMAL
- en: Pass `min_itemsize` on the first table creation to a-priori specify the minimum
    length of a particular string column. `min_itemsize` can be an integer, or a dict
    mapping a column name to an integer. You can pass `values` as a key to allow all
    *indexables* or *data_columns* to have this min_itemsize.
  prefs: []
  type: TYPE_NORMAL
- en: Passing a `min_itemsize` dict will cause all passed columns to be created as
    *data_columns* automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are not passing any `data_columns`, then the `min_itemsize` will be the
    maximum of the length of any string passed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE835]'
  prefs: []
  type: TYPE_PRE
- en: '**nan_rep**'
  prefs: []
  type: TYPE_NORMAL
- en: String columns will serialize a `np.nan` (a missing value) with the `nan_rep`
    string representation. This defaults to the string value `nan`. You could inadvertently
    turn an actual `nan` value into a missing value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE836]'
  prefs: []
  type: TYPE_PRE
- en: Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`tables` format come with a writing performance penalty as compared to `fixed`
    stores. The benefit is the ability to append/delete and query (potentially very
    large amounts of data). Write times are generally longer as compared with regular
    stores. Query times can be quite fast, especially on an indexed axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `chunksize=<int>` to `append`, specifying the write chunksize (default
    is 50000). This will significantly lower your memory usage on writing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `expectedrows=<int>` to the first `append`, to set the TOTAL number
    of rows that `PyTables` will expect. This will optimize read/write performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate rows can be written to tables, but are filtered out in selection (with
    the last items being selected; thus a table is unique on major, minor pairs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `PerformanceWarning` will be raised if you are attempting to store types that
    will be pickled by PyTables (rather than stored as endemic types). See [Here](https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190)
    for more information and some solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read/write API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf`
    for writing, similar to how `read_csv` and `to_csv` work.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE837]'
  prefs: []
  type: TYPE_PRE
- en: HDFStore will by default not drop rows that are all missing. This behavior can
    be changed by setting `dropna=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE838]'
  prefs: []
  type: TYPE_PRE
- en: '### Fixed format'
  prefs: []
  type: TYPE_NORMAL
- en: The examples above show storing using `put`, which write the HDF5 to `PyTables`
    in a fixed array format, called the `fixed` format. These types of stores are
    **not** appendable once written (though you can simply remove them and rewrite).
    Nor are they **queryable**; they must be retrieved in their entirety. They also
    do not support dataframes with non-unique column names. The `fixed` format stores
    offer very fast writing and slightly faster reading than `table` stores. This
    format is specified by default when using `put` or `to_hdf` or by `format='fixed'`
    or `format='f'`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: 'A `fixed` format will raise a `TypeError` if you try to retrieve using a `where`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE839]'
  prefs: []
  type: TYPE_PRE
- en: '### Table format'
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` supports another `PyTables` format on disk, the `table` format.
    Conceptually a `table` is shaped very much like a DataFrame, with rows and columns.
    A `table` may be appended to in the same or other sessions. In addition, delete
    and query type operations are supported. This format is specified by `format=''table''`
    or `format=''t''` to `append` or `put` or `to_hdf`.'
  prefs: []
  type: TYPE_NORMAL
- en: This format can be set as an option as well `pd.set_option('io.hdf.default_format','table')`
    to enable `put/append/to_hdf` to by default store in the `table` format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE840]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can also create a `table` by passing `format='table'` or `format='t'` to
    a `put` operation.
  prefs: []
  type: TYPE_NORMAL
- en: '### Hierarchical keys'
  prefs: []
  type: TYPE_NORMAL
- en: Keys to a store can be specified as a string. These can be in a hierarchical
    path-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of
    sub-stores (or `Groups` in PyTables parlance). Keys can be specified without the
    leading ‘/’ and are **always** absolute (e.g. ‘foo’ refers to ‘/foo’). Removal
    operations can remove everything in the sub-store and **below**, so be *careful*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE841]'
  prefs: []
  type: TYPE_PRE
- en: You can walk through the group hierarchy using the `walk` method which will
    yield a tuple for each group key along with the relative keys of its contents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE842]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical keys cannot be retrieved as dotted (attribute) access as described
    above for items stored under the root node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE843]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE844]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, use explicit string based keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE845]'
  prefs: []
  type: TYPE_PRE
- en: '### Storing types'
  prefs: []
  type: TYPE_NORMAL
- en: Storing mixed types in a table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing mixed-dtype data is supported. Strings are stored as a fixed-width using
    the maximum size of the appended column. Subsequent attempts at appending longer
    strings will raise a `ValueError`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Passing `min_itemsize={`values`: size}` as a parameter to append will set a
    larger minimum for the string columns. Storing `floats, strings, ints, bools,
    datetime64` are currently supported. For string columns, passing `nan_rep = ''nan''`
    to append will change the default nan representation on disk (which converts to/from
    `np.nan`), this defaults to `nan`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE846]'
  prefs: []
  type: TYPE_PRE
- en: Storing MultiIndex DataFrames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting
    from homogeneous index `DataFrames`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE847]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `index` keyword is reserved and cannot be use as a level name.
  prefs: []
  type: TYPE_NORMAL
- en: Storing mixed types in a table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing mixed-dtype data is supported. Strings are stored as a fixed-width using
    the maximum size of the appended column. Subsequent attempts at appending longer
    strings will raise a `ValueError`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Passing `min_itemsize={`values`: size}` as a parameter to append will set a
    larger minimum for the string columns. Storing `floats, strings, ints, bools,
    datetime64` are currently supported. For string columns, passing `nan_rep = ''nan''`
    to append will change the default nan representation on disk (which converts to/from
    `np.nan`), this defaults to `nan`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE848]'
  prefs: []
  type: TYPE_PRE
- en: Storing MultiIndex DataFrames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storing MultiIndex `DataFrames` as tables is very similar to storing/selecting
    from homogeneous index `DataFrames`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE849]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `index` keyword is reserved and cannot be use as a level name.
  prefs: []
  type: TYPE_NORMAL
- en: '### Querying'
  prefs: []
  type: TYPE_NORMAL
- en: Querying a table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`select` and `delete` operations have an optional criterion that can be specified
    to select/delete only a subset of the data. This allows one to have a very large
    on-disk table and retrieve only a portion of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: A query is specified using the `Term` class under the hood, as a boolean expression.
  prefs: []
  type: TYPE_NORMAL
- en: '`index` and `columns` are supported indexers of `DataFrames`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `data_columns` are specified, these can be used as additional indexers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: level name in a MultiIndex, with default name `level_0`, `level_1`, … if not
    provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valid comparison operators are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`=, ==, !=, >, >=, <, <=`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid boolean expressions are combined with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`|` : or'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`&` : and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(` and `)` : for grouping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are similar to how boolean expressions are used in pandas for indexing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`=` will be automatically expanded to the comparison operator `==`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~` is the not operator, but can only be used in very limited circumstances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a list/tuple of expressions is passed they will be combined via `&`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are valid expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''index >= date''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns = [''A'', ''D'']"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns in [''A'', ''D'']"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns = A''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns == A''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"~(columns = [''A'', ''B''])"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''index > df.index[3] & string = "bar"''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"ts >= Timestamp(''2012-02-01'')"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"major_axis>=20130101"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `indexers` are on the left-hand side of the sub-expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '`columns`, `major_axis`, `ts`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-hand side of the sub-expression (after a comparison operator) can
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strings, e.g. `"bar"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: date-like, e.g. `20130101`, or `"20130101"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lists, e.g. `"['A', 'B']"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variables that are defined in the local names space, e.g. `date`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Passing a string to a query by interpolating it into the query expression is
    not recommended. Simply assign the string of interest to a variable and use that
    variable in an expression. For example, do this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE850]'
  prefs: []
  type: TYPE_PRE
- en: instead of this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE851]'
  prefs: []
  type: TYPE_PRE
- en: The latter will **not** work and will raise a `SyntaxError`.Note that there’s
    a single quote followed by a double quote in the `string` variable.
  prefs: []
  type: TYPE_NORMAL
- en: If you *must* interpolate, use the `'%r'` format specifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE852]'
  prefs: []
  type: TYPE_PRE
- en: which will quote `string`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE853]'
  prefs: []
  type: TYPE_PRE
- en: Use boolean expressions, with in-line function evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE854]'
  prefs: []
  type: TYPE_PRE
- en: Use inline column reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE855]'
  prefs: []
  type: TYPE_PRE
- en: 'The `columns` keyword can be supplied to select a list of columns to be returned,
    this is equivalent to passing a `''columns=list_of_columns_to_filter''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE856]'
  prefs: []
  type: TYPE_PRE
- en: '`start` and `stop` parameters can be specified to limit the total search space.
    These are in terms of the total number of rows in a table.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `ValueError` if the query expression has an unknown variable
    reference. Usually this means that you are trying to select on a column that is
    **not** a data_column.'
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `SyntaxError` if the query expression is not valid.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query timedelta64[ns]'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can store and query using the `timedelta64[ns]` type. Terms can be specified
    in the format: `<float>(<unit>)`, where float may be signed (and fractional),
    and unit can be `D,s,ms,us,ns` for the timedelta. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE857]  #### Query MultiIndex'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting from a `MultiIndex` can be achieved by using the name of the level.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE858]'
  prefs: []
  type: TYPE_PRE
- en: If the `MultiIndex` levels names are `None`, the levels are automatically made
    available via the `level_n` keyword with `n` the level of the `MultiIndex` you
    want to select from.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE859]'
  prefs: []
  type: TYPE_PRE
- en: Indexing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can create/modify an index for a table with `create_table_index` after data
    is already in the table (after and `append/put` operation). Creating a table index
    is **highly** encouraged. This will speed your queries a great deal when you use
    a `select` with the indexed dimension as the `where`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Indexes are automagically created on the indexables and any data columns you
    specify. This behavior can be turned off by passing `index=False` to `append`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE860]'
  prefs: []
  type: TYPE_PRE
- en: Oftentimes when appending large amounts of data to a store, it is useful to
    turn off index creation for each append, then recreate at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE861]'
  prefs: []
  type: TYPE_PRE
- en: Then create the index when finished appending.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE862]'
  prefs: []
  type: TYPE_PRE
- en: See [here](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)
    for how to create a completely-sorted-index (CSI) on an existing store.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query via data columns'
  prefs: []
  type: TYPE_NORMAL
- en: You can designate (and index) certain columns that you want to be able to perform
    queries (other than the `indexable` columns, which you can always query). For
    instance say you want to perform this common operation, on-disk, and return just
    the frame that matches this query. You can specify `data_columns = True` to force
    all columns to be `data_columns`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE863]'
  prefs: []
  type: TYPE_PRE
- en: There is some performance degradation by making lots of columns into `data columns`,
    so it is up to the user to designate these. In addition, you cannot change data
    columns (nor indexables) after the first append/put operation (Of course you can
    simply read in the data and create a new table!).
  prefs: []
  type: TYPE_NORMAL
- en: Iterator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and
    `select_as_multiple` to return an iterator on the results. The default is 50,000
    rows returned in a chunk.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE864]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the iterator with `read_hdf` which will open, then automatically
    close the store when finished iterating.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE865]'
  prefs: []
  type: TYPE_PRE
- en: Note, that the chunksize keyword applies to the **source** rows. So if you are
    doing a query, then the chunksize will subdivide the total rows in the table and
    the query applied, returning an iterator on potentially unequal sized chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a recipe for generating a query and using it to create equal sized return
    chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE866]'
  prefs: []
  type: TYPE_PRE
- en: Advanced queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Select a single column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE867]'
  prefs: []
  type: TYPE_PRE
- en: '##### Selecting coordinates'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE868]  ##### Selecting using a where mask'
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE869]'
  prefs: []
  type: TYPE_PRE
- en: Storer object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE870]'
  prefs: []
  type: TYPE_PRE
- en: Multiple table queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The methods `append_to_multiple` and `select_as_multiple` can perform appending/selecting
    from multiple tables at once. The idea is to have one table (call it the selector
    table) that you index most/all of the columns, and perform your queries. The other
    table(s) are data tables with an index matching the selector table’s index. You
    can then perform a very fast query on the selector table, yet get lots of data
    back. This method is similar to having a very wide table, but enables more efficient
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: The `append_to_multiple` method splits a given single DataFrame into multiple
    tables according to `d`, a dictionary that maps the table names to a list of ‘columns’
    you want in that table. If `None` is used in place of a list, that table will
    have the remaining unspecified columns of the given DataFrame. The argument `selector`
    defines which table is the selector table (which you can make queries from). The
    argument `dropna` will drop rows from the input `DataFrame` to ensure tables are
    synchronized. This means that if a row for one of the tables being written to
    is entirely `np.nan`, that row will be dropped from all tables.
  prefs: []
  type: TYPE_NORMAL
- en: If `dropna` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
    Remember that entirely `np.Nan` rows are not written to the HDFStore, so if you
    choose to call `dropna=False`, some tables may have more rows than others, and
    therefore `select_as_multiple` may not work or it may return unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE871]'
  prefs: []
  type: TYPE_PRE
- en: Querying a table
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`select` and `delete` operations have an optional criterion that can be specified
    to select/delete only a subset of the data. This allows one to have a very large
    on-disk table and retrieve only a portion of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: A query is specified using the `Term` class under the hood, as a boolean expression.
  prefs: []
  type: TYPE_NORMAL
- en: '`index` and `columns` are supported indexers of `DataFrames`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if `data_columns` are specified, these can be used as additional indexers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: level name in a MultiIndex, with default name `level_0`, `level_1`, … if not
    provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valid comparison operators are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`=, ==, !=, >, >=, <, <=`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Valid boolean expressions are combined with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`|` : or'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`&` : and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(` and `)` : for grouping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are similar to how boolean expressions are used in pandas for indexing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`=` will be automatically expanded to the comparison operator `==`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~` is the not operator, but can only be used in very limited circumstances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a list/tuple of expressions is passed they will be combined via `&`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are valid expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''index >= date''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns = [''A'', ''D'']"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"columns in [''A'', ''D'']"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns = A''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''columns == A''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"~(columns = [''A'', ''B''])"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''index > df.index[3] & string = "bar"''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''(index > df.index[3] & index <= df.index[6]) | string = "bar"''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"ts >= Timestamp(''2012-02-01'')"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"major_axis>=20130101"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `indexers` are on the left-hand side of the sub-expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '`columns`, `major_axis`, `ts`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right-hand side of the sub-expression (after a comparison operator) can
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: functions that will be evaluated, e.g. `Timestamp('2012-02-01')`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: strings, e.g. `"bar"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: date-like, e.g. `20130101`, or `"20130101"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lists, e.g. `"['A', 'B']"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: variables that are defined in the local names space, e.g. `date`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Passing a string to a query by interpolating it into the query expression is
    not recommended. Simply assign the string of interest to a variable and use that
    variable in an expression. For example, do this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE872]'
  prefs: []
  type: TYPE_PRE
- en: instead of this
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE873]'
  prefs: []
  type: TYPE_PRE
- en: The latter will **not** work and will raise a `SyntaxError`.Note that there’s
    a single quote followed by a double quote in the `string` variable.
  prefs: []
  type: TYPE_NORMAL
- en: If you *must* interpolate, use the `'%r'` format specifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE874]'
  prefs: []
  type: TYPE_PRE
- en: which will quote `string`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE875]'
  prefs: []
  type: TYPE_PRE
- en: Use boolean expressions, with in-line function evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE876]'
  prefs: []
  type: TYPE_PRE
- en: Use inline column reference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE877]'
  prefs: []
  type: TYPE_PRE
- en: 'The `columns` keyword can be supplied to select a list of columns to be returned,
    this is equivalent to passing a `''columns=list_of_columns_to_filter''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE878]'
  prefs: []
  type: TYPE_PRE
- en: '`start` and `stop` parameters can be specified to limit the total search space.
    These are in terms of the total number of rows in a table.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `ValueError` if the query expression has an unknown variable
    reference. Usually this means that you are trying to select on a column that is
    **not** a data_column.'
  prefs: []
  type: TYPE_NORMAL
- en: '`select` will raise a `SyntaxError` if the query expression is not valid.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query timedelta64[ns]'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can store and query using the `timedelta64[ns]` type. Terms can be specified
    in the format: `<float>(<unit>)`, where float may be signed (and fractional),
    and unit can be `D,s,ms,us,ns` for the timedelta. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE879]'
  prefs: []
  type: TYPE_PRE
- en: '#### Query MultiIndex'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting from a `MultiIndex` can be achieved by using the name of the level.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE880]'
  prefs: []
  type: TYPE_PRE
- en: If the `MultiIndex` levels names are `None`, the levels are automatically made
    available via the `level_n` keyword with `n` the level of the `MultiIndex` you
    want to select from.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE881]'
  prefs: []
  type: TYPE_PRE
- en: Indexing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can create/modify an index for a table with `create_table_index` after data
    is already in the table (after and `append/put` operation). Creating a table index
    is **highly** encouraged. This will speed your queries a great deal when you use
    a `select` with the indexed dimension as the `where`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Indexes are automagically created on the indexables and any data columns you
    specify. This behavior can be turned off by passing `index=False` to `append`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE882]'
  prefs: []
  type: TYPE_PRE
- en: Oftentimes when appending large amounts of data to a store, it is useful to
    turn off index creation for each append, then recreate at the end.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE883]'
  prefs: []
  type: TYPE_PRE
- en: Then create the index when finished appending.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE884]'
  prefs: []
  type: TYPE_PRE
- en: See [here](https://stackoverflow.com/questions/17893370/ptrepack-sortby-needs-full-index)
    for how to create a completely-sorted-index (CSI) on an existing store.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Query via data columns'
  prefs: []
  type: TYPE_NORMAL
- en: You can designate (and index) certain columns that you want to be able to perform
    queries (other than the `indexable` columns, which you can always query). For
    instance say you want to perform this common operation, on-disk, and return just
    the frame that matches this query. You can specify `data_columns = True` to force
    all columns to be `data_columns`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE885]'
  prefs: []
  type: TYPE_PRE
- en: There is some performance degradation by making lots of columns into `data columns`,
    so it is up to the user to designate these. In addition, you cannot change data
    columns (nor indexables) after the first append/put operation (Of course you can
    simply read in the data and create a new table!).
  prefs: []
  type: TYPE_NORMAL
- en: Iterator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and
    `select_as_multiple` to return an iterator on the results. The default is 50,000
    rows returned in a chunk.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE886]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the iterator with `read_hdf` which will open, then automatically
    close the store when finished iterating.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE887]'
  prefs: []
  type: TYPE_PRE
- en: Note, that the chunksize keyword applies to the **source** rows. So if you are
    doing a query, then the chunksize will subdivide the total rows in the table and
    the query applied, returning an iterator on potentially unequal sized chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a recipe for generating a query and using it to create equal sized return
    chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE888]'
  prefs: []
  type: TYPE_PRE
- en: Advanced queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Select a single column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE889]'
  prefs: []
  type: TYPE_PRE
- en: '##### Selecting coordinates'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE890]  ##### Selecting using a where mask'
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE891]'
  prefs: []
  type: TYPE_PRE
- en: Storer object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE892]'
  prefs: []
  type: TYPE_PRE
- en: Select a single column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To retrieve a single indexable or data column, use the method `select_column`.
    This will, for example, enable you to get the index very quickly. These return
    a `Series` of the result, indexed by the row number. These do not currently accept
    the `where` selector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE893]'
  prefs: []
  type: TYPE_PRE
- en: '##### Selecting coordinates'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you want to get the coordinates (a.k.a the index locations) of your
    query. This returns an `Index` of the resulting locations. These coordinates can
    also be passed to subsequent `where` operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE894]'
  prefs: []
  type: TYPE_PRE
- en: '##### Selecting using a where mask'
  prefs: []
  type: TYPE_NORMAL
- en: Sometime your query can involve creating a list of rows to select. Usually this
    `mask` would be a resulting `index` from an indexing operation. This example selects
    the months of a datetimeindex which are 5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE895]'
  prefs: []
  type: TYPE_PRE
- en: Storer object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you want to inspect the stored object, retrieve via `get_storer`. You could
    use this programmatically to say get the number of rows in an object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE896]'
  prefs: []
  type: TYPE_PRE
- en: Multiple table queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The methods `append_to_multiple` and `select_as_multiple` can perform appending/selecting
    from multiple tables at once. The idea is to have one table (call it the selector
    table) that you index most/all of the columns, and perform your queries. The other
    table(s) are data tables with an index matching the selector table’s index. You
    can then perform a very fast query on the selector table, yet get lots of data
    back. This method is similar to having a very wide table, but enables more efficient
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: The `append_to_multiple` method splits a given single DataFrame into multiple
    tables according to `d`, a dictionary that maps the table names to a list of ‘columns’
    you want in that table. If `None` is used in place of a list, that table will
    have the remaining unspecified columns of the given DataFrame. The argument `selector`
    defines which table is the selector table (which you can make queries from). The
    argument `dropna` will drop rows from the input `DataFrame` to ensure tables are
    synchronized. This means that if a row for one of the tables being written to
    is entirely `np.nan`, that row will be dropped from all tables.
  prefs: []
  type: TYPE_NORMAL
- en: If `dropna` is False, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**.
    Remember that entirely `np.Nan` rows are not written to the HDFStore, so if you
    choose to call `dropna=False`, some tables may have more rows than others, and
    therefore `select_as_multiple` may not work or it may return unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE897]'
  prefs: []
  type: TYPE_PRE
- en: Delete from a table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can delete from a table selectively by specifying a `where`. In deleting
    rows, it is important to understand the `PyTables` deletes rows by erasing the
    rows, then **moving** the following data. Thus deleting can potentially be a very
    expensive operation depending on the orientation of your data. To get optimal
    performance, it’s worthwhile to have the dimension you are deleting be the first
    of the `indexables`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is ordered (on the disk) in terms of the `indexables`. Here’s a simple
    use case. You store panel-type data, with dates in the `major_axis` and ids in
    the `minor_axis`. The data is then interleaved like this:'
  prefs: []
  type: TYPE_NORMAL
- en: date_1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_2
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: date_2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: id_1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: .
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: id_n
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be clear that a delete operation on the `major_axis` will be fairly
    quick, as one chunk is removed, then the following data moved. On the other hand
    a delete operation on the `minor_axis` will be very expensive. In this case it
    would almost certainly be faster to rewrite the table using a `where` that selects
    all but the missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files automatically.
    Thus, repeatedly deleting (or removing nodes) and adding again, **WILL TEND TO
    INCREASE THE FILE SIZE**.
  prefs: []
  type: TYPE_NORMAL
- en: To *repack and clean* the file, use [ptrepack](#io-hdf5-ptrepack).
  prefs: []
  type: TYPE_NORMAL
- en: '### Notes & caveats'
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`PyTables` allows the stored data to be compressed. This applies to all kinds
    of stores, not just tables. Two parameters are used to control compression: `complevel`
    and `complib`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`complevel` specifies if and how hard data is to be compressed. `complevel=0`
    and `complevel=None` disables compression and `0<complevel<10` enables compression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`complib` specifies which compression library to use. If nothing is specified
    the default library `zlib` is used. A compression library usually optimizes for
    either good compression rates or speed and the results will depend on the type
    of data. Which type of compression to choose depends on your specific needs and
    data. The list of supported compression libraries:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[zlib](https://zlib.net/): The default compression library. A classic in terms
    of compression, achieves good compression rates but is somewhat slow.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lzo](https://www.oberhumer.com/opensource/lzo/): Fast compression and decompression.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bzip2](https://sourceware.org/bzip2/): Good compression rates.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc](https://www.blosc.org/): Fast compression and decompression.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for alternative blosc compressors:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[blosc:blosclz](https://www.blosc.org/) This is the default compressor for
    `blosc`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html): A compact, very
    popular and fast compressor.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html): A tweaked version
    of LZ4, produces better compression ratios at the expense of speed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:snappy](https://google.github.io/snappy/): A popular compressor used
    in many places.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zlib](https://zlib.net/): A classic; somewhat slower than the previous
    ones, but achieving better compression ratios.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zstd](https://facebook.github.io/zstd/): An extremely well balanced
    codec; it provides the best compression ratios among the others above, and at
    reasonably fast speed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `complib` is defined as something other than the listed libraries a `ValueError`
    exception is issued.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the library specified with the `complib` option is missing on your platform,
    compression defaults to `zlib` without further ado.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable compression for all objects within the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE898]'
  prefs: []
  type: TYPE_PRE
- en: 'Or on-the-fly compression (this only applies to tables) in stores where compression
    is not enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE899]'
  prefs: []
  type: TYPE_PRE
- en: '#### ptrepack'
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` offers better write performance when tables are compressed after
    they are written, as opposed to turning on compression at the very beginning.
    You can use the supplied `PyTables` utility `ptrepack`. In addition, `ptrepack`
    can change compression levels after the fact.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE900]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow you to
    reuse previously deleted space. Alternatively, one can simply remove the file
    and write again, or use the `copy` method.  #### Caveats'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is **not-threadsafe for writing**. The underlying `PyTables` only
    supports concurrent reads (via threading or processes). If you need reading and
    writing *at the same time*, you need to serialize these operations in a single
    thread in a single process. You will corrupt your data otherwise. See the ([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397)) for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: If you use locks to manage write access between multiple processes, you may
    want to use [`fsync()`](https://docs.python.org/3/library/os.html#os.fsync "(in
    Python v3.12)") before releasing write locks. For convenience you can use `store.flush(fsync=True)`
    to do this for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a `table` is created columns (DataFrame) are fixed; only exactly the same
    columns can be appended
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not necessarily
    equal across timezone versions. So if data is localized to a specific timezone
    in the HDFStore using one version of a timezone library and that data is updated
    with another version, the data will be converted to UTC since these timezones
    are not considered equal. Either use the same version of timezone library or use
    `tz_convert` with the updated timezone definition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` will show a `NaturalNameWarning` if a column name cannot be used
    as an attribute selector. *Natural* identifiers contain only letters, numbers,
    and underscores, and may not begin with a number. Other identifiers cannot be
    used in a `where` clause and are generally a bad idea.'
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`PyTables` allows the stored data to be compressed. This applies to all kinds
    of stores, not just tables. Two parameters are used to control compression: `complevel`
    and `complib`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`complevel` specifies if and how hard data is to be compressed. `complevel=0`
    and `complevel=None` disables compression and `0<complevel<10` enables compression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`complib` specifies which compression library to use. If nothing is specified
    the default library `zlib` is used. A compression library usually optimizes for
    either good compression rates or speed and the results will depend on the type
    of data. Which type of compression to choose depends on your specific needs and
    data. The list of supported compression libraries:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[zlib](https://zlib.net/): The default compression library. A classic in terms
    of compression, achieves good compression rates but is somewhat slow.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[lzo](https://www.oberhumer.com/opensource/lzo/): Fast compression and decompression.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[bzip2](https://sourceware.org/bzip2/): Good compression rates.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc](https://www.blosc.org/): Fast compression and decompression.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Support for alternative blosc compressors:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[blosc:blosclz](https://www.blosc.org/) This is the default compressor for
    `blosc`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4](https://fastcompression.blogspot.com/p/lz4.html): A compact, very
    popular and fast compressor.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:lz4hc](https://fastcompression.blogspot.com/p/lz4.html): A tweaked version
    of LZ4, produces better compression ratios at the expense of speed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:snappy](https://google.github.io/snappy/): A popular compressor used
    in many places.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zlib](https://zlib.net/): A classic; somewhat slower than the previous
    ones, but achieving better compression ratios.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[blosc:zstd](https://facebook.github.io/zstd/): An extremely well balanced
    codec; it provides the best compression ratios among the others above, and at
    reasonably fast speed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If `complib` is defined as something other than the listed libraries a `ValueError`
    exception is issued.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If the library specified with the `complib` option is missing on your platform,
    compression defaults to `zlib` without further ado.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable compression for all objects within the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE901]'
  prefs: []
  type: TYPE_PRE
- en: 'Or on-the-fly compression (this only applies to tables) in stores where compression
    is not enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE902]'
  prefs: []
  type: TYPE_PRE
- en: '#### ptrepack'
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` offers better write performance when tables are compressed after
    they are written, as opposed to turning on compression at the very beginning.
    You can use the supplied `PyTables` utility `ptrepack`. In addition, `ptrepack`
    can change compression levels after the fact.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE903]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore `ptrepack in.h5 out.h5` will *repack* the file to allow you to reuse
    previously deleted space. Alternatively, one can simply remove the file and write
    again, or use the `copy` method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Caveats'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` is **not-threadsafe for writing**. The underlying `PyTables` only
    supports concurrent reads (via threading or processes). If you need reading and
    writing *at the same time*, you need to serialize these operations in a single
    thread in a single process. You will corrupt your data otherwise. See the ([GH
    2397](https://github.com/pandas-dev/pandas/issues/2397)) for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: If you use locks to manage write access between multiple processes, you may
    want to use [`fsync()`](https://docs.python.org/3/library/os.html#os.fsync "(in
    Python v3.12)") before releasing write locks. For convenience you can use `store.flush(fsync=True)`
    to do this for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a `table` is created columns (DataFrame) are fixed; only exactly the same
    columns can be appended
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not necessarily
    equal across timezone versions. So if data is localized to a specific timezone
    in the HDFStore using one version of a timezone library and that data is updated
    with another version, the data will be converted to UTC since these timezones
    are not considered equal. Either use the same version of timezone library or use
    `tz_convert` with the updated timezone definition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTables` will show a `NaturalNameWarning` if a column name cannot be used
    as an attribute selector. *Natural* identifiers contain only letters, numbers,
    and underscores, and may not begin with a number. Other identifiers cannot be
    used in a `where` clause and are generally a bad idea.'
  prefs: []
  type: TYPE_NORMAL
- en: '### DataTypes'
  prefs: []
  type: TYPE_NORMAL
- en: '`HDFStore` will map an object dtype to the `PyTables` underlying dtype. This
    means the following types are known to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Represents missing values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| floating : `float64, float32, float16` | `np.nan` |'
  prefs: []
  type: TYPE_TB
- en: '| integer : `int64, int32, int8, uint64,uint32, uint8` |  |'
  prefs: []
  type: TYPE_TB
- en: '| boolean |  |'
  prefs: []
  type: TYPE_TB
- en: '| `datetime64[ns]` | `NaT` |'
  prefs: []
  type: TYPE_TB
- en: '| `timedelta64[ns]` | `NaT` |'
  prefs: []
  type: TYPE_TB
- en: '| categorical : see the section below |  |'
  prefs: []
  type: TYPE_TB
- en: '| object : `strings` | `np.nan` |'
  prefs: []
  type: TYPE_TB
- en: '`unicode` columns are not supported, and **WILL FAIL**.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  prefs: []
  type: TYPE_NORMAL
- en: You can write data that contains `category` dtypes to a `HDFStore`. Queries
    work the same as if it was an object array. However, the `category` dtyped data
    is stored in a more efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE904]'
  prefs: []
  type: TYPE_PRE
- en: String columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**min_itemsize**'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation of `HDFStore` uses a fixed column width (itemsize)
    for string columns. A string column itemsize is calculated as the maximum of the
    length of data (for that column) that is passed to the `HDFStore`, **in the first
    append**. Subsequent appends, may introduce a string for a column **larger** than
    the column can hold, an Exception will be raised (otherwise you could have a silent
    truncation of these columns, leading to loss of information). In the future we
    may relax this and allow a user-specified truncation to occur.
  prefs: []
  type: TYPE_NORMAL
- en: Pass `min_itemsize` on the first table creation to a-priori specify the minimum
    length of a particular string column. `min_itemsize` can be an integer, or a dict
    mapping a column name to an integer. You can pass `values` as a key to allow all
    *indexables* or *data_columns* to have this min_itemsize.
  prefs: []
  type: TYPE_NORMAL
- en: Passing a `min_itemsize` dict will cause all passed columns to be created as
    *data_columns* automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are not passing any `data_columns`, then the `min_itemsize` will be the
    maximum of the length of any string passed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE905]'
  prefs: []
  type: TYPE_PRE
- en: '**nan_rep**'
  prefs: []
  type: TYPE_NORMAL
- en: String columns will serialize a `np.nan` (a missing value) with the `nan_rep`
    string representation. This defaults to the string value `nan`. You could inadvertently
    turn an actual `nan` value into a missing value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE906]'
  prefs: []
  type: TYPE_PRE
- en: '#### Categorical data'
  prefs: []
  type: TYPE_NORMAL
- en: You can write data that contains `category` dtypes to a `HDFStore`. Queries
    work the same as if it was an object array. However, the `category` dtyped data
    is stored in a more efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE907]'
  prefs: []
  type: TYPE_PRE
- en: String columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**min_itemsize**'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying implementation of `HDFStore` uses a fixed column width (itemsize)
    for string columns. A string column itemsize is calculated as the maximum of the
    length of data (for that column) that is passed to the `HDFStore`, **in the first
    append**. Subsequent appends, may introduce a string for a column **larger** than
    the column can hold, an Exception will be raised (otherwise you could have a silent
    truncation of these columns, leading to loss of information). In the future we
    may relax this and allow a user-specified truncation to occur.
  prefs: []
  type: TYPE_NORMAL
- en: Pass `min_itemsize` on the first table creation to a-priori specify the minimum
    length of a particular string column. `min_itemsize` can be an integer, or a dict
    mapping a column name to an integer. You can pass `values` as a key to allow all
    *indexables* or *data_columns* to have this min_itemsize.
  prefs: []
  type: TYPE_NORMAL
- en: Passing a `min_itemsize` dict will cause all passed columns to be created as
    *data_columns* automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are not passing any `data_columns`, then the `min_itemsize` will be the
    maximum of the length of any string passed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE908]'
  prefs: []
  type: TYPE_PRE
- en: '**nan_rep**'
  prefs: []
  type: TYPE_NORMAL
- en: String columns will serialize a `np.nan` (a missing value) with the `nan_rep`
    string representation. This defaults to the string value `nan`. You could inadvertently
    turn an actual `nan` value into a missing value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE909]'
  prefs: []
  type: TYPE_PRE
- en: Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`tables` format come with a writing performance penalty as compared to `fixed`
    stores. The benefit is the ability to append/delete and query (potentially very
    large amounts of data). Write times are generally longer as compared with regular
    stores. Query times can be quite fast, especially on an indexed axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `chunksize=<int>` to `append`, specifying the write chunksize (default
    is 50000). This will significantly lower your memory usage on writing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass `expectedrows=<int>` to the first `append`, to set the TOTAL number
    of rows that `PyTables` will expect. This will optimize read/write performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate rows can be written to tables, but are filtered out in selection (with
    the last items being selected; thus a table is unique on major, minor pairs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `PerformanceWarning` will be raised if you are attempting to store types that
    will be pickled by PyTables (rather than stored as endemic types). See [Here](https://stackoverflow.com/questions/14355151/how-to-make-pandas-hdfstore-put-operation-faster/14370190#14370190)
    for more information and some solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '## Feather'
  prefs: []
  type: TYPE_NORMAL
- en: Feather provides binary columnar serialization for data frames. It is designed
    to make reading and writing data frames efficient, and to make sharing data across
    data analysis languages easy.
  prefs: []
  type: TYPE_NORMAL
- en: Feather is designed to faithfully serialize and de-serialize DataFrames, supporting
    all of the pandas dtypes, including extension dtypes such as categorical and datetime
    with tz.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: The format will NOT write an `Index`, or `MultiIndex` for the `DataFrame` and
    will raise an error if a non-default one is provided. You can `.reset_index()`
    to store the index or `.reset_index(drop=True)` to ignore it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate column names and non-string columns names are not supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actual Python objects in object dtype columns are not supported. These will
    raise a helpful error message on an attempt at serialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See the [Full Documentation](https://github.com/wesm/feather).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE910]'
  prefs: []
  type: TYPE_PRE
- en: Write to a feather file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE911]'
  prefs: []
  type: TYPE_PRE
- en: Read from a feather file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE912]'
  prefs: []
  type: TYPE_PRE
- en: '## Parquet'
  prefs: []
  type: TYPE_NORMAL
- en: '[Apache Parquet](https://parquet.apache.org/) provides a partitioned binary
    columnar serialization for data frames. It is designed to make reading and writing
    data frames efficient, and to make sharing data across data analysis languages
    easy. Parquet can use a variety of compression techniques to shrink the file size
    as much as possible while still maintaining good read performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Parquet is designed to faithfully serialize and de-serialize `DataFrame` s,
    supporting all of the pandas dtypes, including extension dtypes such as datetime
    with tz.
  prefs: []
  type: TYPE_NORMAL
- en: Several caveats.
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate column names and non-string columns names are not supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine always writes the index to the output, but `fastparquet`
    only writes non-default indexes. This extra column can cause problems for non-pandas
    consumers that are not expecting it. You can force including or omitting indexes
    with the `index` argument, regardless of the underlying engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index level names, if specified, must be strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `pyarrow` engine, categorical dtypes for non-string types can be serialized
    to parquet, but will de-serialize as their primitive dtype.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine preserves the `ordered` flag of categorical dtypes with
    string types. `fastparquet` does not preserve the `ordered` flag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non supported types include `Interval` and actual Python object types. These
    will raise a helpful error message on an attempt at serialization. `Period` type
    is supported with pyarrow >= 0.16.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pyarrow` engine preserves extension data types such as the nullable integer
    and string data type (requiring pyarrow >= 0.16.0, and requiring the extension
    type to implement the needed protocols, see the [extension types documentation](../development/extending.html#extending-extension-arrow)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can specify an `engine` to direct the serialization. This can be one of
    `pyarrow`, or `fastparquet`, or `auto`. If the engine is NOT specified, then the
    `pd.options.io.parquet.engine` option is checked; if this is also `auto`, then
    `pyarrow` is tried, and falling back to `fastparquet`.
  prefs: []
  type: TYPE_NORMAL
- en: See the documentation for [pyarrow](https://arrow.apache.org/docs/python/) and
    [fastparquet](https://fastparquet.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: These engines are very similar and should read/write nearly identical parquet
    format files. `pyarrow>=8.0.0` supports timedelta data, `fastparquet>=0.1.4` supports
    timezone aware datetimes. These libraries differ by having different underlying
    dependencies (`fastparquet` by using `numba`, while `pyarrow` uses a c-library).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE913]'
  prefs: []
  type: TYPE_PRE
- en: Write to a parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE914]'
  prefs: []
  type: TYPE_PRE
- en: Read from a parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE915]'
  prefs: []
  type: TYPE_PRE
- en: By setting the `dtype_backend` argument you can control the default dtypes used
    for the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE916]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is not supported for `fastparquet`.
  prefs: []
  type: TYPE_NORMAL
- en: Read only certain columns of a parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE917]'
  prefs: []
  type: TYPE_PRE
- en: Handling indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Serializing a `DataFrame` to parquet may include the implicit index as one
    or more columns in the output file. Thus, this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE918]'
  prefs: []
  type: TYPE_PRE
- en: 'creates a parquet file with *three* columns if you use `pyarrow` for serialization:
    `a`, `b`, and `__index_level_0__`. If you’re using `fastparquet`, the index [may
    or may not](https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write)
    be written to the file.'
  prefs: []
  type: TYPE_NORMAL
- en: This unexpected extra column causes some databases like Amazon Redshift to reject
    the file, because that column doesn’t exist in the target table.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to omit a dataframe’s indexes when writing, pass `index=False`
    to [`to_parquet()`](../reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet
    "pandas.DataFrame.to_parquet"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE919]'
  prefs: []
  type: TYPE_PRE
- en: This creates a parquet file with just the two expected columns, `a` and `b`.
    If your `DataFrame` has a custom index, you won’t get it back when you load this
    file into a `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Passing `index=True` will *always* write the index, even if that’s not the underlying
    engine’s default behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning Parquet files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parquet supports partitioning of data based on the values of one or more columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE920]'
  prefs: []
  type: TYPE_PRE
- en: 'The `path` specifies the parent directory to which data will be saved. The
    `partition_cols` are the column names by which the dataset will be partitioned.
    Columns are partitioned in the order they are given. The partition splits are
    determined by the unique values in the partition columns. The above example creates
    a partitioned dataset that may look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE921]'
  prefs: []
  type: TYPE_PRE
- en: Handling indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Serializing a `DataFrame` to parquet may include the implicit index as one
    or more columns in the output file. Thus, this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE922]'
  prefs: []
  type: TYPE_PRE
- en: 'creates a parquet file with *three* columns if you use `pyarrow` for serialization:
    `a`, `b`, and `__index_level_0__`. If you’re using `fastparquet`, the index [may
    or may not](https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write)
    be written to the file.'
  prefs: []
  type: TYPE_NORMAL
- en: This unexpected extra column causes some databases like Amazon Redshift to reject
    the file, because that column doesn’t exist in the target table.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to omit a dataframe’s indexes when writing, pass `index=False`
    to [`to_parquet()`](../reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet
    "pandas.DataFrame.to_parquet"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE923]'
  prefs: []
  type: TYPE_PRE
- en: This creates a parquet file with just the two expected columns, `a` and `b`.
    If your `DataFrame` has a custom index, you won’t get it back when you load this
    file into a `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Passing `index=True` will *always* write the index, even if that’s not the underlying
    engine’s default behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning Parquet files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parquet supports partitioning of data based on the values of one or more columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE924]'
  prefs: []
  type: TYPE_PRE
- en: 'The `path` specifies the parent directory to which data will be saved. The
    `partition_cols` are the column names by which the dataset will be partitioned.
    Columns are partitioned in the order they are given. The partition splits are
    determined by the unique values in the partition columns. The above example creates
    a partitioned dataset that may look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE925]'
  prefs: []
  type: TYPE_PRE
- en: '## ORC'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the [parquet](#io-parquet) format, the [ORC Format](https://orc.apache.org/)
    is a binary columnar serialization for data frames. It is designed to make reading
    data frames efficient. pandas provides both the reader and the writer for the
    ORC format, [`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc
    "pandas.read_orc") and [`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc"). This requires the [pyarrow](https://arrow.apache.org/docs/python/)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: It is *highly recommended* to install pyarrow using conda due to some issues
    occurred by pyarrow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc") requires pyarrow>=7.0.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`read_orc()`](../reference/api/pandas.read_orc.html#pandas.read_orc "pandas.read_orc")
    and [`to_orc()`](../reference/api/pandas.DataFrame.to_orc.html#pandas.DataFrame.to_orc
    "pandas.DataFrame.to_orc") are not supported on Windows yet, you can find valid
    environments on [install optional dependencies](../getting_started/install.html#install-warn-orc).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For supported dtypes please refer to [supported ORC features in Arrow](https://arrow.apache.org/docs/cpp/orc.html#data-types).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently timezones in datetime columns are not preserved when a dataframe is
    converted into ORC files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE926]'
  prefs: []
  type: TYPE_PRE
- en: Write to an orc file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE927]'
  prefs: []
  type: TYPE_PRE
- en: Read from an orc file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE928]'
  prefs: []
  type: TYPE_PRE
- en: Read only certain columns of an orc file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE929]'
  prefs: []
  type: TYPE_PRE
- en: '## SQL queries'
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas.io.sql` module provides a collection of query wrappers to both facilitate
    data retrieval and to reduce dependency on DB-specific API.
  prefs: []
  type: TYPE_NORMAL
- en: Where available, users may first want to opt for [Apache Arrow ADBC](https://arrow.apache.org/adbc/current/index.html)
    drivers. These drivers should provide the best performance, null handling, and
    type detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'New in version 2.2.0: Added native support for ADBC drivers'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For a full list of ADBC drivers and their development status, see the [ADBC
    Driver Implementation Status](https://arrow.apache.org/adbc/current/driver/status.html)
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Where an ADBC driver is not available or may be missing functionality, users
    should opt for installing SQLAlchemy alongside their database driver library.
    Examples of such drivers are [psycopg2](https://www.psycopg.org/) for PostgreSQL
    or [pymysql](https://github.com/PyMySQL/PyMySQL) for MySQL. For [SQLite](https://docs.python.org/3/library/sqlite3.html)
    this is included in Python’s standard library by default. You can find an overview
    of supported drivers for each SQL dialect in the [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/dialects/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: If SQLAlchemy is not installed, you can use a [`sqlite3.Connection`](https://docs.python.org/3/library/sqlite3.html#sqlite3.Connection
    "(in Python v3.12)") in place of a SQLAlchemy engine, connection, or URI string.
  prefs: []
  type: TYPE_NORMAL
- en: See also some [cookbook examples](cookbook.html#cookbook-sql) for some advanced
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '| [`read_sql_table`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table")(table_name, con[, schema, ...]) | Read SQL database table
    into a DataFrame. |'
  prefs: []
  type: TYPE_TB
- en: '| [`read_sql_query`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query")(sql, con[, index_col, ...]) | Read SQL query into a DataFrame.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [`read_sql`](../reference/api/pandas.read_sql.html#pandas.read_sql "pandas.read_sql")(sql, con[, index_col, ...])
    | Read SQL query or database table into a DataFrame. |'
  prefs: []
  type: TYPE_TB
- en: '| [`DataFrame.to_sql`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql")(name, con, *[, schema, ...]) | Write records stored
    in a DataFrame to a SQL database. |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The function [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") is a convenience wrapper around [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") (and for backward compatibility) and will delegate to
    specific function depending on the provided input (database table name or sql
    query). Table names do not need to be quoted if they have special characters.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we use the [SQlite](https://www.sqlite.org/index.html)
    SQL database engine. You can use a temporary SQLite database where data are stored
    in “memory”.
  prefs: []
  type: TYPE_NORMAL
- en: To connect using an ADBC driver you will want to install the `adbc_driver_sqlite`
    using your package manager. Once installed, you can use the DBAPI interface provided
    by the ADBC driver to connect to your database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE930]'
  prefs: []
  type: TYPE_PRE
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to. For more information on `create_engine()` and
    the URI formatting, see the examples below and the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE931]'
  prefs: []
  type: TYPE_PRE
- en: If you want to manage your own connections you can pass one of those instead.
    The example below opens a connection to the database using a Python context manager
    that automatically closes the connection after the block has completed. See the
    [SQLAlchemy docs](https://docs.sqlalchemy.org/en/latest/core/connections.html#basic-usage)
    for an explanation of how the database connection is handled.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE932]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: When you open a connection to a database you are also responsible for closing
    it. Side effects of leaving a connection open may include locking the database
    or other breaking behaviour.
  prefs: []
  type: TYPE_NORMAL
- en: Writing DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assuming the following data is in a `DataFrame` `data`, we can insert it into
    the database using [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql").
  prefs: []
  type: TYPE_NORMAL
- en: '| id | Date | Col_1 | Col_2 | Col_3 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 2012-10-18 | X | 25.7 | True |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | 2012-10-19 | Y | -12.4 | False |'
  prefs: []
  type: TYPE_TB
- en: '| 63 | 2012-10-20 | Z | 5.73 | True |'
  prefs: []
  type: TYPE_TB
- en: '[PRE933]'
  prefs: []
  type: TYPE_PRE
- en: 'With some databases, writing large DataFrames can result in errors due to packet
    size limitations being exceeded. This can be avoided by setting the `chunksize`
    parameter when calling `to_sql`. For example, the following writes `data` to the
    database in batches of 1000 rows at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE934]'
  prefs: []
  type: TYPE_PRE
- en: SQL data types
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensuring consistent data type management across SQL databases is challenging.
    Not every SQL database offers the same types, and even when they do the implementation
    of a given type can vary in ways that have subtle effects on how types can be
    preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the best odds at preserving database types users are advised to use ADBC
    drivers when available. The Arrow type system offers a wider array of types that
    more closely match database types than the historical pandas/NumPy type system.
    To illustrate, note this (non-exhaustive) listing of types available in different
    databases and pandas backends:'
  prefs: []
  type: TYPE_NORMAL
- en: '| numpy/pandas | arrow | postgres | sqlite |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| int32/Int32 | int32 | INTEGER | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| int64/Int64 | int64 | BIGINT | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| float32 | float32 | REAL | REAL |'
  prefs: []
  type: TYPE_TB
- en: '| float64 | float64 | DOUBLE PRECISION | REAL |'
  prefs: []
  type: TYPE_TB
- en: '| object | string | TEXT | TEXT |'
  prefs: []
  type: TYPE_TB
- en: '| bool | `bool_` | BOOLEAN |  |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | date32 | DATE |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | month_day_nano_interval | INTERVAL |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | binary | BINARY | BLOB |'
  prefs: []
  type: TYPE_TB
- en: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | list | ARRAY [[1]](#f1) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | struct |'
  prefs: []
  type: TYPE_TB
- en: COMPOSITE TYPE
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]](#f1)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: Footnotes
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in preserving database types as best as possible throughout
    the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"`
    argument of [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE935]'
  prefs: []
  type: TYPE_PRE
- en: This will prevent your data from being converted to the traditional pandas/NumPy
    type system, which often converts SQL types in ways that make them impossible
    to round-trip.
  prefs: []
  type: TYPE_NORMAL
- en: In case an ADBC driver is not available, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") will try to map your data to an appropriate SQL data
    type based on the dtype of the data. When you have columns of dtype `object`,
    pandas will try to infer the data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always override the default type by specifying the desired SQL type
    of any of the columns by using the `dtype` argument. This argument needs a dictionary
    mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback
    mode). For example, specifying to use the sqlalchemy `String` type instead of
    the default `Text` type for string columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE936]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limited support for timedelta’s in the different database flavors,
    columns with type `timedelta64` will be written as integer values as nanoseconds
    to the database and a warning will be raised. The only exception to this is when
    using the ADBC PostgreSQL driver in which case a timedelta will be written to
    the database as an `INTERVAL`
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Columns of `category` dtype will be converted to the dense representation as
    you would get with `np.asarray(categorical)` (e.g. for string categories this
    gives an array of strings). Because of this, reading the database table back in
    does **not** generate a categorical.
  prefs: []
  type: TYPE_NORMAL
- en: '### Datetime data types'
  prefs: []
  type: TYPE_NORMAL
- en: Using ADBC or SQLAlchemy, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") is capable of writing datetime data that is timezone
    naive or timezone aware. However, the resulting data stored in the database ultimately
    depends on the supported data type for datetime data of the database system being
    used.
  prefs: []
  type: TYPE_NORMAL
- en: The following table lists supported data types for datetime data for some common
    databases. Other database dialects may have different data types for datetime
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '| Database | SQL Datetime Types | Timezone Support |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SQLite | `TEXT` | No |'
  prefs: []
  type: TYPE_TB
- en: '| MySQL | `TIMESTAMP` or `DATETIME` | No |'
  prefs: []
  type: TYPE_TB
- en: '| PostgreSQL | `TIMESTAMP` or `TIMESTAMP WITH TIME ZONE` | Yes |'
  prefs: []
  type: TYPE_TB
- en: When writing timezone aware data to databases that do not support timezones,
    the data will be written as timezone naive timestamps that are in local time with
    respect to the timezone.
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") is also capable of reading datetime data that is timezone
    aware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will convert
    the data to UTC.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Insertion method'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `method` controls the SQL insertion clause used. Possible values
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`None`: Uses standard SQL `INSERT` clause (one per row).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''multi''`: Pass multiple values in a single `INSERT` clause. It uses a *special*
    SQL syntax not supported by all backends. This usually provides better performance
    for analytic databases like *Presto* and *Redshift*, but has worse performance
    for traditional SQL backend if the table contains many columns. For more information
    check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'callable with signature `(pd_table, conn, keys, data_iter)`: This can be used
    to implement a more performant insertion method based on specific backend dialect
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE937]'
  prefs: []
  type: TYPE_PRE
- en: Reading tables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") will read a database table given the table name and optionally
    a subset of columns to read.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In order to use [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table"), you **must** have the ADBC driver or SQLAlchemy optional
    dependency installed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE938]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: ADBC drivers will map database types directly back to arrow types. For other
    drivers note that pandas infers column dtypes from query outputs, and not by looking
    up data types in the physical database schema. For example, assume `userid` is
    an integer column in a table. Then, intuitively, `select userid ...` will return
    integer-valued series, while `select cast(userid as text) ...` will return object-valued
    (str) series. Accordingly, if the query output is empty, then all resulting columns
    will be returned as object-valued (since they are most general). If you foresee
    that your query will sometimes generate an empty result, you may want to explicitly
    typecast afterwards to ensure dtype integrity.
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify the name of the column as the `DataFrame` index, and specify
    a subset of columns to be read.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE939]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can explicitly force columns to be parsed as dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE940]'
  prefs: []
  type: TYPE_PRE
- en: 'If needed you can explicitly specify a format string, or a dict of arguments
    to pass to [`pandas.to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE941]'
  prefs: []
  type: TYPE_PRE
- en: You can check if a table exists using `has_table()`
  prefs: []
  type: TYPE_NORMAL
- en: Schema support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reading from and writing to different schema’s is supported through the `schema`
    keyword in the [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") functions. Note however that this depends on the database
    flavor (sqlite does not have schema’s). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE942]'
  prefs: []
  type: TYPE_PRE
- en: Querying
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can query using raw SQL in the [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function. In this case you must use the SQL variant appropriate
    for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression
    language constructs, which are database-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE943]'
  prefs: []
  type: TYPE_PRE
- en: Of course, you can specify a more “complex” query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE944]'
  prefs: []
  type: TYPE_PRE
- en: 'The [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function supports a `chunksize` argument. Specifying
    this will return an iterator through chunks of the query result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE945]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE946]'
  prefs: []
  type: TYPE_PRE
- en: Engine connection examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE947]'
  prefs: []
  type: TYPE_PRE
- en: For more information see the examples the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  prefs: []
  type: TYPE_NORMAL
- en: Advanced SQLAlchemy queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use SQLAlchemy constructs to describe your query.
  prefs: []
  type: TYPE_NORMAL
- en: Use `sqlalchemy.text()` to specify query parameters in a backend-neutral way
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE948]'
  prefs: []
  type: TYPE_PRE
- en: If you have an SQLAlchemy description of your database you can express where
    conditions using SQLAlchemy expressions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE949]'
  prefs: []
  type: TYPE_PRE
- en: You can combine SQLAlchemy expressions with parameters passed to [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") using `sqlalchemy.bindparam()`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE950]'
  prefs: []
  type: TYPE_PRE
- en: Sqlite fallback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of sqlite is supported without using SQLAlchemy. This mode requires
    a Python database adapter which respect the [Python DB-API](https://www.python.org/dev/peps/pep-0249/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create connections like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE951]'
  prefs: []
  type: TYPE_PRE
- en: 'And then issue the following queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE952]'
  prefs: []
  type: TYPE_PRE
- en: Writing DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assuming the following data is in a `DataFrame` `data`, we can insert it into
    the database using [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql").
  prefs: []
  type: TYPE_NORMAL
- en: '| id | Date | Col_1 | Col_2 | Col_3 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 2012-10-18 | X | 25.7 | True |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | 2012-10-19 | Y | -12.4 | False |'
  prefs: []
  type: TYPE_TB
- en: '| 63 | 2012-10-20 | Z | 5.73 | True |'
  prefs: []
  type: TYPE_TB
- en: '[PRE953]'
  prefs: []
  type: TYPE_PRE
- en: 'With some databases, writing large DataFrames can result in errors due to packet
    size limitations being exceeded. This can be avoided by setting the `chunksize`
    parameter when calling `to_sql`. For example, the following writes `data` to the
    database in batches of 1000 rows at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE954]'
  prefs: []
  type: TYPE_PRE
- en: SQL data types
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensuring consistent data type management across SQL databases is challenging.
    Not every SQL database offers the same types, and even when they do the implementation
    of a given type can vary in ways that have subtle effects on how types can be
    preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the best odds at preserving database types users are advised to use ADBC
    drivers when available. The Arrow type system offers a wider array of types that
    more closely match database types than the historical pandas/NumPy type system.
    To illustrate, note this (non-exhaustive) listing of types available in different
    databases and pandas backends:'
  prefs: []
  type: TYPE_NORMAL
- en: '| numpy/pandas | arrow | postgres | sqlite |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| int32/Int32 | int32 | INTEGER | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| int64/Int64 | int64 | BIGINT | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| float32 | float32 | REAL | REAL |'
  prefs: []
  type: TYPE_TB
- en: '| float64 | float64 | DOUBLE PRECISION | REAL |'
  prefs: []
  type: TYPE_TB
- en: '| object | string | TEXT | TEXT |'
  prefs: []
  type: TYPE_TB
- en: '| bool | `bool_` | BOOLEAN |  |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | date32 | DATE |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | month_day_nano_interval | INTERVAL |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | binary | BINARY | BLOB |'
  prefs: []
  type: TYPE_TB
- en: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | list | ARRAY [[1]](#f1) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | struct |'
  prefs: []
  type: TYPE_TB
- en: COMPOSITE TYPE
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]](#f1)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: Footnotes
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in preserving database types as best as possible throughout
    the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"`
    argument of [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE955]'
  prefs: []
  type: TYPE_PRE
- en: This will prevent your data from being converted to the traditional pandas/NumPy
    type system, which often converts SQL types in ways that make them impossible
    to round-trip.
  prefs: []
  type: TYPE_NORMAL
- en: In case an ADBC driver is not available, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") will try to map your data to an appropriate SQL data
    type based on the dtype of the data. When you have columns of dtype `object`,
    pandas will try to infer the data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always override the default type by specifying the desired SQL type
    of any of the columns by using the `dtype` argument. This argument needs a dictionary
    mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback
    mode). For example, specifying to use the sqlalchemy `String` type instead of
    the default `Text` type for string columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE956]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limited support for timedelta’s in the different database flavors,
    columns with type `timedelta64` will be written as integer values as nanoseconds
    to the database and a warning will be raised. The only exception to this is when
    using the ADBC PostgreSQL driver in which case a timedelta will be written to
    the database as an `INTERVAL`
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Columns of `category` dtype will be converted to the dense representation as
    you would get with `np.asarray(categorical)` (e.g. for string categories this
    gives an array of strings). Because of this, reading the database table back in
    does **not** generate a categorical.
  prefs: []
  type: TYPE_NORMAL
- en: SQL data types
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensuring consistent data type management across SQL databases is challenging.
    Not every SQL database offers the same types, and even when they do the implementation
    of a given type can vary in ways that have subtle effects on how types can be
    preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the best odds at preserving database types users are advised to use ADBC
    drivers when available. The Arrow type system offers a wider array of types that
    more closely match database types than the historical pandas/NumPy type system.
    To illustrate, note this (non-exhaustive) listing of types available in different
    databases and pandas backends:'
  prefs: []
  type: TYPE_NORMAL
- en: '| numpy/pandas | arrow | postgres | sqlite |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| int16/Int16 | int16 | SMALLINT | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| int32/Int32 | int32 | INTEGER | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| int64/Int64 | int64 | BIGINT | INTEGER |'
  prefs: []
  type: TYPE_TB
- en: '| float32 | float32 | REAL | REAL |'
  prefs: []
  type: TYPE_TB
- en: '| float64 | float64 | DOUBLE PRECISION | REAL |'
  prefs: []
  type: TYPE_TB
- en: '| object | string | TEXT | TEXT |'
  prefs: []
  type: TYPE_TB
- en: '| bool | `bool_` | BOOLEAN |  |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns] | timestamp(us) | TIMESTAMP |  |'
  prefs: []
  type: TYPE_TB
- en: '| datetime64[ns,tz] | timestamp(us,tz) | TIMESTAMPTZ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | date32 | DATE |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | month_day_nano_interval | INTERVAL |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | binary | BINARY | BLOB |'
  prefs: []
  type: TYPE_TB
- en: '|  | decimal128 | DECIMAL [[1]](#f1) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | list | ARRAY [[1]](#f1) |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | struct |'
  prefs: []
  type: TYPE_TB
- en: COMPOSITE TYPE
  prefs: []
  type: TYPE_NORMAL
- en: '[[1]](#f1)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: Footnotes
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in preserving database types as best as possible throughout
    the lifecycle of your DataFrame, users are encouraged to leverage the `dtype_backend="pyarrow"`
    argument of [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE957]'
  prefs: []
  type: TYPE_PRE
- en: This will prevent your data from being converted to the traditional pandas/NumPy
    type system, which often converts SQL types in ways that make them impossible
    to round-trip.
  prefs: []
  type: TYPE_NORMAL
- en: In case an ADBC driver is not available, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") will try to map your data to an appropriate SQL data
    type based on the dtype of the data. When you have columns of dtype `object`,
    pandas will try to infer the data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always override the default type by specifying the desired SQL type
    of any of the columns by using the `dtype` argument. This argument needs a dictionary
    mapping column names to SQLAlchemy types (or strings for the sqlite3 fallback
    mode). For example, specifying to use the sqlalchemy `String` type instead of
    the default `Text` type for string columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE958]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limited support for timedelta’s in the different database flavors,
    columns with type `timedelta64` will be written as integer values as nanoseconds
    to the database and a warning will be raised. The only exception to this is when
    using the ADBC PostgreSQL driver in which case a timedelta will be written to
    the database as an `INTERVAL`
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Columns of `category` dtype will be converted to the dense representation as
    you would get with `np.asarray(categorical)` (e.g. for string categories this
    gives an array of strings). Because of this, reading the database table back in
    does **not** generate a categorical.
  prefs: []
  type: TYPE_NORMAL
- en: '### Datetime data types'
  prefs: []
  type: TYPE_NORMAL
- en: Using ADBC or SQLAlchemy, [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") is capable of writing datetime data that is timezone
    naive or timezone aware. However, the resulting data stored in the database ultimately
    depends on the supported data type for datetime data of the database system being
    used.
  prefs: []
  type: TYPE_NORMAL
- en: The following table lists supported data types for datetime data for some common
    databases. Other database dialects may have different data types for datetime
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '| Database | SQL Datetime Types | Timezone Support |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SQLite | `TEXT` | No |'
  prefs: []
  type: TYPE_TB
- en: '| MySQL | `TIMESTAMP` or `DATETIME` | No |'
  prefs: []
  type: TYPE_TB
- en: '| PostgreSQL | `TIMESTAMP` or `TIMESTAMP WITH TIME ZONE` | Yes |'
  prefs: []
  type: TYPE_TB
- en: When writing timezone aware data to databases that do not support timezones,
    the data will be written as timezone naive timestamps that are in local time with
    respect to the timezone.
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") is also capable of reading datetime data that is timezone
    aware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will convert
    the data to UTC.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### Insertion method'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `method` controls the SQL insertion clause used. Possible values
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`None`: Uses standard SQL `INSERT` clause (one per row).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''multi''`: Pass multiple values in a single `INSERT` clause. It uses a *special*
    SQL syntax not supported by all backends. This usually provides better performance
    for analytic databases like *Presto* and *Redshift*, but has worse performance
    for traditional SQL backend if the table contains many columns. For more information
    check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'callable with signature `(pd_table, conn, keys, data_iter)`: This can be used
    to implement a more performant insertion method based on specific backend dialect
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE959]  #### Insertion method'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `method` controls the SQL insertion clause used. Possible values
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`None`: Uses standard SQL `INSERT` clause (one per row).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''multi''`: Pass multiple values in a single `INSERT` clause. It uses a *special*
    SQL syntax not supported by all backends. This usually provides better performance
    for analytic databases like *Presto* and *Redshift*, but has worse performance
    for traditional SQL backend if the table contains many columns. For more information
    check the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/dml.html#sqlalchemy.sql.expression.Insert.values.params.*args).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'callable with signature `(pd_table, conn, keys, data_iter)`: This can be used
    to implement a more performant insertion method based on specific backend dialect
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of a callable using PostgreSQL [COPY clause](https://www.postgresql.org/docs/current/sql-copy.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE960]'
  prefs: []
  type: TYPE_PRE
- en: Reading tables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") will read a database table given the table name and optionally
    a subset of columns to read.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In order to use [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table"), you **must** have the ADBC driver or SQLAlchemy optional
    dependency installed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE961]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: ADBC drivers will map database types directly back to arrow types. For other
    drivers note that pandas infers column dtypes from query outputs, and not by looking
    up data types in the physical database schema. For example, assume `userid` is
    an integer column in a table. Then, intuitively, `select userid ...` will return
    integer-valued series, while `select cast(userid as text) ...` will return object-valued
    (str) series. Accordingly, if the query output is empty, then all resulting columns
    will be returned as object-valued (since they are most general). If you foresee
    that your query will sometimes generate an empty result, you may want to explicitly
    typecast afterwards to ensure dtype integrity.
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify the name of the column as the `DataFrame` index, and specify
    a subset of columns to be read.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE962]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can explicitly force columns to be parsed as dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE963]'
  prefs: []
  type: TYPE_PRE
- en: 'If needed you can explicitly specify a format string, or a dict of arguments
    to pass to [`pandas.to_datetime()`](../reference/api/pandas.to_datetime.html#pandas.to_datetime
    "pandas.to_datetime"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE964]'
  prefs: []
  type: TYPE_PRE
- en: You can check if a table exists using `has_table()`
  prefs: []
  type: TYPE_NORMAL
- en: Schema support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reading from and writing to different schema’s is supported through the `schema`
    keyword in the [`read_sql_table()`](../reference/api/pandas.read_sql_table.html#pandas.read_sql_table
    "pandas.read_sql_table") and [`to_sql()`](../reference/api/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql
    "pandas.DataFrame.to_sql") functions. Note however that this depends on the database
    flavor (sqlite does not have schema’s). For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE965]'
  prefs: []
  type: TYPE_PRE
- en: Querying
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can query using raw SQL in the [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function. In this case you must use the SQL variant appropriate
    for your database. When using SQLAlchemy, you can also pass SQLAlchemy Expression
    language constructs, which are database-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE966]'
  prefs: []
  type: TYPE_PRE
- en: Of course, you can specify a more “complex” query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE967]'
  prefs: []
  type: TYPE_PRE
- en: 'The [`read_sql_query()`](../reference/api/pandas.read_sql_query.html#pandas.read_sql_query
    "pandas.read_sql_query") function supports a `chunksize` argument. Specifying
    this will return an iterator through chunks of the query result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE968]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE969]'
  prefs: []
  type: TYPE_PRE
- en: Engine connection examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To connect with SQLAlchemy you use the `create_engine()` function to create
    an engine object from database URI. You only need to create the engine once per
    database you are connecting to.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE970]'
  prefs: []
  type: TYPE_PRE
- en: For more information see the examples the SQLAlchemy [documentation](https://docs.sqlalchemy.org/en/latest/core/engines.html)
  prefs: []
  type: TYPE_NORMAL
- en: Advanced SQLAlchemy queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use SQLAlchemy constructs to describe your query.
  prefs: []
  type: TYPE_NORMAL
- en: Use `sqlalchemy.text()` to specify query parameters in a backend-neutral way
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE971]'
  prefs: []
  type: TYPE_PRE
- en: If you have an SQLAlchemy description of your database you can express where
    conditions using SQLAlchemy expressions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE972]'
  prefs: []
  type: TYPE_PRE
- en: You can combine SQLAlchemy expressions with parameters passed to [`read_sql()`](../reference/api/pandas.read_sql.html#pandas.read_sql
    "pandas.read_sql") using `sqlalchemy.bindparam()`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE973]'
  prefs: []
  type: TYPE_PRE
- en: Sqlite fallback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of sqlite is supported without using SQLAlchemy. This mode requires
    a Python database adapter which respect the [Python DB-API](https://www.python.org/dev/peps/pep-0249/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create connections like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE974]'
  prefs: []
  type: TYPE_PRE
- en: 'And then issue the following queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE975]'
  prefs: []
  type: TYPE_PRE
- en: '## Google BigQuery'
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas-gbq` package provides functionality to read/write from Google BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: pandas integrates with this external package. if `pandas-gbq` is installed,
    you can use the pandas methods `pd.read_gbq` and `DataFrame.to_gbq`, which will
    call the respective functions from `pandas-gbq`.
  prefs: []
  type: TYPE_NORMAL
- en: Full documentation can be found [here](https://pandas-gbq.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: '## Stata format'
  prefs: []
  type: TYPE_NORMAL
- en: '### Writing to stata format'
  prefs: []
  type: TYPE_NORMAL
- en: The method `DataFrame.to_stata()` will write a DataFrame into a .dta file. The
    format version of this file is always 115 (Stata 12).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE976]'
  prefs: []
  type: TYPE_PRE
- en: '*Stata* data files have limited data type support; only strings with 244 or
    fewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be stored
    in `.dta` files. Additionally, *Stata* reserves certain values to represent missing
    data. Exporting a non-missing value that is outside of the permitted range in
    Stata for a particular data type will retype the variable to the next larger size.
    For example, `int8` values are restricted to lie between -127 and 100 in Stata,
    and so variables with values above 100 will trigger a conversion to `int16`. `nan`
    values in floating points data types are stored as the basic missing data type
    (`.` in *Stata*).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is not possible to export missing data values for integer data types.
  prefs: []
  type: TYPE_NORMAL
- en: The *Stata* writer gracefully handles other data types including `int64`, `bool`,
    `uint8`, `uint16`, `uint32` by casting to the smallest supported type that can
    represent the data. For example, data with a type of `uint8` will be cast to `int8`
    if all values are less than 100 (the upper bound for non-missing `int8` data in
    *Stata*), or, if values are outside of this range, the variable is cast to `int16`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from `int64` to `float64` may result in a loss of precision if `int64`
    values are larger than 2**53.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`StataWriter` and `DataFrame.to_stata()` only support fixed width strings containing
    up to 244 characters, a limitation imposed by the version 115 dta file format.
    Attempting to write *Stata* dta files with strings longer than 244 characters
    raises a `ValueError`.  ### Reading from Stata format'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function `read_stata` will read a dta file and return either a
    `DataFrame` or a `pandas.api.typing.StataReader` that can be used to read the
    file incrementally.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE977]'
  prefs: []
  type: TYPE_PRE
- en: Specifying a `chunksize` yields a `pandas.api.typing.StataReader` instance that
    can be used to read `chunksize` lines from the file at a time. The `StataReader`
    object can be used as an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE978]'
  prefs: []
  type: TYPE_PRE
- en: For more fine-grained control, use `iterator=True` and specify `chunksize` with
    each call to `read()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE979]'
  prefs: []
  type: TYPE_PRE
- en: Currently the `index` is retrieved as a column.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_categoricals` indicates whether value labels should be
    read and used to create a `Categorical` variable from them. Value labels can also
    be retrieved by the function `value_labels`, which requires `read()` to be called
    before use.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_missing` indicates whether missing value representations
    in Stata should be preserved. If `False` (the default), missing values are represented
    as `np.nan`. If `True`, missing values are represented using `StataMissingValue`
    objects, and columns containing missing values will have `object` data type.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") and `StataReader` support .dta formats 113-115 (Stata 10-12),
    117 (Stata 13), and 118 (Stata 14).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `preserve_dtypes=False` will upcast to the standard pandas data types:
    `int64` for all integer types and `float64` for floating point data. By default,
    the Stata data types are preserved when importing.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All `StataReader` objects, whether created by [`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") (when using `iterator=True` or `chunksize`) or instantiated
    by hand, must be used as context managers (e.g. the `with` statement). While the
    `close()` method is available, its use is unsupported. It is not part of the public
    API and will be removed in with future without warning.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` data can be exported to *Stata* data files as value labeled data.
    The exported data consists of the underlying category codes as integer data values
    and the categories as value labels. *Stata* does not have an explicit equivalent
    to a `Categorical` and information about *whether* the variable is ordered is
    lost when exporting.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* only supports string value labels, and so `str` is called on the categories
    when exporting data. Exporting `Categorical` variables with non-string categories
    produces a warning, and can result a loss of information if the `str` representations
    of the categories are not unique.'
  prefs: []
  type: TYPE_NORMAL
- en: Labeled data can similarly be imported from *Stata* data files as `Categorical`
    variables using the keyword argument `convert_categoricals` (`True` by default).
    The keyword argument `order_categoricals` (`True` by default) determines whether
    imported `Categorical` variables are ordered.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since `Categorical` variables always use integer data
    types between `-1` and `n-1` where `n` is the number of categories. If the original
    values in the *Stata* data file are required, these can be imported by setting
    `convert_categoricals=False`, which will import original data (but not the variable
    labels). The original values can be matched to the imported categorical data since
    there is a simple mapping between the original *Stata* data values and the category
    codes of imported Categorical variables: missing values are assigned code `-1`,
    and the smallest original value is assigned `0`, the second smallest is assigned
    `1` and so on until the largest original value is assigned the code `n-1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a `Categorical` with string categories for the values that are labeled and numeric
    categories for values with no label.  ### Writing to stata format'
  prefs: []
  type: TYPE_NORMAL
- en: The method `DataFrame.to_stata()` will write a DataFrame into a .dta file. The
    format version of this file is always 115 (Stata 12).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE980]'
  prefs: []
  type: TYPE_PRE
- en: '*Stata* data files have limited data type support; only strings with 244 or
    fewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be stored
    in `.dta` files. Additionally, *Stata* reserves certain values to represent missing
    data. Exporting a non-missing value that is outside of the permitted range in
    Stata for a particular data type will retype the variable to the next larger size.
    For example, `int8` values are restricted to lie between -127 and 100 in Stata,
    and so variables with values above 100 will trigger a conversion to `int16`. `nan`
    values in floating points data types are stored as the basic missing data type
    (`.` in *Stata*).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is not possible to export missing data values for integer data types.
  prefs: []
  type: TYPE_NORMAL
- en: The *Stata* writer gracefully handles other data types including `int64`, `bool`,
    `uint8`, `uint16`, `uint32` by casting to the smallest supported type that can
    represent the data. For example, data with a type of `uint8` will be cast to `int8`
    if all values are less than 100 (the upper bound for non-missing `int8` data in
    *Stata*), or, if values are outside of this range, the variable is cast to `int16`.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from `int64` to `float64` may result in a loss of precision if `int64`
    values are larger than 2**53.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '`StataWriter` and `DataFrame.to_stata()` only support fixed width strings containing
    up to 244 characters, a limitation imposed by the version 115 dta file format.
    Attempting to write *Stata* dta files with strings longer than 244 characters
    raises a `ValueError`.'
  prefs: []
  type: TYPE_NORMAL
- en: '### Reading from Stata format'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function `read_stata` will read a dta file and return either a
    `DataFrame` or a `pandas.api.typing.StataReader` that can be used to read the
    file incrementally.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE981]'
  prefs: []
  type: TYPE_PRE
- en: Specifying a `chunksize` yields a `pandas.api.typing.StataReader` instance that
    can be used to read `chunksize` lines from the file at a time. The `StataReader`
    object can be used as an iterator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE982]'
  prefs: []
  type: TYPE_PRE
- en: For more fine-grained control, use `iterator=True` and specify `chunksize` with
    each call to `read()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE983]'
  prefs: []
  type: TYPE_PRE
- en: Currently the `index` is retrieved as a column.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_categoricals` indicates whether value labels should be
    read and used to create a `Categorical` variable from them. Value labels can also
    be retrieved by the function `value_labels`, which requires `read()` to be called
    before use.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `convert_missing` indicates whether missing value representations
    in Stata should be preserved. If `False` (the default), missing values are represented
    as `np.nan`. If `True`, missing values are represented using `StataMissingValue`
    objects, and columns containing missing values will have `object` data type.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '[`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") and `StataReader` support .dta formats 113-115 (Stata 10-12),
    117 (Stata 13), and 118 (Stata 14).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `preserve_dtypes=False` will upcast to the standard pandas data types:
    `int64` for all integer types and `float64` for floating point data. By default,
    the Stata data types are preserved when importing.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All `StataReader` objects, whether created by [`read_stata()`](../reference/api/pandas.read_stata.html#pandas.read_stata
    "pandas.read_stata") (when using `iterator=True` or `chunksize`) or instantiated
    by hand, must be used as context managers (e.g. the `with` statement). While the
    `close()` method is available, its use is unsupported. It is not part of the public
    API and will be removed in with future without warning.
  prefs: []
  type: TYPE_NORMAL
- en: '#### Categorical data'
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` data can be exported to *Stata* data files as value labeled data.
    The exported data consists of the underlying category codes as integer data values
    and the categories as value labels. *Stata* does not have an explicit equivalent
    to a `Categorical` and information about *whether* the variable is ordered is
    lost when exporting.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* only supports string value labels, and so `str` is called on the categories
    when exporting data. Exporting `Categorical` variables with non-string categories
    produces a warning, and can result a loss of information if the `str` representations
    of the categories are not unique.'
  prefs: []
  type: TYPE_NORMAL
- en: Labeled data can similarly be imported from *Stata* data files as `Categorical`
    variables using the keyword argument `convert_categoricals` (`True` by default).
    The keyword argument `order_categoricals` (`True` by default) determines whether
    imported `Categorical` variables are ordered.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since `Categorical` variables always use integer data
    types between `-1` and `n-1` where `n` is the number of categories. If the original
    values in the *Stata* data file are required, these can be imported by setting
    `convert_categoricals=False`, which will import original data (but not the variable
    labels). The original values can be matched to the imported categorical data since
    there is a simple mapping between the original *Stata* data values and the category
    codes of imported Categorical variables: missing values are assigned code `-1`,
    and the smallest original value is assigned `0`, the second smallest is assigned
    `1` and so on until the largest original value is assigned the code `n-1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a `Categorical` with string categories for the values that are labeled and numeric
    categories for values with no label.  #### Categorical data'
  prefs: []
  type: TYPE_NORMAL
- en: '`Categorical` data can be exported to *Stata* data files as value labeled data.
    The exported data consists of the underlying category codes as integer data values
    and the categories as value labels. *Stata* does not have an explicit equivalent
    to a `Categorical` and information about *whether* the variable is ordered is
    lost when exporting.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* only supports string value labels, and so `str` is called on the categories
    when exporting data. Exporting `Categorical` variables with non-string categories
    produces a warning, and can result a loss of information if the `str` representations
    of the categories are not unique.'
  prefs: []
  type: TYPE_NORMAL
- en: Labeled data can similarly be imported from *Stata* data files as `Categorical`
    variables using the keyword argument `convert_categoricals` (`True` by default).
    The keyword argument `order_categoricals` (`True` by default) determines whether
    imported `Categorical` variables are ordered.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When importing categorical data, the values of the variables in the *Stata*
    data file are not preserved since `Categorical` variables always use integer data
    types between `-1` and `n-1` where `n` is the number of categories. If the original
    values in the *Stata* data file are required, these can be imported by setting
    `convert_categoricals=False`, which will import original data (but not the variable
    labels). The original values can be matched to the imported categorical data since
    there is a simple mapping between the original *Stata* data values and the category
    codes of imported Categorical variables: missing values are assigned code `-1`,
    and the smallest original value is assigned `0`, the second smallest is assigned
    `1` and so on until the largest original value is assigned the code `n-1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Stata* supports partially labeled series. These series have value labels for
    some but not all data values. Importing a partially labeled series will produce
    a `Categorical` with string categories for the values that are labeled and numeric
    categories for values with no label.'
  prefs: []
  type: TYPE_NORMAL
- en: '## SAS formats'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function [`read_sas()`](../reference/api/pandas.read_sas.html#pandas.read_sas
    "pandas.read_sas") can read (but not write) SAS XPORT (.xpt) and SAS7BDAT (.sas7bdat)
    format files.
  prefs: []
  type: TYPE_NORMAL
- en: 'SAS files only contain two value types: ASCII text and floating point values
    (usually 8 bytes but sometimes truncated). For xport files, there is no automatic
    type conversion to integers, dates, or categoricals. For SAS7BDAT files, the format
    codes may allow date variables to be automatically converted to dates. By default
    the whole file is read and returned as a `DataFrame`.'
  prefs: []
  type: TYPE_NORMAL
- en: Specify a `chunksize` or use `iterator=True` to obtain reader objects (`XportReader`
    or `SAS7BDATReader`) for incrementally reading the file. The reader objects also
    have attributes that contain additional information about the file and its variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read a SAS7BDAT file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE984]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtain an iterator and read an XPORT file 100,000 lines at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE985]'
  prefs: []
  type: TYPE_PRE
- en: The [specification](https://support.sas.com/content/dam/SAS/support/en/technical-papers/record-layout-of-a-sas-version-5-or-6-data-set-in-sas-transport-xport-format.pdf)
    for the xport file format is available from the SAS web site.
  prefs: []
  type: TYPE_NORMAL
- en: No official documentation is available for the SAS7BDAT format.
  prefs: []
  type: TYPE_NORMAL
- en: '## SPSS formats'
  prefs: []
  type: TYPE_NORMAL
- en: The top-level function [`read_spss()`](../reference/api/pandas.read_spss.html#pandas.read_spss
    "pandas.read_spss") can read (but not write) SPSS SAV (.sav) and ZSAV (.zsav)
    format files.
  prefs: []
  type: TYPE_NORMAL
- en: SPSS files contain column names. By default the whole file is read, categorical
    columns are converted into `pd.Categorical`, and a `DataFrame` with all columns
    is returned.
  prefs: []
  type: TYPE_NORMAL
- en: Specify the `usecols` parameter to obtain a subset of columns. Specify `convert_categoricals=False`
    to avoid converting categorical columns into `pd.Categorical`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read an SPSS file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE986]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract a subset of columns contained in `usecols` from an SPSS file and avoid
    converting categorical columns into `pd.Categorical`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE987]'
  prefs: []
  type: TYPE_PRE
- en: More information about the SAV and ZSAV file formats is available [here](https://www.ibm.com/docs/en/spss-statistics/22.0.0).
  prefs: []
  type: TYPE_NORMAL
- en: '## Other file formats'
  prefs: []
  type: TYPE_NORMAL
- en: pandas itself only supports IO with a limited set of file formats that map cleanly
    to its tabular data model. For reading and writing other file formats into and
    from pandas, we recommend these packages from the broader community.
  prefs: []
  type: TYPE_NORMAL
- en: netCDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[xarray](https://xarray.pydata.org/en/stable/) provides data structures inspired
    by the pandas `DataFrame` for working with multi-dimensional datasets, with a
    focus on the netCDF file format and easy conversion to and from pandas.'
  prefs: []
  type: TYPE_NORMAL
- en: netCDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[xarray](https://xarray.pydata.org/en/stable/) provides data structures inspired
    by the pandas `DataFrame` for working with multi-dimensional datasets, with a
    focus on the netCDF file format and easy conversion to and from pandas.'
  prefs: []
  type: TYPE_NORMAL
- en: '## Performance considerations'
  prefs: []
  type: TYPE_NORMAL
- en: This is an informal comparison of various IO methods, using pandas 0.24.2\.
    Timings are machine dependent and small differences should be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE988]'
  prefs: []
  type: TYPE_PRE
- en: 'The following test functions will be used below to compare the performance
    of several IO methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE989]'
  prefs: []
  type: TYPE_PRE
- en: When writing, the top three functions in terms of speed are `test_feather_write`,
    `test_hdf_fixed_write` and `test_hdf_fixed_write_compress`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE990]'
  prefs: []
  type: TYPE_PRE
- en: When reading, the top three functions in terms of speed are `test_feather_read`,
    `test_pickle_read` and `test_hdf_fixed_read`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE991]'
  prefs: []
  type: TYPE_PRE
- en: The files `test.pkl.compress`, `test.parquet` and `test.feather` took the least
    space on disk (in bytes).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE992]'
  prefs: []
  type: TYPE_PRE
