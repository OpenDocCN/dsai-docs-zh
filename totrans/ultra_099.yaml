- en: Ultralytics YOLO Hyperparameter Tuning Guide
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[`docs.ultralytics.com/guides/hyperparameter-tuning/`](https://docs.ultralytics.com/guides/hyperparameter-tuning/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hyperparameter tuning is not just a one-time set-up but an iterative process
    aimed at optimizing the machine learning model's performance metrics, such as
    accuracy, precision, and recall. In the context of Ultralytics YOLO, these hyperparameters
    could range from learning rate to architectural details, such as the number of
    layers or types of activation functions used.
  prefs: []
  type: TYPE_NORMAL
- en: What are Hyperparameters?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hyperparameters are high-level, structural settings for the algorithm. They
    are set prior to the training phase and remain constant during it. Here are some
    commonly tuned hyperparameters in Ultralytics YOLO:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate** `lr0`: Determines the step size at each iteration while moving
    towards a minimum in the loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Size** `batch`: Number of images processed simultaneously in a forward
    pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of Epochs** `epochs`: An epoch is one complete forward and backward
    pass of all the training examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Architecture Specifics**: Such as channel counts, number of layers, types
    of activation functions, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Hyperparameter Tuning Visual](img/bfd62960bd9f74faa1c910ed78fdde4a.png)'
  prefs: []
  type: TYPE_IMG
- en: For a full list of augmentation hyperparameters used in YOLOv8 please refer
    to the configurations page.
  prefs: []
  type: TYPE_NORMAL
- en: Genetic Evolution and Mutation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ultralytics YOLO uses genetic algorithms to optimize hyperparameters. Genetic
    algorithms are inspired by the mechanism of natural selection and genetics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutation**: In the context of Ultralytics YOLO, mutation helps in locally
    searching the hyperparameter space by applying small, random changes to existing
    hyperparameters, producing new candidates for evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crossover**: Although crossover is a popular genetic algorithm technique,
    it is not currently used in Ultralytics YOLO for hyperparameter tuning. The focus
    is mainly on mutation for generating new hyperparameter sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing for Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you begin the tuning process, it''s important to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify the Metrics**: Determine the metrics you will use to evaluate the
    model''s performance. This could be AP50, F1-score, or others.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Set the Tuning Budget**: Define how much computational resources you''re
    willing to allocate. Hyperparameter tuning can be computationally intensive.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps Involved
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Initialize Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start with a reasonable set of initial hyperparameters. This could either be
    the default hyperparameters set by Ultralytics YOLO or something based on your
    domain knowledge or previous experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Mutate Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use the `_mutate` method to produce a new set of hyperparameters based on the
    existing set.
  prefs: []
  type: TYPE_NORMAL
- en: Train Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training is performed using the mutated set of hyperparameters. The training
    performance is then assessed.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use metrics like AP50, F1-score, or custom metrics to evaluate the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Log Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's crucial to log both the performance metrics and the corresponding hyperparameters
    for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process is repeated until either the set number of iterations is reached
    or the performance metric is satisfactory.
  prefs: []
  type: TYPE_NORMAL
- en: Usage Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here's how to use the `model.tune()` method to utilize the `Tuner` class for
    hyperparameter tuning of YOLOv8n on COCO8 for 30 epochs with an AdamW optimizer
    and skipping plotting, checkpointing and validation other than on final epoch
    for faster Tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After you''ve successfully completed the hyperparameter tuning process, you
    will obtain several files and directories that encapsulate the results of the
    tuning. The following describes each:'
  prefs: []
  type: TYPE_NORMAL
- en: File Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here''s what the directory structure of the results will look like. Training
    directories like `train1/` contain individual tuning iterations, i.e. one model
    trained with one set of hyperparameters. The `tune/` directory contains tuning
    results from all the individual model trainings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: File Descriptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: best_hyperparameters.yaml
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This YAML file contains the best-performing hyperparameters found during the
    tuning process. You can use this file to initialize future trainings with these
    optimized settings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Format**: YAML'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage**: Hyperparameter results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: best_fitness.png
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is a plot displaying fitness (typically a performance metric like AP50)
    against the number of iterations. It helps you visualize how well the genetic
    algorithm performed over time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Format**: PNG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage**: Performance visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Hyperparameter Tuning Fitness vs Iteration](img/a5d9bd0a7515da79d1c8e48ee4c0399f.png)'
  prefs: []
  type: TYPE_IMG
- en: tune_results.csv
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A CSV file containing detailed results of each iteration during the tuning.
    Each row in the file represents one iteration, and it includes metrics like fitness
    score, precision, recall, as well as the hyperparameters used.
  prefs: []
  type: TYPE_NORMAL
- en: '**Format**: CSV'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage**: Per-iteration results tracking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: tune_scatter_plots.png
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This file contains scatter plots generated from `tune_results.csv`, helping
    you visualize relationships between different hyperparameters and performance
    metrics. Note that hyperparameters initialized to 0 will not be tuned, such as
    `degrees` and `shear` below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Format**: PNG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usage**: Exploratory data analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Hyperparameter Tuning Scatter Plots](img/6d8ca000b8ab3a7f94038f024ef3a9a4.png)'
  prefs: []
  type: TYPE_IMG
- en: weights/
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This directory contains the saved PyTorch models for the last and the best iterations
    during the hyperparameter tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: '**`last.pt`**: The last.pt are the weights from the last epoch of training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`best.pt`**: The best.pt weights for the iteration that achieved the best
    fitness score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these results, you can make more informed decisions for your future model
    trainings and analyses. Feel free to consult these artifacts to understand how
    well your model performed and how you might improve it further.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The hyperparameter tuning process in Ultralytics YOLO is simplified yet powerful,
    thanks to its genetic algorithm-based approach focused on mutation. Following
    the steps outlined in this guide will assist you in systematically tuning your
    model to achieve better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Hyperparameter Optimization in Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_optimization)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: YOLOv5 Hyperparameter Evolution Guide
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Efficient Hyperparameter Tuning with Ray Tune and YOLOv8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For deeper insights, you can explore the `Tuner` class source code and accompanying
    documentation. Should you have any questions, feature requests, or need further
    assistance, feel free to reach out to us on [GitHub](https://github.com/ultralytics/ultralytics/issues/new/choose)
    or [Discord](https://ultralytics.com/discord).
  prefs: []
  type: TYPE_NORMAL
- en: FAQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do I optimize the learning rate for Ultralytics YOLO during hyperparameter
    tuning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To optimize the learning rate for Ultralytics YOLO, start by setting an initial
    learning rate using the `lr0` parameter. Common values range from `0.001` to `0.01`.
    During the hyperparameter tuning process, this value will be mutated to find the
    optimal setting. You can utilize the `model.tune()` method to automate this process.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For more details, check the Ultralytics YOLO configuration page.
  prefs: []
  type: TYPE_NORMAL
- en: What are the benefits of using genetic algorithms for hyperparameter tuning
    in YOLOv8?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Genetic algorithms in Ultralytics YOLOv8 provide a robust method for exploring
    the hyperparameter space, leading to highly optimized model performance. Key benefits
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficient Search**: Genetic algorithms like mutation can quickly explore
    a large set of hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoiding Local Minima**: By introducing randomness, they help in avoiding
    local minima, ensuring better global optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance Metrics**: They adapt based on performance metrics such as AP50
    and F1-score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To see how genetic algorithms can optimize hyperparameters, check out the hyperparameter
    evolution guide.
  prefs: []
  type: TYPE_NORMAL
- en: How long does the hyperparameter tuning process take for Ultralytics YOLO?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The time required for hyperparameter tuning with Ultralytics YOLO largely depends
    on several factors such as the size of the dataset, the complexity of the model
    architecture, the number of iterations, and the computational resources available.
    For instance, tuning YOLOv8n on a dataset like COCO8 for 30 epochs might take
    several hours to days, depending on the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: To effectively manage tuning time, define a clear tuning budget beforehand (internal
    section link). This helps in balancing resource allocation and optimization goals.
  prefs: []
  type: TYPE_NORMAL
- en: What metrics should I use to evaluate model performance during hyperparameter
    tuning in YOLO?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When evaluating model performance during hyperparameter tuning in YOLO, you
    can use several key metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AP50**: The average precision at IoU threshold of 0.50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1-Score**: The harmonic mean of precision and recall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision and Recall**: Individual metrics indicating the model''s accuracy
    in identifying true positives versus false positives and false negatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These metrics help you understand different aspects of your model's performance.
    Refer to the Ultralytics YOLO performance metrics guide for a comprehensive overview.
  prefs: []
  type: TYPE_NORMAL
- en: Can I use Ultralytics HUB for hyperparameter tuning of YOLO models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Yes, you can use Ultralytics HUB for hyperparameter tuning of YOLO models. The
    HUB offers a no-code platform to easily upload datasets, train models, and perform
    hyperparameter tuning efficiently. It provides real-time tracking and visualization
    of tuning progress and results.
  prefs: []
  type: TYPE_NORMAL
- en: Explore more about using Ultralytics HUB for hyperparameter tuning in the Ultralytics
    HUB Cloud Training documentation.
  prefs: []
  type: TYPE_NORMAL
