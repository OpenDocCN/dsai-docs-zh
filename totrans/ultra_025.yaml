- en: Segment Anything Model (SAM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Segment Anything 模型（SAM）
- en: 原文：[`docs.ultralytics.com/models/sam/`](https://docs.ultralytics.com/models/sam/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[`docs.ultralytics.com/models/sam/`](https://docs.ultralytics.com/models/sam/)
- en: Welcome to the frontier of image segmentation with the Segment Anything Model,
    or SAM. This revolutionary model has changed the game by introducing promptable
    image segmentation with real-time performance, setting new standards in the field.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到 Segment Anything 模型（SAM）在图像分割领域的前沿。这一革命性的模型通过引入实时性能的可提示图像分割，改变了游戏规则，树立了领域新标准。
- en: 'Introduction to SAM: The Segment Anything Model'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SAM 简介：Segment Anything 模型
- en: The Segment Anything Model, or SAM, is a cutting-edge image segmentation model
    that allows for promptable segmentation, providing unparalleled versatility in
    image analysis tasks. SAM forms the heart of the Segment Anything initiative,
    a groundbreaking project that introduces a novel model, task, and dataset for
    image segmentation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 模型（SAM）是一种先进的图像分割模型，支持可提示的分割，为图像分析任务提供了前所未有的灵活性。SAM 是 Segment
    Anything 项目的核心，该项目引入了一种新的模型、任务和数据集，用于图像分割。
- en: SAM's advanced design allows it to adapt to new image distributions and tasks
    without prior knowledge, a feature known as zero-shot transfer. Trained on the
    expansive [SA-1B dataset](https://ai.facebook.com/datasets/segment-anything/),
    which contains more than 1 billion masks spread over 11 million carefully curated
    images, SAM has displayed impressive zero-shot performance, surpassing previous
    fully supervised results in many cases.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: SAM 的先进设计使其能够在没有先验知识的情况下适应新的图像分布和任务，这一特性称为零射击传输。SAM 在广泛的[SA-1B 数据集](https://ai.facebook.com/datasets/segment-anything/)上进行训练，该数据集包含超过11百万张精心策划的图像和10亿多的蒙版，SAM
    展示了令人印象深刻的零射击性能，在许多情况下超越了以往的完全监督结果。
- en: '![Dataset sample image](img/af0aee249f56af250a272709d2c33ce9.png) **SA-1B Example
    images.** Dataset images overlaid masks from the newly introduced SA-1B dataset.
    SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting
    images and 1.1B high-quality segmentation masks. These masks were annotated fully
    automatically by SAM, and as verified by human ratings and numerous experiments,
    are of high quality and diversity. Images are grouped by number of masks per image
    for visualization (there are ∼100 masks per image on average).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![数据集示例图像](img/af0aee249f56af250a272709d2c33ce9.png) **SA-1B 示例图像。** 数据集图像上叠加了新引入的
    SA-1B 数据集的蒙版。SA-1B 包含了1100万多样化、高分辨率、有许可和隐私保护的图像以及11亿高质量的分割蒙版。这些蒙版由 SAM 完全自动注释，并经过人类评分和大量实验验证，质量和多样性很高。图像按照每个图像平均∼100个蒙版进行了分组可视化。'
- en: Key Features of the Segment Anything Model (SAM)
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Segment Anything 模型（SAM）的关键特性
- en: '**Promptable Segmentation Task:** SAM was designed with a promptable segmentation
    task in mind, allowing it to generate valid segmentation masks from any given
    prompt, such as spatial or text clues identifying an object.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可提示分割任务：** SAM 设计时考虑了可提示的分割任务，允许它从任何给定的提示生成有效的分割蒙版，例如空间或文本线索，用于识别对象。'
- en: '**Advanced Architecture:** The Segment Anything Model employs a powerful image
    encoder, a prompt encoder, and a lightweight mask decoder. This unique architecture
    enables flexible prompting, real-time mask computation, and ambiguity awareness
    in segmentation tasks.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先进的架构：** Segment Anything 模型采用强大的图像编码器、提示编码器和轻量级蒙版解码器。这种独特的架构支持灵活的提示、实时蒙版计算和分割任务中的歧义感知。'
- en: '**The SA-1B Dataset:** Introduced by the Segment Anything project, the SA-1B
    dataset features over 1 billion masks on 11 million images. As the largest segmentation
    dataset to date, it provides SAM with a diverse and large-scale training data
    source.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SA-1B 数据集：** 由 Segment Anything 项目引入的 SA-1B 数据集包含超过10亿张图像和1100万个蒙版。作为迄今为止最大的分割数据集，它为
    SAM 提供了一个多样化且大规模的训练数据来源。'
- en: '**Zero-Shot Performance:** SAM displays outstanding zero-shot performance across
    various segmentation tasks, making it a ready-to-use tool for diverse applications
    with minimal need for prompt engineering.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零射击性能：** SAM 在各种分割任务中展现出卓越的零射击性能，使其成为一款可即插即用的工具，在最小的提示工程需求下，适用于多样的应用场景。'
- en: For an in-depth look at the Segment Anything Model and the SA-1B dataset, please
    visit the [Segment Anything website](https://segment-anything.com) and check out
    the research paper [Segment Anything](https://arxiv.org/abs/2304.02643).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 欲深入了解 Segment Anything 模型和 SA-1B 数据集，请访问[Segment Anything 网站](https://segment-anything.com)，并查阅研究论文[Segment
    Anything](https://arxiv.org/abs/2304.02643)。
- en: Available Models, Supported Tasks, and Operating Modes
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用模型、支持的任务和操作模式
- en: This table presents the available models with their specific pre-trained weights,
    the tasks they support, and their compatibility with different operating modes
    like Inference, Validation, Training, and Export, indicated by ✅ emojis for supported
    modes and ❌ emojis for unsupported modes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此表格显示了可用模型及其特定的预训练权重，它们支持的任务以及它们与不同操作模式（如推断、验证、训练和导出）的兼容性，由✅ 表示支持的模式和❌ 表示不支持的模式。
- en: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| Model Type | Pre-trained Weights | Tasks Supported | Inference | Validation
    | Training | Export |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| SAM base | [sam_b.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam_b.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| SAM base | [sam_b.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam_b.pt)
    | 实例分割 | ✅ | ❌ | ❌ | ❌ |'
- en: '| SAM large | [sam_l.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam_l.pt)
    | Instance Segmentation | ✅ | ❌ | ❌ | ❌ |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| SAM large | [sam_l.pt](https://github.com/ultralytics/assets/releases/download/v8.2.0/sam_l.pt)
    | 实例分割 | ✅ | ❌ | ❌ | ❌ |'
- en: 'How to Use SAM: Versatility and Power in Image Segmentation'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 SAM：图像分割中的多功能和强大
- en: The Segment Anything Model can be employed for a multitude of downstream tasks
    that go beyond its training data. This includes edge detection, object proposal
    generation, instance segmentation, and preliminary text-to-mask prediction. With
    prompt engineering, SAM can swiftly adapt to new tasks and data distributions
    in a zero-shot manner, establishing it as a versatile and potent tool for all
    your image segmentation needs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Segment Anything 模型可用于多种超出其训练数据范围的下游任务，包括边缘检测、对象提议生成、实例分割和初步文本到遮罩预测。通过提示工程，SAM
    可以以零-shot方式快速适应新任务和数据分布，使其成为您所有图像分割需求的多功能和强大工具。
- en: SAM prediction example
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SAM 预测示例
- en: Segment with prompts
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提示分割
- en: Segment image with given prompts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定提示分割图像。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Segment everything
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 分割所有内容
- en: Segment the whole image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 分割整个图像。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The logic here is to segment the whole image if you don't pass any prompts(bboxes/points/masks).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此处的逻辑是，如果您没有传递任何提示（边界框/点/遮罩），则对整个图像进行分割。
- en: SAMPredictor example
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SAMPredictor 示例
- en: This way you can set image once and run prompts inference multiple times without
    running image encoder multiple times.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此方法，您可以一次设置图像，并多次运行提示推断，而无需多次运行图像编码器。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Segment everything with additional args.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用附加参数分割所有内容。
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All the returned `results` in above examples are Results object which allows
    access predicted masks and source image easily.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述示例中返回的`results`都是Results对象，可以轻松访问预测的遮罩和源图像。
- en: More additional args for `Segment everything` see `Predictor/generate` Reference.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关`Segment everything`的更多附加参数，请参阅`Predictor/generate` 参考资料。
- en: SAM comparison vs YOLOv8
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SAM 与 YOLOv8 的比较
- en: 'Here we compare Meta''s smallest SAM model, SAM-b, with Ultralytics smallest
    segmentation model, YOLOv8n-seg:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将 Meta 的最小 SAM 模型 SAM-b 与 Ultralytics 的最小分割模型 YOLOv8n-seg 进行比较：
- en: '| Model | Size | Parameters | Speed (CPU) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Model | Size | Parameters | Speed (CPU) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Meta''s SAM-b | 358 MB | 94.7 M | 51096 ms/im |'
- en: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| MobileSAM | 40.7 MB | 10.1 M | 46122 ms/im |'
- en: '| FastSAM-s with YOLOv8 backbone | 23.7 MB | 11.8 M | 115 ms/im |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| FastSAM-s with YOLOv8 backbone | 23.7 MB | 11.8 M | 115 ms/im |'
- en: '| Ultralytics YOLOv8n-seg | **6.7 MB** (53.4x smaller) | **3.4 M** (27.9x less)
    | **59 ms/im** (866x faster) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Ultralytics YOLOv8n-seg | **6.7 MB** (53.4倍更小) | **3.4 M** (27.9倍更少) | **59
    ms/im** (866倍更快) |'
- en: This comparison shows the order-of-magnitude differences in the model sizes
    and speeds between models. Whereas SAM presents unique capabilities for automatic
    segmenting, it is not a direct competitor to YOLOv8 segment models, which are
    smaller, faster and more efficient.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此比较显示了不同模型大小和速度之间的数量级差异。虽然 SAM 提供了自动分割的独特能力，但它并非 YOLOv8 分割模型的直接竞争对手，后者更小、更快且更高效。
- en: 'Tests run on a 2023 Apple M2 Macbook with 16GB of RAM. To reproduce this test:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2023 年的 Apple M2 MacBook 上运行的测试，配备 16GB RAM。要重现此测试：
- en: Example
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Auto-Annotation: A Quick Path to Segmentation Datasets'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动标注：快速路径到分割数据集
- en: Auto-annotation is a key feature of SAM, allowing users to generate a segmentation
    dataset using a pre-trained detection model. This feature enables rapid and accurate
    annotation of a large number of images, bypassing the need for time-consuming
    manual labeling.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 自动注释是SAM的一个关键特性，允许用户使用预训练的检测模型生成分割数据集。这一功能使得能够快速而准确地对大量图像进行注释，避免了耗时的手动标记过程。
- en: Generate Your Segmentation Dataset Using a Detection Model
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用检测模型生成您的分割数据集
- en: 'To auto-annotate your dataset with the Ultralytics framework, use the `auto_annotate`
    function as shown below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Ultralytics框架自动注释您的数据集，请按照下面的方式使用`auto_annotate`函数：
- en: Example
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '| Argument | Type | Description | Default |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 类型 | 描述 | 默认值 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `data` | `str` | Path to a folder containing images to be annotated. |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `data` | `str` | 包含待注释图像的文件夹路径。 |  |'
- en: '| `det_model` | `str`, optional | Pre-trained YOLO detection model. Defaults
    to ''yolov8x.pt''. | `''yolov8x.pt''` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `det_model` | `str`，可选项 | 预训练的YOLO检测模型。默认为''yolov8x.pt''。 | `''yolov8x.pt''`
    |'
- en: '| `sam_model` | `str`, optional | Pre-trained SAM segmentation model. Defaults
    to ''sam_b.pt''. | `''sam_b.pt''` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `sam_model` | `str`，可选项 | 预训练的SAM分割模型。默认为''sam_b.pt''。 | `''sam_b.pt''` |'
- en: '| `device` | `str`, optional | Device to run the models on. Defaults to an
    empty string (CPU or GPU, if available). |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `device` | `str`，可选项 | 在其上运行模型的设备。默认为空字符串（CPU或GPU，如果可用）。 |  |'
- en: '| `output_dir` | `str`, None, optional | Directory to save the annotated results.
    Defaults to a ''labels'' folder in the same directory as ''data''. | `None` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `output_dir` | `str`，`None`，可选项 | 保存带注释结果的目录。默认为与''data''文件夹相同目录下的''labels''文件夹。
    | `None` |'
- en: The `auto_annotate` function takes the path to your images, with optional arguments
    for specifying the pre-trained detection and SAM segmentation models, the device
    to run the models on, and the output directory for saving the annotated results.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`auto_annotate`函数接受您的图像路径，可选参数指定预训练检测和SAM分割模型，运行模型的设备以及保存带注释结果的输出目录。'
- en: Auto-annotation with pre-trained models can dramatically cut down the time and
    effort required for creating high-quality segmentation datasets. This feature
    is especially beneficial for researchers and developers dealing with large image
    collections, as it allows them to focus on model development and evaluation rather
    than manual annotation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型进行自动注释，可以大大减少创建高质量分割数据集所需的时间和精力。这一功能对于处理大型图像集合的研究人员和开发人员尤其有益，因为它使他们能够集中精力于模型的开发和评估，而不是手动注释。
- en: Citations and Acknowledgements
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用和致谢
- en: 'If you find SAM useful in your research or development work, please consider
    citing our paper:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在研究或开发工作中发现SAM有用，请考虑引用我们的论文：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We would like to express our gratitude to Meta AI for creating and maintaining
    this valuable resource for the computer vision community.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢Meta AI为计算机视觉社区创建和维护这一宝贵资源。
- en: FAQ
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见问题解答
- en: What is the Segment Anything Model (SAM) by Ultralytics?
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是Ultralytics的段落任意模型（SAM）？
- en: The Segment Anything Model (SAM) by Ultralytics is a revolutionary image segmentation
    model designed for promptable segmentation tasks. It leverages advanced architecture,
    including image and prompt encoders combined with a lightweight mask decoder,
    to generate high-quality segmentation masks from various prompts such as spatial
    or text cues. Trained on the expansive [SA-1B dataset](https://ai.facebook.com/datasets/segment-anything/),
    SAM excels in zero-shot performance, adapting to new image distributions and tasks
    without prior knowledge. Learn more here.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Ultralytics的段落任意模型（SAM）是一种革命性的图像分割模型，专为可提示分割任务设计。它利用先进的架构，包括图像和提示编码器以及轻量级掩模解码器，从诸如空间或文本提示等各种提示生成高质量的分割掩模。在广泛的[SA-1B数据集](https://ai.facebook.com/datasets/segment-anything/)上进行训练，SAM在零-shot性能方面表现出色，能够适应新的图像分布和任务，无需先验知识。在这里了解更多信息。
- en: How can I use the Segment Anything Model (SAM) for image segmentation?
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用段落任意模型（SAM）进行图像分割？
- en: 'You can use the Segment Anything Model (SAM) for image segmentation by running
    inference with various prompts such as bounding boxes or points. Here''s an example
    using Python:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用段落任意模型（SAM）进行图像分割，通过运行推理，使用各种提示，如边界框或点。以下是一个使用Python的示例：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Alternatively, you can run inference with SAM in the command line interface
    (CLI):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在命令行界面（CLI）中使用SAM进行推理：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For more detailed usage instructions, visit the Segmentation section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 获取更详细的使用说明，请访问分割部分。
- en: How do SAM and YOLOv8 compare in terms of performance?
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SAM和YOLOv8在性能上有什么区别？
- en: Compared to YOLOv8, SAM models like SAM-b and FastSAM-s are larger and slower
    but offer unique capabilities for automatic segmentation. For instance, Ultralytics
    YOLOv8n-seg is 53.4 times smaller and 866 times faster than SAM-b. However, SAM's
    zero-shot performance makes it highly flexible and efficient in diverse, untrained
    tasks. Learn more about performance comparisons between SAM and YOLOv8 here.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与YOLOv8相比，SAM模型如SAM-b和FastSAM-s更大且速度较慢，但提供了独特的自动分割能力。例如，Ultralytics的YOLOv8n-seg比SAM-b小53.4倍，速度快866倍。然而，SAM的零样本表现使其在多样化、未训练的任务中非常灵活和高效。了解SAM与YOLOv8的性能比较更多信息，请访问这里。
- en: How can I auto-annotate my dataset using SAM?
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用SAM自动注释我的数据集？
- en: 'Ultralytics'' SAM offers an auto-annotation feature that allows generating
    segmentation datasets using a pre-trained detection model. Here''s an example
    in Python:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Ultralytics的SAM提供了一个自动注释功能，允许使用预训练检测模型生成分割数据集。以下是Python的一个示例：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This function takes the path to your images and optional arguments for pre-trained
    detection and SAM segmentation models, along with device and output directory
    specifications. For a complete guide, see Auto-Annotation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能接受图像路径和预训练检测以及SAM分割模型的可选参数，以及设备和输出目录的规格说明。有关完整指南，请参阅自动注释。
- en: What datasets are used to train the Segment Anything Model (SAM)?
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 哪些数据集用于训练Segment Anything Model（SAM）？
- en: SAM is trained on the extensive [SA-1B dataset](https://ai.facebook.com/datasets/segment-anything/)
    which comprises over 1 billion masks across 11 million images. SA-1B is the largest
    segmentation dataset to date, providing high-quality and diverse training data,
    ensuring impressive zero-shot performance in varied segmentation tasks. For more
    details, visit the Dataset section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: SAM是基于广泛的[SA-1B数据集](https://ai.facebook.com/datasets/segment-anything/)进行训练的，该数据集包含超过11百万张图像中的10亿多个掩码。SA-1B是迄今为止最大的分割数据集，提供高质量且多样化的训练数据，确保在各种分割任务中具有令人印象深刻的零样本性能。有关更多详细信息，请访问数据集部分。
