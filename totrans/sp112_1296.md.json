["```py\nscipy.stats.jarque_bera(x, *, axis=None, nan_policy='propagate', keepdims=False)\n```", "```py\n>>> import numpy as np\n>>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236]) \n```", "```py\n>>> from scipy import stats\n>>> res = stats.jarque_bera(x)\n>>> res.statistic\n6.982848237344646 \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> dist = stats.chi2(df=2)\n>>> jb_val = np.linspace(0, 11, 100)\n>>> pdf = dist.pdf(jb_val)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> def jb_plot(ax):  # we'll reuse this\n...     ax.plot(jb_val, pdf)\n...     ax.set_title(\"Jarque-Bera Null Distribution\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n>>> jb_plot(ax)\n>>> plt.show() \n```", "```py\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> jb_plot(ax)\n>>> pvalue = dist.sf(res.statistic)\n>>> annotation = (f'p-value={pvalue:.6f}\\n(shaded area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (7.5, 0.01), (8, 0.05), arrowprops=props)\n>>> i = jb_val >= res.statistic  # indices of more extreme statistic values\n>>> ax.fill_between(jb_val[i], y1=0, y2=pdf[i])\n>>> ax.set_xlim(0, 11)\n>>> ax.set_ylim(0, 0.3)\n>>> plt.show() \n```", "```py\n>>> res.pvalue\n0.03045746622458189 \n```", "```py\n>>> def statistic(x, axis):\n...     # underlying calculation of the Jarque Bera statistic\n...     s = stats.skew(x, axis=axis)\n...     k = stats.kurtosis(x, axis=axis)\n...     return x.shape[axis]/6 * (s**2 + k**2/4)\n>>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic,\n...                              alternative='greater')\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> jb_plot(ax)\n>>> ax.hist(res.null_distribution, np.linspace(0, 10, 50),\n...         density=True)\n>>> ax.legend(['aymptotic approximation (many observations)',\n...            'Monte Carlo approximation (11 observations)'])\n>>> plt.show() \n```", "```py\n>>> res.pvalue\n0.0097  # may vary \n```"]