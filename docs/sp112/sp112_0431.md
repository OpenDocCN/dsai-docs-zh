# 优化和根查找（`scipy.optimize`）

> 原文链接：[`docs.scipy.org/doc/scipy-1.12.0/reference/optimize.html`](https://docs.scipy.org/doc/scipy-1.12.0/reference/optimize.html)

SciPy `optimize` 提供了用于最小化（或最大化）目标函数的函数，可能受约束条件限制。它包括非线性问题的求解器（支持局部和全局优化算法）、线性规划、约束和非线性最小二乘法、根查找和曲线拟合。

不同求解器共享的常见函数和对象包括：

| `show_options`([solver, method, disp]) | 显示优化求解器的附加选项文档。 |
| --- | --- |
| `OptimizeResult` | 表示优化结果。 |
| `OptimizeWarning` |  |

## 优化

### 标量函数优化

| `minimize_scalar`(fun[, bracket, bounds, ...]) | 对一维标量函数进行局部最小化。 |
| --- | --- |

`minimize_scalar` 函数支持以下方法：

+   minimize_scalar(method=’brent’)

+   minimize_scalar(method=’bounded’)

+   minimize_scalar(method=’golden’)

### 本地（多变量）优化

| `minimize`(fun, x0[, args, method, jac, hess, ...]) | 对一个或多个变量的标量函数进行最小化。 |
| --- | --- |

`minimize` 函数支持以下方法：

+   minimize(method=’Nelder-Mead’)

+   minimize(method=’Powell’)

+   minimize(method=’CG’)

+   minimize(method=’BFGS’)

+   minimize(method=’Newton-CG’)

+   minimize(method=’L-BFGS-B’)

+   minimize(method=’TNC’)

+   minimize(method=’COBYLA’)

+   minimize(method=’SLSQP’)

+   minimize(method=’trust-constr’)

+   minimize(method=’dogleg’)

+   minimize(method=’trust-ncg’)

+   minimize(method=’trust-krylov’)

+   minimize(method=’trust-exact’)

约束以单个对象或来自以下类的对象列表形式传递给`minimize`函数：

| `NonlinearConstraint`(fun, lb, ub[, jac, ...]) | 变量的非线性约束。 |
| --- | --- |
| `LinearConstraint`(A[, lb, ub, keep_feasible]) | 变量的线性约束。 |

简单的边界约束分别处理，并且有一个专门的类：

| `Bounds`([lb, ub, keep_feasible]) | 变量的边界约束。 |
| --- | --- |

实现`HessianUpdateStrategy`接口的拟牛顿策略可用于在`minimize`函数中近似黑塞矩阵（仅适用于“trust-constr”方法）。实现此接口的可用拟牛顿方法包括：

| `BFGS`([exception_strategy, min_curvature, ...]) | BFGS（Broyden-Fletcher-Goldfarb-Shanno）海森更新策略。 |
| --- | --- |
| `SR1`([min_denominator, init_scale]) | 对称秩-1 海森更新策略。 |

### 全局优化

| `basinhopping`(func, x0[, niter, T, stepsize, ...]) | 使用盆地跳跃算法找到函数的全局最小值。 |
| --- | --- |
| `brute`(func, ranges[, args, Ns, full_output, ...]) | 通过蛮力法在给定范围内最小化函数。 |
| `differential_evolution`(func, bounds[, args, ...]) | 多元函数的全局最小值。 |
| `shgo`(func, bounds[, args, constraints, n, ...]) | 使用 SHG 优化找到函数的全局最小值。 |
| `dual_annealing`(func, bounds[, args, ...]) | 使用双退火法找到函数的全局最小值。 |
| `direct`(func, bounds, *[, args, eps, maxfun, ...]) | 使用 DIRECT 算法寻找函数的全局最小值。 |

## 最小二乘和曲线拟合

### 非线性最小二乘

| `least_squares`(fun, x0[, jac, bounds, ...]) | 解决带有变量边界的非线性最小二乘问题。 |
| --- | --- |

### 线性最小二乘

| `nnls`(A, b[, maxiter, atol]) | 解决 `argmin_x | | Ax - b | | _2` 且 `x>=0`。 |
| --- | --- | --- | --- | --- | --- |
| `lsq_linear`(A, b[, bounds, method, tol, ...]) | 解决带有变量边界的线性最小二乘问题。 |
| `isotonic_regression`(y, *[, weights, increasing]) | 非参数等距回归。 |

### 曲线拟合

| `curve_fit`(f, xdata, ydata[, p0, sigma, ...]) | 使用非线性最小二乘拟合函数 `f` 到数据。 |
| --- | --- |

## 根查找

### 标量函数

| `root_scalar`(f[, args, method, bracket, ...]) | 寻找标量函数的根。 |
| --- | --- |
| `brentq`(f, a, b[, args, xtol, rtol, maxiter, ...]) | 使用 Brent 方法在一个区间内寻找函数的根。 |
| `brenth`(f, a, b[, args, xtol, rtol, maxiter, ...]) | 使用 Brent 方法及双曲线外推在一个区间内寻找函数的根。 |
| `ridder`(f, a, b[, args, xtol, rtol, maxiter, ...]) | 使用 Ridder 方法在一个区间内寻找函数的根。 |
| `bisect`(f, a, b[, args, xtol, rtol, maxiter, ...]) | 使用二分法在一个区间内寻找函数的根。 |
| `newton`(func, x0[, fprime, args, tol, ...]) | 使用牛顿-拉弗森（或割线或哈雷）方法寻找实数或复数函数的根。 |
| `toms748`(f, a, b[, args, k, xtol, rtol, ...]) | 使用 TOMS 算法 748 方法寻找根。 |
| `RootResults`(root, iterations, ...) | 表示根查找结果。 |

`root_scalar` 函数支持以下方法：

+   root_scalar(method=’brentq’)

+   root_scalar(method=’brenth’)

+   root_scalar(method=’bisect’)

+   root_scalar(method=’ridder’)

+   root_scalar(method=’newton’)

+   root_scalar(method=’toms748’)

+   root_scalar(method=’secant’)

+   root_scalar(method=’halley’)

下表列出了情况及适当的方法，以及每次迭代（和每次函数评估）的渐近收敛率，以便成功收敛到简单根(*）。二分法是最慢的，每次函数评估增加一位有效数字，但保证收敛。其他括号法（最终）每次函数评估增加大约 50%的准确位数。基于导数的方法，都建立在`newton`上，如果初始值接近根，可以相当快速地收敛。它们也可应用于在复平面（的子集上）定义的函数。

| 函数域 | 是否括号化？ | 是否有导数？ | 求解器 | 收敛性 |
| --- | --- | --- | --- | --- |
| *fprime* | *fprime2* | 是否保证？ | 收敛率(*) |
| --- | --- | --- | --- |
| *R* | 是 | N/A | N/A |

+   二分法

+   brentq

+   brenth

+   ridder

+   toms748

|

+   是

+   是

+   是

+   是

+   是

|

+   1 “线性”

+   >=1, <= 1.62

+   >=1, <= 1.62

+   2.0 (1.41)

+   2.7 (1.65)

|

| *R* 或 *C* | 否 | 否 | 否 | 切线法 | 否 | 1.62 (1.62) |
| --- | --- | --- | --- | --- | --- | --- |
| *R* 或 *C* | 否 | 是 | 否 | 牛顿法 | 否 | 2.00 (1.41) |
| *R* 或 *C* | 否 | 是 | 是 | 亥姆法 | 否 | 3.00 (1.44) |

另见

`scipy.optimize.cython_optimize` – Typed Cython 版本的根查找函数

寻找不动点：

| `fixed_point`(func, x0[, args, xtol, maxiter, ...]) | 查找函数的不动点。 |
| --- | --- |

### 多维的

| `root`(fun, x0[, args, method, jac, tol, ...]) | 查找向量函数的根。 |
| --- | --- |

`root` 函数支持以下方法：

+   root(method=’hybr’)

+   root(method=’lm’)

+   root(method=’broyden1’)

+   root(method=’broyden2’)

+   root(method=’anderson’)

+   root(method=’linearmixing’)

+   root(method=’diagbroyden’)

+   root(method=’excitingmixing’)

+   root(method=’krylov’)

+   root(method=’df-sane’)

## 线性规划 / MILP

| `milp`(c, *[, integrality, bounds, ...]) | 混合整数线性规划 |
| --- | --- |
| `linprog`(c[, A_ub, b_ub, A_eq, b_eq, bounds, ...]) | 线性规划：最小化线性目标函数，满足线性等式和不等式约束。 |

`linprog` 函数支持以下方法：

+   linprog(method=’simplex’)

+   linprog(method=’interior-point’)

+   linprog(method=’revised simplex’)

+   linprog(method=’highs-ipm’)

+   linprog(method=’highs-ds’)

+   linprog(method=’highs’)

简单法、内点法和修订单纯法方法支持回调函数，例如：

| `linprog_verbose_callback`(res) | 演示 linprog 回调接口的样本回调函数。 |
| --- | --- |

## 分配问题

| `linear_sum_assignment` | 解决线性求和分配问题。 |
| --- | --- |
| `quadratic_assignment`(A, B[, method, options]) | 近似解决二次分配问题和图匹配问题。 |

`quadratic_assignment` 函数支持以下方法：

+   quadratic_assignment(method=’faq’)

+   quadratic_assignment(method=’2opt’)

## 实用工具

### Finite-difference approximation

| `approx_fprime`(xk, f[, epsilon]) | 标量或向量值函数的有限差分近似导数。 |
| --- | --- |
| `check_grad`(func, grad, x0, *args[, epsilon, ...]) | 通过将其与梯度的（前向）有限差分近似比较，检查梯度函数的正确性。 |

### 线搜索：

| `bracket`(func[, xa, xb, args, grow_limit, ...]) | 定位函数最小值的区间。 |
| --- | --- |
| `line_search`(f, myfprime, xk, pk[, gfk, ...]) | 寻找满足强 Wolfe 条件的 alpha。 |

### Hessian 近似：

| `LbfgsInvHessProduct`(*args, **kwargs) | L-BFGS 近似逆 Hessian 的线性算子。 |
| --- | --- |
| `HessianUpdateStrategy`() | 实现 Hessian 更新策略的接口。 |

### 基准问题：

| `rosen`(x) | Rosenbrock 函数。 |
| --- | --- |
| `rosen_der`(x) | Rosenbrock 函数的导数（即梯度）。 |
| `rosen_hess`(x) | Rosenbrock 函数的 Hessian 矩阵。 |
| `rosen_hess_prod`(x, p) | Rosenbrock 函数的 Hessian 矩阵与向量的乘积。 |

## 遗留函数：

下面的函数不建议在新脚本中使用；所有这些方法都可以通过提供的更新、更一致的接口访问。

### 优化：

通用多元方法：

| `fmin`(func, x0[, args, xtol, ftol, maxiter, ...]) | 使用下降单纯形算法最小化函数。 |
| --- | --- |
| `fmin_powell`(func, x0[, args, xtol, ftol, ...]) | 使用修改后的 Powell 方法最小化函数。 |
| `fmin_cg`(f, x0[, fprime, args, gtol, norm, ...]) | 使用非线性共轭梯度算法最小化函数。 |
| `fmin_bfgs`(f, x0[, fprime, args, gtol, norm, ...]) | 使用 BFGS 算法最小化函数。 |
| `fmin_ncg`(f, x0, fprime[, fhess_p, fhess, ...]) | 使用牛顿-CG 方法无约束最小化函数。 |

约束多元方法：

| `fmin_l_bfgs_b`(func, x0[, fprime, args, ...]) | 使用 L-BFGS-B 算法最小化函数 func。 |
| --- | --- |
| `fmin_tnc`(func, x0[, fprime, args, ...]) | 使用截断牛顿算法最小化受界限约束的变量函数，并使用梯度信息。 |
| `fmin_cobyla`(func, x0, cons[, args, ...]) | 使用线性逼近约束优化（COBYLA）方法最小化函数。 |
| `fmin_slsqp`(func, x0[, eqcons, f_eqcons, ...]) | 使用顺序最小二乘规划（SLSQP）方法最小化函数。 |

单变量（标量）最小化方法：

| `fminbound`(func, x1, x2[, args, xtol, ...]) | 标量函数的有界最小化。 |
| --- | --- |
| `brent`(func[, args, brack, tol, full_output, ...]) | 给定一个变量函数和可能的区间，返回函数的局部最小化器，精确到 tol 的分数精度。 |
| `golden`(func[, args, brack, tol, ...]) | 使用黄金分割法返回单变量函数的最小化器。 |

### 最小二乘

| `leastsq`(func, x0[, args, Dfun, full_output, ...]) | 最小化一组方程的平方和。 |
| --- | --- |

### 根查找：

一般非线性求解器：

| `fsolve`(func, x0[, args, fprime, ...]) | 找到函数的根。 |
| --- | --- |
| `broyden1`(F, xin[, iter, alpha, ...]) | 使用布罗伊登第一雅可比逼近找到函数的根。 |
| `broyden2`(F, xin[, iter, alpha, ...]) | 使用布罗伊登第二雅可比逼近找到函数的根。 |

大规模非线性求解器：

| `newton_krylov`\(F, xin\[\, iter, rdiff, method, ...\]\) | 使用 Krylov 逼近方法求解函数的根，用于逆雅可比矩阵。 |
| --- | --- |
| `anderson`(F, xin[, iter, alpha, w0, M, ...]) | 使用（扩展的）安德森混合方法寻找函数的根。 |
| `BroydenFirst`([alpha, reduction_method, max_rank]) | 使用 Broyden 第一雅可比逼近方法寻找函数的根。 |
| `InverseJacobian`(jacobian) |  |

属性：

|

| `KrylovJacobian`([rdiff, method, ...]) | 使用 Krylov 逼近方法求解函数的根，用于逆雅可比矩阵。 |
| --- | --- |

简单迭代求解器：

| `excitingmixing`\(F, xin\[\, iter, alpha, ...\]\) | 使用调整的对角雅可比逼近方法寻找函数的根。 |
| --- | --- |
| `linearmixing`\(F, xin\[\, iter, alpha, verbose, ...\]\) | 使用标量雅可比逼近方法寻找函数的根。 |
| `diagbroyden`(F, xin[, iter, alpha, verbose, ...]) | 使用对角 Broyden 雅可比逼近方法寻找函数的根。 |
