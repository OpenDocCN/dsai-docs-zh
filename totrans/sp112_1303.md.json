["```py\nscipy.stats.power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None)\n```", "```py\n>>> import numpy as np\n>>> from scipy.stats import power_divergence\n>>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n(2.006573162632538, 0.84823476779463769) \n```", "```py\n>>> power_divergence([16, 18, 16, 14, 12, 12],\n...                  f_exp=[16, 16, 16, 16, 16, 8],\n...                  lambda_='log-likelihood')\n(3.3281031458963746, 0.6495419288047497) \n```", "```py\n>>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n>>> obs.shape\n(6, 2)\n>>> power_divergence(obs, lambda_=\"log-likelihood\")\n(array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225])) \n```", "```py\n>>> power_divergence(obs, axis=None)\n(23.31034482758621, 0.015975692534127565)\n>>> power_divergence(obs.ravel())\n(23.31034482758621, 0.015975692534127565) \n```", "```py\n>>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n(2.0, 0.73575888234288467) \n```", "```py\n>>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n(2.0, array([ 0.84914504,  0.73575888,  0.5724067 ])) \n```", "```py\n>>> power_divergence([16, 18, 16, 14, 12, 12],\n...                  f_exp=[[16, 16, 16, 16, 16, 8],\n...                         [8, 20, 20, 16, 12, 12]],\n...                  axis=1)\n(array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846])) \n```"]