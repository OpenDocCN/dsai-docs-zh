["```py\n`<class-index> <x> <y> <width> <height> <px1> <py1> <px2> <py2> ... <pxn> <pyn>` \n```", "```py\n`<class-index> <x> <y> <width> <height> <px1> <py1> <p1-visibility> <px2> <py2> <p2-visibility> <pxn> <pyn> <p2-visibility>` \n```", "```py\n`# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..] path:  ../datasets/coco8-pose  # dataset root dir train:  images/train  # train images (relative to 'path') 4 images val:  images/val  # val images (relative to 'path') 4 images test:  # test images (optional)  # Keypoints kpt_shape:  [17,  3]  # number of keypoints, number of dims (2 for x,y or 3 for x,y,visible) flip_idx:  [0,  2,  1,  4,  3,  6,  5,  8,  7,  10,  9,  12,  11,  14,  13,  16,  15]  # Classes dictionary names:   0:  person` \n```", "```py\n`from ultralytics import YOLO  # Load a model model = YOLO(\"yolov8n-pose.pt\")  # load a pretrained model (recommended for training)  # Train the model results = model.train(data=\"coco8-pose.yaml\", epochs=100, imgsz=640)` \n```", "```py\n`# Start training from a pretrained *.pt model yolo  pose  train  data=coco8-pose.yaml  model=yolov8n-pose.pt  epochs=100  imgsz=640` \n```", "```py\n`from ultralytics.data.converter import convert_coco  convert_coco(labels_dir=\"path/to/coco/annotations/\", use_keypoints=True)` \n```", "```py\n```", "```pypython from ultralytics import YOLO  model = YOLO(\"yolov8n-pose.pt\")  # load pretrained model results = model.train(data=\"coco-pose.yaml\", epochs=100, imgsz=640) ```", "```py` \n```", "```py\n```", "```pypython from ultralytics import YOLO  model = YOLO(\"yolov8n-pose.pt\") results = model.train(data=\"your-dataset.yaml\", epochs=100, imgsz=640) ```", "```py` \n```", "```py\n`path:  ../datasets/coco8-pose train:  images/train val:  images/val names:   0:  person` \n```", "```py\n`from ultralytics.data.converter import convert_coco  convert_coco(labels_dir=\"path/to/coco/annotations/\", use_keypoints=True)` \n```"]