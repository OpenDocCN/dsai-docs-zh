- en: scipy.stats.entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.stats.entropy.html#scipy.stats.entropy](https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.stats.entropy.html#scipy.stats.entropy)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Calculate the Shannon entropy/relative entropy of given distribution(s).
  prefs: []
  type: TYPE_NORMAL
- en: If only probabilities *pk* are given, the Shannon entropy is calculated as `H
    = -sum(pk * log(pk))`.
  prefs: []
  type: TYPE_NORMAL
- en: If *qk* is not None, then compute the relative entropy `D = sum(pk * log(pk
    / qk))`. This quantity is also known as the Kullback-Leibler divergence.
  prefs: []
  type: TYPE_NORMAL
- en: This routine will normalize *pk* and *qk* if they don’t sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pk**array_like'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the (discrete) distribution. Along each axis-slice of `pk`, element
    `i` is the (possibly unnormalized) probability of event `i`.
  prefs: []
  type: TYPE_NORMAL
- en: '**qk**array_like, optional'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence against which the relative entropy is computed. Should be in the same
    format as *pk*.
  prefs: []
  type: TYPE_NORMAL
- en: '**base**float, optional'
  prefs: []
  type: TYPE_NORMAL
- en: The logarithmic base to use, defaults to `e` (natural logarithm).
  prefs: []
  type: TYPE_NORMAL
- en: '**axis**int or None, default: 0'
  prefs: []
  type: TYPE_NORMAL
- en: If an int, the axis of the input along which to compute the statistic. The statistic
    of each axis-slice (e.g. row) of the input will appear in a corresponding element
    of the output. If `None`, the input will be raveled before computing the statistic.
  prefs: []
  type: TYPE_NORMAL
- en: '**nan_policy**{‘propagate’, ‘omit’, ‘raise’}'
  prefs: []
  type: TYPE_NORMAL
- en: Defines how to handle input NaNs.
  prefs: []
  type: TYPE_NORMAL
- en: '`propagate`: if a NaN is present in the axis slice (e.g. row) along which the
    statistic is computed, the corresponding entry of the output will be NaN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`omit`: NaNs will be omitted when performing the calculation. If insufficient
    data remains in the axis slice along which the statistic is computed, the corresponding
    entry of the output will be NaN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`raise`: if a NaN is present, a `ValueError` will be raised.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keepdims**bool, default: False'
  prefs: []
  type: TYPE_NORMAL
- en: If this is set to True, the axes which are reduced are left in the result as
    dimensions with size one. With this option, the result will broadcast correctly
    against the input array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S**{float, array_like}'
  prefs: []
  type: TYPE_NORMAL
- en: The calculated entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Notes
  prefs: []
  type: TYPE_NORMAL
- en: Informally, the Shannon entropy quantifies the expected uncertainty inherent
    in the possible outcomes of a discrete random variable. For example, if messages
    consisting of sequences of symbols from a set are to be encoded and transmitted
    over a noiseless channel, then the Shannon entropy `H(pk)` gives a tight lower
    bound for the average number of units of information needed per symbol if the
    symbols occur with frequencies governed by the discrete distribution *pk* [[1]](#r7a63479d8f91-1).
    The choice of base determines the choice of units; e.g., `e` for nats, `2` for
    bits, etc.
  prefs: []
  type: TYPE_NORMAL
- en: The relative entropy, `D(pk|qk)`, quantifies the increase in the average number
    of units of information needed per symbol if the encoding is optimized for the
    probability distribution *qk* instead of the true distribution *pk*. Informally,
    the relative entropy quantifies the expected excess in surprise experienced if
    one believes the true distribution is *qk* when it is actually *pk*.
  prefs: []
  type: TYPE_NORMAL
- en: A related quantity, the cross entropy `CE(pk, qk)`, satisfies the equation `CE(pk,
    qk) = H(pk) + D(pk|qk)` and can also be calculated with the formula `CE = -sum(pk
    * log(qk))`. It gives the average number of units of information needed per symbol
    if an encoding is optimized for the probability distribution *qk* when the true
    distribution is *pk*. It is not computed directly by [`entropy`](#scipy.stats.entropy
    "scipy.stats.entropy"), but it can be computed using two calls to the function
    (see Examples).
  prefs: []
  type: TYPE_NORMAL
- en: See [[2]](#r7a63479d8f91-2) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Beginning in SciPy 1.9, `np.matrix` inputs (not recommended for new code) are
    converted to `np.ndarray` before the calculation is performed. In this case, the
    output will be a scalar or `np.ndarray` of appropriate shape rather than a 2D
    `np.matrix`. Similarly, while masked elements of masked arrays are ignored, the
    output will be a scalar or `np.ndarray` rather than a masked array with `mask=False`.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs: []
  type: TYPE_NORMAL
- en: '[[1](#id1)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shannon, C.E. (1948), A Mathematical Theory of Communication. Bell System Technical
    Journal, 27: 379-423. [https://doi.org/10.1002/j.1538-7305.1948.tb01338.x](https://doi.org/10.1002/j.1538-7305.1948.tb01338.x)'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2](#id2)]'
  prefs: []
  type: TYPE_NORMAL
- en: Thomas M. Cover and Joy A. Thomas. 2006\. Elements of Information Theory (Wiley
    Series in Telecommunications and Signal Processing). Wiley-Interscience, USA.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs: []
  type: TYPE_NORMAL
- en: 'The outcome of a fair coin is the most uncertain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The outcome of a biased coin is less uncertain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The relative entropy between the fair coin and biased coin is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The cross entropy can be calculated as the sum of the entropy and relative
    entropy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
